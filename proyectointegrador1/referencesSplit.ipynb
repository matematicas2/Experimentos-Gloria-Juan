{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(['punkt','stopwords','wordnet','words'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path =  Path(\"~\").expanduser().resolve()\n",
    "input_file_path  = base_path / 'datasets/papers-txt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cmejia3/datasets/papers-txt/1410.2670.txt\n",
      "/home/cmejia3/datasets/papers-txt/1404.3626.txt\n",
      "/home/cmejia3/datasets/papers-txt/1405.0149.txt\n",
      "/home/cmejia3/datasets/papers-txt/1409.2612.txt\n",
      "/home/cmejia3/datasets/papers-txt/1511.00867.txt\n",
      "/home/cmejia3/datasets/papers-txt/1504.01708.txt\n"
     ]
    }
   ],
   "source": [
    "for f in input_file_path.glob('*.txt'):   \n",
    "    #Lectura del archivo\n",
    "    print(f)\n",
    "    input_file = open(f, \"r\", encoding = 'utf-8')\n",
    "    input_aux = input_file.read()\n",
    "    \n",
    "    input_aux = input_aux.lower()\n",
    "    tokens = input_aux.split('[\\nreferences]')\n",
    "    if len(tokens)!= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reactive synthesis without regret\\npaul hunter∗, guillermo a. pérez†, and jean-françois raskin∗\\ndépartement d’informatique, université libre de bruxelles\\n{phunter,gperezme,jraskin}@ulb.ac.be\\n\\narxiv:1504.01708v5 [cs.gt] 19 feb 2018\\n\\naugust 27, 2018\\n\\nabstract\\ntwo-player zero-sum games of infinite duration and their quantitative versions are used in verification to model the interaction between a controller (eve) and its environment (adam). the\\nquestion usually addressed is that of the existence (and computability) of a strategy for eve that can\\nmaximize her payoff against any strategy of adam. in this work, we are interested in strategies of\\neve that minimize her regret, i.e. strategies that minimize the difference between her actual payoff\\nand the payoff she could have achieved if she had known the strategy of adam in advance. we\\ngive algorithms to compute the strategies of eve that ensure minimal regret against an adversary\\nwhose choice of strategy is (i) unrestricted, (ii) limited to positional strategies, or (iii) limited to\\nword strategies, and show that the two last cases have natural modelling applications. these results\\napply for quantitative games defined with the classical payoff functions inf, sup, liminf, limsup, and\\nmean-payoff. we also show that our notion of regret minimization in which adam is limited to word\\nstrategies generalizes the notion of good for games introduced by henzinger and piterman, and is\\nrelated to the notion of determinization by pruning due to aminof, kupferman and lampert.\\n\\n1\\n\\nintroduction\\n\\nthe model of two player games played on graphs is an adequate mathematical tool to solve important\\nproblems in computer science, and in particular the reactive-system synthesis problem [pr89]. in that\\ncontext, the game models the non-terminating interaction between the system to synthesize and its\\nenvironment. games with quantitative objectives are useful to formalize important quantitative aspects\\nsuch as mean-response time or energy consumption. they have attracted large attention recently, see\\ne.g. [cdhr10,bcd+ 11]. most of the contributions in this context are for zero-sum games: the objective\\nof eve (that models the system) is to maximize the value of the game while the objective of adam\\n(that models the environment) is to minimize this value. this is a worst-case assumption: because the\\ncooperation of the environment cannot be assumed, we postulate that it is antagonistic.\\nin this antagonistic approach, the main solution concept is that of a winning strategy. given a\\nthreshold value, a winning strategy for eve ensures a minimal value greater than the threshold against\\nany strategy of adam. however, sometimes there are no winning strategies. what should the behaviour\\nof the system be in such cases? there are several possible answers to this question. one is to consider\\nnon-zero sum extensions of those games: the environment (adam) is not completely antagonistic, rather\\nit has its own speciﬁcation. in such games, a strategy for eve must be winning only when the outcome\\nsatisﬁes the objectives of adam, see e.g. [cdfr14]. another option for eve is to play a strategy which\\nminimizes her regret. the regret is informally deﬁned as the diﬀerence between what a player actually\\nwins and what she could have won if she had known the strategy chosen by the other player. minimization\\nof regret is a central concept in decision theory [bel82]. this notion is important because it usually leads\\nto solutions that agree with common sense.\\nlet us illustrate the notion of regret minimization on the example of fig. 1. in this example, eve\\nowns the squares and adam owns the circles (we do not use the letters labelling edges for the moment).\\n∗ authors\\n† author\\n\\nsupported by the erc invest (279499) project.\\nsupported by f.r.s.-fnrs fellowship.\\n\\n1\\n\\n\\x0cb, −1\\n\\nv1\\n\\n1\\n2\\n\\na, 2\\n\\nv4\\n\\nσ, 2\\n\\nσ, 1\\n\\nσ, 1\\n\\nb,\\n\\nv2\\n\\nv3\\n\\nv5\\n\\nσ, 1\\n\\na, 1\\n\\nfigure 1: example weighted arena g0 .\\n\\nthe game is played for inﬁnitely many rounds and the value of a play for eve is the long-run average\\nof the values of edges traversed during the play (the so-called mean-payoff). in this game, eve is only\\nable to secure a mean-payoﬀ of 21 when adam is fully antagonistic. indeed, if eve (from v1 ) plays to v2\\nthen adam can force a mean-payoﬀ value of 0, and if she plays to v3 then the mean-payoﬀ value is at\\nleast 21 . note also that if adam is not fully antagonistic, then the mean-payoﬀ could be as high as 2.\\nnow, assume that eve does not try to force the highest value in the worst-case but tries to minimize her\\nregret. if she plays v1 7→ v2 then the regret is equal to 1. this is because adam can play the following\\nstrategy: if eve plays to v2 (from v1 ) then he plays v2 7→ v1 (giving a mean-payoﬀ of 0), and if eve plays\\nto v3 then he plays to v5 (giving a mean-payoﬀ of 1). if she plays v1 7→ v3 then her regret is 32 since\\nadam can play the symmetric strategy. it should thus be clear that the strategy of eve which always\\nchooses v1 7→ v2 is indeed minimizing her regret.\\nin this paper, we will study three variants of regret minimization, each corresponding to a diﬀerent\\nset of strategies we allow adam to choose from. the ﬁrst variant is when adam can play any possible\\nstrategy (as in the example above), the second variant is when adam is restricted to playing memoryless\\nstrategies, and the third variant is when adam is restricted to playing word strategies. to illustrate\\nthe last two variants, let us consider again the example of fig. 1. assume now that adam is playing\\nmemoryless strategies only. then in this case, we claim that there is a strategy of eve that ensures\\nregret 0. the strategy is as follows: ﬁrst play to v2 , if adam chooses to go back to v1 , then eve should\\nhenceforth play v1 7→ v3 . we claim that this strategy has regret 0. indeed, when v2 is visited, either\\nadam chooses v2 7→ v4 , and then eve secures a mean-payoﬀ of 2 (which is the maximal possible value),\\nor adam chooses v2 7→ v1 and then we know that v1 7→ v2 is not a good option for eve as cycling between\\nv1 and v2 yields a payoﬀ of only 0. in this case, the mean-payoﬀ is either 1, if adam plays v3 7→ v5 , or 12 ,\\nif he plays v3 7→ v1 . in all the cases, the regret is 0. let us now turn to the restriction to word strategies\\nfor adam. when considering this restriction, we use the letters that label the edges of the graph. a word\\nstrategy for adam is a function w : n → {a, b}. in this setting adam plays a sequence of letters and\\nthis sequence is independent of the current state of the game. it is more convenient to view the latter as\\na game played on a weighted automata—assumed to be total and with at least one transition for every\\naction from every state—in which adam plays letters and eve responds by resolving non-determinism.\\nwhen adam plays word strategies, the strategy that minimizes regret for eve is to always play v1 7→ v2 .\\nindeed, for any word in which the letter a appears, the mean-payoﬀ is equal to 2, and the regret is 0,\\nand for any word in which the letter a does not appear, the mean-payoﬀ is 0 while it would have been\\nequal to 12 when playing v1 7→ v3 . so the regret of this strategy is 21 and it is the minimal regret that\\neve can secure. note that the three diﬀerent strategies give three diﬀerent values in our example. this\\nis in contrast with the worst-case analysis of the same problem (memoryless strategies suﬃce for both\\nplayers).\\nwe claim that at least the two last variants are useful for modelling purposes. for example, the\\nmemoryless restriction is useful when designing a system that needs to perform well in an environment\\nwhich is only partially known. in practical situations, a controller may discover the environment with\\nwhich it is interacting at run time. such a situation can be modelled by an arena in which choices in\\nnodes of the environment model an entire family of environments and each memoryless strategy models\\n\\n2\\n\\n\\x0cpayoff type\\nsup, inf,\\nlimsup\\nliminf\\nmp, mp\\n\\nany strategy\\nptime-c\\n(thm 1)\\nptime-c (thm 1)\\nmp equiv. (thm 1)\\n\\nmemoryless strategies\\npspace (lem 4),\\nconp-h (lem 11)\\npspace-c (thm 2)\\npspace-c (thm 2)\\n\\nword strategies\\nexptime-c\\n(thm 3)\\nexptime-c (thm 3)\\nundecidable (lem 14)\\n\\ntable 1: complexity of deciding the regret threshold problem.\\n\\na speciﬁc environment of the family. in such cases, if we want to design a controller that performs\\nreasonably well against all the possible environments, we can consider a controller that minimizes regret:\\nthe strategy of the controller will be as close as possible to an optimal strategy if we had known the\\nenvironment beforehand. this is, for example, the modelling choice done in the famous canadian\\ntraveller’s problem [py91]: a driver is attempting to reach a speciﬁc location while ensuring the traversed\\ndistance is not too far from the shortest feasible path. the partial knowledge is due to some roads being\\nclosed because of snow. the canadian traveller, when planning his itinerary, is in fact searching for\\na strategy to minimize his regret for the shortest path measure against a memoryless adversary who\\ndetermines the roads that are closed. similar situations naturally arise when synthesizing controllers\\nfor robot motion planning [wet15]. we now illustrate the usefulness of the variant in which adam is\\nrestricted to play word strategies. assume that we need to design a system embedded into an environment\\nthat produces disturbances: if the sequence of disturbances produced by the environment is independent\\nof the behavior of the system, then it is natural to model this sequence not as a function of the state\\nof the system but as a temporal sequence of events, i.e. a word on the alphabet of the disturbances.\\nclearly, if the sequences are not the result of an antagonistic process, then minimizing the regret against\\nall disturbance sequences is an adequate solution concept to obtain a reasonable system and may be\\npreferable to a system obtained from a strategy that is optimal under the antagonistic hypothesis.\\ncontributions. in this paper, we provide algorithms to solve the regret threshold problem (strict and\\nnon-strict) in the three variants explained above, i.e. given a game and a threshold, does there exist a\\nstrategy for eve with a regret that is (strictly) less than the threshold against all (resp. all memoryless,\\nresp. all word) strategies for adam. it is worth mentioning that, in the ﬁrst two cases we consider,\\nwe actually provide algorithms to solve the following search problem: ﬁnd the controller which ensures\\nthe minimal possible regret. indeed, our algorithms are reductions to well-known games and are such\\nthat a winning strategy for eve in the resulting game corresponds to a regret-minimizing strategy in\\nthe original one. conversely, in games played against word strategies for adam, we only explicitly solve\\nthe regret threshold problem. however, since the set of possible regret values of the considered games is\\nﬁnite and easy to describe, it will be obvious that one can implement a binary search to ﬁnd the regret\\nvalue and a corresponding optimal regret-minimizing strategy for eve.\\nwe study this problem for six common quantitative measures: inf, sup, liminf, limsup, mp, mp.\\nfor all measures, but mp, the strict and non-strict threshold problems are equivalent. we state our\\nresults for both cases for consistency. in almost all the cases, we provide matching lower bounds showing\\nthe worst-case optimality of our algorithms. our results are summarized in the table of fig. 1. for the\\nvariant in which adam plays word strategies only, we show that we can recover decidability of meanpayoﬀ objectives when the memory of eve is ﬁxed in advance: in this case, the problem is np-complete\\n(theorems 4 and 5).\\nrelated works. the notion of regret minimization is a central one in game theory, see e.g. [zjbp08]\\nand references therein. also, iterated regret minimization has been recently proposed by halpern et al. as\\na concept for non-zero sum games [hp12]. there, it is applied to matrix games and not to game graphs.\\nin a previous contribution, we have applied the iterated regret minimization concept to non-zero sum\\ngames played on weighted graphs for the shortest path problem [fgr10]. restrictions on how adam is\\nallowed to play were not considered there. as we do not consider an explicit objective for adam, we do\\nnot consider iteration of the regret minimization here.\\nthe disturbance-handling embedded system example was ﬁrst given in [df11]. in that work, the\\nauthors introduce remorsefree strategies, which correspond to strategies which minimize regret in games\\n\\n3\\n\\n\\x0cwith ω-regular objectives. they do not establish lower bounds on the complexity of realizability or\\nsynthesis of remorsefree strategies and they focus on word strategies of adam only.\\nin [hp06], henzinger and piterman introduce the notion of good for games automata. a nondeterministic automaton is good for solving games if it fairly simulates the equivalent deterministic\\nautomaton. we show that our notion of regret minimization for word strategies extends this notion to\\nthe quantitative setting (proposition 3). our deﬁnitions give rise to a natural notion of approximate\\ndeterminization for weighted automata on inﬁnite words.\\nin [akl10], aminof et al. introduce the notion of approximate determinization by pruning for\\nweighted sum automata over ﬁnite words. for α ∈ (0, 1], a weighted sum automaton is α-determinizable\\nby pruning if there exists a ﬁnite state strategy to resolve non-determinism and that constructs a run\\nwhose value is at least α times the value of the maximal run of the given word. so, they consider a\\nnotion of approximation which is a ratio. we will show that our concept of regret, when adam plays\\nword strategies only, deﬁnes instead a notion of approximation with respect to the difference metric\\nfor weighted automata (proposition 2). there are other diﬀerences with their work. first, we consider\\ninﬁnite words while they consider ﬁnite words. second, we study a general notion of regret minimization\\nproblem in which eve can use any strategy while they restrict their study to ﬁxed memory strategies\\nonly and leave the problem open when the memory is not ﬁxed a priori.\\nfinally, the main diﬀerence between these related works and this paper is that we study the inf, sup,\\nliminf, limsup, mp, mp measures while they consider the total sum measure or qualitative objectives.\\n\\n2\\n\\npreliminaries\\n\\na weighted arena is a tuple g = (v, v∃ , e, w, vi ) where (v, e, w) is a ﬁnite edge-weighted graph1 with\\ninteger weights, v∃ ⊆ v , and vi ∈ v is the initial vertex. in the sequel we depict vertices owned by eve\\n(i.e. v∃ ) with squares and vertices owned by adam (i.e. v \\\\ v∃ ) with circles. we denote the maximum\\nabsolute value of a weight in a weighted arena by w .\\na play in a weighted arena is an inﬁnite sequence of vertices π = v0 v1 . . . where v0 = vi and (vi , vi+1 ) ∈\\npl−1\\ne for all i. we extend the weight function to partial plays by setting w(hvi ili=k ) = i=k\\nw(vi , vi+1 ).\\na strategy for eve (adam) is a function σ that maps partial plays ending with a vertex v in v∃\\n(v \\\\ v∃ ) to a successor of v. a strategy has memory m if it can be realized as the output of a ﬁnite\\nstate machine with m states (see e.g. [hpr14] for a formal deﬁnition). a memoryless (or positional)\\nstrategy is a strategy with memory 1, that is, a function that only depends on the last element of the\\ngiven partial play. a play π = v0 v1 . . . is consistent with a strategy σ for eve (adam) if whenever vi ∈ v∃\\n(vi ∈ v \\\\ v∃ ), σ(hvj ij≤i ) = vi+1 . we denote by s∃ (g) (s∀ (g)) the set of all strategies for eve (adam)\\nm\\nand by σm\\n∃ (g) (σ∀ (g)) the set of all strategies for eve (adam) in g that require memory of size at\\nmost m, in particular σ1∃ (g) (σ1∀ (g)) is the set of all memoryless strategies of eve (adam) in g. we\\nomit g if the context is clear.\\npayoff functions. a play in a weighted arena deﬁnes an inﬁnite sequence of weights. we deﬁne\\nbelow several classical payoff functions that map such sequences to real numbers.2 formally, for a play\\nπ = v0 v1 . . . we deﬁne:\\n• the inf (sup) payoﬀ, is the minimum (maximum) weight seen along a play: inf(π) = inf{w(vi , vi+1 )|i ≥\\n0} and sup(π) = sup{w(vi , vi+1 ) | i ≥ 0};\\n• the liminf (limsup) payoﬀ, is the minimum (maximum) weight seen inﬁnitely often: liminf(π) =\\nlim inf i→∞ w(vi , vi+1 ) and, respectively, we have that limsup(π) = lim supi→∞ w(vi , vi+1 );\\n• the mean-payoff value of a play, i.e. the limiting average weight, deﬁned using lim inf or lim sup\\nsince the running averages might not converge: mp(π) = lim inf k→∞ k1 w(hvi ii<k ) and mp(π) =\\nlim supk→∞ k1 w(hvi ii<k ). in words, mp corresponds to the limit inferior of the average weight of\\nincreasingly longer preﬁxes of the play while mp is deﬁned as the limit superior of that same\\nsequence.\\ng is assumed to be total: for each v ∈ v , there exists v′ ∈ v such that (v, v′ ) ∈ e.\\nvalues of all functions are not infinite, and therefore in r since we deal with finite graphs only.\\n\\n1 w.l.o.g.\\n2 the\\n\\n4\\n\\n\\x0ca payoﬀ function val is prefix-independent if for all plays π = v0 v1 . . ., for all i ≥ 0, val(π) =\\nval(hvj ij≥i ). it is well-known that liminf, limsup, mp, and mp are preﬁx-independent. often, the\\narguments that we develop work uniformly for these four measures because of their preﬁx-independent\\nproperty. inf and sup are not preﬁx-independent but often in the sequel we apply a simple transformation\\nto the game and encode inf into a liminf objective, and sup into a limsup objective. the transformation\\nconsists of encoding in the vertices of the arena the minimal (maximal) weight that has been witnessed\\nby a play, and label the edges of the new graph with this same recorded weight. when this simple\\ntransformation does not suﬃce, we mention it explicitly.\\nregret. consider a ﬁxed weighted arena g, and payoﬀ function val. given strategies σ, τ , for eve\\nv\\nand adam respectively, and v ∈ v , we denote by πστ\\nthe unique play starting from v that is consistent\\nv\\nwith σ and τ and denote its value by: valvg (σ, τ ) := val(πστ\\n). we omit g if it is clear from the context.\\nif v is omitted, it is assumed to be vi .\\nlet σ∃ ⊆ s∃ and σ∀ ⊆ s∀ be sets of strategies for eve and adam respectively. given σ ∈ σ∃ we\\ndeﬁne the regret of σ in g w.r.t. σ∃ and σ∀ as:\\nregσς∃ ,σ∀ (g) := supτ ∈σ∀ (supσ′ ∈σ∃ val(σ ′ , τ ) − val(σ, τ )).\\nwe deﬁne the regret of g w.r.t. σ∃ and σ∀ as:\\nregς∃ ,σ∀ (g) := inf σ∈σ∃ regσς∃ ,σ∀ (g).\\nwhen σ∃ or σ∀ are omitted from reg(·) and reg(·) they are assumed to be the set of all strategies for\\neve and adam.\\nremark 1 (ratio vs. diﬀerence). let g be a weighted arena and σ∃ ⊆ s∃ and σ∀ ⊆ s∀ . consider\\nthe regret of g defined using the ratio measure, instead of difference. for inf, sup, liminf, and limsup,\\ndeciding if the regret of g is (strictly) less than a given threshold r reduces (in polynomial time) to\\ndeciding the same problem in glog – which is obtained by replacing every weight x in g with log2 x – for\\nthreshold log2 r with the difference measure.\\nwe will make use of two other values associated with the vertices of an arena: the antagonistic and\\ncooperative values, deﬁned for plays from a vertex v ∈ v as\\navalv (g) := sup inf valv (σ, τ )\\n\\ncvalv (g) := sup sup valv (σ, τ ).\\n\\nσ∈s∃ τ ∈s∀\\n\\nσ∈s∃ τ ∈s∀\\n\\nwhen clear from context g will be omitted, and if v is omitted it is assumed to be vi .\\nremark 2. it is well-known that cval and aval can be computed in polynomial time, w.r.t. the\\nunderlying graph of the given arena, for all payoff functions but mp [cdahs03, cdh10]. for mp, cval\\nis known to be computable in polynomial time, for aval it can be done in up ∩ coup [jur98] and in\\npseudo-polynomial time [zp96, bcd+ 11].\\n\\n3\\n\\nvariant i: adam plays any strategy\\n\\nfor this variant, we establish that for all the payoﬀ functions that we consider, the problem of computing\\nthe antagonistic value and the problem of computing the regret value are inter-reducible in polynomial\\ntime. as a direct consequence, we obtain the following theorem:\\ntheorem 1. deciding if the regret value is less than a given threshold (strictly or non-strictly) is ptimecomplete (under log-space reductions) for inf, sup, liminf, and limsup, and equivalent to mean-payoff\\ngames (under polynomial-time reductions) for mp and mp.\\nupper bounds. we now describe an algorithm to compute regret for all payoﬀ functions. to do so,\\nwe will use the fact that all payoﬀ functions we consider, can be assumed to be preﬁx-independent. thus,\\nlet us ﬁrst convince the reader that one can, in polynomial time, modify inf and sup games so that they\\nbecome preﬁx-independent.\\n5\\n\\n\\x0c0\\n\\ngb1\\nv0\\n\\nvib1\\n\\nw(e) − b1\\n\\n···\\n\\n0\\n\\n0\\n\\nif w ′ (e) > b1\\n0\\n\\n..\\n.\\n\\nv1\\n\\nv⊥\\n\\n−2w − 1\\n\\n0\\nif w ′ (e) > bn\\n\\n0\\n\\nvibn\\n\\nw(e) − bn\\n\\n···\\ngbn\\n\\nfigure 2: weighted arena ĝ, constructed from g. dotted lines represent several edges added when the\\ncondition labelling it is met.\\n\\nlemma 1. for a given weighted arena g, and payoff function sup: reg(g) = reg(gmax ); for payoff\\nfunction inf: reg(g) = reg(gmin ).\\nconsider a weighted arena g = (v, v∃ , vi , e, w). we describe how to construct gmin from g so that\\nthere is a clear bijection between plays in both games deﬁned with the inf payoﬀ function. the arena\\ngmin consists of the following components:\\n• v ′ = v × {w(e) | e ∈ e};\\n• v∃′ = {(v, n) ∈ v ′ | v ∈ v∃ };\\n• vi′ = (vi , w );\\n\\x01\\n• e ′ ∋ (u, n), (v, m) if and only if (u, v) ∈ e and m = min{n, w(u, v)};\\n\\x01\\n• w′ (u, n), (v, m) = m.\\n\\nintuitively, the construction keeps track of the minimal weight witnessed by a play by encoding it into\\nthe vertices themselves. it is not hard to see that plays in gmin indeed have a one-to-one correspondence\\nwith plays in g. furthermore, the liminf and limsup values of a play in gmin are easily seen to be\\nequivalent to the inf value of the play in gmin and the corresponding play in g.\\na similar idea can be used to construct weighted arena gmax from a sup game such that the maximal\\nweight is recorded (instead of the minimal).\\nlemma 2. for payoff functions inf, sup, liminf, limsup, mp, and mp computing the regret of a game\\nis at most as hard as computing the antagonistic value of a (polynomial-size) game with the same payoff\\nfunction.\\nwe now describe the construction used to prove lemma 2.\\nlet us ﬁx a weighted arena g. we deﬁne a new weight function w′ as follows. for any edge e = (u, v)\\n′\\nlet w′ (e) = −∞ if u ∈ v \\\\ v∃ , and if u ∈ v∃ then w′ (e) = max{cvalv | (u, v ′ ) ∈ e \\\\ {e}}. intuitively,\\nw′ represents the best value obtainable for a strategy of eve that diﬀers at the given edge. it is not\\ndiﬃcult to see that in order to minimize regret, eve is trying to minimize the diﬀerence between the\\nvalue given by the original weight function w and the value given by w′ . let range(w′ ) be the set of\\nvalues {w′ (e) | e ∈ e}. for b ∈ range(w′ ) we deﬁne gb to be the graph obtained by restricting g—the\\noriginal weighted arena with weight function w—to edges e with w′ (e) ≤ b.\\nnext, we will construct a new weighted arena ĝ such that the regret of g is a function of the\\nantagonistic value of ĝ. figure 2 depicts the general form of the arena we construct. we have three\\n6\\n\\n\\x0cvertices v0 ∈ v̂ \\\\ vˆ∃ and v1 , v⊥ ∈ vˆ∃ and a “copy” of g as gb for each b ∈ range(w′ ) \\\\ {−∞}. we have\\na self-loop of weight 0 on v0 which is the initial vertex of ĝ, a self-loop of weight −2w − 1 on v⊥ , and\\nweight-0 edges from v0 to v1 and from v1 to the initial vertices of gb for all b. recall that gb might not\\nbe total. to ﬁx this we add, for all vertices without a successor, a weight-0 edge to v⊥ . the remainder\\nof the weight function ŵ, is deﬁned for each edge eb in gb as ŵ(eb ) = w(e) − b.\\nintuitively, in ĝ adam ﬁrst decides whether he can ensure a non-zero regret. if this is the case, then\\nhe moves to v1 . next, eve chooses a maximal value she will allow for strategies which diﬀer from the\\none she will play (this is the choice of b). the play then moves to the corresponding copy of g, i.e. gb .\\nshe can now play to maximize her mean-payoﬀ value. however, if her choice of b was not correct then\\nthe play will end in v⊥ .\\nwe show that, for all preﬁx-independent payoﬀ functions we consider, the following holds:\\nclaim 1. for all prefix-independent payoff functions considered in this work reg(g) = −aval(ĝ).\\nthis implies lemma 2 for all preﬁx-independent payoﬀ functions. together with lemma 1, we get\\nthe same result for inf and sup.\\nof the claim. let us start by arguing that the following equality holds.\\nreg(g) = inf\\n\\nsup\\n\\nsup\\n\\nσ∈s∃ τ ∈s∀ σ′ ∈s∃ \\\\{σ}\\n\\n{0, val(σ ′ , τ ) − val(σ, τ )}.\\n\\n(1)\\n\\nindeed, it follows from the deﬁnition of regret that if σ ′ = σ then the regret of the game is 0. thus, adam\\ncan always ensure the regret of a game is at least 0. now, for b ∈ range(w′ ), deﬁne σ∃ (b) ⊆ s∃ (g) as:\\nσ∃ (b) := {σ | sup\\n\\nval(σ ′ , τ ) ≤ b}.\\n\\nsup\\n\\nτ ∈s∀ σ′ ∈s∃ \\\\{σ}\\n\\nit is clear from the deﬁnitions that σ ∈ σ∃ (b) if and only if σ is a strategy for eve in gb which avoids\\never reaching v⊥ . now, if we let\\nbσ = sup\\n\\nsup\\n\\nval(σ ′ , τ ),\\n\\nτ ∈s∀ σ′ ∈s∃ \\\\{σ}\\n\\nthen σ ∈ σ∃ (b) if and only bσ ≤ b. it follows that for all σ:\\nsup\\n\\nsup\\n\\nτ ∈s∀ σ′ ∈s∃ \\\\{σ}\\n\\nval(σ ′ , τ ) = inf{b | σ ∈ σ∃ (b)}.\\n\\n(2)\\n\\nwe now turn to the mean-payoﬀ game played on ĝ, and make some observations about the strategies\\nwe need to consider. it is well known that memoryless strategies suﬃce for either player to ensure\\nan antagonistic value of at least (resp. at most) aval(ĝ), for all quantitative games considered in\\nthis work, so we can assume that adam and eve play positionally. it follows that all plays either\\nremain in v0 , or move to gb for some b, and adam can ensure a non-positive payoﬀ. note that for\\nbmax = max(range(w′ ) \\\\ {−∞}) we have gbmax = g. so the copy of gbmax in ĝ has no edge to v⊥ ,\\nand by playing to this sub-graph eve can ensure a payoﬀ of at least −|bmax − w | ≥ −2w . as any play\\nthat reaches v⊥ will have a payoﬀ of −2w − 1, we can restrict eve to strategies which avoid v⊥ , and\\nhence all plays either remain in v0 or (eventually) in the copy of gb for some b. now gb contains no\\nrestrictions for adam, so we can assume that he plays the same strategy in all the copies of gb (where\\nhe cannot force the play to v⊥ ), and these strategies have a one-to-one correspondence with strategies in\\ng. likewise, as eve chooses a unique gb to play in, we have a one-to-one correspondence with strategies\\nof eve in ĝ and strategies in g. more precisely, if σ̂ ∈ s∃ (ĝ) is such that σ̂(v1 ) = vib and σ̂ avoids v⊥ ,\\nthen the corresponding strategy σ ∈ s∃ (g) is a valid strategy in gb , and hence:\\nσ̂(v1 ) = vib =⇒ σ ∈ σ(b).\\n\\n(3)\\n\\nnow suppose σ̂ ∈ s∃ (ĝ) is a strategy such that σ̂(v1 ) = vib and σ̂ avoids v⊥ , and τ̂ ∈ s∀ (ĝ) is a strategy\\nsuch that τ̂ (v0 ) = v1 . let σ ∈ σ(b) and τ ∈ s∀ (g) be the strategies in g corresponding to σ̂ and τ̂\\nrespectively. it is easy to show that:\\n− valĝ (σ̂, τ̂ ) = b − valg (σ, τ ).\\n7\\n\\n(4)\\n\\n\\x0cn1\\n\\nm1\\nl\\n\\nvi′\\n\\nm2\\nl\\nn2\\n\\nvi\\narena g\\n\\nfigure 3: gadget to reduce a game to its regret game.\\n\\nputting together equations (1)–(4) gives:\\n−aval(ĝ) = − supσ̂ inf τ̂ valĝ (σ̂, τ̂ )\\n= inf σ̂ sup({−valĝ (σ̂, τ̂ ) | τ̂ (v0 ) = v1 } ∪ {0})\\n= inf{sup({−valĝ (σ̂, τ̂ ) | τ̂ (v0 ) = v1 } ∪ {0}) | σ̂(v1 ) = vib }\\n= inf{supτ ∈s∀ ({b − valg (σ, τ )} ∪ {0}) | σ ∈ σ(b)}\\n= inf σ∈s∃ supτ ∈s∀ ({inf{b | σ ∈ σ(b)} − valg (σ, τ )} ∪ {0})\\n= inf σ∈s∃ supτ ∈s∀ supσ′ ∈s∃ {0, valg (σ ′ , τ ) − valg (σ, τ )}\\n= reg(g) as required.\\n\\nlower bounds. for all the payoﬀ functions, from g we can construct in logarithmic space g′ such\\nthat the antagonistic value of g is a function of the regret value of g′ , and so we have:\\nlemma 3. for payoff functions inf, sup, liminf, limsup, mp, and mp computing the regret of a game\\nis at least as hard as computing the antagonistic value of a (polynomial-size) game with the same payoff\\nfunction.\\nproof. suppose g is a weighted arena with initial vertex vi . consider the weighted arena g′ obtained\\nby adding to g the gadget of figure 3. the initial vertex of g′ is set to be vi′ . in g′ from vi′ eve can\\neither progress to the original game or to the new gadget, both with weight l. we claim that the right\\nchoice of values for the parameters l, m1 , m2 , n1 , n2 makes it so that the antagonistic value of g is a\\nfunction of the regret of the game g′ .\\nlet us ﬁrst give the values of l, m1 , m2 , n1 , and n2 for each of the payoﬀ functions considered.\\nfor all our payoﬀ functions we have m1 = m2 = l; n1 = w + 1; and n2 = −3w − 2. for inf we have\\nl = w , for sup we have l = −w and for the remaining payoﬀ functions we have l = 0.\\nthe following result shows that computing the regret of g is at least as hard as computing the\\n(antagonistic) value of g′ .\\nclaim 2. for all payoff functions:\\naval(g) = w + 1 − reg(g′ ).\\nwe ﬁrst observe that for all payoﬀ functions we consider we have that aval(g) and cval(g) both\\nlie in [−w, w ]. at vi′ eve has a choice: she can choose to remain in the gadget or she can move to the\\noriginal game g. if she chooses to remain in the gadget, her payoﬀ will be −3w − 2, meanwhile adam\\ncould choose a strategy that would have achieved a payoﬀ of cval(g) if she had chosen to play to g.\\nhence her regret in this case is cval(g) + 3w + 2 ≥ 2w + 2. otherwise, if she chooses to play to g she\\n8\\n\\n\\x0c1\\n\\nu\\n6\\n0\\n\\n0\\n\\nv\\n\\n0\\n\\nx\\n\\n0\\n\\nfigure 4: example weighted arena g1 .\\n\\ncan achieve a payoﬀ of at most aval(g). as cval(g) ≤ w is the maximum possible payoﬀ achievable\\nin g, the strategy which now maximizes eve’s regret is the one which remains in the gadget – giving\\na payoﬀ of w + 1. her regret in this case is w + 1 − aval(g) ≤ 2w + 1. therefore, to minimize her\\nregret she will play this strategy, and reg(g′ ) = w + 1 − aval(g).\\nmemory requirements for eve and adam. it follows from the reductions underlying the proof of\\nlemma 2 that eve only requires positional strategies to minimize regret when there is no restriction on\\nadam’s strategies. on the other hand, for any given strategy σ for eve, the strategy τ for adam which\\nwitnesses the maximal regret against it consists of a combination of three positional strategies: ﬁrst he\\nmoves to the optimal vertex for deviating (it is from this vertex that the alternative strategy σ ′ of eve\\nwill achieve a better payoﬀ against τ ), then he plays his optimal (positional) strategy in the antagonistic\\ngame (i.e. against σ). his strategy for the alternative scenario, i.e. against σ ′ , is his optimal strategy in\\nthe co-operative game which is also positional. this combined strategy is clearly realizable as a strategy\\nwith three memory states, giving us:\\ncorollary 1. for payoff functions liminf, limsup, mp and mp: reg(g) = regς1 ,σ3 (g).\\n∃\\n\\n∀\\n\\nthe algorithm we give relies on the preﬁx-independence of the payoﬀ function. as the transformation\\nfrom inf and sup to equivalent preﬁx-independent ones is polynomial it follows that polynomial memory\\n(w.r.t. the size of the underlying graph of the arena) suﬃces for both players.\\n\\n4\\n\\nvariant ii: adam plays memoryless strategies\\n\\nfor this variant, we provide a polynomial space algorithm to solve the problem for all the payoﬀ functions,\\nwe then provide lower bounds.\\ntheorem 2. deciding if the regret value is less than a given threshold (strictly or non-strictly) playing\\nagainst memoryless strategies of adam is pspace-complete for liminf, mp and mp; in pspace and\\nconp-hard for inf, sup and limsup.\\nupper bounds. let us now show how to compute regret against positional adversaries.\\nlemma 4. for payoff functions inf, sup, liminf, limsup, mp and mp, the regret of a game played\\nagainst a positional adversary can be computed in polynomial space.\\ngiven a weighted arena g, we construct a new weighted arena ĝ such that we have that −aval(ĝ)\\nis equivalent to the regret of g.\\nthe vertices of ĝ encode the choices made by adam. for a subset of edges d ⊆ e, let g↾d\\ndenote the weighted arena (v, v∃ , d, w, vi ). the new weighted arena ĝ is the tuple (v̂ , vˆ∃ , ê, ŵ, vˆi )\\nwhere (i) v̂ = v ×\\x01 p(e); (ii) vˆ∃ = {(v, d) ∈ v̂ | v ∈ v∃ }; (iii) vˆi = (vi , e); (iv) ê contains the\\nedge (u, c), (v, d) if and only if (u, v) ∈ c and,\\x01 either u ∈ v∃ and d = c, or u ∈ v \\\\ v∃ and\\nd = c \\\\ {(u, x) ∈ e | x 6= v}; (v) ŵ (u, c), (v, d) = w(u, v) − cval(g↾d). the application of this\\ntransformation for the graph of fig. 4 w.r.t. to the mp payoﬀ function is given in fig. 5.\\n9\\n\\n\\x0c−1\\n\\n−1\\n\\nu, {xv, xu}\\n\\nu, {xu}\\n\\n−2\\n\\nv, {xu}\\n−2\\n\\n−2\\n\\n−2\\n\\n−2\\n4\\n4\\n\\nv, {xv, xu}\\n\\nx, {xv}\\n\\n−2\\n\\n−1\\n\\nx, {xu}\\n0\\n\\n−1\\n−1\\n\\nx, {xv, xu}\\n\\nv, {xv}\\n\\nu, {xv}\\n\\n−1\\n−1\\n\\nfigure 5: weighted arena ĝ1 , constructed from g1 w.r.t the mp payoﬀ function. in the edge set\\ncomponent only edges leaving adam nodes are depicted.\\n\\nconsider a play π̂ = (v0 , c0 )(v1 , c1 ) . . . in ĝ. we denote by [π̂]k , for k ∈ {1, 2}, the sequence hck,i ii≥0 ,\\nwhere ck,i is the k-th component of the i-th pair from π̂. observe that [π̂]1 is a valid play in g. also\\nobserve that e ⊇ cj ⊇ cj+1 for all j. hence [π̂]2 is an inﬁnite descending chain of ﬁnite subsets,\\nand therefore lim [π̂]2 is well-deﬁned. finally, we deﬁne c(π̂) := cval(g↾ lim [π̂]2 ). the following result\\nrelates the value of a play in ĝ to the value of the corresponding play in g.\\nlemma 5. for payoff functions liminf, limsup, mp, mp and for any play π̂ in ĝ we have that val(π̂) =\\nval([π̂]1 ) − c(π̂).\\nproof. we ﬁrst establish the following intermediate result. it follows from the existence of lim [π̂]2 and\\nthe deﬁnition of c(·) that:\\nlim sup\\nn→∞\\n\\nn−1\\nn−1\\n1x\\n1x\\ncval(g↾ci ) = lim inf\\ncval(g↾ci ) = c(π̂).\\nn→∞ n\\nn i=0\\ni=0\\n\\n(5)\\n\\nwe now show that the result holds for mp.\\nval(π̂) = lim inf\\nn→∞\\n\\n!\\nn−1\\n1x\\n(w(vi , vi+1 ) − cval(g↾cj ))\\nn i=0\\n\\n= val([π̂]1 ) − lim sup\\nn→∞\\n\\n= val([π̂]1 ) − c(π̂)\\n\\ndefs. of val(·), ŵ\\n\\nn−1\\n1x\\ncval(g↾cj )\\nn j=0\\n\\ndef. of val(·)\\nfrom eq. (5)\\n\\nthe proofs for the other payoﬀ functions are almost identical (for liminf and limsup replace the use\\nof equation (5) by equation (6)).\\nlim sup cval(g↾ci ) = lim inf cval(g↾ci ) = c(π̂).\\ni→∞\\n\\ni→∞\\n\\n(6)\\n\\nwe now describe how to translate winning strategies for either player from ĝ back to g, i.e. given an\\noptimal maximizing (minimizing) strategy for eve (adam) in ĝ we construct the corresponding optimal\\nregret minimizing strategy (memoryless regret maximizing counter-strategy) for eve (adam) in g. for\\n10\\n\\n\\x0cclarity, we follow this same naming convention throughout this section: again, we say a strategy is an\\noptimal maximizing (minimizing) strategy when we speak about antagonistic and cooperative games, we\\nsay a strategy is an optimal regret maximizing (regret minimizing) when we speak about regret games.\\nwhen this does not suﬃce, we explicitly state which kind of game we are speaking about.\\nlet ε̂ ∈ s∃ (ĝ) be an optimal maximizing strategy of eve in ĝ and α̂ ∈ s∀ (ĝ) be an optimal\\nminimizing strategy of adam. indeed, in [em79] it was shown that mean-payoﬀ games are positionally\\ndetermined. we will now deﬁne a strategy for eve in g which for every play preﬁx s constructs a valid\\nplay preﬁx ŝ in ĝ and plays as ε̂ would in ĝ for ŝ. more formally, for a play preﬁx s from g, denote\\n−1\\nby [s]1 the corresponding sequence of vertex and edge-set pairs in ĝ (indeed, it is the inverse function\\nof [·]1 , which is easily seen to be bijective). deﬁne σ ∈ s∃ (g) as follows: σ(s) = [ε̂([s]−1\\n1 )]1 for all play\\npreﬁxes s ∈ v ∗ · v∃ in g consistent with a positional strategy of adam.\\nfor a ﬁxed strategy of eve we can translate the optimal minimizing strategy of adam in ĝ into an\\noptimal memoryless regret maximizing counter-strategy of his in g. formally, for an arbitrary strategy\\nσ for eve in g, deﬁne σ̂ ∈ s∃ (ĝ) as follows: σ̂(ŝ) = σ([ŝ]1 ) for all ŝ ∈ v̂ ∗ · vˆ∃ . let τσ be an optimal\\n(positional) maximizing strategy for adam in g↾ lim [πσ̂ α̂ ]2 .\\nit is not hard to see the described strategy of eve ensures a regret value of at most −aval(ĝ).\\nslightly less obvious is the fact that for any strategy of eve, the counter-strategy τσ of adam is such\\nthat supσ′ ∈s∃ valg (σ ′ , τσ ) − valg (σ, τσ ) ≥ −aval(ĝ).\\nlemma 6. for payoff functions liminf, limsup, mp, and mp:\\nregs∃ ,σ1 (g) = −aval(ĝ).\\n∀\\n\\nproof. the proof is decomposed into two parts. first, we describe a strategy σ ∈ s∃ (g) which ensures\\na regret value of at most −aval(ĝ). second, we show that for any σ ∈ s∃ (g) there is a τ ∈ σ1∀ (g) such\\nthat\\nsup valg (σ ′ , τ ) − valg (σ, τ ) ≥ −aval(ĝ).\\nσ′ ∈s∃\\n\\nthe result follows.\\nwe have already mentioned earlier that for a play π̂ in ĝ we have that [π̂]1 is a play in g. let\\nppref(g) denote the set of all play preﬁxes consistent with a positional strategy for adam in g. it is not\\ndiﬃcult to see that [·]1 is indeed a bijection between plays of ĝ and plays of g consistent with positional\\nstrategies for adam.\\nit follows from the determinacy of antagonistic games deﬁned by the payoﬀ functions considered in\\nthis work that there are optimal strategies for eve and adam that ensure a payoﬀ of, respectively, at\\nleast and at most a value aval(ĝ) against any strategy of the opposing player. let ε̂ ∈ s∃ (ĝ) be an\\noptimal maximizing strategy of eve in ĝ and α̂ ∈ s∀ (ĝ) be an optimal minimizing strategy of adam.\\n(first part).\\nwe claim that\\n\\n−1\\n\\ndeﬁne a strategy σ from s∃ (g) as follows: σ(s) = [ε̂([s]1 )]1 for all s ∈ ppref(g) · v∃ .\\nregσs∃ ,σ1 (g) ≤ −aval(ĝ).\\n∀\\n\\ntowards a contradiction, assume there are τ ∈ σ1∀ (g) and σ ′ ∈ s∃ (g) such that\\nvalg (σ ′ , τ ) − valg (σ, τ ) > −aval(ĝ).\\ndeﬁne a strategy τ̂ ∈ s∀ (ĝ) as follows: τ̂ (ŝ) = τ ([ŝ]1 ) for all ŝ ∈ v̂ ∗ · v \\\\ˆ v∃ . from the deﬁnition of ε̂\\nand our assumption we get that\\nvalg (σ ′ , τ ) − valg (σ, τ ) > −aval(ĝ) ≥ −valĝ (ε̂, τ̂ ).\\n\\n(7)\\n\\n−1\\n\\nit is straightforward to verify that [πστ ]1 = πε̂τ̂ . therefore, from lemma 5, we have:\\nval(πσ′ τ ) > val(πστ ) − val(πε̂τ̂ ) = cval(g↾ lim [πε̂τ̂ ]2 ).\\n\\n(8)\\n\\nat this point we note that, since τ is a positional strategy, it holds that valg (σ ′ , τ ) is at most the\\nhighest payoﬀ value attainable in g restricted to the edges allowed by τ . formally, if eτ = {(u, v) ∈\\n11\\n\\n\\x0ce | u ∈ v \\\\ v∃ =⇒ v = τ (u)} then val(πσ′ τ ) ≤ cval(g↾eτ ). also, by construction of τ̂ we get that\\neτ ⊆ lim [πε̂τ̂ ]2 . it should be clear that this implies cval(g↾ lim [πε̂τ̂ ]2 ) ≥ cval(g↾eτ ). this contradicts\\nequation (8).\\n\\n(second part). for the second part of the proof we require the following result which relates positional\\nstrategies for adam in g that agree on certain vertices to strategies in sub-graphs deﬁned by plays in ĝ.\\n\\n−1\\n\\nclaim 3. let σ ∈ s∃ (g) and τ, τ ′ ∈ σ1∀ (g). then πστ = πστ ′ if and only if τ ′ ∈ σ1∀ (g↾ lim [[πτ σ ]1 ]2 )\\n\\x01\\nproof. (only if) note that by construction of ĝ we have that once adam chooses an edge (u, c), (v, d)\\nfrom a vertex (v, c) ∈ v̂ \\\\ vˆ∃ then on any subsequent visit to a vertex (u, c ′ ) ∈ v̂ \\\\ vˆ∃ he has no other\\noption but to go to (v, c ′ ). that is, his choice is restricted to be consistent with the history of the\\nplay. for a play π̂ in ĝ, it is clear that the sequence [π̂]2 is the decreasing sequence of sets of edges\\nconsistent (for adam) with the history of the play in the same manner. in particular, for any τ ′ ∈ σ1∀ (g)\\nand any play π in g consistent with τ ′ we have that τ ′ is a valid strategy for adam in g↾e ′ where\\n−1\\ne ′ = lim [[π]1 ]2 . as πστ = πστ ′ is a play consistent with τ ′ , the result follows.\\n(if) suppose πστ 6= πστ ′ , and let v be the last vertex in their common preﬁx. as σ is common\\n−1\\nto both plays, we have v ∈ v \\\\ v∃ , and τ (v) 6= τ ′ (v). in particular, (v, τ ′ (v)) ∈\\n/ lim [[πτ σ ]1 ]2 so\\n−1\\nτ′ ∈\\n/ σ1∀ (g↾ lim [[πτ σ ]1 ]2 ).\\nfor an arbitrary strategy σ for eve in g, deﬁne σ̂ ∈ s∃ (ĝ) as follows: σ̂(ŝ·(v, d)) = (σ([ŝ · (v, d)]−1\\n1 ), d)\\nfor all ŝ·(v, d) ∈ v̂ ∗ ·vˆ∃ . let τσ be an optimal (positional) maximizing strategy for adam in g↾ lim [πσ̂ α̂ ]2 .\\nwe claim that for all σ ∈ s∃ (g) we have that\\nsup valg (σ ′ , τσ ) − valg (σ, τσ ) ≥ −aval(ĝ).\\n\\nσ′ ∈s∃\\n\\ntowards a contradiction, assume that for some σ ∈ s∃ (g) it is the case that for all σ ′ ∈ s∃ (g) the left\\nhand side of the above inequality is strictly smaller than the right hand side. by deﬁnition of α̂ we then\\nget the following inequality.\\nsup valg (σ ′ , τσ ) − valg (σ, τσ ) < −aval(ĝ) ≤ −valĝ (σ̂, α̂)\\n\\n(9)\\n\\nσ′ ∈s∃\\n\\nusing the above claim it is easy to show that [πστσ ]1 = πσ̂ α̂ . hence, by equation (9) and lemma 5 we\\nget that:\\n(10)\\nsup val(πσ′ τσ ) < cval(g↾ lim [πσ̂ α̂ ]2 )\\nσ′ ∈s∃\\n\\nhowever, by choice of τσ , we know that there is a strategy σ ′′ ∈ s∃ (g) such that val(πσ′′ τσ ) =\\ncval(g↾ lim [πσ̂ α̂ ]2 ). this contradicts equation (10) and completes the proof of the theorem.\\nif g was constructed from a inf or sup game h, then one could easily transfer the described strategy\\nof eve, σ into a strategy for her in h which achieves the same regret. in order to have a symmetric\\nresult we still lack the ability to transfer a strategy of adam from ĝ to the original game h. consider\\na modiﬁed construction in which we additionally keep track of the minimal (resp. maximal) weight seen\\nso far by a play, just like described in sect. 3. denote the corresponding game by g̃. the vertex set\\nṽ of g̃ is thus a set of triples of the form (v, c, x) where x is the minimal (resp. maximal) weight the\\nplay has witnessed. we observe that in the proof of the above result the intuition behind why we can\\ntransfer a strategy of adam from ĝ back to g as a memoryless strategy, although the vertices in ĝ\\nalready encode additional information, is that once we have ﬁxed a strategy of eve in g, this gives\\nus enough information about the preﬁx of the play before visiting any adam vertex. in other words, we\\nconstruct a strategy of adam tailored to spoil a speciﬁc strategy of eve, σ, in g using the information\\nwe gather from [·]−1\\n1 and his optimal strategy in ĝ. these properties still hold in g̃. thus, we get the\\nfollowing result.\\n\\n12\\n\\n\\x0clemma 7. for payoff functions inf, sup: regs∃ ,σ1 (g) = −aval(g̃).\\n∀\\n\\nwe recall a result from [em79] which gives us an algorithm for computing the value regs∃ ,σ1 (g) in\\n∀\\npolynomial space. in [em79] the authors show that the value of a mean-payoﬀ game g is equivalent to\\nthe value of a ﬁnite cycle forming game γg played on g. the game is identical to the mean-payoﬀ game\\nexcept that it is ﬁnite. the game is stopped as soon as a cycle is formed and the value of the game is\\ngiven by the mean-payoﬀ value of the cycle.\\nproposition 1 (finite mean-payoﬀ game [em79]). the value of a mean-payoff game g is equal to the\\nvalue of the finite cycle forming game γg played on the same weighted arena.\\nas liminf and limsup games are also equivalent to their ﬁnite cycle forming game (see [ar14]) it\\nfollows that one can use an alternating turing machine to compute the value of a game and that said\\nmachine will stop in time bounded by the length of the largest simple cycle in the arena. we note the\\nlength of the longest simple path in ĝ is bounded by |v |(|e| + 1). hence, we can compute the winner\\nof ĝ in alternating polynomial time. since aptime = pspace, this concludes the proof of lemma 4.\\nlower bounds. we give a reduction from the qsat problem to the problem of determining whether,\\ngiven r ∈ q, regs∃ ,σ1 (g) ⊳ r for the payoﬀ functions liminf, mp, and mp (for ⊳ ∈ {<, ≤}). then we\\n∀\\nprovide a reduction from the complement of the 2-disjoint-paths problem for limsup, sup, and inf.\\nthe crux of the reduction from qsat is a gadget for each clause of the qsat formula. visiting this\\ngadget allows eve to gain information about the highest payoﬀ obtainable in the gadget, each entry point\\ncorresponds to a literal from the clause, and the literal is visited when it is made true by the valuation\\nof variables chosen by eve and adam in the reduction described below. figure 7 depicts an instance of\\nthe gadget for a particular clause. let us focus on the mean-payoﬀ function. note that staying in the\\ninner 6-vertex triangle would yield a mean-payoﬀ value of 4. however, in order to do so, adam needs\\nto cooperate with eve at all three corner vertices. also note that if he does cooperate in at least one of\\nthese vertices then eve can secure a payoﬀ value of at least 11\\n3 .\\nlemma 8. for r ∈ q, weighted arena g and payoff function liminf, mp, or mp, determining whether\\nregs∃ ,σ1 (g) ⊳ r, for ⊳ ∈ {<, ≤}, is pspace-hard.\\n∀\\n\\nthe qsat problem asks whether a given fully quantiﬁed boolean formula (qbf) is satisﬁable.\\nthe problem is known to be pspace-complete [gj79]. it is known the result holds even if the formula\\nis assumed to be in conjunctive normal form with three literals per clause (also known as 3-cnf).\\ntherefore, w.l.o.g., we consider an instance of the qsat problem to be given in the following form:\\n∃x0 ∀x1 ∃x2 . . . φ(x0 , x1 , . . . , xn )\\nwhere φ is in 3-cnf. also w.l.o.g., we assume that every non-trivially true clause has at least one\\nexistentially quantiﬁed variable (as otherwise the answer to the problem is trivial).\\nit is common to consider a qbf as a game between an existential and a universal player. the\\nexistential player chooses a truth value for existentially quantiﬁed variable xi and the universal player\\nresponds with a truth value for xi+1 . after n turns the truth value of φ determines the winner: the\\nexistential player wins if φ is true and the universal player wins otherwise. the game we shall construct\\nmimics the choices of the existential and universal player and makes sure that the regret of the game is\\nsmall if and only if φ is true.\\nlet us ﬁrst consider the strict regret threshold problem. we will construct a weighted arena g in\\nwhich eve wins in the strict regret threshold problem for threshold 2 if and only if φ is true.\\nlemma 9. for weighted arena g and payoff function liminf, mp, or mp, determining whether regs∃ ,σ1 (g) <\\n∀\\n2 is pspace-hard.\\nproof. we ﬁrst describe the value-choosing part of the game (see figure 6). v∃ contains vertices for\\nevery existentially quantiﬁed variable from the qbf and v \\\\ v∃ contains vertices for every universally\\nquantiﬁed variable. at each of this vertices, there are two outgoing edges with weight 0 corresponding to\\na choice of truth value for the variable. for the variable xi vertex, the true edge leads to a vertex from\\n13\\n\\n\\x0cx0\\n\\nx1\\n\\nxn\\n···\\n\\nx0\\n\\nφ\\n\\nx1\\n\\n2\\n\\nxn\\n\\n···\\nci\\n\\ncj\\nfigure 6: depiction of the reduction from qbf.\\n\\n0\\n\\n0\\n\\nxi\\n\\n0\\n\\n3\\n\\n4\\n\\n4\\n\\n4\\n\\n4\\n4\\n4\\n0\\n\\n4\\n\\n3\\n\\nxk\\n\\n0\\n0\\n\\n4\\n3\\n\\n0\\n\\nxj\\n\\n4\\n0\\n\\n0\\n\\nfigure 7: clause gadget for the qbf reduction for clause xi ∨ ¬xj ∨ xk .\\n\\n14\\n\\n\\x0cwhich eve can choose to move to any of the clause gadgets corresponding to clauses where the literal\\nxi occurs (see dotted incoming edge in figure 7) or to advance to xi+1 . the false edge construction is\\nsimilar, while leading to the literal xi rather than to xi . from the vertices encoding the choice of truth\\nvalue for xn eve can either visit the clause gadgets for it or move to a “ﬁnal” vertex φ ∈ v∃ . this ﬁnal\\nvertex has a self-loop with weight 2.\\nto conclude the proof, we describe the strategy of eve which ensures the desired property if the qbf\\nis satisﬁable and a strategy of adam which ensures the property is falsiﬁed otherwise.\\nassume the qbf is true. it follows that there is a strategy of the existential player in the qbf game\\nsuch that for any strategy of the universal player the qbf will be true after they both choose values for\\nthe variables. eve now follows this strategy while visiting all clause gadgets corresponding to occurrences\\nof chosen literals. at every gadget clause she visits she chooses to enter the gadget. if adam now decides\\nto take the weight 4 edge, eve can achieve a mean-payoﬀ value of 11\\n3 or a liminf value of 3 by staying\\nin the gadget. in this case the claim trivially holds. we therefore focus in the case where adam chooses\\nto take eve back to the vertex from which she entered the gadget. she can now go to the next clause\\ngadget and repeat. thus, when the play reaches vertex φ, eve must have visited every clause gadget\\nand adam has chosen to disallow a weight 4 edge in every gadget. now eve can ensure a payoﬀ value of\\n2 by going to φ. as she has witnessed that in every clause gadget there is at least one vertex in which\\nadam is not helping her, alternative strategies might have ensured a mean-payoﬀ of at most 11\\n3 and a\\nliminf value of at most 3. thus, her regret is less than 2.\\nconversely, if the universal player had a winning strategy (or, in other words, the qbf was not\\nsatisﬁable) then the strategy of adam consists in following this strategy in choosing values for the\\nvariables and taking eve out of clause gadgets if she ever enters one. if the play arrives at φ we have\\nthat there is at least one clause gadget that was not visited by the play. we note there is an alternative\\nstrategy of eve which, by choosing a diﬀerent valuation of some variable, reaches this clause gadget and\\nwith the help of adam achieves value 4. hence, this strategy of adam ensures regret of exactly 2. if\\neve avoids reaching φ then she can ensure a value of at most 0, which means an even greater regret for\\nher.\\nwe observe that the above reduction can be readily parameterized. that is, we can replace the 4\\nvalue, the 3 value and the 2 value by arbitrary values a, b, c satisfying the following constraints:\\n• a > b > c,\\n•\\n\\n2a+b\\n3\\n\\n− c < r so that eve wins if φ is true,\\n\\n• a − c ≥ r so that adam wins if φ is false, and\\n• a−\\n\\n2a+b\\n3\\n\\n< r so that he never helps eve in the clause gadgets.\\n\\nindeed, the valuation of a, b, c we chose: 4, 3, 2 with r = 2, satisﬁes these inequalities exactly. it is\\nnot hard to see that if we ﬁnd a valuation for r, a, b, c which meets the ﬁrst restriction and the last\\nthree having changed from strict to non-strict, and vice-versa, we can get a reduction that works for the\\nnon-strict regret threshold problem. that is, ﬁnd values such that\\n• a > b > c,\\n•\\n\\n2a+b\\n3\\n\\n− c ≤ r so that eve wins if φ is true,\\n\\n• a − c > r so that adam wins if φ is false, and\\n• a−\\n\\n2a+b\\n3\\n\\n≤ r so that he never helps eve in the clause gadgets.\\n\\nfor example, one could consider a = 10, b = 7, c = 5 and r = 4.\\nlemma 10. for weighted arena g and payoff function liminf, mp, or mp, determining whether\\nregs∃ ,σ1 (g) ≤ 4 is pspace-hard.\\n∀\\n\\nlemma 11. for r ∈ q, weighted arena g and payoff function inf, sup, or limsup, determining whether\\nregs∃ ,σ1 (g) ⊳ r, for ⊳ ∈ {<, ≤}, is conp-hard.\\n∀\\n\\n15\\n\\n\\x0cv\\n\\nt1\\n\\ns2\\n\\nfigure 8: regret gadget for 2-disjoint-paths reduction.\\n\\nproof. we provide a reduction from the complement of the 2-disjoint-paths problem on directed\\ngraphs [et98]. as the problem is known to be np-complete, the result follows. in other words, we\\nsketch how to translate a given instance of the 2-disjoint-paths problem into a weighted arena in\\nwhich eve can ensure regret value strictly less than 1 if and only if the answer to the 2-disjoint-paths\\nproblem is negative.\\nconsider a directed graph g and distinct vertex pairs (s1 , t1 ) and (s2 , t2 ). w.l.o.g. we assume that\\nfor all i ∈ {1, 2}: (i) ti is reachable from si , and (ii) ti is a sink (i.e. has no outgoing edges). in g. we\\nnow describe the changes we apply to g in order to get the underlying graph structure of the weighted\\narena and then comment on the weight function. let all vertices from g be adam vertices and s1 be\\nthe initial vertex. we replace all edges on t1 —edges of the form (v, t1 ) incident, for some v—by a copy\\nof the gadget shown in figure 8. next, we add self-loops on t1 and t2 with weights 1 and 2, respectively.\\nfinally, the weights of all remaining edges are 0.\\nwe claim that, in this weighted arena, eve can ensure regret strictly less than 1—for payoﬀ functions\\nsup and limsup—if and only if in g the vertex pairs (s1 , t1 ) and (s2 , t2 ) cannot be joined by vertexdisjoint paths. indeed, we claim that the strategy that minimizes the regret of eve is the strategy that,\\nin states where she has a choice, tells her to go to t1 .\\nfirst, let us prove that this strategy has regret strictly less than 1 if and only if no two disjoint paths\\nin the graph exist between the pairs of states (s1 , t1 ) and (s2 , t2 ). assume the latter is the case. then if\\nadam chooses to always avoid t1 , then clearly the regret is 0. if t1 is eventually reached, then the choice\\nof eve secures a value of 1 (for all payoﬀ functions). note that if she had chosen to go towards s2 instead,\\nas there are no two disjoint paths, we know that either the path constructed from s2 by adam never\\nreaches t2 , and then the value of the path is 0—and the regret is 0 for eve—or the path constructed from\\ns2 reaches t1 again since adam is playing positionally—and, again, the regret is 0 for eve. now assume\\nthat two disjoint paths between the source-target pairs exist. if eve changed her strategy to go towards\\ns2 (instead of choosing t1 ) then adam has a strategy to reach t2 and achieve a payoﬀ of 2. thus, her\\nregret would be equal to 1.\\nsecond, we claim that any other strategy of eve has a regret greater than or equal to 1. indeed, if\\neve decides to go towards s2 (instead of choosing to go to t1 ) then adam can choose to loop on the state\\nbefore s2 and the payoﬀ in this case is 0. hence, the regret of eve is at least 1.\\nnote that minimal changes are required for the same construction to imply the result for inf. further,\\nthe weight function and threshold r can be accommodated so that eve wins for the non-strict regret\\nthreshold. hence, the general result follows.\\nmemory requirements for eve. it follows from our algorithms for computing regret in this variant\\nthat eve only requires strategies with exponential memory. examples where exponential memory is\\nnecessary can be easily constructed.\\ncorollary 2. for all payoff functions sup, inf, limsup, liminf, mp and mp, for all game graphs g,\\nthere exists m which is 2o(|g|) such that:\\nregs∃ ,σ1 (g) = regσm ,σ1 (g).\\n∃\\n\\n∀\\n\\n16\\n\\n∀\\n\\n\\x0c5\\n\\nvariant iii: adam plays word strategies\\n\\nfor this variant, we provide tight upper and lower bounds for all the payoﬀ functions: the regret threshold\\nproblem is exptime-complete for sup, inf, limsup, and liminf, and undecidable for mp and mp. for\\nthe later case, the decidability can be recovered when we ﬁx a priori the size of the memory that eve\\ncan use to play, the decision problem is then np-complete. finally, we show that our notion of regret\\nminimization for word strategies generalizes the notion of good for games introduced by henzinger and\\npiterman in [hp06], and we also formalize the relation that exists with the notion of determinization by\\npruning for weighted automata introduced by aminof et al. in [akl10].\\nadditional definitions. we say that a strategy of adam is a word strategy if his strategy can be\\nexpressed as a function τ : n → [max{deg + (v) | v ∈ v }], where [n] = {i | 1 ≤ i ≤ n} and deg + (v) is the\\noutdegree of v (i.e. the number of edges leaving v). intuitively, we consider an order on the successors\\nof each adam vertex. on every turn, the strategy τ of adam will tell him to move to the i-th successor\\n(or to a sink state, if its outdegree is less than i) of the vertex according to the ﬁxed order. we denote\\nby w∀ the set of all such strategies for adam. when considering word strategies, it is more natural to\\nsee the arena as a (weighted) automaton.\\na weighted automaton is a tuple γ = (q, qi , a, ∆, w) where a is a ﬁnite alphabet, q is a ﬁnite set\\nof states, qi is the initial state, ∆ ⊆ q × a × q is the transition relation, w : ∆ → z assigns weights\\nto transitions. a run of γ on a word a0 a1 . . . ∈ aω is a sequence ρ = q0 a0 q1 a1 . . . ∈ (q × a)ω such\\nthat (qi , ai , qi+1 ) ∈ ∆, for all i ≥ 0, and has value val(ρ) determined by the sequence of weights of the\\ntransitions of the run and the payoﬀ function. the value γ assigns to a word is the supremum of the\\nvalues of all its runs on the word. we say the automaton is deterministic if ∆ is functional.\\na game in which adam plays word strategies can be reformulated as a game played on a weighted\\nautomaton γ = (q, qi , a, ∆, w) and strategies of adam – of the form τ : n → a – determine a sequence\\nof input symbols to which eve has to react by choosing ∆-successor states starting from qi . in this\\nsetting a strategy of eve which minimizes regret deﬁnes a run by resolving the non-determinism of ∆ in\\nγ, and ensures the diﬀerence of value given by the constructed run is minimal w.r.t. the value of the best\\nrun on the word spelled out by adam. for instance, if all vertices in fig. 1 are replaced by states, eve\\ncan choose the successor of v1 regardless of what letter adam plays and from v2 and v3 adam chooses\\nthe successor by choosing to play a or b. furthermore, his choice of letter tells eve what would have\\nhappened had the play been at the other state.\\nthe following result summarizes the results of this section:\\ntheorem 3. deciding if the regret value is less than a given threshold (strictly or non-strictly) playing\\nagainst word strategies of adam is exptime-complete for inf, sup, liminf, and limsup; it is undecidable\\nfor mp and mp.\\nupper bounds. there is an exptime algorithm for solving the regret threshold problem for inf, sup,\\nliminf, and limsup. this algorithm is obtained by a reduction to parity and streett games.\\nlemma 12. for r ∈ q, weighted automaton γ and payoff function inf, sup, liminf, or limsup, determining whether regs∃ ,w∀ (γ) ⊳ r, for ⊳ ∈ {<, ≤}, can be done in exponential time.\\nwe show how to decide the strict regret threshold problem. however, the same algorithm can be\\nadapted for the non-strict version by changing strictness of the inequalities used to deﬁne the parity/streett accepting conditions.\\nproof. we focus on the liminf and limsup payoﬀ functions. the result for inf and sup follows from the\\ntranslation to liminf and limsup games given in sect. 3. our decision algorithm consists in ﬁrst building\\na deterministic automaton for γ = (q1 , qi , a, ∆1 , w1 ) using the construction provided in [cdh10]. we\\ndenote by dγ = (q2 , si , a, ∆2 , w2 ) this deterministic automaton and we know that it is at most exponentially larger than γ. next, we consider a simulation game played by eve and adam on the automata\\nγ and dγ . the game is played for an inﬁnite number of rounds and builds runs in the two automata,\\nit starts with the two automata in their respective initial states (qi , si ), and if the current states are q1\\nand q2 , then the next round is played as follows:\\n\\n17\\n\\n\\x0c• adam chooses a letter a ∈ a, and the state of the deterministic automaton is updated accordingly,\\ni.e. q2′ = ∆2 (q2 , a), then\\n• eve updates the state of the non-deterministic automaton to q1′ by reading a using one of the edges\\nlabelled with a in the current state, i.e. she chooses q1′ such that q1′ ∈ ∆1 (q1 , a). the new state of\\nthe game is (q1′ , q2′ ).\\neve wins the simulation game if the minimal weight seen inﬁnitely often in the run of the non-deterministic\\nautomaton is larger than or equal to the minimal weight seen inﬁnitely often in the deterministic automaton minus r. it should be clear that this happens exactly when eve has a regret bounded by r in\\nthe original regret game on the word which is spelled out by adam.\\nlet us focus on the lim inf payoﬀ function now. we will sketch how this game can be translated into\\na parity game. for completeness, we now provide a formal deﬁnition of the latter. a parity game is a\\npair (g, ω) where g is a non-weighted arena and ω : v → n is a function that assigns a priority to each\\nvertex. plays, strategies, and other notions are deﬁned as with games played on weighted arenas. a play\\nin a parity game induces an inﬁnite sequence of priorities. we say a play is winning for eve if and only\\nif the minimal priority seen inﬁnitely often is odd. the parity index of a parity game is the number of\\npriorities labelling its vertices, that is |{ω(v) | v ∈ v }|.\\nto obtain the translation, we keep the structure of the game as above but we assign priorities to the\\nedges of the games instead of weights. we do it in the following way. if x = {x1 , x2 , . . . , xn } is the\\nordered set of weight values that appear in the automata (note that |x| is bounded by the number of\\nedges in the non-deterministic automaton), then we need the set of priorities d = {2, . . . , 2n + 1}. we\\nassign priorities to edges in the game as follows:\\n• when adam chooses a letter a from q2 , then if the weight that labels the edge that leaves q2 with\\nletter a in the deterministic automaton is equal to xi ∈ x, then the priority is set to 2i + 1,\\n• when eve updates the non-deterministic automaton from q1 with a edge labelled with weight w,\\nthen the color is set to 2i where i is the index in x such that xi−1 ≤ w + r < xi .\\nit should be clear then along a run, the minimal color seen inﬁnitely often is odd if and only if the\\ncorresponding run is winning for eve in the simulation game. so, now it remains to solve a parity game\\nwith exponentially many states and polynomially many priorities w.r.t. the size of γ. this can be done\\nin exponential time with classical algorithms for parity games.\\nlimsup to streett games. let us now focus on limsup. in this case we will reduce our problem to\\nthat of determining the winner of a streett game with state-space exponential w.r.t. the original game\\nbut with number of streett pairs polynomial (w.r.t. the original game). recall that a streett game is a\\npair (g, f ) where g is a game graph (with no weight function) and f ⊆ p(v ) × p(v ) is a set of streett\\npairs. we say a play is winning for eve if and only if for all pairs (e, f ) ∈ f, if a vertex in e is visited\\ninﬁnitely often then some vertex in f is visited inﬁnitely often as well.\\nconsider a limsup automaton γ = (q, qi , a, ∆, w). for xi ∈ {w(d) | d ∈ ∆} let us denote by a≥xi\\nthe büchi automaton with büchi transition set equivalent to all transitions with weight of at least xi .\\nwe denote by d≥xi = (qi , qi,i , a, δi , ωi ) the deterministic parity automaton with the same language as\\na≥xi .3 from [pit07] we have that d≥xi has at most 2|q||q| |q|! states and parity index 2|q| (the number\\nof priorities). now, let x1 < x2 < · · · < xl be the weights appearing in transitions of γ. we construct\\nthe (non-weighted) arena gγ = (v, v∃ , e, vi ) and streett pair set f as follows\\nql\\nql\\nql\\n• v = q × i=1 qi ∪ q × i=1 qi × a ∪ q × i=1 qi × a × q;\\nql\\n• v∃ = q × i=1 qi × a;\\n• vi = (qi , q1,i , . . . , ql,i );\\n\\n• e contains\\n\\n3 since\\n\\n\\x01\\n– (p, p1 , . . . , pl )), (p, p1 , . . . , pl , a) for all a ∈ a,\\n\\x01\\n– (p, pl , . . . , pl , a), (p, p1 , . . . , pl , a, q) if (p, a, q) ∈ ∆,\\n\\nδi is deterministic, we sometimes write δi (p, a) to denote the unique q ∈ qi such that (p, a, q) ∈ δi .\\n\\n18\\n\\n\\x0ca, 0\\n\\na, 2\\n\\nbail, 0\\n\\na, 0\\n\\na, 0\\n\\nbail, 0\\n\\n⊥0\\n\\n⊥2\\na \\\\ {bail}, 0\\n\\na \\\\ {bail}, 0\\n\\nfigure 9: initial gadget used in reduction from countdown games.\\n\\n\\x01\\n– (p, pl , . . . , pl , a, q), (q, q1 , . . . , ql ) if for all 1 ≤ i ≤ l: (pi , a, qi ) ∈ δi ;\\n\\n• for all 1 ≤ i ≤ l and all even y such that range(ωi ) ∋ y, f contains the pair (ei , fi ) where\\n– ei,y = {(p, . . . , pi , . . . , pl , a, q) | ωi (pi , a, δ(pi , a)) = y}, and\\n– fi,y = {(p, . . . , pj , . . . , pl , a, q) | (ωi (pi , a, δ(pi , a)) < y ∧ y (mod 2) = 1) ∨ w(p, a, q) ≥ xi − r}.\\nit is not hard to show that in the resulting streett game, a strategy σ of eve is winning against any\\nstrategy τ of adam if and only if for every automaton d≥xi which accepts the word induced by τ then\\nthe run of γ induced by σ has payoﬀ of at least xi − r, if and only if eve has a winning strategy in γ to\\nensure regret is less than r.\\nnote that the number of streett pairs in gγ is polynomial w.r.t. the size of γ, i.e.\\n|f | ≤\\n\\nl\\nx\\n\\n|range(ωi )|\\n\\ni=0\\n\\n≤ l · 2|q|\\n≤ |q|2 · 2|q| = 2|q|3 .\\nfrom [pp06] we have that streett games can be solved in time o(mnk+1 kk!) where n is the number of\\nstates, m the number of transitions and k the number of pairs in f . thus, in this case we have that gγ\\ncan be solved in\\n\\x01\\n3\\no (2|q||q| |q|!)3+2|q| · 2|q|3 · (2|q|3 )! .\\n\\nwhich is still exponential time w.r.t. the size of γ.\\n\\nlower bounds. we ﬁrst establish exptime-hardness for the payoﬀ functions inf, sup, liminf, and\\nlimsup by giving a reduction from countdown games [jsl08]. that is, we show that given a countdown\\ngame, we can construct a game where eve ensures regret less than 2 if and only if counter wins in the\\noriginal countdown game.\\nlemma 13. for r ∈ q, weighted automaton γ and payoff function inf, sup, liminf, or limsup, determining whether regs∃ ,w∀ (γ) ⊳ r, for ⊳ ∈ {<, ≤}, is exptime-hard.\\nlet us ﬁrst formalize what a countdown game is. a countdown game c consists of a weighted graph\\n(s, t ), where s is the set of states and t ⊆ s × (n \\\\ {0}) × s is the transition relation, and a target\\nvalue n ∈ n. if t = (s, d, s′ ) ∈ t then we say that the duration of the transition t is d. a conﬁguration\\nof a countdown game is a pair (s, c), where s ∈ s is a state and c ∈ n. a move of a countdown game\\nfrom a conﬁguration (s, c) consists in player counter choosing a duration d such that (s, d, s′ ) ∈ t for\\nsome s′ ∈ s followed by player spoiler choosing s′′ such that (s, d, s′′ ) ∈ t , the new conﬁguration is then\\n(s′′ , c + d). counter wins if the game reaches a conﬁguration of the form (s, n ) and spoiler wins if the\\ngame reaches a conﬁguration (s, c) such that c < n and for all t = (s, d, ·) ∈ t we have that c + d > n .\\ndeciding the winner in a countdown game c from a conﬁguration (s, 0) – where n and all durations\\nin c are given in binary – is exptime-complete.\\nof lemma 13. let us ﬁx a countdown game c = ((s, t ), n ) and let n = ⌊log2 n ⌋ + 2.\\n19\\n\\n\\x0ca, 2\\n\\na, 2\\n\\nci+1 , 0\\n\\n⊥2\\n\\na, 0\\n\\nxn\\n\\nxi\\n\\n⊥2\\n\\nci+1 , 0\\n\\n...\\n\\nbi , 0\\nci , 0\\n\\na \\\\ {ci+1 }, 0\\n\\nbn , 0\\ncn , 0\\n\\n...\\n\\nbi , 0\\nci , 0\\nci+1 , 0\\n\\nxi\\n\\nxn\\n\\nfigure 10: counter gadget.\\n\\nb0 , 0\\nc1 , 0\\n\\nc1 , 0\\nb0 , 0\\nc2 , 0\\nc2 , 0\\n\\nc3 , 0\\n\\nb4 , 0\\n\\nc3 , 0\\n\\nb4 , 0\\n\\nc5 , 0\\nc5 , 0\\n\\nfigure 11: adder gadget: depicted +9.\\n\\n20\\n\\n\\x0csimplifying assumptions. clearly, if spoiler has a winning strategy and the game continues beyond\\nhis winning the game, then eventually a conﬁguration (s, c), such that c ≥ 2n , is reached. thus, we\\ncan assume w.l.o.g. that plays in c which visit a conﬁguration (s, n ) are winning for counter and plays\\nwhich don’t visit a conﬁguration (s, n ) but eventually get to a conﬁguration (s′ , c) such that c ≥ 2n are\\nwinning for spoiler.\\nadditionally, we can also assume that t in c is total. that is to say, for all s ∈ s there is some\\nduration d such that (s, d, s′ ) ∈ t for some s′ ∈ s. if this were not the case then for every s with no\\noutgoing transitions we could add a transition (s, n + 1, s⊥ ) where s⊥ is a newly added state. it is easy\\nto see that either player has a winning strategy in this new game if and only if he has a winning strategy\\nin the original game.\\nreduction. we will now construct a weighted arena γ with w = 2 such that, in a regret game with\\npayoﬀ function sup played on γ, eve can ensure regret value strictly less than 2 if and only if counter\\nhas a winning strategy in c.\\nas all weights are 0 in the arena we build, with the exception of self-loops on sinks, the result holds\\nfor sup, limsup and inf. we describe the changes required for the inf result at the end.\\nimplementation. the alphabet of the weighted arena γ = (q, qi , a, ∆, w) is a = {bi | 0 ≤ i ≤\\nn} ∪ {ci | 0 < i ≤ n} ∪ {bail, choose} ∪ s. we now describe the structure of γ (i.e. q, ∆ and w).\\ninitial gadget. figure 9 depicts the initial state of the arena. here, eve has the choice of playing\\nleft or right. if she plays to the left then adam can play bail and force her to ⊥0 while the alternative\\nplay resulting from her having chosen to go right goes to ⊥2 . hence, playing left already gives adam a\\nwinning strategy to ensure regret 2, so she plays to the right. if adam now plays bail then eve can go\\nto ⊥2 and as w = 2 this implies the regret will be 0. therefore, adam plays anything but bail.\\ncounter gadget. figure 10 shows the left sub-arena. all states from {xi | 0 ≤ i ≤ n} have incoming\\ntransitions from the left part of the initial gadget with symbol a \\\\ {bail} and weight 0. let y0 . . . yn ∈ b\\nbe the (little-endian) binary representation of n , then for all xi such that yi = 1 there is a transition\\nfrom xi to ⊥0 with weight 0 and symbol bail. similarly, for all xi such that yi = 0 there is a transition\\nfrom xi to ⊥0 with weight 0 and symbol bail. all the remaining transitions not shown in the ﬁgure cycle\\non the same state, e.g. xi goes to xi with symbol choose and weight 0.\\nthe sub-arena we have just described corresponds to a counter gadget (little-endian encoding) which\\nkeeps track of the sum of the durations “spelled” by adam. at any point in time, the states of this subarena in which eve believes alternative plays are now will represent the binary encoding of the current\\nsum of durations. indeed, the initial gadget makes sure eve plays into the right sub-arena and therefore\\nshe knows there are alternative play preﬁxes that could be at any of the xi states. this corresponds to\\nthe 0 value of the initial conﬁguration.\\nadder gadget. let us now focus on the right sub-arena in which eve ﬁnds herself at the moment.\\nthe right transition with symbol a \\\\ {bail} from the initial gadget goes to state s – the initial state from\\nc. it is easy to see how we can simulate counter’s choice of duration and spoiler’s choice of successor.\\nfrom s there are transitions to every (s, c), such that (s, c, s′ ) ∈ t for some s′ ∈ s in c, with symbol\\nchoose and weight 0. transitions with all other symbols and weight 0 going to ⊥1 – a sink with a\\n1-weight cycle with every symbol – from s ensure adam plays choose, lest since w = 2 the regret of the\\ngame will be at most 1 and eve wins.\\nfigure 11 shows how eve forces adam to “spell” the duration c of a transition of c from (s, c). for\\nconcreteness, assume that eve has chosen duration 9. the top source in figure 11 is therefore the state\\n(s, 9). again, transitions with all the symbols not depicted go to ⊥1 with weight 0 are added for all states\\nexcept for the bottom sink. hence, adam will play b0 and eve has the choice of going straight down or\\nmoving to a state where adam is forced to play c1 . recall from the description of the counter gadget\\nthat the belief of eve encodes the binary representation of the current sum of delays. if she believes a\\nplay is in x1 (and therefore none in x1 ) then after adam plays b0 it is important for her to make him\\nplay c1 or this alternative play will end up in ⊥2 . it will be clear from the construction that adam\\nalways has a strategy to keep the play in the right sub-arena without reaching ⊥1 and therefore if any\\nalternative play from the left sub-arena is able to reach ⊥2 then adam wins (i.e. can ensure regret 2).\\nthus, eve decides to force adam to play c1 . as the duration was 9 this gadget now forces adam to play\\n\\n21\\n\\n\\x0cb4 and again presents the choice of forcing adam to play c5 to eve. clearly this can be generalized for\\nany duration. this gadget in fact simulates a cascade configuration of n 1-bit adders.\\nfinally, from the bottom sink in the adder gadget, we have transitions with symbols from s with\\nweight 0 to the corresponding states (thus simulating spoiler’s choice of successor state). additionally,\\nwith any symbol from s and with weight 0 eve can also choose to go to a state qbail where adam is\\nforced to play bail and eve is forced into ⊥0 .\\nargument. note that if the simulation of the counter has been faithful and the belief of eve encodes\\nthe value n then by playing bail, adam forces all of the alternative plays in the left sub-arena into the\\n⊥0 sink. hence, if counter has a winning strategy and eve faithfully simulates the c she can force this\\noutcome of all plays going to ⊥0 . note that from the right sub-arena we have that ⊥2 is not reachable\\nand therefore the highest payoﬀ achievable was 1. therefore, her regret is of at most 1.\\nconversely, if both players faithfully simulate c and the conﬁguration n is never reached, i.e. spoiler\\nhad a winning strategy in c then eventually some alternative play in the left sub-arena will reach xn and\\nfrom there it will go to ⊥2 . again, the construction makes sure that adam always has a strategy to keep\\nthe play in the right sub-arena from reaching ⊥1 and therefore this outcome yields a regret of 2 for eve.\\nchanges for inf. for the same reduction to work for the inf payoﬀ function we add an additional\\nsymbol kick to the alphabet of γ. we also add deterministic transitions with kick, from all states which\\nare not sinks ⊥x for some x, to ⊥0 . finally, all non-loop transitions in the initial gadget are now given a\\nweight of 2; the ones in the counter gadget are given a weight of 2 as well; the ones in the adder gadget\\n(i.e. right sub-arena) are given a weight of 1.\\nwe observe that if counter has a winning strategy in the original game c then eve still has a winning\\nstrategy in γ. the additional symbol kick allows adam to force eve into a 0-loop but also ensures that\\nall alternative plays also go to ⊥0 , thus playing kick is not beneﬁcial to adam unless an alternative play\\nis already at ⊥2 . conversely, if spoiler has a winning strategy in c then adam has a strategy to allow\\nan alternative play to reach ⊥2 while eve remains in the adder gadget. he can then play kick to ensure\\nthe payoﬀ of eve is 0 and achieve a maximal regret of 2.\\nonce again, we observe that the above reduction can be readily parameterized. that is, we can\\nreplace the 2 value, the 1 value and the 0 value from the ⊥2 , ⊥1 , ⊥0 sink loops by arbitrary values a, b,\\nc satisfying the following constraints:\\n• a > b > c,\\n• a − c ≥ r so that eve loses by going left in the initial gadget,\\n• a − b < r so that she does not lose by faithfully simulating the adder if she has a winning strategy\\nfrom the countdown game, or in other words: if adam cheats then a − b is low enough to punish\\nhim,\\n• b − c < r so that she does not regret having faithfully simulated addition, that is, if she plays her\\nwinning strategy from the countdown game then she does not consider b − c too high and regret\\nit.\\nchanging the strictness of the last three constraints and ﬁnding a suitable valuation for r and a, b, c\\nsuﬃces for the reduction to work for the non-strict regret threshold problem. such a valuation is given\\nby a = 2, b = 1, c = 0 with r = 1.\\nto show undecidability of the problem for the mean-payoﬀ function we give a reduction from the\\nthreshold problem in mean-payoff games with partial-observation. this problem was shown to be undecidable in [ddg+ 10, hpr14].\\nlemma 14. for r ∈ q, weighted automaton γ and payoff function mp or mp, determining whether\\nregs∃ ,w∀ (γ) ⊳ r, for ⊳ ∈ {<, ≤}, is undecidable even if eve is only allowed to play finite memory\\nstrategies.\\na mean-payoﬀ game with partial-observation (mpgpo for short) g is a tuple (q, qi , a, ∆, w, obs)\\nwhere q is a set of states, qi is the initial state of the game, a is a ﬁnite set of actions, ∆ ⊆ q×a×q is the\\n22\\n\\n\\x0ctransition relation, w : ∆ → q is a weight function and obs ⊆ p(q) is a partition of q into observations.\\nin these games a play is started by placing a token on qi , eve then chooses an action from a and adam\\nresolves non-determinism by choosing a valid successor (w.r.t. ∆). additionally, eve does not know\\nwhich state adam has chosen as the successor, she is only revealed the observation containing the state.\\nmore formally: a concrete play in such a game is a sequence q0 a0 q1 a1 . . . ∈ (q × a)ω such that q0 = qi\\nand (qi , ai , qi+1 ) ∈ ∆, for all i ≥ 0. an abstract play is then a sequence ψ = o0 a0 o1 a1 . . . ∈ (obs × a)ω\\nsuch that there is some concrete play π = q0 a0 q1 a1 . . . and qi ∈ oi , for all i ≥ 0; in this case we say that\\nπ is a concretization of ψ. strategies of eve in this game are of the form σ : (obs × a)∗ obs → a, that\\nis to say they are observation-based. strategies of adam are not symmetrical, he is allowed to use the\\nexact state information, i.e. his strategies are of the form τ : (q × a)∗ → q.\\nthe threshold problem for mean-payoﬀ games is deﬁned as follows. given ν ∈ q, determining whether\\neve has an observation-based strategy such that, for all counter-strategies of adam, the resulting abstract\\nplay has no concretization with mean-payoﬀ value (strictly) less than ν. for convenience, let us denote\\nthis problem by maxmpgpo(> ν) and by maxmpgpo(≥ ν) when the inequality is strict and nonstrict, respectively. note that in this case eve is playing to maximize the mean-payoﬀ value of all concrete\\nruns corresponding to the abstract play being played while adam is minimizing the same.\\nit was shown in [ddg+ 10, hpr14] that both problems are undecidable for mp and for mp. that\\nis, determining if maxmpgpo(> ν) or maxmpgpo(≥ ν) is undecidable regardless of the deﬁnition\\nused for the mean-payoﬀ function. further, if we ask for the existence of ﬁnite memory observation-based\\nstrategies of eve only, both deﬁnitions (mp and mp) coincide and the problem remains undecidable.\\nconsider a given mpgpo h = (q, qi , a, ∆, w, obs), and denote by h ′ the game obtained by multiplying by −1 all values assigned by w to the transitions of h. clearly, we get that the answer to whether\\nmaxmpgpo(> ν) (resp. maxmpgpo(≥ ν)) in h is aﬃrmative if and only if in h ′ eve has an\\nobservation-based strategy to ensure that against any strategy of adam, the resulting abstract play is\\nsuch that all concretizations have mean-payoﬀ value of less than or equal to −ν (resp. strictly less than\\n−ν). denote these problems by minmpgpo(< µ) and minmpgpo(≤ µ), respectively. it follows\\nthat for any deﬁnition of the mean-payoﬀ function, these problems are undecidable (even if we are only\\ninterested in ﬁnite memory strategies of eve).\\nsimplifying assumptions. we assume, w.l.o.g., that in mean-payoﬀ games with partial-observation\\nthe transition relation is total. as the weights in mean-payoﬀ games with partial-observation can be\\nshifted and scaled, we can assume w.l.o.g. that ν is any integer n . furthermore, we can also assume\\nthat the mean-payoﬀ value of any concrete play in a game is bounded from below by 0 and from above\\nby m (this can again be achieved by shifting and scaling).\\nof lemma 14. we give a reduction from the threshold problem of mean-payoff games with partial observation [ddg+ 10,hpr14] that resembles the reduction used for the proof of lemma 13. more speciﬁcally,\\ngiven a mean-payoﬀ game with partial-observation h = (s, si , t, b, c, obs), we construct a weighted automaton γh = (q, qi , ∆, a, w) with the same payoﬀ function such that\\nregς∃ ,σ1 (γh ) < r\\n∀\\n\\nif and only if the answer to minmpgpo(< n ) is aﬃrmative. the reduction we describe works for any\\nr, n, m, c such that\\n• c < r,\\n•\\n\\nm\\n2\\n\\n− c < r, and\\n\\n•\\n\\nn\\n2\\n\\n≤ r,\\n\\nfor concreteness we consider r = 4, n = 4, m = 6 and c = 3.\\nlet us describe how to construct the weighted arena γh from g. the alphabet of γh is a =\\nb ∪ {bail} ∪ obs. the structure of γh includes a gadget such as the one depicted in figure 9. recall\\nfrom the proof of lemma 12 that this gadget ensures eve chooses to go to the right sub-arena, lest adam\\nhas a spoiling strategy. as the left sub-arena we have a modiﬁed version of h. first, for every state\\ns ∈ s and every action b ∈ b, we add an intermediate state (s, b) such that when b is played from s the\\nplay deterministically goes to (s, b) and for any transition (s, b, s′ ) in h we add a transition in γh from\\n23\\n\\n\\x0c(s, b) to s′ with action os′ , where os′ is the observation containing s′ . second, we add transitions from\\nevery s ∈ s to ⊥c for symbol bail with weight 0 and from every (s, b) to ⊥c with symbol o if there is\\nno s′ ∈ o such that (s, b, s′ ) ∈ t . the sink ⊥c has, for every symbol a ∈ a, a weight c self-loop. as the\\nright sub-arena we will have states qb for all b ∈ b. for any such qb there are transitions with weight\\n0 and symbol b to qobs and transitions with weight 0 and symbols a \\\\ {b} to ⊥c . from qobs with any\\nsymbol from obs, there are 0-weight transitions to qb′ (for any b′ ∈ b) and transitions with weight 0\\nand symbols a \\\\ obs to ⊥c . all qb have incoming edges from the state of the initial gadget which leads\\nto the right sub-arena.\\nwe claim that eve has a strategy σ in γh to ensure regret less than r if and only if the answer to\\nminmpgpo(< n ) is aﬃrmative. assume that the latter is the case, i.e. in h eve has an observationbased strategy to ensure that against any strategy of adam the abstract play has no concretization with\\nmean-payoﬀ value greater than or equal to n . let us describe the strategy of eve in γh . first, she plays\\ninto the right sub-arena of the game. once there, she tries to visit states qb0 qb1 . . . based on her strategy\\nfor h. if adam, at some qbi does not play bi , or at some visit to qobs he plays a non-observation symbol,\\nthen eve goes to ⊥c . the play then has value c. since no alternative play in the left sub-arena can\\nm\\nhave value greater than m\\n2 and we have that 2 − c < r, eve wins. thus, we can assume that adam, at\\nevery qbi plays the symbol bi and at every visit to qobs plays an observation. note that, by construction\\nof the left sub-arena, we are forcing adam to reveal a sequence of observations to eve and allowing her\\nto choose a next action. it follows that the value of the play in γh is 0. any alternative play in the right\\nsub-arena would have value of at most c as the highest weight in it is c. in the left sub-arena, we have\\nthat all alternative plays have value less than n2 . indeed, since she has followed her winning strategy\\nfrom h, and since by construction we have that all alternative plays in the left sub-arena correspond to\\nconcretizations of the abstract path spelled by adam and eve, if there were some play with value of at\\nleast n2 this would contradict her strategy being optimal. as c < r and n2 < r, we have that eve wins\\nthe regret game, i.e. her strategy ensures regret less than r.\\nconversely, assume that the answer to minmpgpo(< n ) is negative. then regardless, of which\\nstrategy from h eve decides to follow, we know there will be some alternative play in the left sub-arena\\nwith value of at least n2 . if adam allows eve to play any such strategy then the value of the play is 0\\nand her regret is at least n2 ≤ r, which concludes the proof for the strict regret threshold problem.\\nwe observe that the restriction on n, m, r and c can easily be adapted to allow for a reduction from\\nminmpgpo(≤ n ) to the non-strict regret threshold problem.\\nfinally, we note that in the above proof eve might require inﬁnite memory as it is known that in\\nmean-payoﬀ games with partial-observation the protagonist might require inﬁnite memory to win. yet,\\nas we have already mentioned, even if we ask whether eve has a winning ﬁnite memory observation-based\\nstrategy, the problem remains undecidable. notice that the above construction – when restricting eve\\nto play with ﬁnite memory – gives us a reduction from this exact problem. hence, even when restricting\\neve to use only ﬁnite memory, the problem is undecidable.\\nmemory requirements for eve and adam. it is known that positional strategies suﬃce for eve\\nin parity games. on the other hand, for streett games she might require exponential memory (see,\\ne.g. [djw97]). this exponential blow-up, however, is only on the number of pairs—which we have\\nalready argued remains polynomial w.r.t. the original automaton. it follows that:\\ncorollary 3. for payoff functions sup, inf, limsup, liminf, for all weighted automata a, there exists\\nm which is 2o(|a|) such that:\\nregs∃ ,w∀ (a) = regσm ,w∀ (a).\\n∃\\n\\nfixed memory for eve. since the problem is exptime-hard for most payoﬀ functions and already\\nundecidable for mp and mp, we now ﬁx the memory eve can use.\\ntheorem 4. for r ∈ q, weighted automaton γ and payoff function inf, sup, liminf, limsup, mp, or\\nmp, determining whether regσm ,w∀ (γ) ⊳ r, for ⊳ ∈ {<, ≤}, can be done in ntime(m2 |γ|2 ).\\n∃\\n\\ndenote by r∀ ⊆ w∀ the set of all word strategies of adam which are regular. that is to say, w ∈ r∀\\nif and only if w is ultimately periodic. it is well-known that the mean-payoﬀ value of ultimately periodic\\nplays in weighted arenas is the same for both mp and mp.\\n\\n24\\n\\n\\x0cbefore proving the theorem we ﬁrst show that ultimately periodic words suﬃce for adam to spoil a\\nﬁnite memory strategy of eve. let us ﬁx some useful notation. given weighted automaton γ and a ﬁnite\\nmemory strategy σ for eve in γ we denote by γσ the deterministic automaton embodied by a reﬁnement\\nof γ that is induced by σ.\\nlemma 15. for r ∈ q, weighted automaton γ, and payoff function inf, sup, liminf, limsup, mp, or\\nmp, if regσm ,w∀ (γ) ⊲ r then regσm ,r∀ (γ) ⊲ r, for ⊲ ∈ {>, ≥}.\\n∃\\n\\n∃\\n\\nproof. for inf, sup, liminf, and limsup the result follows from lemma 12. it is known that positional\\nstrategies suﬃce for either player to win a parity game. thus, if adam wins the parity game deﬁned\\nin the proof of lemma 12 then he has a positional strategy to do so. now, for any strategy of eve in\\nthe original game, one can translate the winning strategy of adam in the parity game into a spoiling\\nstrategy of adam in the regret game. this strategy will have ﬁnite memory and will thus correspond to\\nan ultimately periodic word. hence, it suﬃces for us to show the claim follows for mean-payoﬀ. we do\\nso for mp and ≥ but the result for mp follows from minimal changes to the argument (a small quantiﬁer\\nswap in fact) and for > variations we need only use the strict versions of equations (∗) and (11). we\\nassume without loss of generality that all weights are non-negative.\\nlet σ be the best (regret minimizing) strategy of eve in γ which uses at most memory m. we claim\\nthat if adam has a word strategy to ensure the regret of eve in γ is at least r then he also has a regular\\nword strategy to do so.\\nconsider the bi-weighted graph g constructed by taking the synchronous product of γ and γσ while\\nlabelling every edge with two weights: the value assigned to the transition by the weight function of γσ\\nand the value assigned to the transition by that of γ. for a path π in g, denote by wi (π) the sum of the\\nweights of the edges traversed by π w.r.t. the i-th weight function. also, for an inﬁnite path π, denote\\nby mpi the mean-payoﬀ value of π w.r.t. the i-th weight function. clearly, adam has a word strategy\\nto ensure a regret of at least r against the strategy σ of eve if and only if there is an inﬁnite path π\\nin g such that mp2 (π) − mp1 (π) ≥ r. we claim that if this is the case then there is a simple cycle χ\\n1\\n1\\nin g such that |χ|\\nw2 (χ) − |χ|\\nw1 (χ) ≥ r. the argument is based on the cycle decomposition of π (see,\\ne.g. [em79]).\\nassume, for the sake of contradiction, that all the cycles χ in g satisfy the following:\\n1\\n1\\nw2 (χ) −\\nw1 (χ) ≤ r − ε, for some 0 < ε ≤ r,\\n|χ|\\n|χ|\\n\\n(∗)\\n\\nand let us consider an arbitrary inﬁnite path π = v0 v1 . . . . let l = mp1 (π). we will show\\nlim inf\\nk→∞\\n\\nw2 (hvj ij≤k )\\n− l ≤ r − ε,\\nk\\n\\n(11)\\n\\nfrom which the required contradiction follows.\\nfor any k ≥ 0, the cycle decomposition of hvj ij≤k tells us that apart from a small sub-path, π ′ , of\\nlength at most n (the number of states in g), the preﬁx hvj ij≤k can be decomposed into simple cycles\\npt\\nχ1 , . . . , χt such that wi (hvj ij≤k ) = wi (π ′ ) + j=1 wi (χj ) for i = 1, 2. if w is the maximum weight\\noccurring in g, then from equation (∗) we have:\\nw2 (hvj ij≤k ) ≤\\n\\nnw +\\n\\nt\\nx\\n\\nw2 (χj )\\n\\nj=1\\n\\n≤\\n\\nnw + (r − ε)\\n\\nt\\nx\\nj=1\\n\\n≤\\n\\n|χj | +\\n\\nt\\nx\\n\\nw1 (χj )\\n\\nj=1\\n\\nnw + k(r − ε) + w1 (hvj ij≤k ).\\n\\nnow, it follows from the deﬁnition of the limit inferior that for any ε′ > 0 and any k > 0 there\\nexists k > k such that w1 (hvj ij≤k ) ≤ k(l + ε′ ). thus for any ε′ > 0 and k ′ > 0, there exists\\nk > max{k ′ , nw/ε′ } such that\\nw2 (hvj ij≤k )\\nnw\\n≤\\n+ (r − ε) + (l + ε′ ) < (l + r − ε) + 2ε′ .\\nk\\nk\\n25\\n\\n\\x0cequation (11) then follows from the deﬁnition of limit inferior.\\nthe above implies that adam can, by repeating χ inﬁnitely often, achieve a regret value of at least\\nr against strategy σ of eve. as this can be done by him playing a regular word, the result follows.\\nwe now proceed with the proof of the theorem. the argument is presented for mean-payoﬀ (mp)\\nbut minimal changes are required for the other payoﬀ functions. for simplicity, we use the non-strict\\nthreshold for the emptiness problems. however, the result from [cdh10] is independent of this. further,\\nthe exact same argument presented here works for both cases. thus, if suﬃces to show the result follows\\nfor ≥.\\nof theorem 4. we will “guess” a strategy for eve which uses memory at most m and verify (in polynomial\\ntime w.r.t. m and the size of γ) that it ensures a regret value of strictly less than r.\\nlet a be the mean-payoﬀ (mp) automaton constructed as the synchronous product of γ and γσ .\\nthe new weight function maps a transition to the diﬀerence of the values of the weight functions of the\\ntwo original automata. we claim that the language of a is empty (for accepting threshold ≥ r) if and\\nonly if regσσm ,w∀ (γ) < r. indeed, there is a bijective map from every run of a to a pair of plays π, π ′\\n∃\\nin γ such that both π and π ′ are consistent with the same word strategy of adam and π is consistent\\nwith σ. it will be clear that a has size at most m|γ|. as emptiness of a weighted automaton a can be\\ndecided in o(|a|2 ) time [cdh10], the result will follow.\\nwe now show that if the language of a is not empty then adam can ensure a regret value of at least\\nr against σ in γ and that, conversely, if adam has a spoiling strategy against σ in γ then that implies\\nthe language of a is not empty.\\npi\\nlet ρx be a run of a on x. from the deﬁnition of a we get that mp(ρx ) = lim inf i→∞ 1i j=0 (aj − bj )\\nwhere αx = hai ii≥0 and βx = hbi ii≥0 are the inﬁnite sequences of weights assigned to the transitions\\nof ρ by the weight functions of γ and γσ respectively. it is known that if a mean-payoﬀ automaton\\naccepts a word y then it must accept an ultimately periodic word y ′ , thus we can assume that x is\\nultimately periodic (see, e.g. [cdh10]). furthermore, we can also assume the run of the automaton on\\nx is ultimately periodic. recall that for ultimately periodic runs we have that mp(ρx ) = mp(ρx ). we\\nget the following\\ni\\n\\nmp(ρx ) = lim sup\\ni→∞\\n\\n1x\\n(aj − bj )\\ni j=0\\ni\\n\\n≤ lim sup\\ni→∞\\n\\ni\\n\\n≤ lim sup\\ni→∞\\n\\ni→∞\\n\\nsub-additivity of lim sup\\n\\ni\\n\\n1x\\n1x\\naj − lim inf\\nbj\\ni→∞ i\\ni j=0\\nj=0\\ni\\n\\n≤ lim inf\\n\\ni\\n\\n1x\\n−1 x\\naj + lim sup\\nbj\\ni j=0\\ni j=0\\ni→∞\\n\\ni\\n\\n1x\\n1x\\naj − lim inf\\nbj\\ni→∞\\ni j=0\\ni j=0\\n\\nultimate periodicity.\\n\\nthus, as x and ρx can be be mapped to a strategy of adam in γ which ensures regret of at least r against\\nσ, the claim follows.\\nfor the other direction, assume adam has a word strategy τ in γ which ensures a regret of at least\\nr against σ. from lemma 15 it follows that τ and the run ρ of γ with value γ(τ ) can be assumed to\\nbe ultimately periodic w.l.o.g.. denote by ρσ and wσ the run of γσ on τ and the weight function of γσ\\nrespectively. we then get that\\n1\\n1\\nlim inf wσ (ρσ ) − lim inf w(ρ)\\ni→∞ i\\ni→∞ i\\n1\\n−1\\n= lim inf wσ (ρσ ) + lim sup\\nw(ρ)\\ni→∞ i\\ni\\ni→∞\\n−1\\n1\\nw(ρ)\\n= lim inf wσ (ρσ ) + lim inf\\ni→∞\\ni→∞ i\\ni\\n≤ mp(ψτ )\\n\\nultimate periodicity\\nsuper-additivity of lim inf,\\n26\\n\\n\\x0c1\\n\\nn\\n\\n...\\n#\\n\\n#\\n\\n...\\n\\nn\\n\\n1\\n\\n⊥2\\n\\na, 2\\n\\nfigure 12: clause choosing gadget for the sat reduction. there are as many paths from top to bottom\\n(⊥2 ) as there are clauses (n).\\n\\nwhere ψτ is the corresponding run of a for τ and ρ. hence, a has at least one word in its language.\\nwe provide a matching lower bound. the proof is an adaptation of the np-hardness proof from [akl10].\\ntheorem 5. for r ∈ q, weighted automaton γ and payoff function inf, sup, liminf, limsup, mp, or\\nmp, determining whether regς1 ,w∀ (γ) ⊳ r, for ⊳ ∈ {<, ≤}, is np-hard.\\n∃\\n\\nproof. we give a reduction from the sat problem, i.e. satisﬁability of a cnf formula. the construction\\npresented is based on a proof in [akl10]. the idea is simple: given boolean formula φ in cnf we\\nconstruct a weighted automaton γφ such that eve can ensure regret value of at most 1 with a positional\\nstrategy in γφ if and only if φ is satisﬁable.\\nlet us now ﬁx a boolean formula φ in cnf with n clauses and m boolean variables x1 , . . . , xm . the\\nweighted automaton γφ = (q, qi , a, ∆, w) has alphabet a = {bail, #} ∪ {i | 1 ≤ i ≤ n}. γφ includes an\\ninitial gadget such as the one depicted in figure 9. recall that this gadget forces eve to play into the\\nright sub-arena. as the left sub-arena of γφ we attach the gadget depicted in figure 12. all transitions\\nshown have weight 1 and all missing transitions in order for γφ to be complete lead to a state ⊥0 with a\\nself-loop on every symbol from a with weight 0. intuitively, as eve must go to the right sub-arena then\\nall alternative plays in the left sub-arena correspond to either adam choosing a clause i and spelling\\ni#i to reach ⊥2 or reaching ⊥0 by playing any other sequence of symbols. the right sub-arena of the\\nautomaton is as shown in figure 13, where all transitions shown have weight 1 and all missing transitions\\ngo to ⊥0 again. here, from q0 we have transitions to state xj with symbol i if the i-th clause contains\\nvariable xj . for every state xj we have transitions to jtrue and jf alse with symbol #. the idea is to\\nallow eve to choose the truth value of xj . finally, every state jtrue (or jf alse ) has a transition to ⊥1\\nwith symbol i if the literal xj (resp. ¬xj ) appears in the i-th clause.\\nthe argument to show that eve can ensure regret of 0 if and only if φ is satisﬁable is straightforward.\\nassume the formula is indeed satisﬁable. assume, also, that adam chooses 1 ≤ i ≤ n and spells i#i.\\nsince φ is satisﬁable there is a choice of values for x1 , . . . , xm such that for each clause there must be at\\nleast one literal in the i-th clause which makes the clause true. eve transitions, in the right sub-arena\\nfrom q0 to the corresponding value and when adam plays # she chooses the correct truth value for the\\n27\\n\\n\\x0cq0\\n1, 2, 3\\n\\n1, 2, 3\\n\\nx1\\n\\nx2\\n#\\n\\n#\\n\\n1true\\n\\n1\\n\\n#\\n\\n#\\n\\n1f alse\\n\\n2true\\n\\n2, 3\\n\\n1, 2\\n\\n2f alse\\n\\n3\\n\\n⊥1\\n\\na, 1\\n\\nfigure 13: value choosing gadget for the sat reduction. depicted is the conﬁguration for (x1 ∨ x2 ) ∧\\n(¬x1 ∨ x2 ) ∧ (¬x1 ∨ ¬x2 ).\\n\\n28\\n\\n\\x0cvariable. thus, the play reaches ⊥1 and, as w = 2 in φ it follows that her regret is 0. if adam does not\\nplay i#i, for some i then all runs will have value at most 1 and the regret of eve will thus be at most\\n1. note that the strategy for eve can be realized with a positional strategy by assigning to each xj the\\nchoice of truth value and choosing from q0 any valid transition for all 1 ≤ i ≤ n.\\nconversely, if φ is not satisﬁable then for every valuation of variables x1 , . . . , xm there is at least one\\nclause which is not true. given any positional strategy of eve in γφ we can extract the corresponding\\nvaluation of the boolean variables. now adam chooses 1 ≤ i ≤ n such that the i-th clause is not satisﬁed\\nby the assignment. the play will therefore end in ⊥0 while an alternative play in the left sub-arena will\\nreach ⊥1 . hence the regret of eve in the game is 2.\\nto complete the proof we note that the above analysis is the same for payoﬀ functions inf, liminf,\\nlimsup, and mp. for sup it suﬃces to change all the weights on non-self-loops from 1 to 0.\\nwe observe that, once more, that the same proof shows eve can ensure regret of strictly less than 2\\nif and only if the formula is satisﬁable. hence, the result for the non-strict regret threshold problem as\\nwell.\\nrelation to other works. let us ﬁrst extend the deﬁnitions of approximation, embodiment and refinement from [akl10] to the setting of ω-words. consider two weighted automata a = (qa , qi , a, ∆a , wa )\\nand b = (qb , qi , a, ∆b , wb ) and let d : r × r → r be a metric.4 we say b (strictly) α-approximates a\\n(with respect to d) if d(b(w), a(w)) ≤ α (resp. d(b(w), a(w)) < α) for all words w ∈ aω . we say b embodies a if qa ⊆ qb , ∆a ⊆ ∆b and wa agrees with wb on ∆a . for an automaton a = (q, qi , a, ∆, w)\\nand an integer k ≥ 0, the k-reﬁnement of a is the automaton obtained by reﬁning the state space of a\\nusing k boolean variables. intuitively, this corresponds to having 2k copies of every state, with each copy\\nof p transitioning to all copies of q with a if (p, a, q) ∈ ∆. the automaton a is said to be (strictly) (α, k)determinizable by pruning (dbp, for short) if the k-reﬁnement of a embodies a deterministic automaton\\nwhich (strictly) α-approximates a. the next result follows directly from the above deﬁnitions.\\nproposition 2. for α ∈ q, k ∈ n, a weighted automaton γ is (strictly) (α, k)-dbp (w.r.t. the difference\\nmetric) if and only if regς2k ,w∀ (γ) ≤ α (resp. regς2k ,w∀ (γ) < α).\\n∃\\n\\n∃\\n\\nin [hp06] the authors deﬁne good for games automata. their deﬁnition is based on a game which\\nis played on an ω-automaton by spoiler and simulator. we propose the following generalization of the\\nnotion of good for games automata for weighted automata. a weighted automaton a is (strictly) α-good\\nfor games if simulator, against any word w ∈ aω spelled by spoiler, can resolve non-determinism in a\\nso that the resulting run has value v and d(v, a(w)) ≤ α (resp. d(v, a(w)) < α), for some metric d. we\\nsummarize the relationship that follows from the deﬁnition in the following result:\\nproposition 3. for α ∈ q, a weighted automaton γ is (strictly) α-good for games (w.r.t. the difference\\nmetric) if and only if regs∃ ,w∀ (γ) ≤ α (resp. regs∃ ,w∀ (γ) < α).\\n\\n6\\n\\ndiscussion\\n\\nin this work we have considered the regret threshold problem in quantitative games. we have studied\\nthree variants which corresponds to diﬀerent assumptions regarding the behavior of adam. our deﬁnition\\nof regret is based on the diﬀerence measure: eve attempts to minimize the diﬀerence between the value\\nshe obtains by playing the game, and the value she could have obtained if she had known the strategy\\nof adam in advance. in [akl10] the ratio measure was used instead. we believe some of the results\\nobtained presently can be extended to arbitrary metrics (as in, e.g., [bh14]). in particular, all hardness\\nstatements should hold. we give more precise claims for the ratio measure below.\\nfor inf, sup, liminf, and limsup.\\nwe have already observed that upper bounds for the regret threshold problem follow directly from our results if regret is deﬁned using ratio (see remark 1). furthermore,\\nall hardness results presented here can also be adapted to obtain the same result for ratio. indeed, the\\nsame constructions and gadgets can be used. these, together with correctly chosen regret threshold value\\nr and modiﬁed edge weights and inequalities (such as the ones given to prove, for instance, lemma 8)\\nare suﬃcient to show the same results hold for regret deﬁned with ratio.\\n4 the\\n\\nmetric used in [akl10] is the ratio measure.\\n\\n29\\n\\n\\x0cfor mp. all hardness results also hold for regret deﬁned with ratio. as with the other payoﬀ functions,\\nminimal modiﬁcations are needed for the proofs given in this work to imply the result for the alternative\\ndeﬁnition of regret. regarding the algorithms, we have solved the regret threshold problem for the ﬁrst\\ntwo variants. in the third variant, we have considered a restricted version of the game (theorem 4)\\nand given an algorithm for it by reducing it to an the emptiness problem for mean-payoﬀ automata.\\nwe claim the corresponding problems are in the same complexity classes, respectively, when regret is\\ndeﬁned with ratio. for the ﬁrst two, the proofs are almost identical to the ones we have give in the\\npresent work for the diﬀerence measure. for the third problem, lemma 15 must be restated for ratio,\\nyet the proof requires minimal modiﬁcations to work in that case. finally, the argument used to prove\\ntheorem 4 requires the reduction to mean-payoﬀ automata be replaced by a reduction to ratio automata.\\nhowever, all the properties from mean-payoﬀ automata which were used in the proof, are also true for\\nratio automata (e.g. ultimately periodic words being accepted if an arbitrary word is accepted). the\\nlatter follow from results regarding ratio games in [bcg+ 14].\\n\\nacknowledgements\\nwe thank udi boker for his comments on how to determinize limsup automata.\\n\\nreferences\\n[akl10]\\n\\nbenjamin aminof, orna kupferman, and robby lampert. reasoning about online algorithms with weighted automata. acm transactions on algorithms, 2010.\\n\\n[ar14]\\n\\nbenjamin aminof and sasha rubin. first cycle games. in sr, pages 83–90, 2014.\\n\\n[bcd+ 11] lubos brim, jakub chaloupka, laurent doyen, raﬀaella gentilini, and jean-françois\\nraskin. faster algorithms for mean-payoﬀ games. formal methods in system design,\\n38(2):97–118, 2011.\\n[bcg+ 14] roderick bloem, krishnendu chatterjee, karin greimel, thomas a. henzinger, georg hofferek, barbara jobstmann, bettina könighofer, and robert könighofer. synthesizing robust\\nsystems. acta informatica, 51(3-4):193–220, 2014.\\n[bel82]\\n\\ndavid e. bell. regret in decision making under uncertainty. operations research, 30(5):961–\\n981, 1982.\\n\\n[bh14]\\n\\nudi boker and thomas a. henzinger. exact and approximate determinization of discountedsum automata. lmcs, 10(1), 2014.\\n\\n[cdahs03] arindam chakrabarti, luca de alfaro, thomas a. henzinger, and mariëlle stoelinga. resource interfaces. in emsoft, volume 2855 of lncs, pages 117–133. springer, 2003.\\n[cdfr14] krishnendu chatterjee, laurent doyen, emmanuel filiot, and jean-françois raskin.\\ndoomsday equilibria for omega-regular games. in vmcai, volume 8318, pages 78–97.\\nspringer, 2014.\\n[cdh10]\\n\\nkrishnendu chatterjee, laurent doyen, and thomas a. henzinger. quantitative languages.\\nacm transactions on computational logic, 11(4), 2010.\\n\\n[cdhr10] krishnendu chatterjee, laurent doyen, thomas a. henzinger, and jean-françois raskin.\\ngeneralized mean-payoﬀ and energy games. in fsttcs, pages 505–516, 2010.\\n[ddg+ 10] aldric degorre, laurent doyen, raﬀaella gentilini, jean-françois raskin, and szymon\\ntoruńczyk. energy and mean-payoﬀ games with imperfect information. in csl, pages\\n260–274, 2010.\\n[df11]\\n\\nwerner damm and bernd finkbeiner. does it pay to extend the perimeter of a world model?\\nin fm, volume 6664 of lncs, pages 12–26. springer, 2011.\\n\\n30\\n\\n\\x0c[djw97]\\n\\nstefan dziembowski, marcin jurdziński, and igor walukiewicz. how much memory is needed\\nto win inﬁnite games? in lics, pages 99–110. ieee computer society, 1997.\\n\\n[em79]\\n\\nandrzej ehrenfeucht and jan mycielski. positional strategies for mean payoﬀ games. international journal of game theory, 8:109–113, 1979.\\n\\n[et98]\\n\\ntali eilam-tzoreﬀ. the disjoint shortest paths problem. discrete applied mathematics,\\n85(2):113–138, 1998.\\n\\n[fgr10]\\n\\nemmanuel filiot, tristan le gall, and jean-françois raskin. iterated regret minimization\\nin game graphs. in mfcs, volume 6281 of lncs, pages 342–354. springer, 2010.\\n\\n[gj79]\\n\\nmichael r. garey and david s. johnson. computers and intractability: a guide to the\\ntheory of np-completeness. w. h. freeman and company, 1979.\\n\\n[hp06]\\n\\nthomas a. henzinger and nir piterman. solving games without determinization. in csl,\\npages 395–410, 2006.\\n\\n[hp12]\\n\\njoseph y. halpern and rafael pass. iterated regret minimization: a new solution concept.\\ngames and economic behavior, 74(1):184–207, 2012.\\n\\n[hpr14]\\n\\npaul hunter, guillermo a. pérez, and jean-françois raskin. mean-payoﬀ games with\\npartial-observation - (extended abstract). in reachability problems, pages 163–175, 2014.\\n\\n[jsl08]\\n\\nmarcin jurdzinski, jeremy sproston, and françois laroussinie. model checking probabilistic\\ntimed automata with one or two clocks. lmcs, 4(3), 2008.\\n\\n[jur98]\\n\\nmarcin jurdziński. deciding the winner in parity games is in up∩coup. ipl, 68(3):119–124,\\n1998.\\n\\n[pit07]\\n\\nnir piterman. from nondeterministic büchi and streett automata to deterministic parity\\nautomata. lmcs, 3(3), 2007.\\n\\n[pp06]\\n\\nnir piterman and amir pnueli. faster solutions of rabin and streett games. in lics, pages\\n275–284, 2006.\\n\\n[pr89]\\n\\na. pnueli and r. rosner. on the synthesis of a reactive module. in popl, pages 179–190.\\nacm, acm press, 1989.\\n\\n[py91]\\n\\nchristos h. papadimitriou and mihalis yannakakis. shortest paths without a map. tcs,\\n84(1):127–150, 1991.\\n\\n[wet15]\\n\\nmin wen, rüdiger ehlers, and ufuk topcu. correct-by-synthesis reinforcement learning\\nwith temporal logic constraints. in iros, pages 4983–4990. ieee, 2015.\\n\\n[zjbp08]\\n\\nmartin zinkevich, michael johanson, michael bowling, and carmelo piccione. regret minimization in games with incomplete information. in nips, pages 905–912, 2008.\\n\\n[zp96]\\n\\nuri zwick and mike paterson. the complexity of mean payoﬀ games on graphs. tcs,\\n158(1):343–359, 1996.\\n\\n31\\n\\n\\x0c'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cmejia3/datasets/papers-txt/1504.01708.txt')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metapy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(conda)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
