{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cmejia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cmejia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cmejia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\cmejia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(['punkt','stopwords','wordnet','words'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import *\n",
    "from nltk.corpus import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_file_path es la ruta donde se encuentran los archivos de texto plano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path =  Path(\"~\").expanduser().resolve()\n",
    "#base_path = Path.cwd().expanduser().resolve()\n",
    "#input_file_path  = base_path / 'datasets/papers-txt/'\n",
    "input_file_path = Path('E:/OneDrive - CELSIA S.A E.S.P/Maestría/Almacenamiento/Tika/Tika/limpiados')\n",
    "#datasetOut =base_path / \"datasets/salidas_procesamiento/\"\n",
    "#datasetOut_freq = base_path / \"datasets/salidas_freq/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('E:/OneDrive - CELSIA S.A E.S.P/Maestría/Almacenamiento/Tika/Tika/limpiados')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileSize(fileIn):\n",
    "    size=os.stat(fileIn).st_size\n",
    "    return size\n",
    "\n",
    "def wordsCount(contenido):\n",
    "    totWwords=contenido.split()\n",
    "    return len(totWwords)\n",
    "\n",
    "def cleanWordCount(contenido):\n",
    "    contenido =re.sub('((f|ht)tp(s?)://)?(.*)[.][a-z]+(((\\/.*(\\/?)){1,}?)(.*([.].*)?))?',' ',contenido) # Eliminar las URL\n",
    "    #contenido =re.sub('REFERENCES (\\S|\\w)+',' ',contenido) # Eliminar la bibliografia\n",
    "    contenido =re.sub('[a-zA-Z0-9.?{}]+@\\w+\\.\\w+.\\w*',' ',contenido) # Eliminar los correos\n",
    "    contenido =re.sub('\\[[a-zA-Z0-9\\,\\. ]+\\]',' ',contenido) # Eliminar cualquier contenido entre corchetes\n",
    "    contenido =re.sub('\\([a-zA-Z0-9\\,\\.\\- ]+\\)',' ',contenido) # Eliminar cualquier contenido entre paréntesis\n",
    "    contenido =re.sub('((et al\\.)|(i\\.i\\.d\\.)|(i\\.e\\.)|\\'|\\’|\\`)',' ',contenido) # Eliminar abreviaciones, apostrofes y guion\n",
    "    contenido =re.sub('(á|à|ä)','a',contenido) # Reemplazar a acentuada\n",
    "    contenido =re.sub('(é|è|ë)','e',contenido) # Reemplazar e acentuada\n",
    "    contenido =re.sub('(í|ì|ï)','i',contenido) # Reemplazar i acentuada\n",
    "    contenido =re.sub('(ó|ò|ö)','o',contenido) # Reemplazar o acentuada\n",
    "    contenido =re.sub('(ú|ù|ü)','u',contenido) # Reemplazar u acentuada\n",
    "    contenido =re.sub('-|\\u2212|\\u2012|\\u2013|\\u2014|\\u2015',' ',contenido) # Reemplazar el guión\n",
    "    contenido =re.sub('[^a-zA-Z]',' ',contenido) # Eliminar caracteres que no sean: letra, número o vocales acentuadas\n",
    "    contenido =re.sub('(a-z|A-Z){1,2}',' ',contenido) # Eliminar palabras o números de un caracter de longitud \n",
    "    contenido =re.sub('((\\w*)?xyz(\\w*)?)',' ',contenido) # Eliminar palabras que tiene la cadena xyz \n",
    "    contenido =re.sub('((\\w*)?abc(\\w*)?)',' ',contenido) # Eliminar palabras que tiene la cadena abc\n",
    "    contenido =re.sub(' +',' ',contenido) # Eliminar espacios en blanco\n",
    "    contenidoDuplicado =re.findall(r'(\\w*?(\\w)\\2{2,}.*?)',contenido) #Elminar carecteres repetidos\n",
    "    i=0\n",
    "    longDuplicados=len(contenidoDuplicado)\n",
    "    while i <longDuplicados:\n",
    "        pattern= \"\\w*\" + contenidoDuplicado [i][0] +\"\\w*\"\n",
    "        contenido=re.sub(pattern,' ',contenido) # Eliminar las URL\n",
    "        i=i+1\n",
    "    #contenido =re.sub('[^A-Za-z0-9.,_%+-\\(\\)\\[\\]\\´\\'\\`]',' ',contenido)\n",
    "    #contenido =re.sub('\\[(0-9)+\\]',' ',contenido)    \n",
    "    #totWordDepurado = Counter(map(str, contenido.split()))\n",
    "    #outputFile= open(datasetOut, 'w', encoding='UTF-8')\n",
    "    #outputFile.write(contenido)\n",
    "    #outputFile.close()\n",
    "    totWwords=contenido.split()\n",
    "    setWords = set(totWwords)\n",
    "    #print(\"Total de palabras {}\".format(len(totWwords)))\n",
    "    #print(\"Total de palabras después del pre-procesamiento: {}\".format(totWordDepurado))\n",
    "    return len(totWwords),contenido,setWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_ratio(input):\n",
    "    lang_ratio = {}\n",
    "    tokens = wordpunct_tokenize(input)\n",
    "    words = [word.lower() for word in tokens]\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        word_set = set(words)\n",
    "        common_elements = word_set.intersection(stopwords_set)\n",
    "        lang_ratio[language] = len(common_elements)\n",
    "    return lang_ratio\n",
    "\n",
    "def detect_language(input):\n",
    "    ratios = lang_ratio(input)\n",
    "    language = max(ratios, key = ratios.get)\n",
    "    return language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de las stop words que serán eliminadas ya que no aportan valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palabras que considermos StopWords que no estan incluidas en el conjunto descargado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "listoStopWords = ['www','https','html','figure', 'chapter','abbcbccabcabcabcabcbcbabacba','abcdefghijklmnopqrstuvwxyz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se añaden las palabras que consideramos al conjunto principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords.extend(listoStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictonary = nltk.corpus.words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función principal procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_files(texto,stopWords):\n",
    "    \n",
    "    #Quitar todos los acentos\n",
    "    #texto = unidecode.unidecode(texto)\n",
    "    \n",
    "    #Quitar todos los caracteres especiales\n",
    "    texto = re.sub('[^A-Za-z0-9]+',' ',texto)\n",
    "    \n",
    "    #Pasar todo a minisculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    #Tokenizar\n",
    "    tokens = texto.split()\n",
    "    \n",
    "    tokens2 = [w for w in tokens if (len(w)>1)&(w.isalpha())&(w not in stopWords)]\n",
    "    \n",
    "    #Lematización\n",
    "    word_net_lemmatizar = WordNetLemmatizer()\n",
    "\n",
    "    tokens3 = [word_net_lemmatizar.lemmatize(w, pos = \"v\") for w in tokens2]\n",
    "   \n",
    "    #Stemmer\n",
    "    ps = PorterStemmer() \n",
    "    tokens4 = [ps.stem(w) for w in tokens3]\n",
    "     \n",
    "    #Se retorna el texto nuevamente en un solo string luego de ser procesado\n",
    "    to_return = ' '.join(tokens4)\n",
    "    \n",
    "    #Se retorna el vocabulario de cada documento\n",
    "    set_words = set(tokens4)\n",
    "    \n",
    "    #Y la frecuencia de las palabras\n",
    "    freq = nltk.FreqDist(tokens4)\n",
    "    return to_return,set_words,freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialización de los conjuntos:\n",
    "\n",
    "    - Vocabulary: el conjunto de todas las palabras que contienen los documentos\n",
    "    - results_text: la lista con los documentos ya organizados para construir el bag of words\n",
    "    - results_frecuency: información de cada documento de las palabras que contiene cuántas veces las contiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "results_text = []\n",
    "results_frecuency = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german:E:\\OneDrive - CELSIA S.A E.S.P\\Maestría\\Almacenamiento\\Tika\\Tika\\limpiados\\1508.02340.txt\n"
     ]
    }
   ],
   "source": [
    "fileSummary = \"CleanSummary.csv\"\n",
    "\n",
    "contenido= \"Archivo\" + \";\" + \"Tamaño(K)\" + \";\" + \"Cant Palabras Inicial\" + \";\" + \"Cant Palabras depuradas\"+ \";\" +\"Porc Limpieza\"+ \"\\n\"\n",
    "resultado = pd.DataFrame()\n",
    "indexFiles = []\n",
    "documents = []\n",
    "\n",
    "for f in input_file_path.glob('*.txt'):\n",
    "    #Peso del archivo\n",
    "    tmpSize=round(fileSize(f)/1014)\n",
    "    \n",
    "    #Lectura del archivo\n",
    "    input_file = open(f, \"r\", encoding = 'utf-8')\n",
    "    input_aux = input_file.read()\n",
    "    \n",
    "    #Cuenta de palabras iniciales\n",
    "    tmpWordsOri=wordsCount(input_aux)\n",
    "    #out = datasetOut / str(f).split('/')[-1]\n",
    "    \n",
    "    #Cuenta de palabras despues de la limpieza\n",
    "    tmpWordsEnd,text,setWord=cleanWordCount(input_aux)\n",
    "    #tmpPerClean=round((tmpWordsEnd/tmpWordsOri)*100)\n",
    "    \n",
    "    \n",
    "    #Detectando el idioma\n",
    "    aux = detect_language(text)\n",
    "    \n",
    "    if(aux == 'english'):\n",
    "        text_cleanned,set_words,freq = clean_files(text,stopWords)\n",
    "        #out2 = datasetOut / str(f).split('/')[-1]\n",
    "        documents.append(text_cleanned)\n",
    "        vocabulary = vocabulary.union(set_words)\n",
    "        results_text.append(text_cleanned)\n",
    "        indexFiles.append(str(f).split('\\\\')[-1])\n",
    "        \n",
    "        #Escritura de los resultados del preprocesamiento\n",
    "        auxRes= pd.DataFrame({'Archivo': str(f).split('/')[-1], 'Tamaño(K)': [tmpSize], 'Cant Palabras Inicial': [tmpWordsOri],'Cant Palabras depuradas': [tmpWordsEnd],\"Vocabulario Inicial\":len(setWord),\"Vocabulario Final\":len(set_words)})\n",
    "        resultado = pd.concat([resultado, auxRes])\n",
    "        #break\n",
    "    else:\n",
    "        print(aux + ':' + str(f))\n",
    "\n",
    "resultado.to_csv(fileSummary, sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Archivo</th>\n",
       "      <th>Tamaño(K)</th>\n",
       "      <th>Cant Palabras Inicial</th>\n",
       "      <th>Cant Palabras depuradas</th>\n",
       "      <th>Vocabulario Inicial</th>\n",
       "      <th>Vocabulario Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:\\OneDrive - CELSIA S.A E.S.P\\Maestría\\Almace...</td>\n",
       "      <td>14</td>\n",
       "      <td>2765</td>\n",
       "      <td>2765</td>\n",
       "      <td>515</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:\\OneDrive - CELSIA S.A E.S.P\\Maestría\\Almace...</td>\n",
       "      <td>22</td>\n",
       "      <td>3694</td>\n",
       "      <td>3694</td>\n",
       "      <td>1036</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:\\OneDrive - CELSIA S.A E.S.P\\Maestría\\Almace...</td>\n",
       "      <td>30</td>\n",
       "      <td>5482</td>\n",
       "      <td>5482</td>\n",
       "      <td>1254</td>\n",
       "      <td>798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:\\OneDrive - CELSIA S.A E.S.P\\Maestría\\Almace...</td>\n",
       "      <td>94</td>\n",
       "      <td>18304</td>\n",
       "      <td>18304</td>\n",
       "      <td>1771</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:\\OneDrive - CELSIA S.A E.S.P\\Maestría\\Almace...</td>\n",
       "      <td>32</td>\n",
       "      <td>6227</td>\n",
       "      <td>6227</td>\n",
       "      <td>986</td>\n",
       "      <td>616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Archivo  Tamaño(K)  \\\n",
       "0  E:\\OneDrive - CELSIA S.A E.S.P\\Maestría\\Almace...         14   \n",
       "0  E:\\OneDrive - CELSIA S.A E.S.P\\Maestría\\Almace...         22   \n",
       "0  E:\\OneDrive - CELSIA S.A E.S.P\\Maestría\\Almace...         30   \n",
       "0  E:\\OneDrive - CELSIA S.A E.S.P\\Maestría\\Almace...         94   \n",
       "0  E:\\OneDrive - CELSIA S.A E.S.P\\Maestría\\Almace...         32   \n",
       "\n",
       "   Cant Palabras Inicial  Cant Palabras depuradas  Vocabulario Inicial  \\\n",
       "0                   2765                     2765                  515   \n",
       "0                   3694                     3694                 1036   \n",
       "0                   5482                     5482                 1254   \n",
       "0                  18304                    18304                 1771   \n",
       "0                   6227                     6227                  986   \n",
       "\n",
       "   Vocabulario Final  \n",
       "0                326  \n",
       "0                633  \n",
       "0                798  \n",
       "0               1057  \n",
       "0                616  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49881"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import vstack,save_npz,load_npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construido el vocabulario podemos construir el bag of words, que se hace con la ayuda de la funcion CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",vocabulary =vocabulary , tokenizer = None, preprocessor = None, stop_words = 'english', max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49881"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 0,\n",
       " 'aaa': 1,\n",
       " 'aab': 2,\n",
       " 'aabab': 3,\n",
       " 'aababa': 4,\n",
       " 'aababaababb': 5,\n",
       " 'aabak': 6,\n",
       " 'aabb': 7,\n",
       " 'aachen': 8,\n",
       " 'aaecc': 9,\n",
       " 'aaex': 10,\n",
       " 'aagea': 11,\n",
       " 'aai': 12,\n",
       " 'aakv': 13,\n",
       " 'aalborg': 14,\n",
       " 'aall': 15,\n",
       " 'aalto': 16,\n",
       " 'aam': 17,\n",
       " 'aama': 18,\n",
       " 'aan': 19,\n",
       " 'aand': 20,\n",
       " 'aarabi': 21,\n",
       " 'aaron': 22,\n",
       " 'aaronson': 23,\n",
       " 'aarti': 24,\n",
       " 'aat': 25,\n",
       " 'aatu': 26,\n",
       " 'aatuut': 27,\n",
       " 'aau': 28,\n",
       " 'aaw': 29,\n",
       " 'aawcondit': 30,\n",
       " 'aax': 31,\n",
       " 'aazhang': 32,\n",
       " 'ab': 33,\n",
       " 'aba': 34,\n",
       " 'abab': 35,\n",
       " 'ababa': 36,\n",
       " 'ababac': 37,\n",
       " 'ababb': 38,\n",
       " 'abac': 39,\n",
       " 'abacaxi': 40,\n",
       " 'abad': 41,\n",
       " 'abadi': 42,\n",
       " 'abak': 43,\n",
       " 'abalon': 44,\n",
       " 'aban': 45,\n",
       " 'abandon': 46,\n",
       " 'abaqu': 47,\n",
       " 'abasekmt': 48,\n",
       " 'abb': 49,\n",
       " 'abba': 50,\n",
       " 'abbaa': 51,\n",
       " 'abbaac': 52,\n",
       " 'abbasi': 53,\n",
       " 'abbeel': 54,\n",
       " 'abboud': 55,\n",
       " 'abbr': 56,\n",
       " 'abbrevi': 57,\n",
       " 'abbreviatetb': 58,\n",
       " 'abc': 59,\n",
       " 'abcab': 60,\n",
       " 'abcd': 61,\n",
       " 'abd': 62,\n",
       " 'abdel': 63,\n",
       " 'abdelgawad': 64,\n",
       " 'abdelhak': 65,\n",
       " 'abdoulay': 66,\n",
       " 'abduct': 67,\n",
       " 'abdul': 68,\n",
       " 'abdullah': 69,\n",
       " 'abdur': 70,\n",
       " 'abe': 71,\n",
       " 'abel': 72,\n",
       " 'abelian': 73,\n",
       " 'abelson': 74,\n",
       " 'abep': 75,\n",
       " 'aber': 76,\n",
       " 'abernethi': 77,\n",
       " 'aberth': 78,\n",
       " 'aberystwyth': 79,\n",
       " 'abeyesingh': 80,\n",
       " 'abgrp': 81,\n",
       " 'abhay': 82,\n",
       " 'abhinav': 83,\n",
       " 'abhiruk': 84,\n",
       " 'abhishek': 85,\n",
       " 'abi': 86,\n",
       " 'abid': 87,\n",
       " 'abigail': 88,\n",
       " 'abil': 89,\n",
       " 'abilist': 90,\n",
       " 'abishanka': 91,\n",
       " 'abiteboul': 92,\n",
       " 'abj': 93,\n",
       " 'abk': 94,\n",
       " 'abka': 95,\n",
       " 'abkbk': 96,\n",
       " 'abl': 97,\n",
       " 'ablat': 98,\n",
       " 'ablayev': 99,\n",
       " 'abli': 100,\n",
       " 'abm': 101,\n",
       " 'abmash': 102,\n",
       " 'abn': 103,\n",
       " 'abnor': 104,\n",
       " 'abnorm': 105,\n",
       " 'abolish': 106,\n",
       " 'aboost': 107,\n",
       " 'abort': 108,\n",
       " 'abortship': 109,\n",
       " 'abound': 110,\n",
       " 'aboutdcomp': 111,\n",
       " 'aboutt': 112,\n",
       " 'abovec': 113,\n",
       " 'abovem': 114,\n",
       " 'abovement': 115,\n",
       " 'abox': 116,\n",
       " 'abp': 117,\n",
       " 'abpq': 118,\n",
       " 'abq': 119,\n",
       " 'abqp': 120,\n",
       " 'abr': 121,\n",
       " 'abraham': 122,\n",
       " 'abramski': 123,\n",
       " 'abramson': 124,\n",
       " 'abrego': 125,\n",
       " 'abreu': 126,\n",
       " 'abridg': 127,\n",
       " 'abroad': 128,\n",
       " 'abrupt': 129,\n",
       " 'abruptli': 130,\n",
       " 'abrusci': 131,\n",
       " 'abruzzi': 132,\n",
       " 'abrxbkx': 133,\n",
       " 'absb': 134,\n",
       " 'abscissa': 135,\n",
       " 'absenc': 136,\n",
       " 'absent': 137,\n",
       " 'abso': 138,\n",
       " 'absolut': 139,\n",
       " 'absorb': 140,\n",
       " 'absorpt': 141,\n",
       " 'abstent': 142,\n",
       " 'abstrac': 143,\n",
       " 'abstract': 144,\n",
       " 'abstractiong': 145,\n",
       " 'abstractli': 146,\n",
       " 'abstractlinkedlist': 147,\n",
       " 'abstractorderedmapdecor': 148,\n",
       " 'abstractst': 149,\n",
       " 'abstrus': 150,\n",
       " 'absurd': 151,\n",
       " 'absurdo': 152,\n",
       " 'absurdum': 153,\n",
       " 'abt': 154,\n",
       " 'abtahi': 155,\n",
       " 'abu': 156,\n",
       " 'abualrub': 157,\n",
       " 'abubakr': 158,\n",
       " 'abun': 159,\n",
       " 'abund': 160,\n",
       " 'abundanceecosystem': 161,\n",
       " 'abundanceproject': 162,\n",
       " 'abundantli': 163,\n",
       " 'abuot': 164,\n",
       " 'abus': 165,\n",
       " 'abut': 166,\n",
       " 'abuz': 167,\n",
       " 'abv': 168,\n",
       " 'abw': 169,\n",
       " 'abx': 170,\n",
       " 'abyk': 171,\n",
       " 'abyn': 172,\n",
       " 'abyp': 173,\n",
       " 'ac': 174,\n",
       " 'aca': 175,\n",
       " 'acad': 176,\n",
       " 'academ': 177,\n",
       " 'academi': 178,\n",
       " 'academia': 179,\n",
       " 'acarchau': 180,\n",
       " 'acas': 181,\n",
       " 'acaus': 182,\n",
       " 'acb': 183,\n",
       " 'acbc': 184,\n",
       " 'acbct': 185,\n",
       " 'acbd': 186,\n",
       " 'acbt': 187,\n",
       " 'acc': 188,\n",
       " 'accardi': 189,\n",
       " 'accd': 190,\n",
       " 'accdhyp': 191,\n",
       " 'accel': 192,\n",
       " 'acceler': 193,\n",
       " 'accelera': 194,\n",
       " 'acceleromet': 195,\n",
       " 'accent': 196,\n",
       " 'accentu': 197,\n",
       " 'accentur': 198,\n",
       " 'accep': 199,\n",
       " 'accept': 200,\n",
       " 'acceptor': 201,\n",
       " 'acces': 202,\n",
       " 'access': 203,\n",
       " 'accessi': 204,\n",
       " 'accessiblilti': 205,\n",
       " 'accessor': 206,\n",
       " 'accessori': 207,\n",
       " 'accg': 208,\n",
       " 'acchyp': 209,\n",
       " 'accid': 210,\n",
       " 'accident': 211,\n",
       " 'accl': 212,\n",
       " 'acclaim': 213,\n",
       " 'accn': 214,\n",
       " 'accom': 215,\n",
       " 'accommod': 216,\n",
       " 'accommoda': 217,\n",
       " 'accomod': 218,\n",
       " 'accompani': 219,\n",
       " 'accomplic': 220,\n",
       " 'accomplish': 221,\n",
       " 'accord': 222,\n",
       " 'accordingli': 223,\n",
       " 'account': 224,\n",
       " 'accountmanag': 225,\n",
       " 'accountmanagerservic': 226,\n",
       " 'accrat': 227,\n",
       " 'accratef': 228,\n",
       " 'accratefhyp': 229,\n",
       " 'accrateg': 230,\n",
       " 'accrateghyp': 231,\n",
       " 'accratemhyp': 232,\n",
       " 'accrel': 233,\n",
       " 'accru': 234,\n",
       " 'acct': 235,\n",
       " 'accu': 236,\n",
       " 'accumu': 237,\n",
       " 'accumul': 238,\n",
       " 'accumula': 239,\n",
       " 'accumulat': 240,\n",
       " 'accur': 241,\n",
       " 'accuraci': 242,\n",
       " 'accus': 243,\n",
       " 'accx': 244,\n",
       " 'acd': 245,\n",
       " 'acdb': 246,\n",
       " 'ace': 247,\n",
       " 'acemog': 248,\n",
       " 'acemoglu': 249,\n",
       " 'acentr': 250,\n",
       " 'acerbi': 251,\n",
       " 'acerca': 252,\n",
       " 'aceto': 253,\n",
       " 'acf': 254,\n",
       " 'acfor': 255,\n",
       " 'acg': 256,\n",
       " 'ach': 257,\n",
       " 'achanta': 258,\n",
       " 'acharya': 259,\n",
       " 'achiev': 260,\n",
       " 'achievabil': 261,\n",
       " 'achievabiliy': 262,\n",
       " 'achievabl': 263,\n",
       " 'achievebl': 264,\n",
       " 'achieveso': 265,\n",
       " 'achillea': 266,\n",
       " 'achilleo': 267,\n",
       " 'achiv': 268,\n",
       " 'achliopta': 269,\n",
       " 'achterberg': 270,\n",
       " 'achtergracht': 271,\n",
       " 'aci': 272,\n",
       " 'acial': 273,\n",
       " 'acid': 274,\n",
       " 'acifar': 275,\n",
       " 'acit': 276,\n",
       " 'acizl': 277,\n",
       " 'ack': 278,\n",
       " 'ackerman': 279,\n",
       " 'ackermann': 280,\n",
       " 'acki': 281,\n",
       " 'ackley': 282,\n",
       " 'acklm': 283,\n",
       " 'acknowl': 284,\n",
       " 'acknowledg': 285,\n",
       " 'acknowlegd': 286,\n",
       " 'ackowledg': 287,\n",
       " 'acl': 288,\n",
       " 'aclf': 289,\n",
       " 'aclong': 290,\n",
       " 'acm': 291,\n",
       " 'acmmust': 292,\n",
       " 'acn': 293,\n",
       " 'aco': 294,\n",
       " 'acolor': 295,\n",
       " 'acompi': 296,\n",
       " 'acomput': 297,\n",
       " 'aconst': 298,\n",
       " 'acontrol': 299,\n",
       " 'acopf': 300,\n",
       " 'acou': 301,\n",
       " 'acount': 302,\n",
       " 'acoust': 303,\n",
       " 'acp': 304,\n",
       " 'acpa': 305,\n",
       " 'acpj': 306,\n",
       " 'acqpa': 307,\n",
       " 'acquaint': 308,\n",
       " 'acqui': 309,\n",
       " 'acquir': 310,\n",
       " 'acquisi': 311,\n",
       " 'acquisit': 312,\n",
       " 'acquisti': 313,\n",
       " 'acrf': 314,\n",
       " 'acronym': 315,\n",
       " 'across': 316,\n",
       " 'acsysimp': 317,\n",
       " 'act': 318,\n",
       " 'acta': 319,\n",
       " 'acter': 320,\n",
       " 'acteris': 321,\n",
       " 'acterist': 322,\n",
       " 'acti': 323,\n",
       " 'actin': 324,\n",
       " 'action': 325,\n",
       " 'activ': 326,\n",
       " 'activa': 327,\n",
       " 'activationdesc': 328,\n",
       " 'activationgroup': 329,\n",
       " 'activationsystem': 330,\n",
       " 'activebodi': 331,\n",
       " 'activi': 332,\n",
       " 'activist': 333,\n",
       " 'activit': 334,\n",
       " 'activityaspect': 335,\n",
       " 'activityedg': 336,\n",
       " 'activityedgeaspect': 337,\n",
       " 'activityedgeimpl': 338,\n",
       " 'activityimpl': 339,\n",
       " 'activitynet': 340,\n",
       " 'activitynod': 341,\n",
       " 'activitynodeactivationgroup': 342,\n",
       " 'activitynodeaspect': 343,\n",
       " 'activitynodeimpl': 344,\n",
       " 'activityparameternod': 345,\n",
       " 'activityu': 346,\n",
       " 'actli': 347,\n",
       " 'acton': 348,\n",
       " 'actonto': 349,\n",
       " 'actor': 350,\n",
       " 'actorfoundri': 351,\n",
       " 'actpbq': 352,\n",
       " 'actpq': 353,\n",
       " 'actpqq': 354,\n",
       " 'actq': 355,\n",
       " 'actscr': 356,\n",
       " 'actsh': 357,\n",
       " 'actshr': 358,\n",
       " 'actsi': 359,\n",
       " 'actslo': 360,\n",
       " 'actt': 361,\n",
       " 'actu': 362,\n",
       " 'actual': 363,\n",
       " 'actualis': 364,\n",
       " 'actualment': 365,\n",
       " 'actualtypeargu': 366,\n",
       " 'actuari': 367,\n",
       " 'actuat': 368,\n",
       " 'acu': 369,\n",
       " 'acut': 370,\n",
       " 'acvt': 371,\n",
       " 'acx': 372,\n",
       " 'acyc': 373,\n",
       " 'acycl': 374,\n",
       " 'acyclicli': 375,\n",
       " 'acyl': 376,\n",
       " 'aczel': 377,\n",
       " 'ad': 378,\n",
       " 'ada': 379,\n",
       " 'adaboost': 380,\n",
       " 'adachi': 381,\n",
       " 'adadelta': 382,\n",
       " 'adadi': 383,\n",
       " 'adag': 384,\n",
       " 'adalf': 385,\n",
       " 'adam': 386,\n",
       " 'adamczak': 387,\n",
       " 'adami': 388,\n",
       " 'adap': 389,\n",
       " 'adapt': 390,\n",
       " 'adapta': 391,\n",
       " 'adar': 392,\n",
       " 'adaricheva': 393,\n",
       " 'adat': 394,\n",
       " 'adata': 395,\n",
       " 'adavid': 396,\n",
       " 'adc': 397,\n",
       " 'adcd': 398,\n",
       " 'adcock': 399,\n",
       " 'add': 400,\n",
       " 'addad': 401,\n",
       " 'addaiu': 402,\n",
       " 'addcandrul': 403,\n",
       " 'addclocklisten': 404,\n",
       " 'addend': 405,\n",
       " 'addendum': 406,\n",
       " 'adder': 407,\n",
       " 'addflip': 408,\n",
       " 'addi': 409,\n",
       " 'addict': 410,\n",
       " 'addison': 411,\n",
       " 'addit': 412,\n",
       " 'additionalnod': 413,\n",
       " 'addiu': 414,\n",
       " 'addn': 415,\n",
       " 'addnod': 416,\n",
       " 'addon': 417,\n",
       " 'addr': 418,\n",
       " 'addrandomappl': 419,\n",
       " 'address': 420,\n",
       " 'addresse': 421,\n",
       " 'addscor': 422,\n",
       " 'addtion': 423,\n",
       " 'addu': 424,\n",
       " 'adduc': 425,\n",
       " 'ade': 426,\n",
       " 'adecod': 427,\n",
       " 'adel': 428,\n",
       " 'adelaid': 429,\n",
       " 'adelgren': 430,\n",
       " 'adelmann': 431,\n",
       " 'adepart': 432,\n",
       " 'adequ': 433,\n",
       " 'adequaci': 434,\n",
       " 'ader': 435,\n",
       " 'adesnik': 436,\n",
       " 'adf': 437,\n",
       " 'adfsdca': 438,\n",
       " 'adg': 439,\n",
       " 'adga': 440,\n",
       " 'adher': 441,\n",
       " 'adhes': 442,\n",
       " 'adi': 443,\n",
       " 'adiabat': 444,\n",
       " 'adiac': 445,\n",
       " 'adiag': 446,\n",
       " 'adibi': 447,\n",
       " 'adic': 448,\n",
       " 'adifor': 449,\n",
       " 'adimat': 450,\n",
       " 'adipartimento': 451,\n",
       " 'adistm': 452,\n",
       " 'aditya': 453,\n",
       " 'adj': 454,\n",
       " 'adja': 455,\n",
       " 'adjac': 456,\n",
       " 'adjacen': 457,\n",
       " 'adjacent': 458,\n",
       " 'adject': 459,\n",
       " 'adjnoun': 460,\n",
       " 'adjoin': 461,\n",
       " 'adjoint': 462,\n",
       " 'adjud': 463,\n",
       " 'adjug': 464,\n",
       " 'adjunct': 465,\n",
       " 'adjust': 466,\n",
       " 'adl': 467,\n",
       " 'adlakha': 468,\n",
       " 'adleman': 469,\n",
       " 'adler': 470,\n",
       " 'admetox': 471,\n",
       " 'admi': 472,\n",
       " 'admin': 473,\n",
       " 'admini': 474,\n",
       " 'administ': 475,\n",
       " 'administr': 476,\n",
       " 'admir': 477,\n",
       " 'admira': 478,\n",
       " 'admiss': 479,\n",
       " 'admissi': 480,\n",
       " 'admissibil': 481,\n",
       " 'admistr': 482,\n",
       " 'admit': 483,\n",
       " 'admitt': 484,\n",
       " 'admittedli': 485,\n",
       " 'admm': 486,\n",
       " 'adn': 487,\n",
       " 'adnhztyclxk': 488,\n",
       " 'ado': 489,\n",
       " 'adob': 490,\n",
       " 'adol': 491,\n",
       " 'adolesc': 492,\n",
       " 'adolfo': 493,\n",
       " 'adom': 494,\n",
       " 'adop': 495,\n",
       " 'adopt': 496,\n",
       " 'adorn': 497,\n",
       " 'adpt': 498,\n",
       " 'adress': 499,\n",
       " 'adresses': 500,\n",
       " 'adria': 501,\n",
       " 'adrian': 502,\n",
       " 'adriana': 503,\n",
       " 'adriano': 504,\n",
       " 'adrien': 505,\n",
       " 'adsaf': 506,\n",
       " 'adsorpt': 507,\n",
       " 'adu': 508,\n",
       " 'adult': 509,\n",
       " 'adulthood': 510,\n",
       " 'adv': 511,\n",
       " 'advan': 512,\n",
       " 'advanc': 513,\n",
       " 'advancepoint': 514,\n",
       " 'advanta': 515,\n",
       " 'advantag': 516,\n",
       " 'advcdha': 517,\n",
       " 'advclassdummi': 518,\n",
       " 'advdummi': 519,\n",
       " 'advect': 520,\n",
       " 'advent': 521,\n",
       " 'adventur': 522,\n",
       " 'adver': 523,\n",
       " 'advers': 524,\n",
       " 'adversar': 525,\n",
       " 'adversari': 526,\n",
       " 'adversarywho': 527,\n",
       " 'advertis': 528,\n",
       " 'advi': 529,\n",
       " 'advic': 530,\n",
       " 'advis': 531,\n",
       " 'advisor': 532,\n",
       " 'advisori': 533,\n",
       " 'advo': 534,\n",
       " 'advoc': 535,\n",
       " 'advocaci': 536,\n",
       " 'adwait': 537,\n",
       " 'adword': 538,\n",
       " 'adx': 539,\n",
       " 'adxd': 540,\n",
       " 'adz': 541,\n",
       " 'ae': 542,\n",
       " 'aeb': 543,\n",
       " 'aecor': 544,\n",
       " 'aeefma': 545,\n",
       " 'aeeopt': 546,\n",
       " 'aef': 547,\n",
       " 'aegi': 548,\n",
       " 'aei': 549,\n",
       " 'aej': 550,\n",
       " 'aek': 551,\n",
       " 'aekd': 552,\n",
       " 'aem': 553,\n",
       " 'aen': 554,\n",
       " 'aenc': 555,\n",
       " 'aeolian': 556,\n",
       " 'aep': 557,\n",
       " 'aeq': 558,\n",
       " 'aeqxagfbnlulzhpzmpynip': 559,\n",
       " 'aerial': 560,\n",
       " 'aero': 561,\n",
       " 'aerodynam': 562,\n",
       " 'aeronaut': 563,\n",
       " 'aeroplan': 564,\n",
       " 'aerospac': 565,\n",
       " 'aesn': 566,\n",
       " 'aesop': 567,\n",
       " 'aesthet': 568,\n",
       " 'aeterna': 569,\n",
       " 'aex': 570,\n",
       " 'aexp': 571,\n",
       " 'aexppol': 572,\n",
       " 'aext': 573,\n",
       " 'af': 574,\n",
       " 'afa': 575,\n",
       " 'afaculti': 576,\n",
       " 'afag': 577,\n",
       " 'afar': 578,\n",
       " 'afc': 579,\n",
       " 'afellar': 580,\n",
       " 'afem': 581,\n",
       " 'afew': 582,\n",
       " 'aff': 583,\n",
       " 'affair': 584,\n",
       " 'affdim': 585,\n",
       " 'affdimp': 586,\n",
       " 'affec': 587,\n",
       " 'affect': 588,\n",
       " 'affection': 589,\n",
       " 'affil': 590,\n",
       " 'affili': 591,\n",
       " 'affin': 592,\n",
       " 'affir': 593,\n",
       " 'affirm': 594,\n",
       " 'affix': 595,\n",
       " 'afflict': 596,\n",
       " 'affm': 597,\n",
       " 'afford': 598,\n",
       " 'affr': 599,\n",
       " 'affymetrix': 600,\n",
       " 'afh': 601,\n",
       " 'afi': 602,\n",
       " 'afield': 603,\n",
       " 'afk': 604,\n",
       " 'aflw': 605,\n",
       " 'afm': 606,\n",
       " 'afmm': 607,\n",
       " 'afonso': 608,\n",
       " 'afor': 609,\n",
       " 'aforemen': 610,\n",
       " 'aforement': 611,\n",
       " 'aforesaid': 612,\n",
       " 'afosr': 613,\n",
       " 'afoul': 614,\n",
       " 'afout': 615,\n",
       " 'afp': 616,\n",
       " 'afpr': 617,\n",
       " 'afrabandpeyb': 618,\n",
       " 'afraid': 619,\n",
       " 'afrati': 620,\n",
       " 'afresh': 621,\n",
       " 'afriat': 622,\n",
       " 'africa': 623,\n",
       " 'african': 624,\n",
       " 'afrl': 625,\n",
       " 'afrom': 626,\n",
       " 'afshin': 627,\n",
       " 'afsr': 628,\n",
       " 'aft': 629,\n",
       " 'aftermath': 630,\n",
       " 'afternoon': 631,\n",
       " 'aftershock': 632,\n",
       " 'afterward': 633,\n",
       " 'afterwardsin': 634,\n",
       " 'afterword': 635,\n",
       " 'aftn': 636,\n",
       " 'afu': 637,\n",
       " 'afuk': 638,\n",
       " 'afw': 639,\n",
       " 'afz': 640,\n",
       " 'ag': 641,\n",
       " 'agafonov': 642,\n",
       " 'againmun': 643,\n",
       " 'agarawala': 644,\n",
       " 'agarw': 645,\n",
       " 'agat': 646,\n",
       " 'agatz': 647,\n",
       " 'agbgf': 648,\n",
       " 'agc': 649,\n",
       " 'agct': 650,\n",
       " 'agcta': 651,\n",
       " 'agda': 652,\n",
       " 'age': 653,\n",
       " 'ageefma': 654,\n",
       " 'ageeopt': 655,\n",
       " 'agement': 656,\n",
       " 'agenc': 657,\n",
       " 'agenda': 658,\n",
       " 'agent': 659,\n",
       " 'agentcontrol': 660,\n",
       " 'agenth': 661,\n",
       " 'agentkd': 662,\n",
       " 'ager': 663,\n",
       " 'agerad': 664,\n",
       " 'agfa': 665,\n",
       " 'agg': 666,\n",
       " 'aggarw': 667,\n",
       " 'agglom': 668,\n",
       " 'agglomer': 669,\n",
       " 'agglomera': 670,\n",
       " 'aggr': 671,\n",
       " 'aggrav': 672,\n",
       " 'aggreg': 673,\n",
       " 'aggrega': 674,\n",
       " 'aggress': 675,\n",
       " 'aggverifi': 676,\n",
       " 'agh': 677,\n",
       " 'agha': 678,\n",
       " 'aghabozorgi': 679,\n",
       " 'aghae': 680,\n",
       " 'agichtein': 681,\n",
       " 'agil': 682,\n",
       " 'agit': 683,\n",
       " 'aglio': 684,\n",
       " 'agm': 685,\n",
       " 'agn': 686,\n",
       " 'agneessen': 687,\n",
       " 'agner': 688,\n",
       " 'agnost': 689,\n",
       " 'ago': 690,\n",
       " 'agon': 691,\n",
       " 'agonist': 692,\n",
       " 'agr': 693,\n",
       " 'agram': 694,\n",
       " 'agraw': 695,\n",
       " 'agre': 696,\n",
       " 'agreement': 697,\n",
       " 'agribio': 698,\n",
       " 'agricultur': 699,\n",
       " 'agroparistech': 700,\n",
       " 'agrophi': 701,\n",
       " 'agrothendieckunivers': 702,\n",
       " 'agsactsi': 703,\n",
       " 'agsli': 704,\n",
       " 'agt': 705,\n",
       " 'agu': 706,\n",
       " 'aguirr': 707,\n",
       " 'agv': 708,\n",
       " 'agvm': 709,\n",
       " 'ah': 710,\n",
       " 'aha': 711,\n",
       " 'aharon': 712,\n",
       " 'aharoni': 713,\n",
       " 'aharonov': 714,\n",
       " 'ahav': 715,\n",
       " 'ahb': 716,\n",
       " 'ahc': 717,\n",
       " 'ahdki': 718,\n",
       " 'ahe': 719,\n",
       " 'ahead': 720,\n",
       " 'ahebrew': 721,\n",
       " 'ahel': 722,\n",
       " 'ahi': 723,\n",
       " 'ahigh': 724,\n",
       " 'ahk': 725,\n",
       " 'ahl': 726,\n",
       " 'ahlgren': 727,\n",
       " 'ahlswed': 728,\n",
       " 'ahm': 729,\n",
       " 'ahmad': 730,\n",
       " 'ahmadi': 731,\n",
       " 'ahn': 732,\n",
       " 'ahonen': 733,\n",
       " 'ahpi': 734,\n",
       " 'ahr': 735,\n",
       " 'ahrc': 736,\n",
       " 'ahrweil': 737,\n",
       " 'ahst': 738,\n",
       " 'ahv': 739,\n",
       " 'ahx': 740,\n",
       " 'ai': 741,\n",
       " 'aia': 742,\n",
       " 'aiaa': 743,\n",
       " 'aiai': 744,\n",
       " 'aiaj': 745,\n",
       " 'aiajbibj': 746,\n",
       " 'aiajii': 747,\n",
       " 'aiania': 748,\n",
       " 'aib': 749,\n",
       " 'aibi': 750,\n",
       " 'aibj': 751,\n",
       " 'aic': 752,\n",
       " 'aicc': 753,\n",
       " 'aich': 754,\n",
       " 'aichholz': 755,\n",
       " 'aichi': 756,\n",
       " 'aid': 757,\n",
       " 'aida': 758,\n",
       " 'aiden': 759,\n",
       " 'aidwyc': 760,\n",
       " 'aiee': 761,\n",
       " 'aiei': 762,\n",
       " 'aiello': 763,\n",
       " 'aig': 764,\n",
       " 'aigd': 765,\n",
       " 'aigi': 766,\n",
       " 'aigner': 767,\n",
       " 'aii': 768,\n",
       " 'aiitrk': 769,\n",
       " 'aij': 770,\n",
       " 'aijaik': 771,\n",
       " 'aijaikajk': 772,\n",
       " 'aijajk': 773,\n",
       " 'aijakj': 774,\n",
       " 'aijbij': 775,\n",
       " 'aijeyj': 776,\n",
       " 'aijf': 777,\n",
       " 'aijj': 778,\n",
       " 'aijk': 779,\n",
       " 'aijkj': 780,\n",
       " 'aijklxixjykyl': 781,\n",
       " 'aijn': 782,\n",
       " 'aijsj': 783,\n",
       " 'aijwjk': 784,\n",
       " 'aijx': 785,\n",
       " 'aijyj': 786,\n",
       " 'aik': 787,\n",
       " 'aikb': 788,\n",
       " 'aikj': 789,\n",
       " 'aikukj': 790,\n",
       " 'ail': 791,\n",
       " 'ailo': 792,\n",
       " 'ailon': 793,\n",
       " 'aim': 794,\n",
       " 'ain': 795,\n",
       " 'aingl': 796,\n",
       " 'aingworth': 797,\n",
       " 'ainstitut': 798,\n",
       " 'aintellig': 799,\n",
       " 'aiocj': 800,\n",
       " 'aip': 801,\n",
       " 'aipr': 802,\n",
       " 'aiqi': 803,\n",
       " 'air': 804,\n",
       " 'airavat': 805,\n",
       " 'airbnb': 806,\n",
       " 'airborn': 807,\n",
       " 'airbu': 808,\n",
       " 'aircondit': 809,\n",
       " 'aircraft': 810,\n",
       " 'airflow': 811,\n",
       " 'airfoil': 812,\n",
       " 'airhopp': 813,\n",
       " 'airjack': 814,\n",
       " 'airlin': 815,\n",
       " 'airoldi': 816,\n",
       " 'airplan': 817,\n",
       " 'airport': 818,\n",
       " 'airspac': 819,\n",
       " 'airtim': 820,\n",
       " 'aisb': 821,\n",
       " 'aisi': 822,\n",
       " 'aisl': 823,\n",
       " 'aisr': 824,\n",
       " 'aiss': 825,\n",
       " 'aistituto': 826,\n",
       " 'ait': 827,\n",
       " 'aiui': 828,\n",
       " 'aiv': 829,\n",
       " 'aivi': 830,\n",
       " 'aivsv': 831,\n",
       " 'aix': 832,\n",
       " 'aixi': 833,\n",
       " 'aixki': 834,\n",
       " 'aiyer': 835,\n",
       " 'aiz': 836,\n",
       " 'aizawa': 837,\n",
       " 'aizenman': 838,\n",
       " 'aizerman': 839,\n",
       " 'aizi': 840,\n",
       " 'aizingerb': 841,\n",
       " 'aj': 842,\n",
       " 'aja': 843,\n",
       " 'ajapan': 844,\n",
       " 'ajax': 845,\n",
       " 'ajbj': 846,\n",
       " 'ajdt': 847,\n",
       " 'aje': 848,\n",
       " 'ajg': 849,\n",
       " 'ajh': 850,\n",
       " 'aji': 851,\n",
       " 'ajii': 852,\n",
       " 'ajik': 853,\n",
       " 'ajj': 854,\n",
       " 'ajk': 855,\n",
       " 'ajki': 856,\n",
       " 'ajkj': 857,\n",
       " 'ajl': 858,\n",
       " 'ajlbjl': 859,\n",
       " 'ajm': 860,\n",
       " 'ajo': 861,\n",
       " 'ajoint': 862,\n",
       " 'ajp': 863,\n",
       " 'ajq': 864,\n",
       " 'ajqujp': 865,\n",
       " 'ajqujpt': 866,\n",
       " 'ajr': 867,\n",
       " 'ajt': 868,\n",
       " 'ajtai': 869,\n",
       " 'aju': 870,\n",
       " 'ajvj': 871,\n",
       " 'ajw': 872,\n",
       " 'ajx': 873,\n",
       " 'ajxj': 874,\n",
       " 'ajxkj': 875,\n",
       " 'ajyj': 876,\n",
       " 'ajz': 877,\n",
       " 'ak': 878,\n",
       " 'aka': 879,\n",
       " 'akaik': 880,\n",
       " 'akamai': 881,\n",
       " 'akamatsu': 882,\n",
       " 'akb': 883,\n",
       " 'akbar': 884,\n",
       " 'akbari': 885,\n",
       " 'akbarpour': 886,\n",
       " 'akd': 887,\n",
       " 'ake': 888,\n",
       " 'akg': 889,\n",
       " 'akh': 890,\n",
       " 'akhvai': 891,\n",
       " 'aki': 892,\n",
       " 'akiban': 893,\n",
       " 'akibkj': 894,\n",
       " 'akibkjak': 895,\n",
       " 'akidj': 896,\n",
       " 'akihisa': 897,\n",
       " 'akii': 898,\n",
       " 'akij': 899,\n",
       " 'akin': 900,\n",
       " 'akipi': 901,\n",
       " 'akitoshi': 902,\n",
       " 'akiva': 903,\n",
       " 'akix': 904,\n",
       " 'akixi': 905,\n",
       " 'akiyama': 906,\n",
       " 'akiyoshi': 907,\n",
       " 'akj': 908,\n",
       " 'akji': 909,\n",
       " 'akk': 910,\n",
       " 'akka': 911,\n",
       " 'akker': 912,\n",
       " 'akl': 913,\n",
       " 'aklbkl': 914,\n",
       " 'aklk': 915,\n",
       " 'akm': 916,\n",
       " 'akma': 917,\n",
       " 'akmak': 918,\n",
       " 'akmt': 919,\n",
       " 'akn': 920,\n",
       " 'ako': 921,\n",
       " 'akoglu': 922,\n",
       " 'akop': 923,\n",
       " 'akowski': 924,\n",
       " 'akpt': 925,\n",
       " 'akq': 926,\n",
       " 'akqiaki': 927,\n",
       " 'akr': 928,\n",
       " 'akram': 929,\n",
       " 'akrba': 930,\n",
       " 'akrita': 931,\n",
       " 'akselo': 932,\n",
       " 'akshay': 933,\n",
       " 'aksoylar': 934,\n",
       " 'aku': 935,\n",
       " 'akuk': 936,\n",
       " 'akum': 937,\n",
       " 'akutsu': 938,\n",
       " 'akv': 939,\n",
       " 'akvk': 940,\n",
       " 'akwk': 941,\n",
       " 'akxn': 942,\n",
       " 'akz': 943,\n",
       " 'al': 944,\n",
       " 'ala': 945,\n",
       " 'alaa': 946,\n",
       " 'alabama': 947,\n",
       " 'alaboratori': 948,\n",
       " 'alaei': 949,\n",
       " 'alain': 950,\n",
       " 'alaiz': 951,\n",
       " 'alali': 952,\n",
       " 'alamb': 953,\n",
       " 'alamito': 954,\n",
       " 'alamo': 955,\n",
       " 'alamouti': 956,\n",
       " 'alan': 957,\n",
       " 'alantha': 958,\n",
       " 'alaoglu': 959,\n",
       " 'alap': 960,\n",
       " 'alarm': 961,\n",
       " 'alarmist': 962,\n",
       " 'alart': 963,\n",
       " 'alasdair': 964,\n",
       " 'alashw': 965,\n",
       " 'alavijeh': 966,\n",
       " 'alb': 967,\n",
       " 'albadi': 968,\n",
       " 'albahpt': 969,\n",
       " 'albanes': 970,\n",
       " 'albano': 971,\n",
       " 'albanova': 972,\n",
       " 'albash': 973,\n",
       " 'albedo': 974,\n",
       " 'albeit': 975,\n",
       " 'alber': 976,\n",
       " 'alberelli': 977,\n",
       " 'albert': 978,\n",
       " 'alberta': 979,\n",
       " 'alberti': 980,\n",
       " 'alberto': 981,\n",
       " 'albrecht': 982,\n",
       " 'alc': 983,\n",
       " 'alcala': 984,\n",
       " 'alcoclan': 985,\n",
       " 'alcohol': 986,\n",
       " 'ald': 987,\n",
       " 'aldea': 988,\n",
       " 'aldecoa': 989,\n",
       " 'aldo': 990,\n",
       " 'aldou': 991,\n",
       " 'aldridg': 992,\n",
       " 'aldroubi': 993,\n",
       " 'ale': 994,\n",
       " 'aleco': 995,\n",
       " 'alegbra': 996,\n",
       " 'alegr': 997,\n",
       " 'alejandro': 998,\n",
       " 'alek': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = Path('E:/OneDrive - CELSIA S.A E.S.P/Maestría/Almacenamiento/Tika/Tika/limpiados')\n",
    "\n",
    "file1 = open(\"E:/OneDrive - CELSIA S.A E.S.P/Maestría/Almacenamiento/proyectointegrador1/data/cranfield/cranfield.dat\",\"w\", encoding=\"utf-8\")\n",
    "indexMeta = {}\n",
    "i = 0\n",
    "for file in txt_path.glob('*.txt'):\n",
    "    #Leer Informacion\n",
    "    auxkey = str(file).split('\\\\')[-1]\n",
    "    indexMeta[i] = auxkey\n",
    "    i = i+1\n",
    "    input_file = open(file,\"r\",encoding='utf-8')\n",
    "    texto = input_file.read().replace('\\n',' ')\n",
    "    texto = re.sub('[^A-Za-z0-9]+',' ',texto).lower()\n",
    "    file1.writelines(texto+'\\n')\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "estructuraFinal = {'vectorizer' : vectorizer, 'idexFiles' :indexFiles, 'matriz':train_data_features, 'metapyIndex':indexMeta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.txt', 'w') as f:\n",
    "    for item in vectorizer.get_feature_names():\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'estructuraDatos.sav'\n",
    "pickle.dump(estructuraFinal, open(filename, 'wb'))\n",
    "#loaded_model = pickle.load(open('BoW1.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8469"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total = 0\n",
    "for doc in documents:\n",
    "    Total = Total +len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27544.226762002043"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prom = Total/len(documents)\n",
    "Prom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
