{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de columnas es: 5\n",
      "Las candidatas a ser seleccionada para un procesamiento de texto serían: title, description y subject\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>subject</th>\n",
       "      <th>creator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/0704.3504</td>\n",
       "      <td>Smooth R\\'enyi Entropy of Ergodic Quantum Info...</td>\n",
       "      <td>We prove that the average smooth Renyi entro...</td>\n",
       "      <td>Quantum Physics ; Computer Science - Informati...</td>\n",
       "      <td>Schoenmakers, Berry ; Tjoelker, Jilles ; Tuyls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/0706.1402</td>\n",
       "      <td>Analyzing Design Process and Experiments on th...</td>\n",
       "      <td>In the field of tutoring systems, investigat...</td>\n",
       "      <td>Computer Science - Computers and Society ; Com...</td>\n",
       "      <td>Brust, Matthias R. ; Rothkugel, Steffen ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/0710.0736</td>\n",
       "      <td>Colour image segmentation by the vector-valued...</td>\n",
       "      <td>We propose a new method for the numerical so...</td>\n",
       "      <td>Computer Science - Computer Vision and Pattern...</td>\n",
       "      <td>Kay, David A ; Tomasi, Alessandro ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/0803.2570</td>\n",
       "      <td>Unequal Error Protection: An Information Theor...</td>\n",
       "      <td>An information theoretic framework for unequ...</td>\n",
       "      <td>Computer Science - Information Theory ; Comput...</td>\n",
       "      <td>Borade, Shashi ; Nakiboglu, Baris ; Zheng, Liz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/0808.0084</td>\n",
       "      <td>On the hitting times of quantum versus random ...</td>\n",
       "      <td>In this paper we define new Monte Carlo type...</td>\n",
       "      <td>Quantum Physics ; Computer Science - Data Stru...</td>\n",
       "      <td>Magniez, Frederic ; Nayak, Ashwin ; Richter, P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       identifier  \\\n",
       "0  http://arxiv.org/abs/0704.3504   \n",
       "1  http://arxiv.org/abs/0706.1402   \n",
       "2  http://arxiv.org/abs/0710.0736   \n",
       "3  http://arxiv.org/abs/0803.2570   \n",
       "4  http://arxiv.org/abs/0808.0084   \n",
       "\n",
       "                                               title  \\\n",
       "0  Smooth R\\'enyi Entropy of Ergodic Quantum Info...   \n",
       "1  Analyzing Design Process and Experiments on th...   \n",
       "2  Colour image segmentation by the vector-valued...   \n",
       "3  Unequal Error Protection: An Information Theor...   \n",
       "4  On the hitting times of quantum versus random ...   \n",
       "\n",
       "                                         description  \\\n",
       "0    We prove that the average smooth Renyi entro...   \n",
       "1    In the field of tutoring systems, investigat...   \n",
       "2    We propose a new method for the numerical so...   \n",
       "3    An information theoretic framework for unequ...   \n",
       "4    In this paper we define new Monte Carlo type...   \n",
       "\n",
       "                                             subject  \\\n",
       "0  Quantum Physics ; Computer Science - Informati...   \n",
       "1  Computer Science - Computers and Society ; Com...   \n",
       "2  Computer Science - Computer Vision and Pattern...   \n",
       "3  Computer Science - Information Theory ; Comput...   \n",
       "4  Quantum Physics ; Computer Science - Data Stru...   \n",
       "\n",
       "                                             creator  \n",
       "0  Schoenmakers, Berry ; Tjoelker, Jilles ; Tuyls...  \n",
       "1         Brust, Matthias R. ; Rothkugel, Steffen ;   \n",
       "2               Kay, David A ; Tomasi, Alessandro ;   \n",
       "3  Borade, Shashi ; Nakiboglu, Baris ; Zheng, Liz...  \n",
       "4  Magniez, Frederic ; Nayak, Ashwin ; Richter, P...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_csv('E:/OneDrive - CELSIA S.A E.S.P/Maestría/Almacenamiento/articles.csv',sep = ',', encoding= 'utf-8')\n",
    "print(f\"La cantidad de columnas es: {len(articles.columns)}\")\n",
    "print(f\"Las candidatas a ser seleccionada para un procesamiento de texto serían: title, description y subject\")\n",
    "articles.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['combine_column'] = articles['title']+articles['description']\n",
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_files(texto):\n",
    "    #Quitar todos los acentos\n",
    "    #texto = unidecode.unidecode(texto)\n",
    "    \n",
    "    #Quitar todos los caracteres especiales\n",
    "    texto = re.sub('[^A-Za-z0-9]+',' ',texto)\n",
    "    texto =re.sub('((et al\\.)|(i\\.i\\.d\\.)|(i\\.e\\.)|\\'|\\’|\\`)',' ',texto) # Eliminar abreviaciones, apostrofes y guion\n",
    "    texto =re.sub('(á|à|ä)','a',texto) # Reemplazar a acentuada\n",
    "    texto =re.sub('(é|è|ë)','e',texto) # Reemplazar e acentuada\n",
    "    texto =re.sub('(í|ì|ï)','i',texto) # Reemplazar i acentuada\n",
    "    texto =re.sub('(ó|ò|ö)','o',texto) # Reemplazar o acentuada\n",
    "    texto =re.sub('(ú|ù|ü)','u',texto) # Reemplazar u acentuada\n",
    "    texto =re.sub('-|\\u2212|\\u2012|\\u2013|\\u2014|\\u2015',' ',texto) # Reemplazar el guión\n",
    "    texto =re.sub('[^a-zA-Z]',' ',texto) # Eliminar caracteres que no sean: letra, número o vocales acentuadas\n",
    "    \n",
    "    #Pasar todo a minisculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    #Tokenizar\n",
    "    tokens = texto.split()\n",
    "    \n",
    "    tokens2 = [w for w in tokens if (len(w)>1)&(w.isalpha())&(w not in stopWords)]\n",
    "    \n",
    "    #Lematización\n",
    "    word_net_lemmatizar = WordNetLemmatizer()\n",
    "\n",
    "    tokens3 = [word_net_lemmatizar.lemmatize(w, pos = \"v\") for w in tokens2]\n",
    "   \n",
    "    #Stemmer\n",
    "#     ps = PorterStemmer() \n",
    "#     tokens4 = [ps.stem(w) for w in tokens3]\n",
    "     \n",
    "    #Se retorna el texto nuevamente en un solo string luego de ser procesado\n",
    "    to_return = ' '.join(tokens3)\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['combine_cleaned'] = articles['combine_column'].apply(clean_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['cuenta'] = articles['combine_cleaned'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_palabras = articles['cuenta'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Número de grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Es una matriz dispersa que contiene el TF-IDF\n",
    "\n",
    "#limite de palabras el número total de palabras en max_features y \n",
    "#el min_df el número minimo de documentos en los que debe estar para ser considerada en el BoW\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=total_palabras,\n",
    "                                 min_df=1, stop_words='english',\n",
    "                                 use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(articles['combine_cleaned'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980, 8081)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 75%\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=500 )\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "Xlsa = lsa.fit_transform(X)\n",
    "\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980, 500)\n"
     ]
    }
   ],
   "source": [
    "print(Xlsa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
      "    n_clusters=15, n_init=1, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=0)\n",
      "done in 0.127s\n",
      "Silhouette Coefficient: 0.010\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=1000, n_init=1)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(Xlsa)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(Xlsa, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: aapso abilities abbreviate abmash actors abac accessible acp adapt affine\n",
      "Cluster 1: aapso abbreviate aaronson abilities ability abelianized able academic abramsky abs\n",
      "Cluster 2: aapso activities abstract accord activate accomplish academic abstraction absent absoluteness\n",
      "Cluster 3: aaronson aapso abac abrego absolutely able absolute abelian abmash absoluteness\n",
      "Cluster 4: aapso abelianized absoluteness abridge abbreviate accentuate actually accelerators activities accessible\n",
      "Cluster 5: abboud aapso ability abmash abelian abrupt academic additive acp adjoin\n",
      "Cluster 6: aapso abac abboud abbreviate abramsky abs abelianized abmash absence ability\n",
      "Cluster 7: aapso acceleration abramsky accord abide acquire academics activity achievability act\n",
      "Cluster 8: aapso abelian abbreviate abs abstract accentuate acausal accommodate abstractions abramsky\n",
      "Cluster 9: aapso abide accept abundance absence abrupt ac access absolutely accordingly\n",
      "Cluster 10: aapso abilities abac able absent absence abelian acausal acid accumulate\n",
      "Cluster 11: aapso absolutely abelianized abu acceptable ability abac abridge accurate abrupt\n",
      "Cluster 12: aapso abide abboud absent absolute able abrego abilities account accomplish\n",
      "Cluster 13: aaronson abide aapso abramsky abac abelian ability abilities abelianized abboud\n",
      "Cluster 14: aapso academics abelian ac abstraction abu abelianized accelerate abrupt accurately\n"
     ]
    }
   ],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['cluster'] = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['name_file'] = articles['identifier'].apply(lambda x: x.split('/')[-1]+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_csv('docByCluster.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
