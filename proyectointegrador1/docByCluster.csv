identifier,title,description,subject,creator,combine_column,combine_cleaned,cuenta,cluster,name_file
http://arxiv.org/abs/0704.3504,Smooth R\'enyi Entropy of Ergodic Quantum Information Sources,"  We prove that the average smooth Renyi entropy rate will approach the entropy rate of a stationary, ergodic information source, which is equal to the Shannon entropy rate for a classical information source and the von Neumann entropy rate for a quantum information source. ",Quantum Physics ; Computer Science - Information Theory ; ,"Schoenmakers, Berry ; Tjoelker, Jilles ; Tuyls, Pim ; Verbitskiy, Evgeny ; ","Smooth R\'enyi Entropy of Ergodic Quantum Information Sources  We prove that the average smooth Renyi entropy rate will approach the entropy rate of a stationary, ergodic information source, which is equal to the Shannon entropy rate for a classical information source and the von Neumann entropy rate for a quantum information source. ",smooth enyi entropy ergodic quantum information source prove average smooth renyi entropy rate approach entropy rate stationary ergodic information source equal shannon entropy rate classical information source von neumann entropy rate quantum information source,34,11,0704.3504.txt
http://arxiv.org/abs/0706.1402,Analyzing Design Process and Experiments on the AnITA Generic Tutoring   System,"  In the field of tutoring systems, investigations have shown that there are many tutoring systems specific to a specific domain that, because of their static architecture, cannot be adapted to other domains. As consequence, often neither methods nor knowledge can be reused. In addition, the knowledge engineer must have programming skills in order to enhance and evaluate the system. One particular challenge is to tackle these problems with the development of a generic tutoring system. AnITA, as a stand-alone application, has been developed and implemented particularly for this purpose. However, in the testing phase, we discovered that this architecture did not fully match the user's intuitive understanding of the use of a learning tool. Therefore, AnITA has been redesigned to exclusively work as a client/server application and renamed to AnITA2. This paper discusses the evolvements made on the AnITA tutoring system, the goal of which is to use generic principles for system re-use in any domain. Two experiments were conducted, and the results are presented in this paper. ",Computer Science - Computers and Society ; Computer Science - Human-Computer Interaction ; ,"Brust, Matthias R. ; Rothkugel, Steffen ; ","Analyzing Design Process and Experiments on the AnITA Generic Tutoring   System  In the field of tutoring systems, investigations have shown that there are many tutoring systems specific to a specific domain that, because of their static architecture, cannot be adapted to other domains. As consequence, often neither methods nor knowledge can be reused. In addition, the knowledge engineer must have programming skills in order to enhance and evaluate the system. One particular challenge is to tackle these problems with the development of a generic tutoring system. AnITA, as a stand-alone application, has been developed and implemented particularly for this purpose. However, in the testing phase, we discovered that this architecture did not fully match the user's intuitive understanding of the use of a learning tool. Therefore, AnITA has been redesigned to exclusively work as a client/server application and renamed to AnITA2. This paper discusses the evolvements made on the AnITA tutoring system, the goal of which is to use generic principles for system re-use in any domain. Two experiments were conducted, and the results are presented in this paper. ",analyze design process experiment anita generic tutor system field tutor systems investigations show many tutor systems specific specific domain static architecture cannot adapt domains consequence often neither methods knowledge reuse addition knowledge engineer must program skills order enhance evaluate system one particular challenge tackle problems development generic tutor system anita stand alone application develop implement particularly purpose however test phase discover architecture fully match user intuitive understand use learn tool therefore anita redesign exclusively work client server application rename anita paper discuss evolvements make anita tutor system goal use generic principles system use domain two experiment conduct result present paper,100,10,0706.1402.txt
http://arxiv.org/abs/0710.0736,Colour image segmentation by the vector-valued Allen-Cahn phase-field   model: a multigrid solution,"  We propose a new method for the numerical solution of a PDE-driven model for colour image segmentation and give numerical examples of the results. The method combines the vector-valued Allen-Cahn phase field equation with initial data fitting terms. This method is known to be closely related to the Mumford-Shah problem and the level set segmentation by Chan and Vese. Our numerical solution is performed using a multigrid splitting of a finite element space, thereby producing an efficient and robust method for the segmentation of large images. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Numerical Analysis ; I.4.6 ; G.1.8 ; ,"Kay, David A ; Tomasi, Alessandro ; ","Colour image segmentation by the vector-valued Allen-Cahn phase-field   model: a multigrid solution  We propose a new method for the numerical solution of a PDE-driven model for colour image segmentation and give numerical examples of the results. The method combines the vector-valued Allen-Cahn phase field equation with initial data fitting terms. This method is known to be closely related to the Mumford-Shah problem and the level set segmentation by Chan and Vese. Our numerical solution is performed using a multigrid splitting of a finite element space, thereby producing an efficient and robust method for the segmentation of large images. ",colour image segmentation vector value allen cahn phase field model multigrid solution propose new method numerical solution pde drive model colour image segmentation give numerical examples result method combine vector value allen cahn phase field equation initial data fit term method know closely relate mumford shah problem level set segmentation chan vese numerical solution perform use multigrid split finite element space thereby produce efficient robust method segmentation large image,69,11,0710.0736.txt
http://arxiv.org/abs/0803.2570,Unequal Error Protection: An Information Theoretic Perspective,"  An information theoretic framework for unequal error protection is developed in terms of the exponential error bounds. The fundamental difference between the bit-wise and message-wise unequal error protection (UEP) is demonstrated, for fixed length block codes on DMCs without feedback. Effect of feedback is investigated via variable length block codes. It is shown that, feedback results in a significant improvement in both bit-wise and message-wise UEP (except the single message case for missed detection). The distinction between false-alarm and missed-detection formalizations for message-wise UEP is also considered. All results presented are at rates close to capacity. ",Computer Science - Information Theory ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Borade, Shashi ; Nakiboglu, Baris ; Zheng, Lizhong ; ","Unequal Error Protection: An Information Theoretic Perspective  An information theoretic framework for unequal error protection is developed in terms of the exponential error bounds. The fundamental difference between the bit-wise and message-wise unequal error protection (UEP) is demonstrated, for fixed length block codes on DMCs without feedback. Effect of feedback is investigated via variable length block codes. It is shown that, feedback results in a significant improvement in both bit-wise and message-wise UEP (except the single message case for missed detection). The distinction between false-alarm and missed-detection formalizations for message-wise UEP is also considered. All results presented are at rates close to capacity. ",unequal error protection information theoretic perspective information theoretic framework unequal error protection develop term exponential error bound fundamental difference bite wise message wise unequal error protection uep demonstrate fix length block cod dmcs without feedback effect feedback investigate via variable length block cod show feedback result significant improvement bite wise message wise uep except single message case miss detection distinction false alarm miss detection formalizations message wise uep also consider result present rat close capacity,75,5,0803.2570.txt
http://arxiv.org/abs/0808.0084,On the hitting times of quantum versus random walks,"  In this paper we define new Monte Carlo type classical and quantum hitting times, and we prove several relationships among these and the already existing Las Vegas type definitions. In particular, we show that for some marked state the two types of hitting time are of the same order in both the classical and the quantum case.   Further, we prove that for any reversible ergodic Markov chain $P$, the quantum hitting time of the quantum analogue of $P$ has the same order as the square root of the classical hitting time of $P$. We also investigate the (im)possibility of achieving a gap greater than quadratic using an alternative quantum walk.   Finally, we present new quantum algorithms for the detection and finding problems. The complexities of both algorithms are related to the new, potentially smaller, quantum hitting times. The detection algorithm is based on phase estimation and is particularly simple. The finding algorithm combines a similar phase estimation based procedure with ideas of Tulsi from his recent theorem for the 2D grid. Extending his result, we show that for any state-transitive Markov chain with unique marked state, the quantum hitting time is of the same order for both the detection and finding problems. ",Quantum Physics ; Computer Science - Data Structures and Algorithms ; ,"Magniez, Frederic ; Nayak, Ashwin ; Richter, Peter C. ; Santha, Miklos ; ","On the hitting times of quantum versus random walks  In this paper we define new Monte Carlo type classical and quantum hitting times, and we prove several relationships among these and the already existing Las Vegas type definitions. In particular, we show that for some marked state the two types of hitting time are of the same order in both the classical and the quantum case.   Further, we prove that for any reversible ergodic Markov chain $P$, the quantum hitting time of the quantum analogue of $P$ has the same order as the square root of the classical hitting time of $P$. We also investigate the (im)possibility of achieving a gap greater than quadratic using an alternative quantum walk.   Finally, we present new quantum algorithms for the detection and finding problems. The complexities of both algorithms are related to the new, potentially smaller, quantum hitting times. The detection algorithm is based on phase estimation and is particularly simple. The finding algorithm combines a similar phase estimation based procedure with ideas of Tulsi from his recent theorem for the 2D grid. Extending his result, we show that for any state-transitive Markov chain with unique marked state, the quantum hitting time is of the same order for both the detection and finding problems. ",hit time quantum versus random walk paper define new monte carlo type classical quantum hit time prove several relationships among already exist las vegas type definitions particular show mark state two type hit time order classical quantum case prove reversible ergodic markov chain quantum hit time quantum analogue order square root classical hit time also investigate im possibility achieve gap greater quadratic use alternative quantum walk finally present new quantum algorithms detection find problems complexities algorithms relate new potentially smaller quantum hit time detection algorithm base phase estimation particularly simple find algorithm combine similar phase estimation base procedure ideas tulsi recent theorem grid extend result show state transitive markov chain unique mark state quantum hit time order detection find problems,120,11,0808.0084.txt
http://arxiv.org/abs/0811.1254,Coding Theory and Algebraic Combinatorics,"  This chapter introduces and elaborates on the fruitful interplay of coding theory and algebraic combinatorics, with most of the focus on the interaction of codes with combinatorial designs, finite geometries, simple groups, sphere packings, kissing numbers, lattices, and association schemes. In particular, special interest is devoted to the relationship between codes and combinatorial designs. We describe and recapitulate important results in the development of the state of the art. In addition, we give illustrative examples and constructions, and highlight recent advances. Finally, we provide a collection of significant open problems and challenges concerning future research. ",Mathematics - Combinatorics ; Computer Science - Information Theory ; ,"Huber, Michael ; ","Coding Theory and Algebraic Combinatorics  This chapter introduces and elaborates on the fruitful interplay of coding theory and algebraic combinatorics, with most of the focus on the interaction of codes with combinatorial designs, finite geometries, simple groups, sphere packings, kissing numbers, lattices, and association schemes. In particular, special interest is devoted to the relationship between codes and combinatorial designs. We describe and recapitulate important results in the development of the state of the art. In addition, we give illustrative examples and constructions, and highlight recent advances. Finally, we provide a collection of significant open problems and challenges concerning future research. ",cod theory algebraic combinatorics chapter introduce elaborate fruitful interplay cod theory algebraic combinatorics focus interaction cod combinatorial design finite geometries simple group sphere pack kiss number lattices association scheme particular special interest devote relationship cod combinatorial design describe recapitulate important result development state art addition give illustrative examples constructions highlight recent advance finally provide collection significant open problems challenge concern future research,62,5,0811.1254.txt
http://arxiv.org/abs/0811.2853,Generating Random Networks Without Short Cycles,"  Random graph generation is an important tool for studying large complex networks. Despite abundance of random graph models, constructing models with application-driven constraints is poorly understood. In order to advance state-of-the-art in this area, we focus on random graphs without short cycles as a stylized family of graphs, and propose the RandGraph algorithm for randomly generating them. For any constant k, when m=O(n^{1+1/[2k(k+3)]}), RandGraph generates an asymptotically uniform random graph with n vertices, m edges, and no cycle of length at most k using O(n^2m) operations. We also characterize the approximation error for finite values of n. To the best of our knowledge, this is the first polynomial-time algorithm for the problem. RandGraph works by sequentially adding $m$ edges to an empty graph with n vertices. Recently, such sequential algorithms have been successful for random sampling problems. Our main contributions to this line of research includes introducing a new approach for sequentially approximating edge-specific probabilities at each step of the algorithm, and providing a new method for analyzing such algorithms. ",Computer Science - Data Structures and Algorithms ; Computer Science - Information Theory ; ,"Bayati, Mohsen ; Montanari, Andrea ; Saberi, Amin ; ","Generating Random Networks Without Short Cycles  Random graph generation is an important tool for studying large complex networks. Despite abundance of random graph models, constructing models with application-driven constraints is poorly understood. In order to advance state-of-the-art in this area, we focus on random graphs without short cycles as a stylized family of graphs, and propose the RandGraph algorithm for randomly generating them. For any constant k, when m=O(n^{1+1/[2k(k+3)]}), RandGraph generates an asymptotically uniform random graph with n vertices, m edges, and no cycle of length at most k using O(n^2m) operations. We also characterize the approximation error for finite values of n. To the best of our knowledge, this is the first polynomial-time algorithm for the problem. RandGraph works by sequentially adding $m$ edges to an empty graph with n vertices. Recently, such sequential algorithms have been successful for random sampling problems. Our main contributions to this line of research includes introducing a new approach for sequentially approximating edge-specific probabilities at each step of the algorithm, and providing a new method for analyzing such algorithms. ",generate random network without short cycle random graph generation important tool study large complex network despite abundance random graph model construct model application drive constraints poorly understand order advance state art area focus random graph without short cycle stylize family graph propose randgraph algorithm randomly generate constant randgraph generate asymptotically uniform random graph vertices edge cycle length use operations also characterize approximation error finite value best knowledge first polynomial time algorithm problem randgraph work sequentially add edge empty graph vertices recently sequential algorithms successful random sample problems main contributions line research include introduce new approach sequentially approximate edge specific probabilities step algorithm provide new method analyze algorithms,107,3,0811.2853.txt
http://arxiv.org/abs/0812.2709,Variations on a theme by Schalkwijk and Kailath,"  Schalkwijk and Kailath (1966) developed a class of block codes for Gaussian channels with ideal feedback for which the probability of decoding error decreases as a second-order exponent in block length for rates below capacity. This well-known but surprising result is explained and simply derived here in terms of a result by Elias (1956) concerning the minimum mean-square distortion achievable in transmitting a single Gaussian random variable over multiple uses of the same Gaussian channel. A simple modification of the Schalkwijk-Kailath scheme is then shown to have an error probability that decreases with an exponential order which is linearly increasing with block length. In the infinite bandwidth limit, this scheme produces zero error probability using bounded expected energy at all rates below capacity. A lower bound on error probability for the finite bandwidth case is then derived in which the error probability decreases with an exponential order which is linearly increasing in block length at the same rate as the upper bound. ",Computer Science - Information Theory ; ,"Gallager, Robert G. ; Nakiboglu, Baris ; ","Variations on a theme by Schalkwijk and Kailath  Schalkwijk and Kailath (1966) developed a class of block codes for Gaussian channels with ideal feedback for which the probability of decoding error decreases as a second-order exponent in block length for rates below capacity. This well-known but surprising result is explained and simply derived here in terms of a result by Elias (1956) concerning the minimum mean-square distortion achievable in transmitting a single Gaussian random variable over multiple uses of the same Gaussian channel. A simple modification of the Schalkwijk-Kailath scheme is then shown to have an error probability that decreases with an exponential order which is linearly increasing with block length. In the infinite bandwidth limit, this scheme produces zero error probability using bounded expected energy at all rates below capacity. A lower bound on error probability for the finite bandwidth case is then derived in which the error probability decreases with an exponential order which is linearly increasing in block length at the same rate as the upper bound. ",variations theme schalkwijk kailath schalkwijk kailath develop class block cod gaussian channel ideal feedback probability decode error decrease second order exponent block length rat capacity well know surprise result explain simply derive term result elias concern minimum mean square distortion achievable transmit single gaussian random variable multiple use gaussian channel simple modification schalkwijk kailath scheme show error probability decrease exponential order linearly increase block length infinite bandwidth limit scheme produce zero error probability use bound expect energy rat capacity lower bind error probability finite bandwidth case derive error probability decrease exponential order linearly increase block length rate upper bind,99,5,0812.2709.txt
http://arxiv.org/abs/0903.0197,Rotation Distance is Fixed-Parameter Tractable,"  Rotation distance between trees measures the number of simple operations it takes to transform one tree into another. There are no known polynomial-time algorithms for computing rotation distance. In the case of ordered rooted trees, we show that the rotation distance between two ordered trees is fixed-parameter tractable, in the parameter, k, the rotation distance. The proof relies on the kernalization of the initial trees to trees with size bounded by 7k. ",Computer Science - Data Structures and Algorithms ; ,"Cleary, Sean ; John, Katherine St. ; ","Rotation Distance is Fixed-Parameter Tractable  Rotation distance between trees measures the number of simple operations it takes to transform one tree into another. There are no known polynomial-time algorithms for computing rotation distance. In the case of ordered rooted trees, we show that the rotation distance between two ordered trees is fixed-parameter tractable, in the parameter, k, the rotation distance. The proof relies on the kernalization of the initial trees to trees with size bounded by 7k. ",rotation distance fix parameter tractable rotation distance tree measure number simple operations take transform one tree another know polynomial time algorithms compute rotation distance case order root tree show rotation distance two order tree fix parameter tractable parameter rotation distance proof rely kernalization initial tree tree size bound,48,4,0903.0197.txt
http://arxiv.org/abs/0903.0199,A Linear-Time Approximation Algorithm for Rotation Distance,"  Rotation distance between rooted binary trees measures the number of simple operations it takes to transform one tree into another. There are no known polynomial-time algorithms for computing rotation distance. We give an efficient, linear-time approximation algorithm, which estimates the rotation distance, within a provable factor of 2, between ordered rooted binary trees. . ",Computer Science - Data Structures and Algorithms ; ,"Cleary, Sean ; John, Katherine St. ; ","A Linear-Time Approximation Algorithm for Rotation Distance  Rotation distance between rooted binary trees measures the number of simple operations it takes to transform one tree into another. There are no known polynomial-time algorithms for computing rotation distance. We give an efficient, linear-time approximation algorithm, which estimates the rotation distance, within a provable factor of 2, between ordered rooted binary trees. . ",linear time approximation algorithm rotation distance rotation distance root binary tree measure number simple operations take transform one tree another know polynomial time algorithms compute rotation distance give efficient linear time approximation algorithm estimate rotation distance within provable factor order root binary tree,43,4,0903.0199.txt
http://arxiv.org/abs/0903.1291,The quantum query complexity of certification,"  We study the quantum query complexity of finding a certificate for a d-regular, k-level balanced NAND formula. Up to logarithmic factors, we show that the query complexity is Theta(d^{(k+1)/2}) for 0-certificates, and Theta(d^{k/2}) for 1-certificates. In particular, this shows that the zero-error quantum query complexity of evaluating such formulas is O(d^{(k+1)/2}) (again neglecting a logarithmic factor). Our lower bound relies on the fact that the quantum adversary method obeys a direct sum theorem. ",Quantum Physics ; Computer Science - Computational Complexity ; ,"Ambainis, Andris ; Childs, Andrew M. ; Gall, François Le ; Tani, Seiichiro ; ","The quantum query complexity of certification  We study the quantum query complexity of finding a certificate for a d-regular, k-level balanced NAND formula. Up to logarithmic factors, we show that the query complexity is Theta(d^{(k+1)/2}) for 0-certificates, and Theta(d^{k/2}) for 1-certificates. In particular, this shows that the zero-error quantum query complexity of evaluating such formulas is O(d^{(k+1)/2}) (again neglecting a logarithmic factor). Our lower bound relies on the fact that the quantum adversary method obeys a direct sum theorem. ",quantum query complexity certification study quantum query complexity find certificate regular level balance nand formula logarithmic factor show query complexity theta certificate theta certificate particular show zero error quantum query complexity evaluate formulas neglect logarithmic factor lower bind rely fact quantum adversary method obey direct sum theorem,47,1,0903.1291.txt
http://arxiv.org/abs/0903.2923,On uncertainty principles in the finite dimensional setting,  The aim of this paper is to prove an uncertainty principle for the representation of a vector in two bases. Our result extends previously known qualitative uncertainty principles into quantitative estimates. We then show how to transfer this result to the discrete version of the Short Time Fourier Transform. An application to trigonometric polynomials is also given. ,Mathematics - Classical Analysis and ODEs ; Computer Science - Information Theory ; ,"Ghobber, Saifallah ; Jaming, Philippe ; ",On uncertainty principles in the finite dimensional setting  The aim of this paper is to prove an uncertainty principle for the representation of a vector in two bases. Our result extends previously known qualitative uncertainty principles into quantitative estimates. We then show how to transfer this result to the discrete version of the Short Time Fourier Transform. An application to trigonometric polynomials is also given. ,uncertainty principles finite dimensional set aim paper prove uncertainty principle representation vector two base result extend previously know qualitative uncertainty principles quantitative estimate show transfer result discrete version short time fourier transform application trigonometric polynomials also give,37,7,0903.2923.txt
http://arxiv.org/abs/0903.4386,Error-and-Erasure Decoding for Block Codes with Feedback,"  Inner and outer bounds are derived on the optimal performance of fixed length block codes on discrete memoryless channels with feedback and errors-and-erasures decoding. First an inner bound is derived using a two phase encoding scheme with communication and control phases together with the optimal decoding rule for the given encoding scheme, among decoding rules that can be represented in terms of pairwise comparisons between the messages. Then an outer bound is derived using a generalization of the straight-line bound to errors-and-erasures decoders and the optimal error exponent trade off of a feedback encoder with two messages. In addition upper and lower bounds are derived, for the optimal erasure exponent of error free block codes in terms of the rate. Finally we present a proof of the fact that the optimal trade off between error exponents of a two message code does not increase with feedback on DMCs. ",Computer Science - Information Theory ; ,"Nakiboglu, Baris ; Zheng, Lizhong ; ","Error-and-Erasure Decoding for Block Codes with Feedback  Inner and outer bounds are derived on the optimal performance of fixed length block codes on discrete memoryless channels with feedback and errors-and-erasures decoding. First an inner bound is derived using a two phase encoding scheme with communication and control phases together with the optimal decoding rule for the given encoding scheme, among decoding rules that can be represented in terms of pairwise comparisons between the messages. Then an outer bound is derived using a generalization of the straight-line bound to errors-and-erasures decoders and the optimal error exponent trade off of a feedback encoder with two messages. In addition upper and lower bounds are derived, for the optimal erasure exponent of error free block codes in terms of the rate. Finally we present a proof of the fact that the optimal trade off between error exponents of a two message code does not increase with feedback on DMCs. ",error erasure decode block cod feedback inner outer bound derive optimal performance fix length block cod discrete memoryless channel feedback errors erasures decode first inner bind derive use two phase encode scheme communication control phase together optimal decode rule give encode scheme among decode rule represent term pairwise comparisons message outer bind derive use generalization straight line bind errors erasures decoders optimal error exponent trade feedback encoder two message addition upper lower bound derive optimal erasure exponent error free block cod term rate finally present proof fact optimal trade error exponents two message code increase feedback dmcs,97,5,0903.4386.txt
http://arxiv.org/abs/0904.2051,Joint-sparse recovery from multiple measurements,"  The joint-sparse recovery problem aims to recover, from sets of compressed measurements, unknown sparse matrices with nonzero entries restricted to a subset of rows. This is an extension of the single-measurement-vector (SMV) problem widely studied in compressed sensing. We analyze the recovery properties for two types of recovery algorithms. First, we show that recovery using sum-of-norm minimization cannot exceed the uniform recovery rate of sequential SMV using $\ell_1$ minimization, and that there are problems that can be solved with one approach but not with the other. Second, we analyze the performance of the ReMBo algorithm [M. Mishali and Y. Eldar, IEEE Trans. Sig. Proc., 56 (2008)] in combination with $\ell_1$ minimization, and show how recovery improves as more measurements are taken. From this analysis it follows that having more measurements than number of nonzero rows does not improve the potential theoretical recovery rate. ",Computer Science - Information Theory ; ,"Berg, Ewout van den ; Friedlander, Michael P. ; ","Joint-sparse recovery from multiple measurements  The joint-sparse recovery problem aims to recover, from sets of compressed measurements, unknown sparse matrices with nonzero entries restricted to a subset of rows. This is an extension of the single-measurement-vector (SMV) problem widely studied in compressed sensing. We analyze the recovery properties for two types of recovery algorithms. First, we show that recovery using sum-of-norm minimization cannot exceed the uniform recovery rate of sequential SMV using $\ell_1$ minimization, and that there are problems that can be solved with one approach but not with the other. Second, we analyze the performance of the ReMBo algorithm [M. Mishali and Y. Eldar, IEEE Trans. Sig. Proc., 56 (2008)] in combination with $\ell_1$ minimization, and show how recovery improves as more measurements are taken. From this analysis it follows that having more measurements than number of nonzero rows does not improve the potential theoretical recovery rate. ",joint sparse recovery multiple measurements joint sparse recovery problem aim recover set compress measurements unknown sparse matrices nonzero entries restrict subset row extension single measurement vector smv problem widely study compress sense analyze recovery properties two type recovery algorithms first show recovery use sum norm minimization cannot exceed uniform recovery rate sequential smv use ell minimization problems solve one approach second analyze performance rembo algorithm mishali eldar ieee trans sig proc combination ell minimization show recovery improve measurements take analysis follow measurements number nonzero row improve potential theoretical recovery rate,90,9,0904.2051.txt
http://arxiv.org/abs/0907.3220,Inter Genre Similarity Modelling For Automatic Music Genre   Classification,"  Music genre classification is an essential tool for music information retrieval systems and it has been finding critical applications in various media platforms. Two important problems of the automatic music genre classification are feature extraction and classifier design. This paper investigates inter-genre similarity modelling (IGS) to improve the performance of automatic music genre classification. Inter-genre similarity information is extracted over the mis-classified feature population. Once the inter-genre similarity is modelled, elimination of the inter-genre similarity reduces the inter-genre confusion and improves the identification rates. Inter-genre similarity modelling is further improved with iterative IGS modelling(IIGS) and score modelling for IGS elimination(SMIGS). Experimental results with promising classification improvements are provided. ",Computer Science - Sound ; Computer Science - Artificial Intelligence ; Statistics - Machine Learning ; H.5.5 ; I.5 ; ,"Bagci, Ulas ; Erzin, Engin ; ","Inter Genre Similarity Modelling For Automatic Music Genre   Classification  Music genre classification is an essential tool for music information retrieval systems and it has been finding critical applications in various media platforms. Two important problems of the automatic music genre classification are feature extraction and classifier design. This paper investigates inter-genre similarity modelling (IGS) to improve the performance of automatic music genre classification. Inter-genre similarity information is extracted over the mis-classified feature population. Once the inter-genre similarity is modelled, elimination of the inter-genre similarity reduces the inter-genre confusion and improves the identification rates. Inter-genre similarity modelling is further improved with iterative IGS modelling(IIGS) and score modelling for IGS elimination(SMIGS). Experimental results with promising classification improvements are provided. ",inter genre similarity model automatic music genre classification music genre classification essential tool music information retrieval systems find critical applications various media platforms two important problems automatic music genre classification feature extraction classifier design paper investigate inter genre similarity model igs improve performance automatic music genre classification inter genre similarity information extract mis classify feature population inter genre similarity model elimination inter genre similarity reduce inter genre confusion improve identification rat inter genre similarity model improve iterative igs model iigs score model igs elimination smigs experimental result promise classification improvements provide,91,11,0907.3220.txt
http://arxiv.org/abs/0907.3965,P != NP Proof,"  This paper demonstrates that P \not= NP. The way was to generalize the traditional definitions of the classes P and NP, to construct an artificial problem (a generalization to SAT: The XG-SAT, much more difficult than the former) and then to demonstrate that it is in NP but not in P (where the classes P and NP are generalized and called too simply P and NP in this paper, and then it is explained why the traditional classes P and NP should be fixed and replaced by these generalized ones into Theory of Computer Science). The demonstration consists of: 1. Definition of Restricted Type X Program; 2. Definition of the General Extended Problem of Satisfiability of a Boolean Formula - XG-SAT; 3. Generalization to classes P and NP; 4. Demonstration that the XG-SAT is in NP; 5. Demonstration that the XG-SAT is not in P; 6. Demonstration that the Baker-Gill-Solovay Theorem does not refute the proof; 7. Demonstration that the Razborov-Rudich Theorem does not refute the proof; 8. Demonstration that the Aaronson-Wigderson Theorem does not refute the proof. ","Computer Science - Computational Complexity ; 68Q15 (Primary), 68Q17 (Secondary) ; ","Barbosa, André Luiz ; ","P != NP Proof  This paper demonstrates that P \not= NP. The way was to generalize the traditional definitions of the classes P and NP, to construct an artificial problem (a generalization to SAT: The XG-SAT, much more difficult than the former) and then to demonstrate that it is in NP but not in P (where the classes P and NP are generalized and called too simply P and NP in this paper, and then it is explained why the traditional classes P and NP should be fixed and replaced by these generalized ones into Theory of Computer Science). The demonstration consists of: 1. Definition of Restricted Type X Program; 2. Definition of the General Extended Problem of Satisfiability of a Boolean Formula - XG-SAT; 3. Generalization to classes P and NP; 4. Demonstration that the XG-SAT is in NP; 5. Demonstration that the XG-SAT is not in P; 6. Demonstration that the Baker-Gill-Solovay Theorem does not refute the proof; 7. Demonstration that the Razborov-Rudich Theorem does not refute the proof; 8. Demonstration that the Aaronson-Wigderson Theorem does not refute the proof. ",np proof paper demonstrate np way generalize traditional definitions class np construct artificial problem generalization sit xg sit much difficult former demonstrate np class np generalize call simply np paper explain traditional class np fix replace generalize ones theory computer science demonstration consist definition restrict type program definition general extend problem satisfiability boolean formula xg sit generalization class np demonstration xg sit np demonstration xg sit demonstration baker gill solovay theorem refute proof demonstration razborov rudich theorem refute proof demonstration aaronson wigderson theorem refute proof,85,8,0907.3965.txt
http://arxiv.org/abs/0910.2912,Universally Composable Quantum Multi-Party Computation,"  The Universal Composability model (UC) by Canetti (FOCS 2001) allows for secure composition of arbitrary protocols. We present a quantum version of the UC model which enjoys the same compositionality guarantees. We prove that in this model statistically secure oblivious transfer protocols can be constructed from commitments. Furthermore, we show that every statistically classically UC secure protocol is also statistically quantum UC secure. Such implications are not known for other quantum security definitions. As a corollary, we get that quantum UC secure protocols for general multi-party computation can be constructed from commitments. ",Quantum Physics ; Computer Science - Cryptography and Security ; ,"Unruh, Dominique ; ","Universally Composable Quantum Multi-Party Computation  The Universal Composability model (UC) by Canetti (FOCS 2001) allows for secure composition of arbitrary protocols. We present a quantum version of the UC model which enjoys the same compositionality guarantees. We prove that in this model statistically secure oblivious transfer protocols can be constructed from commitments. Furthermore, we show that every statistically classically UC secure protocol is also statistically quantum UC secure. Such implications are not known for other quantum security definitions. As a corollary, we get that quantum UC secure protocols for general multi-party computation can be constructed from commitments. ",universally composable quantum multi party computation universal composability model uc canetti focs allow secure composition arbitrary protocols present quantum version uc model enjoy compositionality guarantee prove model statistically secure oblivious transfer protocols construct commitments furthermore show every statistically classically uc secure protocol also statistically quantum uc secure implications know quantum security definitions corollary get quantum uc secure protocols general multi party computation construct commitments,64,2,0910.2912.txt
http://arxiv.org/abs/0910.5577,On the stability of two-chunk file-sharing systems,"  We consider five different peer-to-peer file sharing systems with two chunks, with the aim of finding chunk selection algorithms that have provably stable performance with any input rate and assuming non-altruistic peers who leave the system immediately after downloading the second chunk. We show that many algorithms that first looked promising lead to unstable or oscillating behavior. However, we end up with a system with desirable properties. Most of our rigorous results concern the corresponding deterministic large system limits, but in two simplest cases we provide proofs for the stochastic systems also. ","Computer Science - Operating Systems ; Mathematics - Probability ; 60K25, 68M14 ; ","Norros, Ilkka ; Reittu, Hannu ; Eirola, Timo ; ","On the stability of two-chunk file-sharing systems  We consider five different peer-to-peer file sharing systems with two chunks, with the aim of finding chunk selection algorithms that have provably stable performance with any input rate and assuming non-altruistic peers who leave the system immediately after downloading the second chunk. We show that many algorithms that first looked promising lead to unstable or oscillating behavior. However, we end up with a system with desirable properties. Most of our rigorous results concern the corresponding deterministic large system limits, but in two simplest cases we provide proofs for the stochastic systems also. ",stability two chunk file share systems consider five different peer peer file share systems two chunk aim find chunk selection algorithms provably stable performance input rate assume non altruistic peer leave system immediately download second chunk show many algorithms first look promise lead unstable oscillate behavior however end system desirable properties rigorous result concern correspond deterministic large system limit two simplest case provide proof stochastic systems also,67,4,0910.5577.txt
http://arxiv.org/abs/0911.1507,MAC Layer Hurdles in BSNs,"  The last few decades have seen considerable research progress in microelectronics and integrated circuits, system-on-chip design, wireless communication, and sensor technology. This progress has enabled the seamless integration of autonomous wireless sensor nodes around a human body to create a Body Sensor Network (BSN). The development of a proactive and ambulatory BSN induces a number of enormous issues and challenges. This paper presents the technical hurdles during the design and implementation of a low-power Medium Access Control (MAC) protocol for in-body and on-body sensor networks. We analyze the performance of IEEE 802.15.4 protocol for the on-body sensor network. We also provide a comprehensive insight into the heterogeneous characteristics of the in-body sensor network. A low-power technique called Pattern-Based Wake-up Table is proposed to handle the normal traffic in a BSN. The proposed technique provides a reliable solution towards low-power communication in the in-body sensor network. ",Computer Science - Networking and Internet Architecture ; ,"Ullah, Sana ; Khan, Pervez ; Choi, Young-Woo ; Lee, Hyung-Soo ; Kwak, Kyung Sup ; ","MAC Layer Hurdles in BSNs  The last few decades have seen considerable research progress in microelectronics and integrated circuits, system-on-chip design, wireless communication, and sensor technology. This progress has enabled the seamless integration of autonomous wireless sensor nodes around a human body to create a Body Sensor Network (BSN). The development of a proactive and ambulatory BSN induces a number of enormous issues and challenges. This paper presents the technical hurdles during the design and implementation of a low-power Medium Access Control (MAC) protocol for in-body and on-body sensor networks. We analyze the performance of IEEE 802.15.4 protocol for the on-body sensor network. We also provide a comprehensive insight into the heterogeneous characteristics of the in-body sensor network. A low-power technique called Pattern-Based Wake-up Table is proposed to handle the normal traffic in a BSN. The proposed technique provides a reliable solution towards low-power communication in the in-body sensor network. ",mac layer hurdle bsns last decades see considerable research progress microelectronics integrate circuit system chip design wireless communication sensor technology progress enable seamless integration autonomous wireless sensor nod around human body create body sensor network bsn development proactive ambulatory bsn induce number enormous issue challenge paper present technical hurdle design implementation low power medium access control mac protocol body body sensor network analyze performance ieee protocol body sensor network also provide comprehensive insight heterogeneous characteristics body sensor network low power technique call pattern base wake table propose handle normal traffic bsn propose technique provide reliable solution towards low power communication body sensor network,103,2,0911.1507.txt
http://arxiv.org/abs/0911.1509,On the Development of Low Power MAC Protocol for WBANs,"  Current advances in wireless communication, microelectronics, semiconductor technologies, and intelligent sensors have contributed to the development of unobtrusive WBANs. These networks provide long term health monitoring of patients without any constraint in their normal activities. Traditional MAC protocols do not accommodate the assorted WBAN traffic requirements in a power efficient manner. In this paper, we present a brief discussion on the development process of a low power MAC protocol for WBANs. We observe the behavior of a beacon-enabled IEEE 802.15.4 for on-body sensor networks. We further propose a low power technique called traffic based wakeup mechanism for a WBAN that exploits the traffic patterns of the BAN Nodes to ensure power efficient and reliable communication. ",Computer Science - Networking and Internet Architecture ; ,"Ullah, Sana ; Khan, Pervez ; Kwak, Kyung Sup ; ","On the Development of Low Power MAC Protocol for WBANs  Current advances in wireless communication, microelectronics, semiconductor technologies, and intelligent sensors have contributed to the development of unobtrusive WBANs. These networks provide long term health monitoring of patients without any constraint in their normal activities. Traditional MAC protocols do not accommodate the assorted WBAN traffic requirements in a power efficient manner. In this paper, we present a brief discussion on the development process of a low power MAC protocol for WBANs. We observe the behavior of a beacon-enabled IEEE 802.15.4 for on-body sensor networks. We further propose a low power technique called traffic based wakeup mechanism for a WBAN that exploits the traffic patterns of the BAN Nodes to ensure power efficient and reliable communication. ",development low power mac protocol wbans current advance wireless communication microelectronics semiconductor technologies intelligent sensors contribute development unobtrusive wbans network provide long term health monitor patients without constraint normal activities traditional mac protocols accommodate assort wban traffic requirements power efficient manner paper present brief discussion development process low power mac protocol wbans observe behavior beacon enable ieee body sensor network propose low power technique call traffic base wakeup mechanism wban exploit traffic pattern ban nod ensure power efficient reliable communication,80,2,0911.1509.txt
http://arxiv.org/abs/0911.1544,Towards Power Efficient MAC Protocol for In-Body and On-Body Sensor   Networks,"  This paper presents an empirical discussion on the design and implementation of a power-efficient Medium Access Control (MAC) protocol for in-body and on-body sensor networks. We analyze the performance of a beacon-enabled IEEE 802.15.4, PB-TDMA, and S-MAC protocols for on-body sensor networks. We further present a Traffic Based Wakeup Mechanism that utilizes the traffic patterns of the BAN Nodes (BNs) to accommodate the entire BSN traffic. To enable a logical connection between different BNs working on different frequency bands, a method called Bridging function is proposed. The Bridging function integrates all BNs working on different bands into a complete BSN. ",Computer Science - Networking and Internet Architecture ; ,"Ullah, Sana ; An, Xizhi ; Kwak, Kyung Sup ; ","Towards Power Efficient MAC Protocol for In-Body and On-Body Sensor   Networks  This paper presents an empirical discussion on the design and implementation of a power-efficient Medium Access Control (MAC) protocol for in-body and on-body sensor networks. We analyze the performance of a beacon-enabled IEEE 802.15.4, PB-TDMA, and S-MAC protocols for on-body sensor networks. We further present a Traffic Based Wakeup Mechanism that utilizes the traffic patterns of the BAN Nodes (BNs) to accommodate the entire BSN traffic. To enable a logical connection between different BNs working on different frequency bands, a method called Bridging function is proposed. The Bridging function integrates all BNs working on different bands into a complete BSN. ",towards power efficient mac protocol body body sensor network paper present empirical discussion design implementation power efficient medium access control mac protocol body body sensor network analyze performance beacon enable ieee pb tdma mac protocols body sensor network present traffic base wakeup mechanism utilize traffic pattern ban nod bns accommodate entire bsn traffic enable logical connection different bns work different frequency band method call bridge function propose bridge function integrate bns work different band complete bsn,76,2,0911.1544.txt
http://arxiv.org/abs/0911.1546,A Study of Implanted and Wearable Body Sensor Networks,"  Recent advances in intelligent sensors, microelectronics and integrated circuit, system-on-chip design and low power wireless communication introduced the development of miniaturised and autonomous sensor nodes. These tiny sensor nodes can be deployed to develop a proactive Body Sensor Network (BSN). The rapid advancement in ultra low-power RF (radio frequency) technology enables invasive and non-invasive devices to communicate with a remote station. This communication revolutionizes healthcare system by enabling long term health monitoring of a patient and providing real time feedback to the medical experts. In this paper, we present In-body and On-body communication networks with a special focus on the methodologies of wireless communication between implanted medical devices with external monitoring equipment and recent technological growth in both areas. We also discuss open issues and challenges in a BSN. ",Computer Science - Networking and Internet Architecture ; ,"Ullah, Sana ; Higgins, Henry ; Siddiqui, M. Arif ; Kwak, Kyung Sup ; ","A Study of Implanted and Wearable Body Sensor Networks  Recent advances in intelligent sensors, microelectronics and integrated circuit, system-on-chip design and low power wireless communication introduced the development of miniaturised and autonomous sensor nodes. These tiny sensor nodes can be deployed to develop a proactive Body Sensor Network (BSN). The rapid advancement in ultra low-power RF (radio frequency) technology enables invasive and non-invasive devices to communicate with a remote station. This communication revolutionizes healthcare system by enabling long term health monitoring of a patient and providing real time feedback to the medical experts. In this paper, we present In-body and On-body communication networks with a special focus on the methodologies of wireless communication between implanted medical devices with external monitoring equipment and recent technological growth in both areas. We also discuss open issues and challenges in a BSN. ",study implant wearable body sensor network recent advance intelligent sensors microelectronics integrate circuit system chip design low power wireless communication introduce development miniaturise autonomous sensor nod tiny sensor nod deploy develop proactive body sensor network bsn rapid advancement ultra low power rf radio frequency technology enable invasive non invasive devices communicate remote station communication revolutionize healthcare system enable long term health monitor patient provide real time feedback medical experts paper present body body communication network special focus methodologies wireless communication implant medical devices external monitor equipment recent technological growth areas also discuss open issue challenge bsn,96,2,0911.1546.txt
http://arxiv.org/abs/0911.2538,Euclidean versus hyperbolic congestion in idealized versus experimental   networks,"  This paper proposes a mathematical justification of the phenomenon of extreme congestion at a very limited number of nodes in very large networks. It is argued that this phenomenon occurs as a combination of the negative curvature property of the network together with minimum length routing. More specifically, it is shown that, in a large n-dimensional hyperbolic ball B of radius R viewed as a roughly similar model of a Gromov hyperbolic network, the proportion of traffic paths transiting through a small ball near the center is independent of the radius R whereas, in a Euclidean ball, the same proportion scales as 1/R^{n-1}. This discrepancy persists for the traffic load, which at the center of the hyperbolic ball scales as the square of the volume, whereas the same traffic load scales as the volume to the power (n+1)/n in the Euclidean ball. This provides a theoretical justification of the experimental exponent discrepancy observed by Narayan and Saniee between traffic loads in Gromov-hyperbolic networks from the Rocketfuel data base and synthetic Euclidean lattice networks. It is further conjectured that for networks that do not enjoy the obvious symmetry of hyperbolic and Euclidean balls, the point of maximum traffic is near the center of mass of the network. ",Computer Science - Networking and Internet Architecture ; ,"Jonckheere, Edmond ; Lou, Mingji ; Bonahon, Francis ; Baryshnikov, Yuliy ; ","Euclidean versus hyperbolic congestion in idealized versus experimental   networks  This paper proposes a mathematical justification of the phenomenon of extreme congestion at a very limited number of nodes in very large networks. It is argued that this phenomenon occurs as a combination of the negative curvature property of the network together with minimum length routing. More specifically, it is shown that, in a large n-dimensional hyperbolic ball B of radius R viewed as a roughly similar model of a Gromov hyperbolic network, the proportion of traffic paths transiting through a small ball near the center is independent of the radius R whereas, in a Euclidean ball, the same proportion scales as 1/R^{n-1}. This discrepancy persists for the traffic load, which at the center of the hyperbolic ball scales as the square of the volume, whereas the same traffic load scales as the volume to the power (n+1)/n in the Euclidean ball. This provides a theoretical justification of the experimental exponent discrepancy observed by Narayan and Saniee between traffic loads in Gromov-hyperbolic networks from the Rocketfuel data base and synthetic Euclidean lattice networks. It is further conjectured that for networks that do not enjoy the obvious symmetry of hyperbolic and Euclidean balls, the point of maximum traffic is near the center of mass of the network. ",euclidean versus hyperbolic congestion idealize versus experimental network paper propose mathematical justification phenomenon extreme congestion limit number nod large network argue phenomenon occur combination negative curvature property network together minimum length rout specifically show large dimensional hyperbolic ball radius view roughly similar model gromov hyperbolic network proportion traffic paths transit small ball near center independent radius whereas euclidean ball proportion scale discrepancy persist traffic load center hyperbolic ball scale square volume whereas traffic load scale volume power euclidean ball provide theoretical justification experimental exponent discrepancy observe narayan saniee traffic load gromov hyperbolic network rocketfuel data base synthetic euclidean lattice network conjecture network enjoy obvious symmetry hyperbolic euclidean ball point maximum traffic near center mass network,115,6,0911.2538.txt
http://arxiv.org/abs/0911.2746,Model Selection: Two Fundamental Measures of Coherence and Their   Algorithmic Significance,"  The problem of model selection arises in a number of contexts, such as compressed sensing, subset selection in linear regression, estimation of structures in graphical models, and signal denoising. This paper generalizes the notion of \emph{incoherence} in the existing literature on model selection and introduces two fundamental measures of coherence---termed as the worst-case coherence and the average coherence---among the columns of a design matrix. In particular, it utilizes these two measures of coherence to provide an in-depth analysis of a simple one-step thresholding (OST) algorithm for model selection. One of the key insights offered by the ensuing analysis is that OST is feasible for model selection as long as the design matrix obeys an easily verifiable property. In addition, the paper also characterizes the model-selection performance of OST in terms of the worst-case coherence, \mu, and establishes that OST performs near-optimally in the low signal-to-noise ratio regime for N x C design matrices with \mu = O(N^{-1/2}). Finally, in contrast to some of the existing literature on model selection, the analysis in the paper is nonasymptotic in nature, it does not require knowledge of the true model order, it is applicable to generic (random or deterministic) design matrices, and it neither requires submatrices of the design matrix to have full rank, nor does it assume a statistical prior on the values of the nonzero entries of the data vector. ",Computer Science - Information Theory ; Mathematics - Statistics Theory ; ,"Bajwa, Waheed U. ; Calderbank, Robert ; Jafarpour, Sina ; ","Model Selection: Two Fundamental Measures of Coherence and Their   Algorithmic Significance  The problem of model selection arises in a number of contexts, such as compressed sensing, subset selection in linear regression, estimation of structures in graphical models, and signal denoising. This paper generalizes the notion of \emph{incoherence} in the existing literature on model selection and introduces two fundamental measures of coherence---termed as the worst-case coherence and the average coherence---among the columns of a design matrix. In particular, it utilizes these two measures of coherence to provide an in-depth analysis of a simple one-step thresholding (OST) algorithm for model selection. One of the key insights offered by the ensuing analysis is that OST is feasible for model selection as long as the design matrix obeys an easily verifiable property. In addition, the paper also characterizes the model-selection performance of OST in terms of the worst-case coherence, \mu, and establishes that OST performs near-optimally in the low signal-to-noise ratio regime for N x C design matrices with \mu = O(N^{-1/2}). Finally, in contrast to some of the existing literature on model selection, the analysis in the paper is nonasymptotic in nature, it does not require knowledge of the true model order, it is applicable to generic (random or deterministic) design matrices, and it neither requires submatrices of the design matrix to have full rank, nor does it assume a statistical prior on the values of the nonzero entries of the data vector. ",model selection two fundamental measure coherence algorithmic significance problem model selection arise number contexts compress sense subset selection linear regression estimation structure graphical model signal denoising paper generalize notion emph incoherence exist literature model selection introduce two fundamental measure coherence term worst case coherence average coherence among columns design matrix particular utilize two measure coherence provide depth analysis simple one step thresholding ost algorithm model selection one key insights offer ensue analysis ost feasible model selection long design matrix obey easily verifiable property addition paper also characterize model selection performance ost term worst case coherence mu establish ost perform near optimally low signal noise ratio regime design matrices mu finally contrast exist literature model selection analysis paper nonasymptotic nature require knowledge true model order applicable generic random deterministic design matrices neither require submatrices design matrix full rank assume statistical prior value nonzero entries data vector,145,9,0911.2746.txt
http://arxiv.org/abs/0911.5153,Self-Reference Ultra-Wideband Systems,"  Towards employing low complexity transceivers for signal reception in Ultra-Wideband (UWB) systems, Transmitted Reference (TR) and Differential TR (DTR) schemes have attracted researchers attention. In this letter, we introduce an alternative, less complex scheme, called Self Reference (SR) UWB transceiver, which uses a modified replica of the received signal itself as reference pulse, resulting in double data rates compared to TR schemes. Moreover, SR eliminates the need for delay lines at the receiver side, which constitute a major drawback of the conventional TR and DTR schemes, while it also requires no channel estimations, resulting in lower complexity implementations and power savings. The performance of the SR scheme is investigated in high-frequency (HF) channels, showing that it offers a better or comparable performance to that of DTR, depending on the channel conditions. ",Computer Science - Networking and Internet Architecture ; ,"Lioumpas, Athanasios S. ; ","Self-Reference Ultra-Wideband Systems  Towards employing low complexity transceivers for signal reception in Ultra-Wideband (UWB) systems, Transmitted Reference (TR) and Differential TR (DTR) schemes have attracted researchers attention. In this letter, we introduce an alternative, less complex scheme, called Self Reference (SR) UWB transceiver, which uses a modified replica of the received signal itself as reference pulse, resulting in double data rates compared to TR schemes. Moreover, SR eliminates the need for delay lines at the receiver side, which constitute a major drawback of the conventional TR and DTR schemes, while it also requires no channel estimations, resulting in lower complexity implementations and power savings. The performance of the SR scheme is investigated in high-frequency (HF) channels, showing that it offers a better or comparable performance to that of DTR, depending on the channel conditions. ",self reference ultra wideband systems towards employ low complexity transceivers signal reception ultra wideband uwb systems transmit reference tr differential tr dtr scheme attract researchers attention letter introduce alternative less complex scheme call self reference sr uwb transceiver use modify replica receive signal reference pulse result double data rat compare tr scheme moreover sr eliminate need delay line receiver side constitute major drawback conventional tr dtr scheme also require channel estimations result lower complexity implementations power save performance sr scheme investigate high frequency hf channel show offer better comparable performance dtr depend channel condition,94,12,0911.5153.txt
http://arxiv.org/abs/1001.1435,"JBotSim, a Tool for Fast Prototyping of Distributed Algorithms in   Dynamic Networks","  JBotSim is a java library that offers basic primitives for prototyping, running, and visualizing distributed algorithms in dynamic networks. With JBotSim, one can implement an idea in minutes and interact with it ({\it e.g.}, add, move, or delete nodes) while it is running. JBotSim is well suited to prepare live demonstrations of your algorithms to colleagues or students; it can also be used to evaluate performance at the algorithmic level (number of messages, number of rounds, etc.). Unlike most tools, JBotSim is not an integrated environment. It is a lightweight library to be used in your program. In this paper, we present an overview of its distinctive features and architecture. ","Computer Science - Mathematical Software ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Networking and Internet Architecture ; ","Casteigts, Arnaud ; ","JBotSim, a Tool for Fast Prototyping of Distributed Algorithms in   Dynamic Networks  JBotSim is a java library that offers basic primitives for prototyping, running, and visualizing distributed algorithms in dynamic networks. With JBotSim, one can implement an idea in minutes and interact with it ({\it e.g.}, add, move, or delete nodes) while it is running. JBotSim is well suited to prepare live demonstrations of your algorithms to colleagues or students; it can also be used to evaluate performance at the algorithmic level (number of messages, number of rounds, etc.). Unlike most tools, JBotSim is not an integrated environment. It is a lightweight library to be used in your program. In this paper, we present an overview of its distinctive features and architecture. ",jbotsim tool fast prototyping distribute algorithms dynamic network jbotsim java library offer basic primitives prototyping run visualize distribute algorithms dynamic network jbotsim one implement idea minutes interact add move delete nod run jbotsim well suit prepare live demonstrations algorithms colleagues students also use evaluate performance algorithmic level number message number round etc unlike tool jbotsim integrate environment lightweight library use program paper present overview distinctive feature architecture,67,6,1001.1435.txt
http://arxiv.org/abs/1001.3780,Combinatorial Bounds and Characterizations of Splitting Authentication   Codes,"  We present several generalizations of results for splitting authentication codes by studying the aspect of multi-fold security. As the two primary results, we prove a combinatorial lower bound on the number of encoding rules and a combinatorial characterization of optimal splitting authentication codes that are multi-fold secure against spoofing attacks. The characterization is based on a new type of combinatorial designs, which we introduce and for which basic necessary conditions are given regarding their existence. ",Computer Science - Cryptography and Security ; Computer Science - Information Theory ; G.2.3 ; ,"Huber, Michael ; ","Combinatorial Bounds and Characterizations of Splitting Authentication   Codes  We present several generalizations of results for splitting authentication codes by studying the aspect of multi-fold security. As the two primary results, we prove a combinatorial lower bound on the number of encoding rules and a combinatorial characterization of optimal splitting authentication codes that are multi-fold secure against spoofing attacks. The characterization is based on a new type of combinatorial designs, which we introduce and for which basic necessary conditions are given regarding their existence. ",combinatorial bound characterizations split authentication cod present several generalizations result split authentication cod study aspect multi fold security two primary result prove combinatorial lower bind number encode rule combinatorial characterization optimal split authentication cod multi fold secure spoof attack characterization base new type combinatorial design introduce basic necessary condition give regard existence,52,5,1001.3780.txt
http://arxiv.org/abs/1002.0747,Efficient Bayesian Learning in Social Networks with Gaussian Estimators,"  We consider a group of Bayesian agents who try to estimate a state of the world $\theta$ through interaction on a social network. Each agent $v$ initially receives a private measurement of $\theta$: a number $S_v$ picked from a Gaussian distribution with mean $\theta$ and standard deviation one. Then, in each discrete time iteration, each reveals its estimate of $\theta$ to its neighbors, and, observing its neighbors' actions, updates its belief using Bayes' Law.   This process aggregates information efficiently, in the sense that all the agents converge to the belief that they would have, had they access to all the private measurements. We show that this process is computationally efficient, so that each agent's calculation can be easily carried out. We also show that on any graph the process converges after at most $2N \cdot D$ steps, where $N$ is the number of agents and $D$ is the diameter of the network. Finally, we show that on trees and on distance transitive-graphs the process converges after $D$ steps, and that it preserves privacy, so that agents learn very little about the private signal of most other agents, despite the efficient aggregation of information. Our results extend those in an unpublished manuscript of the first and last authors. ",Statistics - Applications ; Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Mossel, Elchanan ; Olsman, Noah ; Tamuz, Omer ; ","Efficient Bayesian Learning in Social Networks with Gaussian Estimators  We consider a group of Bayesian agents who try to estimate a state of the world $\theta$ through interaction on a social network. Each agent $v$ initially receives a private measurement of $\theta$: a number $S_v$ picked from a Gaussian distribution with mean $\theta$ and standard deviation one. Then, in each discrete time iteration, each reveals its estimate of $\theta$ to its neighbors, and, observing its neighbors' actions, updates its belief using Bayes' Law.   This process aggregates information efficiently, in the sense that all the agents converge to the belief that they would have, had they access to all the private measurements. We show that this process is computationally efficient, so that each agent's calculation can be easily carried out. We also show that on any graph the process converges after at most $2N \cdot D$ steps, where $N$ is the number of agents and $D$ is the diameter of the network. Finally, we show that on trees and on distance transitive-graphs the process converges after $D$ steps, and that it preserves privacy, so that agents learn very little about the private signal of most other agents, despite the efficient aggregation of information. Our results extend those in an unpublished manuscript of the first and last authors. ",efficient bayesian learn social network gaussian estimators consider group bayesian agents try estimate state world theta interaction social network agent initially receive private measurement theta number pick gaussian distribution mean theta standard deviation one discrete time iteration reveal estimate theta neighbor observe neighbor action update belief use bay law process aggregate information efficiently sense agents converge belief would access private measurements show process computationally efficient agent calculation easily carry also show graph process converge cdot step number agents diameter network finally show tree distance transitive graph process converge step preserve privacy agents learn little private signal agents despite efficient aggregation information result extend unpublished manuscript first last author,108,6,1002.0747.txt
http://arxiv.org/abs/1003.1628,Having Fun with Lambert W(x) Function,"  This short note presents the Lambert W(x) function and its possible application in the framework of physics related to the Pierre Auger Observatory. The actual numerical implementation in C++ consists of Halley's and Fritsch's iteration with branch-point expansion, asymptotic series and rational fits as initial approximations. ",Computer Science - Mathematical Software ; Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; ,"Veberic, Darko ; ","Having Fun with Lambert W(x) Function  This short note presents the Lambert W(x) function and its possible application in the framework of physics related to the Pierre Auger Observatory. The actual numerical implementation in C++ consists of Halley's and Fritsch's iteration with branch-point expansion, asymptotic series and rational fits as initial approximations. ",fun lambert function short note present lambert function possible application framework physics relate pierre auger observatory actual numerical implementation consist halley fritsch iteration branch point expansion asymptotic series rational fit initial approximations,32,11,1003.1628.txt
http://arxiv.org/abs/1004.0208,Delay-rate tradeoff in ergodic interference alignment,"  Ergodic interference alignment, as introduced by Nazer et al (NGJV), is a technique that allows high-rate communication in n-user interference networks with fast fading. It works by splitting communication across a pair of fading matrices. However, it comes with the overhead of a long time delay until matchable matrices occur: the delay is q^n^2 for field size q.   In this paper, we outline two new families of schemes, called JAP and JAP-B, that reduce the expected delay, sometimes at the cost of a reduction in rate from the NGJV scheme. In particular, we give examples of good schemes for networks with few users, and show that in large n-user networks, the delay scales like q^T, where T is quadratic in n for a constant per-user rate and T is constant for a constant sum-rate. We also show that half the single-user rate can be achieved while reducing NGJV's delay from q^n^2 to q^(n-1)(n-2).   This extended version includes complete proofs and more details of good schemes for small n. ",Computer Science - Information Theory ; Mathematics - Probability ; ,"Johnson, Oliver ; Aldridge, Matthew ; Piechocki, Robert ; ","Delay-rate tradeoff in ergodic interference alignment  Ergodic interference alignment, as introduced by Nazer et al (NGJV), is a technique that allows high-rate communication in n-user interference networks with fast fading. It works by splitting communication across a pair of fading matrices. However, it comes with the overhead of a long time delay until matchable matrices occur: the delay is q^n^2 for field size q.   In this paper, we outline two new families of schemes, called JAP and JAP-B, that reduce the expected delay, sometimes at the cost of a reduction in rate from the NGJV scheme. In particular, we give examples of good schemes for networks with few users, and show that in large n-user networks, the delay scales like q^T, where T is quadratic in n for a constant per-user rate and T is constant for a constant sum-rate. We also show that half the single-user rate can be achieved while reducing NGJV's delay from q^n^2 to q^(n-1)(n-2).   This extended version includes complete proofs and more details of good schemes for small n. ",delay rate tradeoff ergodic interference alignment ergodic interference alignment introduce nazer et al ngjv technique allow high rate communication user interference network fast fade work split communication across pair fade matrices however come overhead long time delay matchable matrices occur delay field size paper outline two new families scheme call jap jap reduce expect delay sometimes cost reduction rate ngjv scheme particular give examples good scheme network users show large user network delay scale like quadratic constant per user rate constant constant sum rate also show half single user rate achieve reduce ngjv delay extend version include complete proof detail good scheme small,103,12,1004.0208.txt
http://arxiv.org/abs/1004.4940,FauxCrypt - A Method of Text Obfuscation,"  Warnings have been raised about the steady diminution of privacy. More and more personal information, such as that contained electronic mail, is moving to cloud computing servers where it might be machine-searched and indexed. FauxCrypt is an algorithm for modification of a plaintext document that leaves it generally readable by a person but not readily searched or indexed by machine. The algorithm employs a dictionary substitution of selected words, and an obfuscating transposition of letters in other words. The obfuscation is designed to leave the words understandable, although they are badly spelled. FauxCrypt is free, open source software, with source code available. ",Computer Science - Cryptography and Security ; D.4.6 ; ,"Gualtieri, Devlin M. ; ","FauxCrypt - A Method of Text Obfuscation  Warnings have been raised about the steady diminution of privacy. More and more personal information, such as that contained electronic mail, is moving to cloud computing servers where it might be machine-searched and indexed. FauxCrypt is an algorithm for modification of a plaintext document that leaves it generally readable by a person but not readily searched or indexed by machine. The algorithm employs a dictionary substitution of selected words, and an obfuscating transposition of letters in other words. The obfuscation is designed to leave the words understandable, although they are badly spelled. FauxCrypt is free, open source software, with source code available. ",fauxcrypt method text obfuscation warn raise steady diminution privacy personal information contain electronic mail move cloud compute servers might machine search index fauxcrypt algorithm modification plaintext document leave generally readable person readily search index machine algorithm employ dictionary substitution select word obfuscate transposition letter word obfuscation design leave word understandable although badly spell fauxcrypt free open source software source code available,61,14,1004.4940.txt
http://arxiv.org/abs/1005.2894,Optimal Gradient Clock Synchronization in Dynamic Networks,"  We study the problem of clock synchronization in highly dynamic networks, where communication links can appear or disappear at any time. The nodes in the network are equipped with hardware clocks, but the rate of the hardware clocks can vary arbitrarily within specific bounds, and the estimates that nodes can obtain about the clock values of other nodes are inherently inaccurate. Our goal in this setting is to output a logical clock at each node such that the logical clocks of any two nodes are not too far apart, and nodes that remain close to each other in the network for a long time are better synchronized than distant nodes. This property is called gradient clock synchronization.   Gradient clock synchronization has been widely studied in the static setting, where the network topology does not change. We show that the asymptotically optimal bounds obtained for the static case also apply to our highly dynamic setting: if two nodes remain at distance $d$ from each other for sufficiently long, it is possible to upper bound the difference between their clock values by $O(d \log (D / d))$, where $D$ is the diameter of the network. This is known to be optimal even for static networks. Furthermore, we show that our algorithm has optimal stabilization time: when a path of length $d$ appears between two nodes, the time required until the clock skew between the two nodes is reduced to $O(d \log (D / d))$ is $O(D)$, which we prove to be optimal. Finally, the techniques employed for the more intricate analysis of the algorithm for dynamic graphs provide additional insights that are also of interest for the static setting. In particular, we establish self-stabilization of the gradient property within $O(D)$ time. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Data Structures and Algorithms ; F.2.2 ; G.2.2 ; ","Kuhn, Fabian ; Lenzen, Christoph ; Locher, Thomas ; Oshman, Rotem ; ","Optimal Gradient Clock Synchronization in Dynamic Networks  We study the problem of clock synchronization in highly dynamic networks, where communication links can appear or disappear at any time. The nodes in the network are equipped with hardware clocks, but the rate of the hardware clocks can vary arbitrarily within specific bounds, and the estimates that nodes can obtain about the clock values of other nodes are inherently inaccurate. Our goal in this setting is to output a logical clock at each node such that the logical clocks of any two nodes are not too far apart, and nodes that remain close to each other in the network for a long time are better synchronized than distant nodes. This property is called gradient clock synchronization.   Gradient clock synchronization has been widely studied in the static setting, where the network topology does not change. We show that the asymptotically optimal bounds obtained for the static case also apply to our highly dynamic setting: if two nodes remain at distance $d$ from each other for sufficiently long, it is possible to upper bound the difference between their clock values by $O(d \log (D / d))$, where $D$ is the diameter of the network. This is known to be optimal even for static networks. Furthermore, we show that our algorithm has optimal stabilization time: when a path of length $d$ appears between two nodes, the time required until the clock skew between the two nodes is reduced to $O(d \log (D / d))$ is $O(D)$, which we prove to be optimal. Finally, the techniques employed for the more intricate analysis of the algorithm for dynamic graphs provide additional insights that are also of interest for the static setting. In particular, we establish self-stabilization of the gradient property within $O(D)$ time. ",optimal gradient clock synchronization dynamic network study problem clock synchronization highly dynamic network communication link appear disappear time nod network equip hardware clock rate hardware clock vary arbitrarily within specific bound estimate nod obtain clock value nod inherently inaccurate goal set output logical clock node logical clock two nod far apart nod remain close network long time better synchronize distant nod property call gradient clock synchronization gradient clock synchronization widely study static set network topology change show asymptotically optimal bound obtain static case also apply highly dynamic set two nod remain distance sufficiently long possible upper bind difference clock value log diameter network know optimal even static network furthermore show algorithm optimal stabilization time path length appear two nod time require clock skew two nod reduce log prove optimal finally techniques employ intricate analysis algorithm dynamic graph provide additional insights also interest static set particular establish self stabilization gradient property within time,152,6,1005.2894.txt
http://arxiv.org/abs/1005.3010,A Proof for P =? NP Problem,"  The $\textbf{P} =? \textbf{NP}$ problem is an important problem in contemporary mathematics and theoretical computer science. Many proofs have been proposed to this problem. This paper proposes a theoretic proof for $\textbf{P} =? \textbf{NP}$ problem. The central idea of this proof is a recursive definition for Turing machine (shortly TM) that accepts the encoding strings of valid TMs within any given alphabet. As the concepts ""Tao"", ""Yin"" and ""Yang"" described in Chinese philosopy, an infinite sequence of TM, within any given alphabet, is constructed recursively, and it is proven that the sequence includes all valid TMs, and each of them run in polynomial time. Based on these TMs, the class \textbf{D} that includes all decidable languages is defined, and then the class $\textbf{P}$ and $\textbf{NP}$ are defined. By proving $\textbf{P} \subseteq \textbf{NP}$ and $\textbf{NP} \subseteq \textbf{P}$, the result $\textbf{P}=\textbf{NP}$ is proven. ",Computer Science - Computational Complexity ; F.1.3 ; G.2.1 ; ,"Wan, Changlin ; ","A Proof for P =? NP Problem  The $\textbf{P} =? \textbf{NP}$ problem is an important problem in contemporary mathematics and theoretical computer science. Many proofs have been proposed to this problem. This paper proposes a theoretic proof for $\textbf{P} =? \textbf{NP}$ problem. The central idea of this proof is a recursive definition for Turing machine (shortly TM) that accepts the encoding strings of valid TMs within any given alphabet. As the concepts ""Tao"", ""Yin"" and ""Yang"" described in Chinese philosopy, an infinite sequence of TM, within any given alphabet, is constructed recursively, and it is proven that the sequence includes all valid TMs, and each of them run in polynomial time. Based on these TMs, the class \textbf{D} that includes all decidable languages is defined, and then the class $\textbf{P}$ and $\textbf{NP}$ are defined. By proving $\textbf{P} \subseteq \textbf{NP}$ and $\textbf{NP} \subseteq \textbf{P}$, the result $\textbf{P}=\textbf{NP}$ is proven. ",proof np problem textbf textbf np problem important problem contemporary mathematics theoretical computer science many proof propose problem paper propose theoretic proof textbf textbf np problem central idea proof recursive definition turing machine shortly tm accept encode string valid tms within give alphabet concepts tao yin yang describe chinese philosopy infinite sequence tm within give alphabet construct recursively prove sequence include valid tms run polynomial time base tms class textbf include decidable languages define class textbf textbf np define prove textbf subseteq textbf np textbf np subseteq textbf result textbf textbf np prove,93,8,1005.3010.txt
http://arxiv.org/abs/1006.0719,Why Gabor Frames? Two Fundamental Measures of Coherence and Their Role   in Model Selection,"  This paper studies non-asymptotic model selection for the general case of arbitrary design matrices and arbitrary nonzero entries of the signal. In this regard, it generalizes the notion of incoherence in the existing literature on model selection and introduces two fundamental measures of coherence---termed as the worst-case coherence and the average coherence---among the columns of a design matrix. It utilizes these two measures of coherence to provide an in-depth analysis of a simple, model-order agnostic one-step thresholding (OST) algorithm for model selection and proves that OST is feasible for exact as well as partial model selection as long as the design matrix obeys an easily verifiable property. One of the key insights offered by the ensuing analysis in this regard is that OST can successfully carry out model selection even when methods based on convex optimization such as the lasso fail due to the rank deficiency of the submatrices of the design matrix. In addition, the paper establishes that if the design matrix has reasonably small worst-case and average coherence then OST performs near-optimally when either (i) the energy of any nonzero entry of the signal is close to the average signal energy per nonzero entry or (ii) the signal-to-noise ratio in the measurement system is not too high. Finally, two other key contributions of the paper are that (i) it provides bounds on the average coherence of Gaussian matrices and Gabor frames, and (ii) it extends the results on model selection using OST to low-complexity, model-order agnostic recovery of sparse signals with arbitrary nonzero entries. ",Mathematics - Statistics Theory ; Computer Science - Information Theory ; Statistics - Machine Learning ; ,"Bajwa, Waheed U. ; Calderbank, Robert ; Jafarpour, Sina ; ","Why Gabor Frames? Two Fundamental Measures of Coherence and Their Role   in Model Selection  This paper studies non-asymptotic model selection for the general case of arbitrary design matrices and arbitrary nonzero entries of the signal. In this regard, it generalizes the notion of incoherence in the existing literature on model selection and introduces two fundamental measures of coherence---termed as the worst-case coherence and the average coherence---among the columns of a design matrix. It utilizes these two measures of coherence to provide an in-depth analysis of a simple, model-order agnostic one-step thresholding (OST) algorithm for model selection and proves that OST is feasible for exact as well as partial model selection as long as the design matrix obeys an easily verifiable property. One of the key insights offered by the ensuing analysis in this regard is that OST can successfully carry out model selection even when methods based on convex optimization such as the lasso fail due to the rank deficiency of the submatrices of the design matrix. In addition, the paper establishes that if the design matrix has reasonably small worst-case and average coherence then OST performs near-optimally when either (i) the energy of any nonzero entry of the signal is close to the average signal energy per nonzero entry or (ii) the signal-to-noise ratio in the measurement system is not too high. Finally, two other key contributions of the paper are that (i) it provides bounds on the average coherence of Gaussian matrices and Gabor frames, and (ii) it extends the results on model selection using OST to low-complexity, model-order agnostic recovery of sparse signals with arbitrary nonzero entries. ",gabor frame two fundamental measure coherence role model selection paper study non asymptotic model selection general case arbitrary design matrices arbitrary nonzero entries signal regard generalize notion incoherence exist literature model selection introduce two fundamental measure coherence term worst case coherence average coherence among columns design matrix utilize two measure coherence provide depth analysis simple model order agnostic one step thresholding ost algorithm model selection prove ost feasible exact well partial model selection long design matrix obey easily verifiable property one key insights offer ensue analysis regard ost successfully carry model selection even methods base convex optimization lasso fail due rank deficiency submatrices design matrix addition paper establish design matrix reasonably small worst case average coherence ost perform near optimally either energy nonzero entry signal close average signal energy per nonzero entry ii signal noise ratio measurement system high finally two key contributions paper provide bound average coherence gaussian matrices gabor frame ii extend result model selection use ost low complexity model order agnostic recovery sparse signal arbitrary nonzero entries,170,9,1006.0719.txt
http://arxiv.org/abs/1006.1029,Chi-square-based scoring function for categorization of MEDLINE   citations,"  Objectives: Text categorization has been used in biomedical informatics for identifying documents containing relevant topics of interest. We developed a simple method that uses a chi-square-based scoring function to determine the likelihood of MEDLINE citations containing genetic relevant topic. Methods: Our procedure requires construction of a genetic and a nongenetic domain document corpus. We used MeSH descriptors assigned to MEDLINE citations for this categorization task. We compared frequencies of MeSH descriptors between two corpora applying chi-square test. A MeSH descriptor was considered to be a positive indicator if its relative observed frequency in the genetic domain corpus was greater than its relative observed frequency in the nongenetic domain corpus. The output of the proposed method is a list of scores for all the citations, with the highest score given to those citations containing MeSH descriptors typical for the genetic domain. Results: Validation was done on a set of 734 manually annotated MEDLINE citations. It achieved predictive accuracy of 0.87 with 0.69 recall and 0.64 precision. We evaluated the method by comparing it to three machine learning algorithms (support vector machines, decision trees, na\""ive Bayes). Although the differences were not statistically significantly different, results showed that our chi-square scoring performs as good as compared machine learning algorithms. Conclusions: We suggest that the chi-square scoring is an effective solution to help categorize MEDLINE citations. The algorithm is implemented in the BITOLA literature-based discovery support system as a preprocessor for gene symbol disambiguation process. ",Computer Science - Information Retrieval ; Statistics - Applications ; Statistics - Machine Learning ; ,"Kastrin, Andrej ; Peterlin, Borut ; Hristovski, Dimitar ; ","Chi-square-based scoring function for categorization of MEDLINE   citations  Objectives: Text categorization has been used in biomedical informatics for identifying documents containing relevant topics of interest. We developed a simple method that uses a chi-square-based scoring function to determine the likelihood of MEDLINE citations containing genetic relevant topic. Methods: Our procedure requires construction of a genetic and a nongenetic domain document corpus. We used MeSH descriptors assigned to MEDLINE citations for this categorization task. We compared frequencies of MeSH descriptors between two corpora applying chi-square test. A MeSH descriptor was considered to be a positive indicator if its relative observed frequency in the genetic domain corpus was greater than its relative observed frequency in the nongenetic domain corpus. The output of the proposed method is a list of scores for all the citations, with the highest score given to those citations containing MeSH descriptors typical for the genetic domain. Results: Validation was done on a set of 734 manually annotated MEDLINE citations. It achieved predictive accuracy of 0.87 with 0.69 recall and 0.64 precision. We evaluated the method by comparing it to three machine learning algorithms (support vector machines, decision trees, na\""ive Bayes). Although the differences were not statistically significantly different, results showed that our chi-square scoring performs as good as compared machine learning algorithms. Conclusions: We suggest that the chi-square scoring is an effective solution to help categorize MEDLINE citations. The algorithm is implemented in the BITOLA literature-based discovery support system as a preprocessor for gene symbol disambiguation process. ",chi square base score function categorization medline citations objectives text categorization use biomedical informatics identify document contain relevant topics interest develop simple method use chi square base score function determine likelihood medline citations contain genetic relevant topic methods procedure require construction genetic nongenetic domain document corpus use mesh descriptors assign medline citations categorization task compare frequencies mesh descriptors two corpora apply chi square test mesh descriptor consider positive indicator relative observe frequency genetic domain corpus greater relative observe frequency nongenetic domain corpus output propose method list score citations highest score give citations contain mesh descriptors typical genetic domain result validation do set manually annotate medline citations achieve predictive accuracy recall precision evaluate method compare three machine learn algorithms support vector machine decision tree na ive bay although differences statistically significantly different result show chi square score perform good compare machine learn algorithms conclusions suggest chi square score effective solution help categorize medline citations algorithm implement bitola literature base discovery support system preprocessor gene symbol disambiguation process,166,4,1006.1029.txt
http://arxiv.org/abs/1006.1030,Rasch-based high-dimensionality data reduction and class prediction with   applications to microarray gene expression data,"  Class prediction is an important application of microarray gene expression data analysis. The high-dimensionality of microarray data, where number of genes (variables) is very large compared to the number of samples (obser- vations), makes the application of many prediction techniques (e.g., logistic regression, discriminant analysis) difficult. An efficient way to solve this prob- lem is by using dimension reduction statistical techniques. Increasingly used in psychology-related applications, Rasch model (RM) provides an appealing framework for handling high-dimensional microarray data. In this paper, we study the potential of RM-based modeling in dimensionality reduction with binarized microarray gene expression data and investigate its prediction ac- curacy in the context of class prediction using linear discriminant analysis. Two different publicly available microarray data sets are used to illustrate a general framework of the approach. Performance of the proposed method is assessed by re-randomization scheme using principal component analysis (PCA) as a benchmark method. Our results show that RM-based dimension reduction is as effective as PCA-based dimension reduction. The method is general and can be applied to the other high-dimensional data problems. ",Computer Science - Artificial Intelligence ; Statistics - Applications ; Statistics - Methodology ; Statistics - Machine Learning ; ,"Kastrin, Andrej ; Peterlin, Borut ; ","Rasch-based high-dimensionality data reduction and class prediction with   applications to microarray gene expression data  Class prediction is an important application of microarray gene expression data analysis. The high-dimensionality of microarray data, where number of genes (variables) is very large compared to the number of samples (obser- vations), makes the application of many prediction techniques (e.g., logistic regression, discriminant analysis) difficult. An efficient way to solve this prob- lem is by using dimension reduction statistical techniques. Increasingly used in psychology-related applications, Rasch model (RM) provides an appealing framework for handling high-dimensional microarray data. In this paper, we study the potential of RM-based modeling in dimensionality reduction with binarized microarray gene expression data and investigate its prediction ac- curacy in the context of class prediction using linear discriminant analysis. Two different publicly available microarray data sets are used to illustrate a general framework of the approach. Performance of the proposed method is assessed by re-randomization scheme using principal component analysis (PCA) as a benchmark method. Our results show that RM-based dimension reduction is as effective as PCA-based dimension reduction. The method is general and can be applied to the other high-dimensional data problems. ",rasch base high dimensionality data reduction class prediction applications microarray gene expression data class prediction important application microarray gene expression data analysis high dimensionality microarray data number genes variables large compare number sample obser vations make application many prediction techniques logistic regression discriminant analysis difficult efficient way solve prob lem use dimension reduction statistical techniques increasingly use psychology relate applications rasch model rm provide appeal framework handle high dimensional microarray data paper study potential rm base model dimensionality reduction binarized microarray gene expression data investigate prediction ac curacy context class prediction use linear discriminant analysis two different publicly available microarray data set use illustrate general framework approach performance propose method assess randomization scheme use principal component analysis pca benchmark method result show rm base dimension reduction effective pca base dimension reduction method general apply high dimensional data problems,138,10,1006.1030.txt
http://arxiv.org/abs/1006.5892,Computational complexity of reconstruction and isomorphism testing for   designs and line graphs,"  Graphs with high symmetry or regularity are the main source for experimentally hard instances of the notoriously difficult graph isomorphism problem. In this paper, we study the computational complexity of isomorphism testing for line graphs of $t$-$(v,k,\lambda)$ designs. For this class of highly regular graphs, we obtain a worst-case running time of $O(v^{\log v + O(1)})$ for bounded parameters $t,k,\lambda$. In a first step, our approach makes use of the Babai--Luks algorithm to compute canonical forms of $t$-designs. In a second step, we show that $t$-designs can be reconstructed from their line graphs in polynomial-time. The first is algebraic in nature, the second purely combinatorial. For both, profound structural knowledge in design theory is required. Our results extend earlier complexity results about isomorphism testing of graphs generated from Steiner triple systems and block designs. ",Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; F.2.2 ; G.2.2 ; ,"Huber, Michael ; ","Computational complexity of reconstruction and isomorphism testing for   designs and line graphs  Graphs with high symmetry or regularity are the main source for experimentally hard instances of the notoriously difficult graph isomorphism problem. In this paper, we study the computational complexity of isomorphism testing for line graphs of $t$-$(v,k,\lambda)$ designs. For this class of highly regular graphs, we obtain a worst-case running time of $O(v^{\log v + O(1)})$ for bounded parameters $t,k,\lambda$. In a first step, our approach makes use of the Babai--Luks algorithm to compute canonical forms of $t$-designs. In a second step, we show that $t$-designs can be reconstructed from their line graphs in polynomial-time. The first is algebraic in nature, the second purely combinatorial. For both, profound structural knowledge in design theory is required. Our results extend earlier complexity results about isomorphism testing of graphs generated from Steiner triple systems and block designs. ",computational complexity reconstruction isomorphism test design line graph graph high symmetry regularity main source experimentally hard instance notoriously difficult graph isomorphism problem paper study computational complexity isomorphism test line graph lambda design class highly regular graph obtain worst case run time log bound parameters lambda first step approach make use babai luks algorithm compute canonical form design second step show design reconstruct line graph polynomial time first algebraic nature second purely combinatorial profound structural knowledge design theory require result extend earlier complexity result isomorphism test graph generate steiner triple systems block design,92,3,1006.5892.txt
http://arxiv.org/abs/1007.5491,The Coarsest Precongruences Respecting Safety and Liveness Properties,"  This paper characterises the coarsest refinement preorders on labelled transition systems that are precongruences for renaming and partially synchronous interleaving operators, and respect all safety, liveness, and conditional liveness properties, respectively. ",Computer Science - Logic in Computer Science ; ,"van Glabbeek, Rob ; ","The Coarsest Precongruences Respecting Safety and Liveness Properties  This paper characterises the coarsest refinement preorders on labelled transition systems that are precongruences for renaming and partially synchronous interleaving operators, and respect all safety, liveness, and conditional liveness properties, respectively. ",coarsest precongruences respect safety liven properties paper characterise coarsest refinement preorders label transition systems precongruences rename partially synchronous interleave operators respect safety liven conditional liven properties respectively,27,8,1007.5491.txt
http://arxiv.org/abs/1008.0851,Identification of Parametric Underspread Linear Systems and   Super-Resolution Radar,"  Identification of time-varying linear systems, which introduce both time-shifts (delays) and frequency-shifts (Doppler-shifts), is a central task in many engineering applications. This paper studies the problem of identification of underspread linear systems (ULSs), whose responses lie within a unit-area region in the delay Doppler space, by probing them with a known input signal. It is shown that sufficiently-underspread parametric linear systems, described by a finite set of delays and Doppler-shifts, are identifiable from a single observation as long as the time bandwidth product of the input signal is proportional to the square of the total number of delay Doppler pairs in the system. In addition, an algorithm is developed that enables identification of parametric ULSs from an input train of pulses in polynomial time by exploiting recent results on sub-Nyquist sampling for time delay estimation and classical results on recovery of frequencies from a sum of complex exponentials. Finally, application of these results to super-resolution target detection using radar is discussed. Specifically, it is shown that the proposed procedure allows to distinguish between multiple targets with very close proximity in the delay Doppler space, resulting in a resolution that substantially exceeds that of standard matched-filtering based techniques without introducing leakage effects inherent in recently proposed compressed sensing-based radar methods. ",Computer Science - Information Theory ; ,"Bajwa, Waheed U. ; Gedalyahu, Kfir ; Eldar, Yonina C. ; ","Identification of Parametric Underspread Linear Systems and   Super-Resolution Radar  Identification of time-varying linear systems, which introduce both time-shifts (delays) and frequency-shifts (Doppler-shifts), is a central task in many engineering applications. This paper studies the problem of identification of underspread linear systems (ULSs), whose responses lie within a unit-area region in the delay Doppler space, by probing them with a known input signal. It is shown that sufficiently-underspread parametric linear systems, described by a finite set of delays and Doppler-shifts, are identifiable from a single observation as long as the time bandwidth product of the input signal is proportional to the square of the total number of delay Doppler pairs in the system. In addition, an algorithm is developed that enables identification of parametric ULSs from an input train of pulses in polynomial time by exploiting recent results on sub-Nyquist sampling for time delay estimation and classical results on recovery of frequencies from a sum of complex exponentials. Finally, application of these results to super-resolution target detection using radar is discussed. Specifically, it is shown that the proposed procedure allows to distinguish between multiple targets with very close proximity in the delay Doppler space, resulting in a resolution that substantially exceeds that of standard matched-filtering based techniques without introducing leakage effects inherent in recently proposed compressed sensing-based radar methods. ",identification parametric underspread linear systems super resolution radar identification time vary linear systems introduce time shift delay frequency shift doppler shift central task many engineer applications paper study problem identification underspread linear systems ulss whose responses lie within unit area region delay doppler space probe know input signal show sufficiently underspread parametric linear systems describe finite set delay doppler shift identifiable single observation long time bandwidth product input signal proportional square total number delay doppler pair system addition algorithm develop enable identification parametric ulss input train pulse polynomial time exploit recent result sub nyquist sample time delay estimation classical result recovery frequencies sum complex exponentials finally application result super resolution target detection use radar discuss specifically show propose procedure allow distinguish multiple target close proximity delay doppler space result resolution substantially exceed standard match filter base techniques without introduce leakage effect inherent recently propose compress sense base radar methods,149,9,1008.0851.txt
http://arxiv.org/abs/1009.5894,Some Theorems on the Algorithmic Approach to Probability Theory and   Information Theory,"  This is a 1971 dissertation. Only its extended abstract was published at the time. While some results appeared in other publications, a number of details remained unpublished and may still have relevance. ",Computer Science - Information Theory ; ,"Levin, Leonid A. ; ","Some Theorems on the Algorithmic Approach to Probability Theory and   Information Theory  This is a 1971 dissertation. Only its extended abstract was published at the time. While some results appeared in other publications, a number of details remained unpublished and may still have relevance. ",theorems algorithmic approach probability theory information theory dissertation extend abstract publish time result appear publications number detail remain unpublished may still relevance,22,11,1009.5894.txt
http://arxiv.org/abs/1011.0774,"Leaders, Followers, and Community Detectio","  Communities in social networks or graphs are sets of well-connected, overlapping vertices. The effectiveness of a community detection algorithm is determined by accuracy in finding the ground-truth communities and ability to scale with the size of the data. In this work, we provide three contributions. First, we show that a popular measure of accuracy known as the F1 score, which is between 0 and 1, with 1 being perfect detection, has an information lower bound is 0.5. We provide a trivial algorithm that produces communities with an F1 score of 0.5 for any graph! Somewhat surprisingly, we find that popular algorithms such as modularity optimization, BigClam and CESNA have F1 scores less than 0.5 for the popular IMDB graph. To rectify this, as the second contribution we propose a generative model for community formation, the sequential community graph, which is motivated by the formation of social networks. Third, motivated by our generative model, we propose the leader-follower algorithm (LFA). We prove that it recovers all communities for sequential community graphs by establishing a structural result that sequential community graphs are chordal. For a large number of popular social networks, it recovers communities with a much higher F1 score than other popular algorithms. For the IMDB graph, it obtains an F1 score of 0.81. We also propose a modification to the LFA called the fast leader-follower algorithm (FLFA) which in addition to being highly accurate, is also fast, with a scaling that is almost linear in the network size. ",Statistics - Machine Learning ; Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Parthasarathy, Dhruv ; Shah, Devavrat ; Zaman, Tauhid ; ","Leaders, Followers, and Community Detectio  Communities in social networks or graphs are sets of well-connected, overlapping vertices. The effectiveness of a community detection algorithm is determined by accuracy in finding the ground-truth communities and ability to scale with the size of the data. In this work, we provide three contributions. First, we show that a popular measure of accuracy known as the F1 score, which is between 0 and 1, with 1 being perfect detection, has an information lower bound is 0.5. We provide a trivial algorithm that produces communities with an F1 score of 0.5 for any graph! Somewhat surprisingly, we find that popular algorithms such as modularity optimization, BigClam and CESNA have F1 scores less than 0.5 for the popular IMDB graph. To rectify this, as the second contribution we propose a generative model for community formation, the sequential community graph, which is motivated by the formation of social networks. Third, motivated by our generative model, we propose the leader-follower algorithm (LFA). We prove that it recovers all communities for sequential community graphs by establishing a structural result that sequential community graphs are chordal. For a large number of popular social networks, it recovers communities with a much higher F1 score than other popular algorithms. For the IMDB graph, it obtains an F1 score of 0.81. We also propose a modification to the LFA called the fast leader-follower algorithm (FLFA) which in addition to being highly accurate, is also fast, with a scaling that is almost linear in the network size. ",leaders followers community detectio communities social network graph set well connect overlap vertices effectiveness community detection algorithm determine accuracy find grind truth communities ability scale size data work provide three contributions first show popular measure accuracy know score perfect detection information lower bind provide trivial algorithm produce communities score graph somewhat surprisingly find popular algorithms modularity optimization bigclam cesna score less popular imdb graph rectify second contribution propose generative model community formation sequential community graph motivate formation social network third motivate generative model propose leader follower algorithm lfa prove recover communities sequential community graph establish structural result sequential community graph chordal large number popular social network recover communities much higher score popular algorithms imdb graph obtain score also propose modification lfa call fast leader follower algorithm flfa addition highly accurate also fast scale almost linear network size,137,3,1011.0774.txt
http://arxiv.org/abs/1012.4019,Constructing elliptic curve isogenies in quantum subexponential time,"  Given two elliptic curves over a finite field having the same cardinality and endomorphism ring, it is known that the curves admit an isogeny between them, but finding such an isogeny is believed to be computationally difficult. The fastest known classical algorithm takes exponential time, and prior to our work no faster quantum algorithm was known. Recently, public-key cryptosystems based on the presumed hardness of this problem have been proposed as candidates for post-quantum cryptography. In this paper, we give a subexponential-time quantum algorithm for constructing isogenies, assuming the Generalized Riemann Hypothesis (but with no other assumptions). Our algorithm is based on a reduction to a hidden shift problem, together with a new subexponential-time algorithm for evaluating isogenies from kernel ideals (under only GRH), and represents the first nontrivial application of Kuperberg's quantum algorithm for the hidden shift problem. This result suggests that isogeny-based cryptosystems may be uncompetitive with more mainstream quantum-resistant cryptosystems such as lattice-based cryptosystems. ",Quantum Physics ; Computer Science - Computational Complexity ; Mathematics - Number Theory ; ,"Childs, Andrew M. ; Jao, David ; Soukharev, Vladimir ; ","Constructing elliptic curve isogenies in quantum subexponential time  Given two elliptic curves over a finite field having the same cardinality and endomorphism ring, it is known that the curves admit an isogeny between them, but finding such an isogeny is believed to be computationally difficult. The fastest known classical algorithm takes exponential time, and prior to our work no faster quantum algorithm was known. Recently, public-key cryptosystems based on the presumed hardness of this problem have been proposed as candidates for post-quantum cryptography. In this paper, we give a subexponential-time quantum algorithm for constructing isogenies, assuming the Generalized Riemann Hypothesis (but with no other assumptions). Our algorithm is based on a reduction to a hidden shift problem, together with a new subexponential-time algorithm for evaluating isogenies from kernel ideals (under only GRH), and represents the first nontrivial application of Kuperberg's quantum algorithm for the hidden shift problem. This result suggests that isogeny-based cryptosystems may be uncompetitive with more mainstream quantum-resistant cryptosystems such as lattice-based cryptosystems. ",construct elliptic curve isogenies quantum subexponential time give two elliptic curve finite field cardinality endomorphism ring know curve admit isogeny find isogeny believe computationally difficult fastest know classical algorithm take exponential time prior work faster quantum algorithm know recently public key cryptosystems base presume hardness problem propose candidates post quantum cryptography paper give subexponential time quantum algorithm construct isogenies assume generalize riemann hypothesis assumptions algorithm base reduction hide shift problem together new subexponential time algorithm evaluate isogenies kernel ideals grh represent first nontrivial application kuperberg quantum algorithm hide shift problem result suggest isogeny base cryptosystems may uncompetitive mainstream quantum resistant cryptosystems lattice base cryptosystems,104,4,1012.4019.txt
http://arxiv.org/abs/1012.4290,Bit recycling for scaling random number generators,"  Many Random Number Generators (RNG) are available nowadays; they are divided in two categories, hardware RNG, that provide ""true"" random numbers, and algorithmic RNG, that generate pseudo random numbers (PRNG). Both types usually generate random numbers $(X_n)$ as independent uniform samples in a range $0,\cdots,2^{b-1}$, with $b = 8, 16, 32$ or $b = 64$. In applications, it is instead sometimes desirable to draw random numbers as independent uniform samples $(Y_n)$ in a range $1, \cdots, M$, where moreover M may change between drawings. Transforming the sequence $(X_n)$ to $(Y_n)$ is sometimes known as scaling. We discuss different methods for scaling the RNG, both in term of mathematical efficiency and of computational speed. ",Computer Science - Information Theory ; Mathematics - Numerical Analysis ; Mathematics - Probability ; ,"Mennucci, Andrea C. G. ; ","Bit recycling for scaling random number generators  Many Random Number Generators (RNG) are available nowadays; they are divided in two categories, hardware RNG, that provide ""true"" random numbers, and algorithmic RNG, that generate pseudo random numbers (PRNG). Both types usually generate random numbers $(X_n)$ as independent uniform samples in a range $0,\cdots,2^{b-1}$, with $b = 8, 16, 32$ or $b = 64$. In applications, it is instead sometimes desirable to draw random numbers as independent uniform samples $(Y_n)$ in a range $1, \cdots, M$, where moreover M may change between drawings. Transforming the sequence $(X_n)$ to $(Y_n)$ is sometimes known as scaling. We discuss different methods for scaling the RNG, both in term of mathematical efficiency and of computational speed. ",bite recycle scale random number generators many random number generators rng available nowadays divide two categories hardware rng provide true random number algorithmic rng generate pseudo random number prng type usually generate random number independent uniform sample range cdots applications instead sometimes desirable draw random number independent uniform sample range cdots moreover may change draw transform sequence sometimes know scale discuss different methods scale rng term mathematical efficiency computational speed,70,12,1012.4290.txt
http://arxiv.org/abs/1101.1169,Almost Settling the Hardness of Noncommutative Determinant,"  In this paper, we study the complexity of computing the determinant of a matrix over a non-commutative algebra. In particular, we ask the question, ""over which algebras, is the determinant easier to compute than the permanent?"" Towards resolving this question, we show the following hardness and easiness of noncommutative determinant computation.   * [Hardness] Computing the determinant of an n \times n matrix whose entries are themselves 2 \times 2 matrices over a field is as hard as computing the permanent over the field. This extends the recent result of Arvind and Srinivasan, who proved a similar result which however required the entries to be of linear dimension.   * [Easiness] Determinant of an n \times n matrix whose entries are themselves d \times d upper triangular matrices can be computed in poly(n^d) time.   Combining the above with the decomposition theorem of finite dimensional algebras (in particular exploiting the simple structure of 2 \times 2 matrix algebras), we can extend the above hardness and easiness statements to more general algebras as follows. Let A be a finite dimensional algebra over a finite field with radical R(A).   * [Hardness] If the quotient A/R(A) is non-commutative, then computing the determinant over the algebra A is as hard as computing the permanent.   * [Easiness] If the quotient A/R(A) is commutative and furthermore, R(A) has nilpotency index d (i.e., the smallest d such that R(A)d = 0), then there exists a poly(n^d)-time algorithm that computes determinants over the algebra A.   In particular, for any constant dimensional algebra A over a finite field, since the nilpotency index of R(A) is at most a constant, we have the following dichotomy theorem: if A/R(A) is commutative, then efficient determinant computation is feasible and otherwise determinant is as hard as permanent. ",Computer Science - Computational Complexity ; ,"Chien, Steve ; Harsha, Prahladh ; Sinclair, Alistair ; Srinivasan, Srikanth ; ","Almost Settling the Hardness of Noncommutative Determinant  In this paper, we study the complexity of computing the determinant of a matrix over a non-commutative algebra. In particular, we ask the question, ""over which algebras, is the determinant easier to compute than the permanent?"" Towards resolving this question, we show the following hardness and easiness of noncommutative determinant computation.   * [Hardness] Computing the determinant of an n \times n matrix whose entries are themselves 2 \times 2 matrices over a field is as hard as computing the permanent over the field. This extends the recent result of Arvind and Srinivasan, who proved a similar result which however required the entries to be of linear dimension.   * [Easiness] Determinant of an n \times n matrix whose entries are themselves d \times d upper triangular matrices can be computed in poly(n^d) time.   Combining the above with the decomposition theorem of finite dimensional algebras (in particular exploiting the simple structure of 2 \times 2 matrix algebras), we can extend the above hardness and easiness statements to more general algebras as follows. Let A be a finite dimensional algebra over a finite field with radical R(A).   * [Hardness] If the quotient A/R(A) is non-commutative, then computing the determinant over the algebra A is as hard as computing the permanent.   * [Easiness] If the quotient A/R(A) is commutative and furthermore, R(A) has nilpotency index d (i.e., the smallest d such that R(A)d = 0), then there exists a poly(n^d)-time algorithm that computes determinants over the algebra A.   In particular, for any constant dimensional algebra A over a finite field, since the nilpotency index of R(A) is at most a constant, we have the following dichotomy theorem: if A/R(A) is commutative, then efficient determinant computation is feasible and otherwise determinant is as hard as permanent. ",almost settle hardness noncommutative determinant paper study complexity compute determinant matrix non commutative algebra particular ask question algebras determinant easier compute permanent towards resolve question show follow hardness easiness noncommutative determinant computation hardness compute determinant time matrix whose entries time matrices field hard compute permanent field extend recent result arvind srinivasan prove similar result however require entries linear dimension easiness determinant time matrix whose entries time upper triangular matrices compute poly time combine decomposition theorem finite dimensional algebras particular exploit simple structure time matrix algebras extend hardness easiness statements general algebras follow let finite dimensional algebra finite field radical hardness quotient non commutative compute determinant algebra hard compute permanent easiness quotient commutative furthermore nilpotency index smallest exist poly time algorithm compute determinants algebra particular constant dimensional algebra finite field since nilpotency index constant follow dichotomy theorem commutative efficient determinant computation feasible otherwise determinant hard permanent,145,4,1101.1169.txt
http://arxiv.org/abs/1101.1477,Asynchronous Code-Division Random Access Using Convex Optimization,"  Many applications in cellular systems and sensor networks involve a random subset of a large number of users asynchronously reporting activity to a base station. This paper examines the problem of multiuser detection (MUD) in random access channels for such applications. Traditional orthogonal signaling ignores the random nature of user activity in this problem and limits the total number of users to be on the order of the number of signal space dimensions. Contention-based schemes, on the other hand, suffer from delays caused by colliding transmissions and the hidden node problem. In contrast, this paper presents a novel pairing of an asynchronous non-orthogonal code-division random access scheme with a convex optimization-based MUD algorithm that overcomes the issues associated with orthogonal signaling and contention-based methods. Two key distinguishing features of the proposed MUD algorithm are that it does not require knowledge of the delay or channel state information of every user and it has polynomial-time computational complexity. The main analytical contribution of this paper is the relationship between the performance of the proposed MUD algorithm in the presence of arbitrary or random delays and two simple metrics of the set of user codewords. The study of these metrics is then focused on two specific sets of codewords, random binary codewords and specially constructed algebraic codewords, for asynchronous random access. The ensuing analysis confirms that the proposed scheme together with either of these two codeword sets significantly outperforms the orthogonal signaling-based random access in terms of the total number of users in the system. ",Computer Science - Information Theory ; ,"Applebaum, Lorne ; Bajwa, Waheed U. ; Duarte, Marco F. ; Calderbank, Robert ; ","Asynchronous Code-Division Random Access Using Convex Optimization  Many applications in cellular systems and sensor networks involve a random subset of a large number of users asynchronously reporting activity to a base station. This paper examines the problem of multiuser detection (MUD) in random access channels for such applications. Traditional orthogonal signaling ignores the random nature of user activity in this problem and limits the total number of users to be on the order of the number of signal space dimensions. Contention-based schemes, on the other hand, suffer from delays caused by colliding transmissions and the hidden node problem. In contrast, this paper presents a novel pairing of an asynchronous non-orthogonal code-division random access scheme with a convex optimization-based MUD algorithm that overcomes the issues associated with orthogonal signaling and contention-based methods. Two key distinguishing features of the proposed MUD algorithm are that it does not require knowledge of the delay or channel state information of every user and it has polynomial-time computational complexity. The main analytical contribution of this paper is the relationship between the performance of the proposed MUD algorithm in the presence of arbitrary or random delays and two simple metrics of the set of user codewords. The study of these metrics is then focused on two specific sets of codewords, random binary codewords and specially constructed algebraic codewords, for asynchronous random access. The ensuing analysis confirms that the proposed scheme together with either of these two codeword sets significantly outperforms the orthogonal signaling-based random access in terms of the total number of users in the system. ",asynchronous code division random access use convex optimization many applications cellular systems sensor network involve random subset large number users asynchronously report activity base station paper examine problem multiuser detection mud random access channel applications traditional orthogonal signal ignore random nature user activity problem limit total number users order number signal space dimension contention base scheme hand suffer delay cause collide transmissions hide node problem contrast paper present novel pair asynchronous non orthogonal code division random access scheme convex optimization base mud algorithm overcome issue associate orthogonal signal contention base methods two key distinguish feature propose mud algorithm require knowledge delay channel state information every user polynomial time computational complexity main analytical contribution paper relationship performance propose mud algorithm presence arbitrary random delay two simple metrics set user codewords study metrics focus two specific set codewords random binary codewords specially construct algebraic codewords asynchronous random access ensue analysis confirm propose scheme together either two codeword set significantly outperform orthogonal signal base random access term total number users system,168,12,1101.1477.txt
http://arxiv.org/abs/1101.1640,Restarting Automata with Auxiliary Symbols and Small Lookahead,"  We present a study on lookahead hierarchies for restarting automata with auxiliary symbols and small lookahead. In particular, we show that there are just two different classes of languages recognised RRWW automata, through the restriction of lookahead size. We also show that the respective (left-) monotone restarting automaton models characterise the context-free languages and that the respective right-left-monotone restarting automata characterise the linear languages both with just lookahead length 2. ",Computer Science - Formal Languages and Automata Theory ; ,"Schluter, Natalie ; ","Restarting Automata with Auxiliary Symbols and Small Lookahead  We present a study on lookahead hierarchies for restarting automata with auxiliary symbols and small lookahead. In particular, we show that there are just two different classes of languages recognised RRWW automata, through the restriction of lookahead size. We also show that the respective (left-) monotone restarting automaton models characterise the context-free languages and that the respective right-left-monotone restarting automata characterise the linear languages both with just lookahead length 2. ",restart automata auxiliary symbols small lookahead present study lookahead hierarchies restart automata auxiliary symbols small lookahead particular show two different class languages recognise rrww automata restriction lookahead size also show respective leave monotone restart automaton model characterise context free languages respective right leave monotone restart automata characterise linear languages lookahead length,51,14,1101.1640.txt
http://arxiv.org/abs/1101.1934,Bit-wise Unequal Error Protection for Variable Length Block Codes with   Feedback,"  The bit-wise unequal error protection problem, for the case when the number of groups of bits $\ell$ is fixed, is considered for variable length block codes with feedback. An encoding scheme based on fixed length block codes with erasures is used to establish inner bounds to the achievable performance for finite expected decoding time. A new technique for bounding the performance of variable length block codes is used to establish outer bounds to the performance for a given expected decoding time. The inner and the outer bounds match one another asymptotically and characterize the achievable region of rate-exponent vectors, completely. The single message message-wise unequal error protection problem for variable length block codes with feedback is also solved as a necessary step on the way. ",Computer Science - Information Theory ; ,"Nakiboglu, Baris ; Gorantla, Siva K. ; Zheng, Lizhong ; Coleman, Todd P. ; ","Bit-wise Unequal Error Protection for Variable Length Block Codes with   Feedback  The bit-wise unequal error protection problem, for the case when the number of groups of bits $\ell$ is fixed, is considered for variable length block codes with feedback. An encoding scheme based on fixed length block codes with erasures is used to establish inner bounds to the achievable performance for finite expected decoding time. A new technique for bounding the performance of variable length block codes is used to establish outer bounds to the performance for a given expected decoding time. The inner and the outer bounds match one another asymptotically and characterize the achievable region of rate-exponent vectors, completely. The single message message-wise unequal error protection problem for variable length block codes with feedback is also solved as a necessary step on the way. ",bite wise unequal error protection variable length block cod feedback bite wise unequal error protection problem case number group bits ell fix consider variable length block cod feedback encode scheme base fix length block cod erasures use establish inner bound achievable performance finite expect decode time new technique bound performance variable length block cod use establish outer bound performance give expect decode time inner outer bound match one another asymptotically characterize achievable region rate exponent vectors completely single message message wise unequal error protection problem variable length block cod feedback also solve necessary step way,95,5,1101.1934.txt
http://arxiv.org/abs/1101.5460,A Human-Centric Approach to Group-Based Context-Awareness,"  The emerging need for qualitative approaches in context-aware information processing calls for proper modeling of context information and efficient handling of its inherent uncertainty resulted from human interpretation and usage. Many of the current approaches to context-awareness either lack a solid theoretical basis for modeling or ignore important requirements such as modularity, high-order uncertainty management and group-based context-awareness. Therefore, their real-world application and extendability remains limited. In this paper, we present f-Context as a service-based context-awareness framework, based on language-action perspective (LAP) theory for modeling. Then we identify some of the complex, informational parts of context which contain high-order uncertainties due to differences between members of the group in defining them. An agent-based perceptual computer architecture is proposed for implementing f-Context that uses computing with words (CWW) for handling uncertainty. The feasibility of f-Context is analyzed using a realistic scenario involving a group of mobile users. We believe that the proposed approach can open the door to future research on context-awareness by offering a theoretical foundation based on human communication, and a service-based layered architecture which exploits CWW for context-aware, group-based and platform-independent access to information systems. ",Computer Science - Artificial Intelligence ; Computer Science - Human-Computer Interaction ; ,"Ghadiri, Nasser ; Baraani-Dastjerdi, Ahmad ; Ghasem-Aghaee, Nasser ; Nematbakhsh, Mohammad A. ; ","A Human-Centric Approach to Group-Based Context-Awareness  The emerging need for qualitative approaches in context-aware information processing calls for proper modeling of context information and efficient handling of its inherent uncertainty resulted from human interpretation and usage. Many of the current approaches to context-awareness either lack a solid theoretical basis for modeling or ignore important requirements such as modularity, high-order uncertainty management and group-based context-awareness. Therefore, their real-world application and extendability remains limited. In this paper, we present f-Context as a service-based context-awareness framework, based on language-action perspective (LAP) theory for modeling. Then we identify some of the complex, informational parts of context which contain high-order uncertainties due to differences between members of the group in defining them. An agent-based perceptual computer architecture is proposed for implementing f-Context that uses computing with words (CWW) for handling uncertainty. The feasibility of f-Context is analyzed using a realistic scenario involving a group of mobile users. We believe that the proposed approach can open the door to future research on context-awareness by offering a theoretical foundation based on human communication, and a service-based layered architecture which exploits CWW for context-aware, group-based and platform-independent access to information systems. ",human centric approach group base context awareness emerge need qualitative approach context aware information process call proper model context information efficient handle inherent uncertainty result human interpretation usage many current approach context awareness either lack solid theoretical basis model ignore important requirements modularity high order uncertainty management group base context awareness therefore real world application extendability remain limit paper present context service base context awareness framework base language action perspective lap theory model identify complex informational part context contain high order uncertainties due differences members group define agent base perceptual computer architecture propose implement context use compute word cww handle uncertainty feasibility context analyze use realistic scenario involve group mobile users believe propose approach open door future research context awareness offer theoretical foundation base human communication service base layer architecture exploit cww context aware group base platform independent access information systems,141,11,1101.5460.txt
http://arxiv.org/abs/1102.0969,On the Complexity of Newman's Community Finding Approach for Biological   and Social Networks,"  Given a graph of interactions, a module (also called a community or cluster) is a subset of nodes whose fitness is a function of the statistical significance of the pairwise interactions of nodes in the module. The topic of this paper is a model-based community finding approach, commonly referred to as modularity clustering, that was originally proposed by Newman and has subsequently been extremely popular in practice. Various heuristic methods are currently employed for finding the optimal solution. However, the exact computational complexity of this approach is still largely unknown.   To this end, we initiate a systematic study of the computational complexity of modularity clustering. Due to the specific quadratic nature of the modularity function, it is necessary to study its value on sparse graphs and dense graphs separately. Our main results include a (1+\eps)-inapproximability for dense graphs and a logarithmic approximation for sparse graphs. We make use of several combinatorial properties of modularity to get these results. These are the first non-trivial approximability results beyond the previously known NP-hardness results. ","Physics - Physics and Society ; Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; Computer Science - Social and Information Networks ; 68Q25, 68R01, 05C85 ; F.2.2 ; G.2.2 ; ","DasGupta, Bhaskar ; Desai, Devendra ; ","On the Complexity of Newman's Community Finding Approach for Biological   and Social Networks  Given a graph of interactions, a module (also called a community or cluster) is a subset of nodes whose fitness is a function of the statistical significance of the pairwise interactions of nodes in the module. The topic of this paper is a model-based community finding approach, commonly referred to as modularity clustering, that was originally proposed by Newman and has subsequently been extremely popular in practice. Various heuristic methods are currently employed for finding the optimal solution. However, the exact computational complexity of this approach is still largely unknown.   To this end, we initiate a systematic study of the computational complexity of modularity clustering. Due to the specific quadratic nature of the modularity function, it is necessary to study its value on sparse graphs and dense graphs separately. Our main results include a (1+\eps)-inapproximability for dense graphs and a logarithmic approximation for sparse graphs. We make use of several combinatorial properties of modularity to get these results. These are the first non-trivial approximability results beyond the previously known NP-hardness results. ",complexity newman community find approach biological social network give graph interactions module also call community cluster subset nod whose fitness function statistical significance pairwise interactions nod module topic paper model base community find approach commonly refer modularity cluster originally propose newman subsequently extremely popular practice various heuristic methods currently employ find optimal solution however exact computational complexity approach still largely unknown end initiate systematic study computational complexity modularity cluster due specific quadratic nature modularity function necessary study value sparse graph dense graph separately main result include eps inapproximability dense graph logarithmic approximation sparse graph make use several combinatorial properties modularity get result first non trivial approximability result beyond previously know np hardness result,113,3,1102.0969.txt
http://arxiv.org/abs/1102.4802,A generalization of heterochromatic graphs,"  In 2006, Suzuki, and Akbari & Alipour independently presented a necessary and sufficient condition for edge-colored graphs to have a heterochromatic spanning tree, where a heterochromatic spanning tree is a spanning tree whose edges have distinct colors. In this paper, we propose $f$-chromatic graphs as a generalization of heterochromatic graphs. An edge-colored graph is $f$-chromatic if each color $c$ appears on at most $f(c)$ edges. We also present a necessary and sufficient condition for edge-colored graphs to have an $f$-chromatic spanning forest with exactly $m$ components. Moreover, using this criterion, we show that a $g$-chromatic graph $G$ of order $n$ with $|E(G)|>\binom{n-m}{2}$ has an $f$-chromatic spanning forest with exactly $m$ ($1 \le m \le n-1$) components if $g(c) \le \frac{|E(G)|}{n-m}f(c)$ for any color $c$. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C05, 05C15 ; ","Suzuki, Kazuhiro ; ","A generalization of heterochromatic graphs  In 2006, Suzuki, and Akbari & Alipour independently presented a necessary and sufficient condition for edge-colored graphs to have a heterochromatic spanning tree, where a heterochromatic spanning tree is a spanning tree whose edges have distinct colors. In this paper, we propose $f$-chromatic graphs as a generalization of heterochromatic graphs. An edge-colored graph is $f$-chromatic if each color $c$ appears on at most $f(c)$ edges. We also present a necessary and sufficient condition for edge-colored graphs to have an $f$-chromatic spanning forest with exactly $m$ components. Moreover, using this criterion, we show that a $g$-chromatic graph $G$ of order $n$ with $|E(G)|>\binom{n-m}{2}$ has an $f$-chromatic spanning forest with exactly $m$ ($1 \le m \le n-1$) components if $g(c) \le \frac{|E(G)|}{n-m}f(c)$ for any color $c$. ",generalization heterochromatic graph suzuki akbari alipour independently present necessary sufficient condition edge color graph heterochromatic span tree heterochromatic span tree span tree whose edge distinct color paper propose chromatic graph generalization heterochromatic graph edge color graph chromatic color appear edge also present necessary sufficient condition edge color graph chromatic span forest exactly components moreover use criterion show chromatic graph order binom chromatic span forest exactly le le components le frac color,71,13,1102.4802.txt
http://arxiv.org/abs/1103.1091,A generalization of Hopcroft-Karp algorithm for semi-matchings and   covers in bipartite graphs (Maximum semi-matching problem in bipartite   graphs),"  An $(f,g)$-semi-matching in a bipartite graph $G=(U \cup V,E)$ is a set of edges $M \subseteq E$ such that each vertex $u\in U$ is incident with at most $f(u)$ edges of $M$, and each vertex $v\in V$ is incident with at most $g(v)$ edges of $M$. In this paper we give an algorithm that for a graph with $n$ vertices and $m$ edges, $n\le m$, constructs a maximum $(f,g)$-semi-matching in running time $O(m\cdot \min (\sqrt{\sum_{u\in U}f(u)}, \sqrt{\sum_{v\in V}g(v)}))$. Using the reduction of [5], our result on maximum $(f,g)$-semi-matching problem directly implies an algorithm for the optimal semi-matching problem with running time $O(\sqrt{n}m \log n)$. ",Computer Science - Data Structures and Algorithms ; ,"Katrenic, Ján ; Semanisin, Gabriel ; ","A generalization of Hopcroft-Karp algorithm for semi-matchings and   covers in bipartite graphs (Maximum semi-matching problem in bipartite   graphs)  An $(f,g)$-semi-matching in a bipartite graph $G=(U \cup V,E)$ is a set of edges $M \subseteq E$ such that each vertex $u\in U$ is incident with at most $f(u)$ edges of $M$, and each vertex $v\in V$ is incident with at most $g(v)$ edges of $M$. In this paper we give an algorithm that for a graph with $n$ vertices and $m$ edges, $n\le m$, constructs a maximum $(f,g)$-semi-matching in running time $O(m\cdot \min (\sqrt{\sum_{u\in U}f(u)}, \sqrt{\sum_{v\in V}g(v)}))$. Using the reduction of [5], our result on maximum $(f,g)$-semi-matching problem directly implies an algorithm for the optimal semi-matching problem with running time $O(\sqrt{n}m \log n)$. ",generalization hopcroft karp algorithm semi match cover bipartite graph maximum semi match problem bipartite graph semi match bipartite graph cup set edge subseteq vertex incident edge vertex incident edge paper give algorithm graph vertices edge le construct maximum semi match run time cdot min sqrt sum sqrt sum use reduction result maximum semi match problem directly imply algorithm optimal semi match problem run time sqrt log,66,3,1103.1091.txt
http://arxiv.org/abs/1103.1951,Constructive proof of the existence of equilibrium in competitive   economy with sequentially locally non-constant excess demand functions,"  We present a constructive proof of the existence of an equilibrium in a competitive economy with sequentially locally non-constant excess demand functions. And we will show that the existence of such an equilibrium implies Sperner's lemma. Since the existence of an equilibrium is derived from the existence an approximate fixed point of uniformly continuous functions, which is derived from Sperner's lemma, the existence of an equilibrium in a competitive economy with sequentially locally non-constant excess demand functions is equivalent to Sperner's lemma. ",Mathematics - Logic ; Computer Science - Computer Science and Game Theory ; ,"Tanaka, Yasuhito ; ","Constructive proof of the existence of equilibrium in competitive   economy with sequentially locally non-constant excess demand functions  We present a constructive proof of the existence of an equilibrium in a competitive economy with sequentially locally non-constant excess demand functions. And we will show that the existence of such an equilibrium implies Sperner's lemma. Since the existence of an equilibrium is derived from the existence an approximate fixed point of uniformly continuous functions, which is derived from Sperner's lemma, the existence of an equilibrium in a competitive economy with sequentially locally non-constant excess demand functions is equivalent to Sperner's lemma. ",constructive proof existence equilibrium competitive economy sequentially locally non constant excess demand function present constructive proof existence equilibrium competitive economy sequentially locally non constant excess demand function show existence equilibrium imply sperner lemma since existence equilibrium derive existence approximate fix point uniformly continuous function derive sperner lemma existence equilibrium competitive economy sequentially locally non constant excess demand function equivalent sperner lemma,61,7,1103.1951.txt
http://arxiv.org/abs/1103.1980,Constructive proof of the existence of Nash Equilibrium in a finite   strategic game with sequentially locally non-constant payoff functions by   Sperner's lemma,  Using Sperner's lemma for modified partition of a simplex we will constructively prove the existence of a Nash equilibrium in a finite strategic game with sequentially locally non-constant payoff functions. We follow the Bishop style constructive mathematics. ,Mathematics - Logic ; Computer Science - Computer Science and Game Theory ; ,"Tanaka, Yasuhito ; ",Constructive proof of the existence of Nash Equilibrium in a finite   strategic game with sequentially locally non-constant payoff functions by   Sperner's lemma  Using Sperner's lemma for modified partition of a simplex we will constructively prove the existence of a Nash equilibrium in a finite strategic game with sequentially locally non-constant payoff functions. We follow the Bishop style constructive mathematics. ,constructive proof existence nash equilibrium finite strategic game sequentially locally non constant payoff function sperner lemma use sperner lemma modify partition simplex constructively prove existence nash equilibrium finite strategic game sequentially locally non constant payoff function follow bishop style constructive mathematics,41,7,1103.1980.txt
http://arxiv.org/abs/1104.0471,Quantum Bayesian implementation,"  Bayesian implementation concerns decision making problems when agents have incomplete information. This paper proposes that the traditional sufficient conditions for Bayesian implementation shall be amended by virtue of a quantum Bayesian mechanism. In addition, by using an algorithmic Bayesian mechanism, this amendment holds in the macro world. ","Physics - Data Analysis, Statistics and Probability ; Computer Science - Computer Science and Game Theory ; ","Wu, Haoyang ; ","Quantum Bayesian implementation  Bayesian implementation concerns decision making problems when agents have incomplete information. This paper proposes that the traditional sufficient conditions for Bayesian implementation shall be amended by virtue of a quantum Bayesian mechanism. In addition, by using an algorithmic Bayesian mechanism, this amendment holds in the macro world. ",quantum bayesian implementation bayesian implementation concern decision make problems agents incomplete information paper propose traditional sufficient condition bayesian implementation shall amend virtue quantum bayesian mechanism addition use algorithmic bayesian mechanism amendment hold macro world,34,8,1104.0471.txt
http://arxiv.org/abs/1104.0746,"Quantifier Elimination over Finite Fields Using Gr\""obner Bases","  We give an algebraic quantifier elimination algorithm for the first-order theory over any given finite field using Gr\""obner basis methods. The algorithm relies on the strong Nullstellensatz and properties of elimination ideals over finite fields. We analyze the theoretical complexity of the algorithm and show its application in the formal analysis of a biological controller model. ",Computer Science - Symbolic Computation ; Computer Science - Logic in Computer Science ; ,"Gao, Sicun ; Platzer, André ; Clarke, Edmund M. ; ","Quantifier Elimination over Finite Fields Using Gr\""obner Bases  We give an algebraic quantifier elimination algorithm for the first-order theory over any given finite field using Gr\""obner basis methods. The algorithm relies on the strong Nullstellensatz and properties of elimination ideals over finite fields. We analyze the theoretical complexity of the algorithm and show its application in the formal analysis of a biological controller model. ",quantifier elimination finite field use gr obner base give algebraic quantifier elimination algorithm first order theory give finite field use gr obner basis methods algorithm rely strong nullstellensatz properties elimination ideals finite field analyze theoretical complexity algorithm show application formal analysis biological controller model,44,4,1104.0746.txt
http://arxiv.org/abs/1104.2373,Hybrid Deterministic-Stochastic Methods for Data Fitting,"  Many structured data-fitting applications require the solution of an optimization problem involving a sum over a potentially large number of measurements. Incremental gradient algorithms offer inexpensive iterations by sampling a subset of the terms in the sum. These methods can make great progress initially, but often slow as they approach a solution. In contrast, full-gradient methods achieve steady convergence at the expense of evaluating the full objective and gradient on each iteration. We explore hybrid methods that exhibit the benefits of both approaches. Rate-of-convergence analysis shows that by controlling the sample size in an incremental gradient algorithm, it is possible to maintain the steady convergence rates of full-gradient methods. We detail a practical quasi-Newton implementation based on this approach. Numerical experiments illustrate its potential benefits. ",Computer Science - Numerical Analysis ; Computer Science - Systems and Control ; Mathematics - Optimization and Control ; Statistics - Machine Learning ; ,"Friedlander, Michael P. ; Schmidt, Mark ; ","Hybrid Deterministic-Stochastic Methods for Data Fitting  Many structured data-fitting applications require the solution of an optimization problem involving a sum over a potentially large number of measurements. Incremental gradient algorithms offer inexpensive iterations by sampling a subset of the terms in the sum. These methods can make great progress initially, but often slow as they approach a solution. In contrast, full-gradient methods achieve steady convergence at the expense of evaluating the full objective and gradient on each iteration. We explore hybrid methods that exhibit the benefits of both approaches. Rate-of-convergence analysis shows that by controlling the sample size in an incremental gradient algorithm, it is possible to maintain the steady convergence rates of full-gradient methods. We detail a practical quasi-Newton implementation based on this approach. Numerical experiments illustrate its potential benefits. ",hybrid deterministic stochastic methods data fit many structure data fit applications require solution optimization problem involve sum potentially large number measurements incremental gradient algorithms offer inexpensive iterations sample subset term sum methods make great progress initially often slow approach solution contrast full gradient methods achieve steady convergence expense evaluate full objective gradient iteration explore hybrid methods exhibit benefit approach rate convergence analysis show control sample size incremental gradient algorithm possible maintain steady convergence rat full gradient methods detail practical quasi newton implementation base approach numerical experiment illustrate potential benefit,89,12,1104.2373.txt
http://arxiv.org/abs/1104.4465,Randomness and Differentiability,"  We characterize some major algorithmic randomness notions via differentiability of effective functions.   (1) As the main result we show that a real number z in [0,1] is computably random if and only if each nondecreasing computable function [0,1]->R is differentiable at z.   (2) We prove that a real number z in [0,1] is weakly 2-random if and only if each almost everywhere differentiable computable function [0,1]->R is differentiable at z.   (3) Recasting in classical language results dating from 1975 of the constructivist Demuth, we show that a real z is ML random if and only if every computable function of bounded variation is differentiable at z, and similarly for absolutely continuous functions.   We also use our analytic methods to show that computable randomness of a real is base invariant, and to derive other preservation results for randomness notions. ","Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03D32, 03F60, 26A27, 26A48, 26A45 ; ","Brattka, Vasco ; Miller, Joseph S. ; Nies, André ; ","Randomness and Differentiability  We characterize some major algorithmic randomness notions via differentiability of effective functions.   (1) As the main result we show that a real number z in [0,1] is computably random if and only if each nondecreasing computable function [0,1]->R is differentiable at z.   (2) We prove that a real number z in [0,1] is weakly 2-random if and only if each almost everywhere differentiable computable function [0,1]->R is differentiable at z.   (3) Recasting in classical language results dating from 1975 of the constructivist Demuth, we show that a real z is ML random if and only if every computable function of bounded variation is differentiable at z, and similarly for absolutely continuous functions.   We also use our analytic methods to show that computable randomness of a real is base invariant, and to derive other preservation results for randomness notions. ",randomness differentiability characterize major algorithmic randomness notions via differentiability effective function main result show real number computably random nondecreasing computable function differentiable prove real number weakly random almost everywhere differentiable computable function differentiable recast classical language result date constructivist demuth show real ml random every computable function bound variation differentiable similarly absolutely continuous function also use analytic methods show computable randomness real base invariant derive preservation result randomness notions,69,7,1104.4465.txt
http://arxiv.org/abs/1104.4987,An improved bound on the number of point-surface incidences in three   dimensions,"  We show that $m$ points and $n$ smooth algebraic surfaces of bounded degree in $\mathbb{R}^3$ satisfying suitable nondegeneracy conditions can have at most $O(m^{\frac{2k}{3k-1}}n^{\frac{3k-3}{3k-1}}+m+n)$ incidences, provided that any collection of $k$ points have at most O(1) surfaces passing through all of them, for some $k\geq 3$. In the case where the surfaces are spheres and no three spheres meet in a common circle, this implies there are $O((mn)^{3/4} + m +n)$ point-sphere incidences. This is a slight improvement over the previous bound of $O((mn)^{3/4} \beta(m,n)+ m +n)$ for $\beta(m,n)$ an (explicit) very slowly growing function. We obtain this bound by using the discrete polynomial ham sandwich theorem to cut $\mathbb{R}^3$ into open cells adapted to the set of points, and within each cell of the decomposition we apply a Turan-type theorem to obtain crude control on the number of point-surface incidences. We then perform a second polynomial ham sandwich decomposition on the irreducible components of the variety defined by the first decomposition. As an application, we obtain a new bound on the maximum number of unit distances amongst $m$ points in $\mathbb{R}^3$. ",Mathematics - Combinatorics ; Computer Science - Computational Geometry ; ,"Zahl, Joshua ; ","An improved bound on the number of point-surface incidences in three   dimensions  We show that $m$ points and $n$ smooth algebraic surfaces of bounded degree in $\mathbb{R}^3$ satisfying suitable nondegeneracy conditions can have at most $O(m^{\frac{2k}{3k-1}}n^{\frac{3k-3}{3k-1}}+m+n)$ incidences, provided that any collection of $k$ points have at most O(1) surfaces passing through all of them, for some $k\geq 3$. In the case where the surfaces are spheres and no three spheres meet in a common circle, this implies there are $O((mn)^{3/4} + m +n)$ point-sphere incidences. This is a slight improvement over the previous bound of $O((mn)^{3/4} \beta(m,n)+ m +n)$ for $\beta(m,n)$ an (explicit) very slowly growing function. We obtain this bound by using the discrete polynomial ham sandwich theorem to cut $\mathbb{R}^3$ into open cells adapted to the set of points, and within each cell of the decomposition we apply a Turan-type theorem to obtain crude control on the number of point-surface incidences. We then perform a second polynomial ham sandwich decomposition on the irreducible components of the variety defined by the first decomposition. As an application, we obtain a new bound on the maximum number of unit distances amongst $m$ points in $\mathbb{R}^3$. ",improve bind number point surface incidences three dimension show point smooth algebraic surface bound degree mathbb satisfy suitable nondegeneracy condition frac frac incidences provide collection point surface pass geq case surface spheres three spheres meet common circle imply mn point sphere incidences slight improvement previous bind mn beta beta explicit slowly grow function obtain bind use discrete polynomial ham sandwich theorem cut mathbb open cells adapt set point within cell decomposition apply turan type theorem obtain crude control number point surface incidences perform second polynomial ham sandwich decomposition irreducible components variety define first decomposition application obtain new bind maximum number unit distance amongst point mathbb,105,4,1104.4987.txt
http://arxiv.org/abs/1104.5286,Doubly Robust Smoothing of Dynamical Processes via Outlier Sparsity   Constraints,"  Coping with outliers contaminating dynamical processes is of major importance in various applications because mismatches from nominal models are not uncommon in practice. In this context, the present paper develops novel fixed-lag and fixed-interval smoothing algorithms that are robust to outliers simultaneously present in the measurements {\it and} in the state dynamics. Outliers are handled through auxiliary unknown variables that are jointly estimated along with the state based on the least-squares criterion that is regularized with the $\ell_1$-norm of the outliers in order to effect sparsity control. The resultant iterative estimators rely on coordinate descent and the alternating direction method of multipliers, are expressed in closed form per iteration, and are provably convergent. Additional attractive features of the novel doubly robust smoother include: i) ability to handle both types of outliers; ii) universality to unknown nominal noise and outlier distributions; iii) flexibility to encompass maximum a posteriori optimal estimators with reliable performance under nominal conditions; and iv) improved performance relative to competing alternatives at comparable complexity, as corroborated via simulated tests. ",Computer Science - Systems and Control ; Mathematics - Optimization and Control ; Statistics - Applications ; ,"Farahmand, Shahrokh ; Giannakis, Georgios B. ; Angelosante, Daniele ; ","Doubly Robust Smoothing of Dynamical Processes via Outlier Sparsity   Constraints  Coping with outliers contaminating dynamical processes is of major importance in various applications because mismatches from nominal models are not uncommon in practice. In this context, the present paper develops novel fixed-lag and fixed-interval smoothing algorithms that are robust to outliers simultaneously present in the measurements {\it and} in the state dynamics. Outliers are handled through auxiliary unknown variables that are jointly estimated along with the state based on the least-squares criterion that is regularized with the $\ell_1$-norm of the outliers in order to effect sparsity control. The resultant iterative estimators rely on coordinate descent and the alternating direction method of multipliers, are expressed in closed form per iteration, and are provably convergent. Additional attractive features of the novel doubly robust smoother include: i) ability to handle both types of outliers; ii) universality to unknown nominal noise and outlier distributions; iii) flexibility to encompass maximum a posteriori optimal estimators with reliable performance under nominal conditions; and iv) improved performance relative to competing alternatives at comparable complexity, as corroborated via simulated tests. ",doubly robust smooth dynamical process via outlier sparsity constraints cop outliers contaminate dynamical process major importance various applications mismatch nominal model uncommon practice context present paper develop novel fix lag fix interval smooth algorithms robust outliers simultaneously present measurements state dynamics outliers handle auxiliary unknown variables jointly estimate along state base least square criterion regularize ell norm outliers order effect sparsity control resultant iterative estimators rely coordinate descent alternate direction method multipliers express close form per iteration provably convergent additional attractive feature novel doubly robust smoother include ability handle type outliers ii universality unknown nominal noise outlier distributions iii flexibility encompass maximum posteriori optimal estimators reliable performance nominal condition iv improve performance relative compete alternatives comparable complexity corroborate via simulate test,121,7,1104.5286.txt
http://arxiv.org/abs/1104.5288,Tracking Target Signal Strengths on a Grid using Sparsity,"  Multi-target tracking is mainly challenged by the nonlinearity present in the measurement equation, and the difficulty in fast and accurate data association. To overcome these challenges, the present paper introduces a grid-based model in which the state captures target signal strengths on a known spatial grid (TSSG). This model leads to \emph{linear} state and measurement equations, which bypass data association and can afford state estimation via sparsity-aware Kalman filtering (KF). Leveraging the grid-induced sparsity of the novel model, two types of sparsity-cognizant TSSG-KF trackers are developed: one effects sparsity through $\ell_1$-norm regularization, and the other invokes sparsity as an extra measurement. Iterative extended KF and Gauss-Newton algorithms are developed for reduced-complexity tracking, along with accurate error covariance updates for assessing performance of the resultant sparsity-aware state estimators. Based on TSSG state estimates, more informative target position and track estimates can be obtained in a follow-up step, ensuring that track association and position estimation errors do not propagate back into TSSG state estimates. The novel TSSG trackers do not require knowing the number of targets or their signal strengths, and exhibit considerably lower complexity than the benchmark hidden Markov model filter, especially for a large number of targets. Numerical simulations demonstrate that sparsity-cognizant trackers enjoy improved root mean-square error performance at reduced complexity when compared to their sparsity-agnostic counterparts. ",Computer Science - Systems and Control ; Mathematics - Optimization and Control ; Statistics - Applications ; ,"Farahmand, Shahrokh ; Giannakis, Georgios B. ; Leus, Geert ; Tian, Zhi ; ","Tracking Target Signal Strengths on a Grid using Sparsity  Multi-target tracking is mainly challenged by the nonlinearity present in the measurement equation, and the difficulty in fast and accurate data association. To overcome these challenges, the present paper introduces a grid-based model in which the state captures target signal strengths on a known spatial grid (TSSG). This model leads to \emph{linear} state and measurement equations, which bypass data association and can afford state estimation via sparsity-aware Kalman filtering (KF). Leveraging the grid-induced sparsity of the novel model, two types of sparsity-cognizant TSSG-KF trackers are developed: one effects sparsity through $\ell_1$-norm regularization, and the other invokes sparsity as an extra measurement. Iterative extended KF and Gauss-Newton algorithms are developed for reduced-complexity tracking, along with accurate error covariance updates for assessing performance of the resultant sparsity-aware state estimators. Based on TSSG state estimates, more informative target position and track estimates can be obtained in a follow-up step, ensuring that track association and position estimation errors do not propagate back into TSSG state estimates. The novel TSSG trackers do not require knowing the number of targets or their signal strengths, and exhibit considerably lower complexity than the benchmark hidden Markov model filter, especially for a large number of targets. Numerical simulations demonstrate that sparsity-cognizant trackers enjoy improved root mean-square error performance at reduced complexity when compared to their sparsity-agnostic counterparts. ",track target signal strengths grid use sparsity multi target track mainly challenge nonlinearity present measurement equation difficulty fast accurate data association overcome challenge present paper introduce grid base model state capture target signal strengths know spatial grid tssg model lead emph linear state measurement equations bypass data association afford state estimation via sparsity aware kalman filter kf leverage grid induce sparsity novel model two type sparsity cognizant tssg kf trackers develop one effect sparsity ell norm regularization invoke sparsity extra measurement iterative extend kf gauss newton algorithms develop reduce complexity track along accurate error covariance update assess performance resultant sparsity aware state estimators base tssg state estimate informative target position track estimate obtain follow step ensure track association position estimation errors propagate back tssg state estimate novel tssg trackers require know number target signal strengths exhibit considerably lower complexity benchmark hide markov model filter especially large number target numerical simulations demonstrate sparsity cognizant trackers enjoy improve root mean square error performance reduce complexity compare sparsity agnostic counterparts,167,2,1104.5288.txt
http://arxiv.org/abs/1105.1302,A Modified Cross Correlation Algorithm for Reference-free Image   Alignment of Non-Circular Projections in Single-Particle Electron Microscopy,"  In this paper we propose a modified cross correlation method to align images from the same class in single-particle electron microscopy of highly non-spherical structures. In this new method, First we coarsely align projection images, and then re-align the resulting images using the cross correlation (CC) method. The coarse alignment is obtained by matching the centers of mass and the principal axes of the images. The distribution of misalignment in this coarse alignment can be quantified based on the statistical properties of the additive background noise. As a consequence, the search space for re-alignment in the cross correlation method can be reduced to achieve better alignment. In order to overcome problems associated with false peaks in the cross correlations function, we use artificially blurred images for the early stage of the iterative cross correlation method and segment the intermediate class average from every iteration step. These two additional manipulations combined with the reduced search space size in the cross correlation method yield better alignments for low signal-to-noise ratio images than both classical cross correlation and maximum likelihood(ML) methods. ",Quantitative Biology - Quantitative Methods ; Computer Science - Computer Vision and Pattern Recognition ; Mathematics - Numerical Analysis ; ,"Park, Wooram ; Chirikjian, Gregory S. ; ","A Modified Cross Correlation Algorithm for Reference-free Image   Alignment of Non-Circular Projections in Single-Particle Electron Microscopy  In this paper we propose a modified cross correlation method to align images from the same class in single-particle electron microscopy of highly non-spherical structures. In this new method, First we coarsely align projection images, and then re-align the resulting images using the cross correlation (CC) method. The coarse alignment is obtained by matching the centers of mass and the principal axes of the images. The distribution of misalignment in this coarse alignment can be quantified based on the statistical properties of the additive background noise. As a consequence, the search space for re-alignment in the cross correlation method can be reduced to achieve better alignment. In order to overcome problems associated with false peaks in the cross correlations function, we use artificially blurred images for the early stage of the iterative cross correlation method and segment the intermediate class average from every iteration step. These two additional manipulations combined with the reduced search space size in the cross correlation method yield better alignments for low signal-to-noise ratio images than both classical cross correlation and maximum likelihood(ML) methods. ",modify cross correlation algorithm reference free image alignment non circular projections single particle electron microscopy paper propose modify cross correlation method align image class single particle electron microscopy highly non spherical structure new method first coarsely align projection image align result image use cross correlation cc method coarse alignment obtain match center mass principal ax image distribution misalignment coarse alignment quantify base statistical properties additive background noise consequence search space alignment cross correlation method reduce achieve better alignment order overcome problems associate false peak cross correlations function use artificially blur image early stage iterative cross correlation method segment intermediate class average every iteration step two additional manipulations combine reduce search space size cross correlation method yield better alignments low signal noise ratio image classical cross correlation maximum likelihood ml methods,130,11,1105.1302.txt
http://arxiv.org/abs/1106.1445,From Classical to Quantum Shannon Theory,"  The aim of this book is to develop ""from the ground up"" many of the major, exciting, pre- and post-millenium developments in the general area of study known as quantum Shannon theory. As such, we spend a significant amount of time on quantum mechanics for quantum information theory (Part II), we give a careful study of the important unit protocols of teleportation, super-dense coding, and entanglement distribution (Part III), and we develop many of the tools necessary for understanding information transmission or compression (Part IV). Parts V and VI are the culmination of this book, where all of the tools developed come into play for understanding many of the important results in quantum Shannon theory. ",Quantum Physics ; Computer Science - Information Theory ; ,"Wilde, Mark M. ; ","From Classical to Quantum Shannon Theory  The aim of this book is to develop ""from the ground up"" many of the major, exciting, pre- and post-millenium developments in the general area of study known as quantum Shannon theory. As such, we spend a significant amount of time on quantum mechanics for quantum information theory (Part II), we give a careful study of the important unit protocols of teleportation, super-dense coding, and entanglement distribution (Part III), and we develop many of the tools necessary for understanding information transmission or compression (Part IV). Parts V and VI are the culmination of this book, where all of the tools developed come into play for understanding many of the important results in quantum Shannon theory. ",classical quantum shannon theory aim book develop grind many major excite pre post millenium developments general area study know quantum shannon theory spend significant amount time quantum mechanics quantum information theory part ii give careful study important unit protocols teleportation super dense cod entanglement distribution part iii develop many tool necessary understand information transmission compression part iv part vi culmination book tool develop come play understand many important result quantum shannon theory,72,4,1106.1445.txt
http://arxiv.org/abs/1106.2327,A framework for coupled deformation-diffusion analysis with application   to degradation/healing,"  This paper deals with the formulation and numerical implementation of a fully coupled continuum model for deformation-diffusion in linearized elastic solids. The mathematical model takes into account the effect of the deformation on the diffusion process, and the affect of the transport of an inert chemical species on the deformation of the solid. We then present a robust computational framework for solving the proposed mathematical model, which consists of coupled non-linear partial differential equations. It should be noted that many popular numerical formulations may produce unphysical negative values for the concentration, particularly, when the diffusion process is anisotropic. The violation of the non-negative constraint by these numerical formulations is not mere numerical noise. In the proposed computational framework we employ a novel numerical formulation that will ensure that the concentration of the diffusant be always non-negative, which is one of the main contributions of this paper. Representative numerical examples are presented to show the robustness, convergence, and performance of the proposed computational framework. Another contribution of this paper is to systematically study the affect of transport of the diffusant on the deformation of the solid and vice-versa, and their implication in modeling degradation/healing of materials. We show that the coupled response is both qualitatively and quantitatively different from the uncoupled response. ","Computer Science - Numerical Analysis ; Computer Science - Computational Engineering, Finance, and Science ; Mathematics - Numerical Analysis ; Physics - Computational Physics ; ","Mudunuru, M. K. ; Nakshatrala, K. B. ; ","A framework for coupled deformation-diffusion analysis with application   to degradation/healing  This paper deals with the formulation and numerical implementation of a fully coupled continuum model for deformation-diffusion in linearized elastic solids. The mathematical model takes into account the effect of the deformation on the diffusion process, and the affect of the transport of an inert chemical species on the deformation of the solid. We then present a robust computational framework for solving the proposed mathematical model, which consists of coupled non-linear partial differential equations. It should be noted that many popular numerical formulations may produce unphysical negative values for the concentration, particularly, when the diffusion process is anisotropic. The violation of the non-negative constraint by these numerical formulations is not mere numerical noise. In the proposed computational framework we employ a novel numerical formulation that will ensure that the concentration of the diffusant be always non-negative, which is one of the main contributions of this paper. Representative numerical examples are presented to show the robustness, convergence, and performance of the proposed computational framework. Another contribution of this paper is to systematically study the affect of transport of the diffusant on the deformation of the solid and vice-versa, and their implication in modeling degradation/healing of materials. We show that the coupled response is both qualitatively and quantitatively different from the uncoupled response. ",framework couple deformation diffusion analysis application degradation heal paper deal formulation numerical implementation fully couple continuum model deformation diffusion linearize elastic solids mathematical model take account effect deformation diffusion process affect transport inert chemical species deformation solid present robust computational framework solve propose mathematical model consist couple non linear partial differential equations note many popular numerical formulations may produce unphysical negative value concentration particularly diffusion process anisotropic violation non negative constraint numerical formulations mere numerical noise propose computational framework employ novel numerical formulation ensure concentration diffusant always non negative one main contributions paper representative numerical examples present show robustness convergence performance propose computational framework another contribution paper systematically study affect transport diffusant deformation solid vice versa implication model degradation heal materials show couple response qualitatively quantitatively different uncouple response,129,9,1106.2327.txt
http://arxiv.org/abs/1106.2441,An f-chromatic spanning forest of edge-colored complete bipartite graphs,"  In 2001, Brualdi and Hollingsworth proved that an edge-colored balanced complete bipartite graph Kn,n with a color set C = {1,2,3,..., 2n-1} has a heterochromatic spanning tree if the number of edges colored with colors in R is more than |R|^2 /4 for any non-empty subset R \subseteq C, where a heterochromatic spanning tree is a spanning tree whose edges have distinct colors, namely, any color appears at most once. In 2010, Suzuki generalized heterochromatic graphs to f-chromatic graphs, where any color c appears at most f(c). Moreover, he presented a necessary and sufficient condition for graphs to have an f-chromatic spanning forest with exactly w components. In this paper, using this necessary and sufficient condition, we generalize the Brualdi-Hollingsworth theorem above. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C05, 05C15 ; ","Suzuki, Kazuhiro ; ","An f-chromatic spanning forest of edge-colored complete bipartite graphs  In 2001, Brualdi and Hollingsworth proved that an edge-colored balanced complete bipartite graph Kn,n with a color set C = {1,2,3,..., 2n-1} has a heterochromatic spanning tree if the number of edges colored with colors in R is more than |R|^2 /4 for any non-empty subset R \subseteq C, where a heterochromatic spanning tree is a spanning tree whose edges have distinct colors, namely, any color appears at most once. In 2010, Suzuki generalized heterochromatic graphs to f-chromatic graphs, where any color c appears at most f(c). Moreover, he presented a necessary and sufficient condition for graphs to have an f-chromatic spanning forest with exactly w components. In this paper, using this necessary and sufficient condition, we generalize the Brualdi-Hollingsworth theorem above. ",chromatic span forest edge color complete bipartite graph brualdi hollingsworth prove edge color balance complete bipartite graph kn color set heterochromatic span tree number edge color color non empty subset subseteq heterochromatic span tree span tree whose edge distinct color namely color appear suzuki generalize heterochromatic graph chromatic graph color appear moreover present necessary sufficient condition graph chromatic span forest exactly components paper use necessary sufficient condition generalize brualdi hollingsworth theorem,71,13,1106.2441.txt
http://arxiv.org/abs/1106.5130,Some Properties of R\'{e}nyi Entropy over Countably Infinite Alphabets,"  In this paper we study certain properties of R\'{e}nyi entropy functionals $H_\alpha(\mathcal{P})$ on the space of probability distributions over $\mathbb{Z}_+$. Primarily, continuity and convergence issues are addressed. Some properties shown parallel those known in the finite alphabet case, while others illustrate a quite different behaviour of R\'enyi entropy in the infinite case. In particular, it is shown that, for any distribution $\mathcal P$ and any $r\in[0,\infty]$, there exists a sequence of distributions $\mathcal{P}_n$ converging to $\mathcal{P}$ with respect to the total variation distance, such that $\lim_{n\to\infty}\lim_{\alpha\to{1+}} H_\alpha(\mathcal{P}_n) = \lim_{\alpha\to{1+}}\lim_{n\to\infty} H_\alpha(\mathcal{P}_n) + r$. ",Computer Science - Information Theory ; 94A17 ; H.1.1 ; ,"Kovačević, Mladen ; Stanojević, Ivan ; Šenk, Vojin ; ","Some Properties of R\'{e}nyi Entropy over Countably Infinite Alphabets  In this paper we study certain properties of R\'{e}nyi entropy functionals $H_\alpha(\mathcal{P})$ on the space of probability distributions over $\mathbb{Z}_+$. Primarily, continuity and convergence issues are addressed. Some properties shown parallel those known in the finite alphabet case, while others illustrate a quite different behaviour of R\'enyi entropy in the infinite case. In particular, it is shown that, for any distribution $\mathcal P$ and any $r\in[0,\infty]$, there exists a sequence of distributions $\mathcal{P}_n$ converging to $\mathcal{P}$ with respect to the total variation distance, such that $\lim_{n\to\infty}\lim_{\alpha\to{1+}} H_\alpha(\mathcal{P}_n) = \lim_{\alpha\to{1+}}\lim_{n\to\infty} H_\alpha(\mathcal{P}_n) + r$. ",properties nyi entropy countably infinite alphabets paper study certain properties nyi entropy functionals alpha mathcal space probability distributions mathbb primarily continuity convergence issue address properties show parallel know finite alphabet case others illustrate quite different behaviour enyi entropy infinite case particular show distribution mathcal infty exist sequence distributions mathcal converge mathcal respect total variation distance lim infty lim alpha alpha mathcal lim alpha lim infty alpha mathcal,67,7,1106.5130.txt
http://arxiv.org/abs/1107.0088,Sparse Sums of Positive Semidefinite Matrices,"  Recently there has been much interest in ""sparsifying"" sums of rank one matrices: modifying the coefficients such that only a few are nonzero, while approximately preserving the matrix that results from the sum. Results of this sort have found applications in many different areas, including sparsifying graphs. In this paper we consider the more general problem of sparsifying sums of positive semidefinite matrices that have arbitrary rank.   We give several algorithms for solving this problem. The first algorithm is based on the method of Batson, Spielman and Srivastava (2009). The second algorithm is based on the matrix multiplicative weights update method of Arora and Kale (2007). We also highlight an interesting connection between these two algorithms.   Our algorithms have numerous applications. We show how they can be used to construct graph sparsifiers with auxiliary constraints, sparsifiers of hypergraphs, and sparse solutions to semidefinite programs. ",Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Computer Science - Numerical Analysis ; Mathematics - Combinatorics ; ,"Silva, Marcel K. de Carli ; Harvey, Nicholas J. A. ; Sato, Cristiane M. ; ","Sparse Sums of Positive Semidefinite Matrices  Recently there has been much interest in ""sparsifying"" sums of rank one matrices: modifying the coefficients such that only a few are nonzero, while approximately preserving the matrix that results from the sum. Results of this sort have found applications in many different areas, including sparsifying graphs. In this paper we consider the more general problem of sparsifying sums of positive semidefinite matrices that have arbitrary rank.   We give several algorithms for solving this problem. The first algorithm is based on the method of Batson, Spielman and Srivastava (2009). The second algorithm is based on the matrix multiplicative weights update method of Arora and Kale (2007). We also highlight an interesting connection between these two algorithms.   Our algorithms have numerous applications. We show how they can be used to construct graph sparsifiers with auxiliary constraints, sparsifiers of hypergraphs, and sparse solutions to semidefinite programs. ",sparse sum positive semidefinite matrices recently much interest sparsifying sum rank one matrices modify coefficients nonzero approximately preserve matrix result sum result sort find applications many different areas include sparsifying graph paper consider general problem sparsifying sum positive semidefinite matrices arbitrary rank give several algorithms solve problem first algorithm base method batson spielman srivastava second algorithm base matrix multiplicative weight update method arora kale also highlight interest connection two algorithms algorithms numerous applications show use construct graph sparsifiers auxiliary constraints sparsifiers hypergraphs sparse solutions semidefinite program,86,7,1107.0088.txt
http://arxiv.org/abs/1107.2465,An Efficient Algorithm for Maximum-Entropy Extension of Block-Circulant   Covariance Matrices,"  This paper deals with maximum entropy completion of partially specified block-circulant matrices. Since positive definite symmetric circulants happen to be covariance matrices of stationary periodic processes, in particular of stationary reciprocal processes, this problem has applications in signal processing, in particular to image modeling. In fact it is strictly related to maximum likelihood estimation of bilateral AR-type representations of acausal signals subject to certain conditional independence constraints. The maximum entropy completion problem for block-circulant matrices has recently been solved by the authors, although leaving open the problem of an efficient computation of the solution. In this paper, we provide an effcient algorithm for computing its solution which compares very favourably with existing algorithms designed for positive definite matrix extension problems. The proposed algorithm benefits from the analysis of the relationship between our problem and the band-extension problem for block-Toeplitz matrices also developed in this paper. ",Mathematics - Optimization and Control ; Computer Science - Information Theory ; Computer Science - Systems and Control ; ,"Carli, Francesca P. ; Ferrante, Augusto ; Pavon, Michele ; Picci, Giorgio ; ","An Efficient Algorithm for Maximum-Entropy Extension of Block-Circulant   Covariance Matrices  This paper deals with maximum entropy completion of partially specified block-circulant matrices. Since positive definite symmetric circulants happen to be covariance matrices of stationary periodic processes, in particular of stationary reciprocal processes, this problem has applications in signal processing, in particular to image modeling. In fact it is strictly related to maximum likelihood estimation of bilateral AR-type representations of acausal signals subject to certain conditional independence constraints. The maximum entropy completion problem for block-circulant matrices has recently been solved by the authors, although leaving open the problem of an efficient computation of the solution. In this paper, we provide an effcient algorithm for computing its solution which compares very favourably with existing algorithms designed for positive definite matrix extension problems. The proposed algorithm benefits from the analysis of the relationship between our problem and the band-extension problem for block-Toeplitz matrices also developed in this paper. ",efficient algorithm maximum entropy extension block circulant covariance matrices paper deal maximum entropy completion partially specify block circulant matrices since positive definite symmetric circulants happen covariance matrices stationary periodic process particular stationary reciprocal process problem applications signal process particular image model fact strictly relate maximum likelihood estimation bilateral ar type representations acausal signal subject certain conditional independence constraints maximum entropy completion problem block circulant matrices recently solve author although leave open problem efficient computation solution paper provide effcient algorithm compute solution compare favourably exist algorithms design positive definite matrix extension problems propose algorithm benefit analysis relationship problem band extension problem block toeplitz matrices also develop paper,106,7,1107.2465.txt
http://arxiv.org/abs/1108.1915,Noise effects in the quantum search algorithm from the computational   complexity point of view,  We analyse the resilience of the quantum search algorithm in the presence of quantum noise modelled as trace preserving completely positive maps. We study the influence of noise on computational complexity of the quantum search algorithm. We show that only for small amounts of noise the quantum search algorithm is still more efficient than any classical algorithm. ,Quantum Physics ; Computer Science - Computational Complexity ; ,"Gawron, Piotr ; Klamka, Jerzy ; Winiarczyk, Ryszard ; ",Noise effects in the quantum search algorithm from the computational   complexity point of view  We analyse the resilience of the quantum search algorithm in the presence of quantum noise modelled as trace preserving completely positive maps. We study the influence of noise on computational complexity of the quantum search algorithm. We show that only for small amounts of noise the quantum search algorithm is still more efficient than any classical algorithm. ,noise effect quantum search algorithm computational complexity point view analyse resilience quantum search algorithm presence quantum noise model trace preserve completely positive map study influence noise computational complexity quantum search algorithm show small amount noise quantum search algorithm still efficient classical algorithm,42,4,1108.1915.txt
http://arxiv.org/abs/1109.0345,Planar and Poly-Arc Lombardi Drawings,"  In Lombardi drawings of graphs, edges are represented as circular arcs, and the edges incident on vertices have perfect angular resolution. However, not every graph has a Lombardi drawing, and not every planar graph has a planar Lombardi drawing. We introduce k-Lombardi drawings, in which each edge may be drawn with k circular arcs, noting that every graph has a smooth 2-Lombardi drawing. We show that every planar graph has a smooth planar 3-Lombardi drawing and further investigate topics connecting planarity and Lombardi drawings. ","Computer Science - Computational Geometry ; Computer Science - Discrete Mathematics ; 05C10, 68R10 ; G.2.2 ; F.2.2 ; ","Duncan, Christian A. ; Eppstein, David ; Goodrich, Michael T. ; Kobourov, Stephen G. ; Löffler, Maarten ; ","Planar and Poly-Arc Lombardi Drawings  In Lombardi drawings of graphs, edges are represented as circular arcs, and the edges incident on vertices have perfect angular resolution. However, not every graph has a Lombardi drawing, and not every planar graph has a planar Lombardi drawing. We introduce k-Lombardi drawings, in which each edge may be drawn with k circular arcs, noting that every graph has a smooth 2-Lombardi drawing. We show that every planar graph has a smooth planar 3-Lombardi drawing and further investigate topics connecting planarity and Lombardi drawings. ",planar poly arc lombardi draw lombardi draw graph edge represent circular arc edge incident vertices perfect angular resolution however every graph lombardi draw every planar graph planar lombardi draw introduce lombardi draw edge may draw circular arc note every graph smooth lombardi draw show every planar graph smooth planar lombardi draw investigate topics connect planarity lombardi draw,57,3,1109.0345.txt
http://arxiv.org/abs/1109.2162,The Complexity of the Empire Colouring Problem,"  We investigate the computational complexity of the empire colouring problem (as defined by Percy Heawood in 1890) for maps containing empires formed by exactly $r > 1$ countries each. We prove that the problem can be solved in polynomial time using $s$ colours on maps whose underlying adjacency graph has no induced subgraph of average degree larger than $s/r$. However, if $s \geq 3$, the problem is NP-hard even if the graph is a forest of paths of arbitrary lengths (for any $r \geq 2$, provided $s < 2r - \sqrt(2r + 1/4+ 3/2)$. Furthermore we obtain a complete characterization of the problem's complexity for the case when the input graph is a tree, whereas our result for arbitrary planar graphs fall just short of a similar dichotomy. Specifically, we prove that the empire colouring problem is NP-hard for trees, for any $r \geq 2$, if $3 \leq s \leq 2r-1$ (and polynomial time solvable otherwise). For arbitrary planar graphs we prove NP-hardness if $s<7$ for $r=2$, and $s < 6r-3$, for $r \geq 3$. The result for planar graphs also proves the NP-hardness of colouring with less than 7 colours graphs of thickness two and less than $6r-3$ colours graphs of thickness $r \geq 3$. ",Computer Science - Computational Complexity ; ,"McGrae, Andrew R. A. ; Zito, Michele ; ","The Complexity of the Empire Colouring Problem  We investigate the computational complexity of the empire colouring problem (as defined by Percy Heawood in 1890) for maps containing empires formed by exactly $r > 1$ countries each. We prove that the problem can be solved in polynomial time using $s$ colours on maps whose underlying adjacency graph has no induced subgraph of average degree larger than $s/r$. However, if $s \geq 3$, the problem is NP-hard even if the graph is a forest of paths of arbitrary lengths (for any $r \geq 2$, provided $s < 2r - \sqrt(2r + 1/4+ 3/2)$. Furthermore we obtain a complete characterization of the problem's complexity for the case when the input graph is a tree, whereas our result for arbitrary planar graphs fall just short of a similar dichotomy. Specifically, we prove that the empire colouring problem is NP-hard for trees, for any $r \geq 2$, if $3 \leq s \leq 2r-1$ (and polynomial time solvable otherwise). For arbitrary planar graphs we prove NP-hardness if $s<7$ for $r=2$, and $s < 6r-3$, for $r \geq 3$. The result for planar graphs also proves the NP-hardness of colouring with less than 7 colours graphs of thickness two and less than $6r-3$ colours graphs of thickness $r \geq 3$. ",complexity empire colour problem investigate computational complexity empire colour problem define percy heawood map contain empires form exactly countries prove problem solve polynomial time use colour map whose underlie adjacency graph induce subgraph average degree larger however geq problem np hard even graph forest paths arbitrary lengths geq provide sqrt furthermore obtain complete characterization problem complexity case input graph tree whereas result arbitrary planar graph fall short similar dichotomy specifically prove empire colour problem np hard tree geq leq leq polynomial time solvable otherwise arbitrary planar graph prove np hardness geq result planar graph also prove np hardness colour less colour graph thickness two less colour graph thickness geq,109,3,1109.2162.txt
http://arxiv.org/abs/1109.2984,A Statistically Modelling Method for Performance Limits in Sensor   Localization,"  In this paper, we study performance limits of sensor localization from a novel perspective. Specifically, we consider the Cramer-Rao Lower Bound (CRLB) in single-hop sensor localization using measurements from received signal strength (RSS), time of arrival (TOA) and bearing, respectively, but differently from the existing work, we statistically analyze the trace of the associated CRLB matrix (i.e. as a scalar metric for performance limits of sensor localization) by assuming anchor locations are random. By the Central Limit Theorems for $U$-statistics, we show that as the number of the anchors increases, this scalar metric is asymptotically normal in the RSS/bearing case, and converges to a random variable which is an affine transformation of a chi-square random variable of degree 2 in the TOA case. Moreover, we provide formulas quantitatively describing the relationship among the mean and standard deviation of the scalar metric, the number of the anchors, the parameters of communication channels, the noise statistics in measurements and the spatial distribution of the anchors. These formulas, though asymptotic in the number of the anchors, in many cases turn out to be remarkably accurate in predicting performance limits, even if the number is small. Simulations are carried out to confirm our results. ",Computer Science - Systems and Control ; Mathematics - Optimization and Control ; ,"Huang, Baoqi ; Li, Tao ; Anderson, Brian D. O. ; Yu, Changbin ; ","A Statistically Modelling Method for Performance Limits in Sensor   Localization  In this paper, we study performance limits of sensor localization from a novel perspective. Specifically, we consider the Cramer-Rao Lower Bound (CRLB) in single-hop sensor localization using measurements from received signal strength (RSS), time of arrival (TOA) and bearing, respectively, but differently from the existing work, we statistically analyze the trace of the associated CRLB matrix (i.e. as a scalar metric for performance limits of sensor localization) by assuming anchor locations are random. By the Central Limit Theorems for $U$-statistics, we show that as the number of the anchors increases, this scalar metric is asymptotically normal in the RSS/bearing case, and converges to a random variable which is an affine transformation of a chi-square random variable of degree 2 in the TOA case. Moreover, we provide formulas quantitatively describing the relationship among the mean and standard deviation of the scalar metric, the number of the anchors, the parameters of communication channels, the noise statistics in measurements and the spatial distribution of the anchors. These formulas, though asymptotic in the number of the anchors, in many cases turn out to be remarkably accurate in predicting performance limits, even if the number is small. Simulations are carried out to confirm our results. ",statistically model method performance limit sensor localization paper study performance limit sensor localization novel perspective specifically consider cramer rao lower bind crlb single hop sensor localization use measurements receive signal strength rss time arrival toa bear respectively differently exist work statistically analyze trace associate crlb matrix scalar metric performance limit sensor localization assume anchor locations random central limit theorems statistics show number anchor increase scalar metric asymptotically normal rss bear case converge random variable affine transformation chi square random variable degree toa case moreover provide formulas quantitatively describe relationship among mean standard deviation scalar metric number anchor parameters communication channel noise statistics measurements spatial distribution anchor formulas though asymptotic number anchor many case turn remarkably accurate predict performance limit even number small simulations carry confirm result,126,2,1109.2984.txt
http://arxiv.org/abs/1110.0685,Energy Aware Scheduling for Weighted Completion Time and Weighted   Tardiness,"  The ever increasing adoption of mobile devices with limited energy storage capacity, on the one hand, and more awareness of the environmental impact of massive data centres and server pools, on the other hand, have both led to an increased interest in energy management algorithms.   The main contribution of this paper is to present several new constant factor approximation algorithms for energy aware scheduling problems where the objective is to minimize weighted completion time plus the cost of the energy consumed, in the one machine non-preemptive setting, while allowing release dates and deadlines.Unlike previous known algorithms these new algorithms can handle general job-dependent energy cost functions, extending the application of these algorithms to settings outside the typical CPU-energy one. These new settings include problems where in addition, or instead, of energy costs we also have maintenance costs, wear and tear, replacement costs, etc., which in general depend on the speed at which the machine runs but also depend on the types of jobs processed. Our algorithms also extend to approximating weighted tardiness plus energy cost, an inherently more difficult problem that has not been addressed in the literature. ",Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Carrasco, Rodrigo A. ; Iyengar, Garud ; Stein, Cliff ; ","Energy Aware Scheduling for Weighted Completion Time and Weighted   Tardiness  The ever increasing adoption of mobile devices with limited energy storage capacity, on the one hand, and more awareness of the environmental impact of massive data centres and server pools, on the other hand, have both led to an increased interest in energy management algorithms.   The main contribution of this paper is to present several new constant factor approximation algorithms for energy aware scheduling problems where the objective is to minimize weighted completion time plus the cost of the energy consumed, in the one machine non-preemptive setting, while allowing release dates and deadlines.Unlike previous known algorithms these new algorithms can handle general job-dependent energy cost functions, extending the application of these algorithms to settings outside the typical CPU-energy one. These new settings include problems where in addition, or instead, of energy costs we also have maintenance costs, wear and tear, replacement costs, etc., which in general depend on the speed at which the machine runs but also depend on the types of jobs processed. Our algorithms also extend to approximating weighted tardiness plus energy cost, an inherently more difficult problem that has not been addressed in the literature. ",energy aware schedule weight completion time weight tardiness ever increase adoption mobile devices limit energy storage capacity one hand awareness environmental impact massive data centre server pool hand lead increase interest energy management algorithms main contribution paper present several new constant factor approximation algorithms energy aware schedule problems objective minimize weight completion time plus cost energy consume one machine non preemptive set allow release date deadlines unlike previous know algorithms new algorithms handle general job dependent energy cost function extend application algorithms settings outside typical cpu energy one new settings include problems addition instead energy cost also maintenance cost wear tear replacement cost etc general depend speed machine run also depend type job process algorithms also extend approximate weight tardiness plus energy cost inherently difficult problem address literature,128,11,1110.0685.txt
http://arxiv.org/abs/1110.0895,Robust inversion via semistochastic dimensionality reduction,"  We consider a class of inverse problems where it is possible to aggregate the results of multiple experiments. This class includes problems where the forward model is the solution operator to linear ODEs or PDEs. The tremendous size of such problems motivates dimensionality reduction techniques based on randomly mixing experiments. These techniques break down, however, when robust data-fitting formulations are used, which are essential in cases of missing data, unusually large errors, and systematic features in the data unexplained by the forward model. We survey robust methods within a statistical framework, and propose a semistochastic optimization approach that allows dimensionality reduction. The efficacy of the methods are demonstrated for a large-scale seismic inverse problem using the robust Student's t-distribution, where a useful synthetic velocity model is recovered in the extreme scenario of 60% data missing at random. The semistochastic approach achieves this recovery using 20% of the effort required by a direct robust approach. ","Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Numerical Analysis ; ","Aravkin, Aleksandr ; Friedlander, Michael P. ; van Leeuwen, Tristan ; ","Robust inversion via semistochastic dimensionality reduction  We consider a class of inverse problems where it is possible to aggregate the results of multiple experiments. This class includes problems where the forward model is the solution operator to linear ODEs or PDEs. The tremendous size of such problems motivates dimensionality reduction techniques based on randomly mixing experiments. These techniques break down, however, when robust data-fitting formulations are used, which are essential in cases of missing data, unusually large errors, and systematic features in the data unexplained by the forward model. We survey robust methods within a statistical framework, and propose a semistochastic optimization approach that allows dimensionality reduction. The efficacy of the methods are demonstrated for a large-scale seismic inverse problem using the robust Student's t-distribution, where a useful synthetic velocity model is recovered in the extreme scenario of 60% data missing at random. The semistochastic approach achieves this recovery using 20% of the effort required by a direct robust approach. ",robust inversion via semistochastic dimensionality reduction consider class inverse problems possible aggregate result multiple experiment class include problems forward model solution operator linear odes pdes tremendous size problems motivate dimensionality reduction techniques base randomly mix experiment techniques break however robust data fit formulations use essential case miss data unusually large errors systematic feature data unexplained forward model survey robust methods within statistical framework propose semistochastic optimization approach allow dimensionality reduction efficacy methods demonstrate large scale seismic inverse problem use robust student distribution useful synthetic velocity model recover extreme scenario data miss random semistochastic approach achieve recovery use effort require direct robust approach,102,10,1110.0895.txt
http://arxiv.org/abs/1110.2053,"Steps Towards a Theory of Visual Information: Active Perception,   Signal-to-Symbol Conversion and the Interplay Between Sensing and Control","  This manuscript describes the elements of a theory of information tailored to control and decision tasks and specifically to visual data. The concept of Actionable Information is described, that relates to a notion of information championed by J. Gibson, and a notion of ""complete information"" that relates to the minimal sufficient statistics of a complete representation. It is shown that the ""actionable information gap"" between the two can be reduced by exercising control on the sensing process. Thus, senging, control and information are inextricably tied. This has consequences in the so-called ""signal-to-symbol barrier"" problem, as well as in the analysis and design of active sensing systems. It has ramifications in vision-based control, navigation, 3-D reconstruction and rendering, as well as detection, localization, recognition and categorization of objects and scenes in live video.   This manuscript has been developed from a set of lecture notes for a summer course at the First International Computer Vision Summer School (ICVSS) in Scicli, Italy, in July of 2008. They were later expanded and amended for subsequent lectures in the same School in July 2009. Starting on November 1, 2009, they were further expanded for a special topics course, CS269, taught at UCLA in the Spring term of 2010. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Soatto, Stefano ; ","Steps Towards a Theory of Visual Information: Active Perception,   Signal-to-Symbol Conversion and the Interplay Between Sensing and Control  This manuscript describes the elements of a theory of information tailored to control and decision tasks and specifically to visual data. The concept of Actionable Information is described, that relates to a notion of information championed by J. Gibson, and a notion of ""complete information"" that relates to the minimal sufficient statistics of a complete representation. It is shown that the ""actionable information gap"" between the two can be reduced by exercising control on the sensing process. Thus, senging, control and information are inextricably tied. This has consequences in the so-called ""signal-to-symbol barrier"" problem, as well as in the analysis and design of active sensing systems. It has ramifications in vision-based control, navigation, 3-D reconstruction and rendering, as well as detection, localization, recognition and categorization of objects and scenes in live video.   This manuscript has been developed from a set of lecture notes for a summer course at the First International Computer Vision Summer School (ICVSS) in Scicli, Italy, in July of 2008. They were later expanded and amended for subsequent lectures in the same School in July 2009. Starting on November 1, 2009, they were further expanded for a special topics course, CS269, taught at UCLA in the Spring term of 2010. ",step towards theory visual information active perception signal symbol conversion interplay sense control manuscript describe elements theory information tailor control decision task specifically visual data concept actionable information describe relate notion information champion gibson notion complete information relate minimal sufficient statistics complete representation show actionable information gap two reduce exercise control sense process thus senging control information inextricably tie consequences call signal symbol barrier problem well analysis design active sense systems ramifications vision base control navigation reconstruction render well detection localization recognition categorization object scenes live video manuscript develop set lecture note summer course first international computer vision summer school icvss scicli italy july later expand amend subsequent lecture school july start november expand special topics course cs teach ucla spring term,122,7,1110.2053.txt
http://arxiv.org/abs/1111.0235,New Methods for Handling Singular Sample Covariance Matrices,"  The estimation of a covariance matrix from an insufficient amount of data is one of the most common problems in fields as diverse as multivariate statistics, wireless communications, signal processing, biology, learning theory and finance. In a joint work of Marzetta, Tucci and Simon, a new approach to handle singular covariance matrices was suggested. The main idea was to use dimensionality reduction in conjunction with an average over the Stiefel manifold. In this paper we continue with this research and we consider some new approaches to solve this problem. One of the methods is called the Ewens estimator and uses a randomization of the sample covariance matrix over all the permutation matrices with respect to the Ewens measure. The techniques used to attack this problem are broad and run from random matrix theory to combinatorics. ","Mathematics - Probability ; Computer Science - Information Theory ; Mathematics - Statistics Theory ; 15B52, 60B20 ; ","Tucci, Gabriel H. ; Wang, Ke ; ","New Methods for Handling Singular Sample Covariance Matrices  The estimation of a covariance matrix from an insufficient amount of data is one of the most common problems in fields as diverse as multivariate statistics, wireless communications, signal processing, biology, learning theory and finance. In a joint work of Marzetta, Tucci and Simon, a new approach to handle singular covariance matrices was suggested. The main idea was to use dimensionality reduction in conjunction with an average over the Stiefel manifold. In this paper we continue with this research and we consider some new approaches to solve this problem. One of the methods is called the Ewens estimator and uses a randomization of the sample covariance matrix over all the permutation matrices with respect to the Ewens measure. The techniques used to attack this problem are broad and run from random matrix theory to combinatorics. ",new methods handle singular sample covariance matrices estimation covariance matrix insufficient amount data one common problems field diverse multivariate statistics wireless communications signal process biology learn theory finance joint work marzetta tucci simon new approach handle singular covariance matrices suggest main idea use dimensionality reduction conjunction average stiefel manifold paper continue research consider new approach solve problem one methods call ewens estimator use randomization sample covariance matrix permutation matrices respect ewens measure techniques use attack problem broad run random matrix theory combinatorics,82,7,1111.0235.txt
http://arxiv.org/abs/1111.0284,A topological interpretation of the walk distances,"  The walk distances in graphs have no direct interpretation in terms of walk weights, since they are introduced via the \emph{logarithms} of walk weights. Only in the limiting cases where the logarithms vanish such representations follow straightforwardly. The interpretation proposed in this paper rests on the identity $\ln\det B=\tr\ln B$ applied to the cofactors of the matrix $I-tA,$ where $A$ is the weighted adjacency matrix of a weighted multigraph and $t$ is a sufficiently small positive parameter. In addition, this interpretation is based on the power series expansion of the logarithm of a matrix. Kasteleyn (1967) was probably the first to apply the foregoing approach to expanding the determinant of $I-A$. We show that using a certain linear transformation the same approach can be extended to the cofactors of $I-tA,$ which provides a topological interpretation of the walk distances. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; Computer Science - Social and Information Networks ; Mathematics - Metric Geometry ; 05C12, 05C50, 51K05, 15A09, 15A15 ; ","Chebotarev, Pavel ; Deza, Michel ; ","A topological interpretation of the walk distances  The walk distances in graphs have no direct interpretation in terms of walk weights, since they are introduced via the \emph{logarithms} of walk weights. Only in the limiting cases where the logarithms vanish such representations follow straightforwardly. The interpretation proposed in this paper rests on the identity $\ln\det B=\tr\ln B$ applied to the cofactors of the matrix $I-tA,$ where $A$ is the weighted adjacency matrix of a weighted multigraph and $t$ is a sufficiently small positive parameter. In addition, this interpretation is based on the power series expansion of the logarithm of a matrix. Kasteleyn (1967) was probably the first to apply the foregoing approach to expanding the determinant of $I-A$. We show that using a certain linear transformation the same approach can be extended to the cofactors of $I-tA,$ which provides a topological interpretation of the walk distances. ",topological interpretation walk distance walk distance graph direct interpretation term walk weight since introduce via emph logarithms walk weight limit case logarithms vanish representations follow straightforwardly interpretation propose paper rest identity ln det tr ln apply cofactors matrix ta weight adjacency matrix weight multigraph sufficiently small positive parameter addition interpretation base power series expansion logarithm matrix kasteleyn probably first apply forego approach expand determinant show use certain linear transformation approach extend cofactors ta provide topological interpretation walk distance,78,11,1111.0284.txt
http://arxiv.org/abs/1111.2480,The Distance Function on a Computable Graph,"  We apply the techniques of computable model theory to the distance function of a graph. This task leads us to adapt the definitions of several truth-table reducibilities so that they apply to functions as well as to sets, and we prove assorted theorems about the new reducibilities and about functions which have nonincreasing computable approximations. Finally, we show that the spectrum of the distance function can consist of an arbitrary single btt-degree which is approximable from above, or of all such btt-degrees at once, or of the bT-degrees of exactly those functions approximable from above in at most n steps. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; ,"Calvert, Wesley ; Miller, Russell ; Reimann, Jennifer Chubb ; ","The Distance Function on a Computable Graph  We apply the techniques of computable model theory to the distance function of a graph. This task leads us to adapt the definitions of several truth-table reducibilities so that they apply to functions as well as to sets, and we prove assorted theorems about the new reducibilities and about functions which have nonincreasing computable approximations. Finally, we show that the spectrum of the distance function can consist of an arbitrary single btt-degree which is approximable from above, or of all such btt-degrees at once, or of the bT-degrees of exactly those functions approximable from above in at most n steps. ",distance function computable graph apply techniques computable model theory distance function graph task lead us adapt definitions several truth table reducibilities apply function well set prove assort theorems new reducibilities function nonincreasing computable approximations finally show spectrum distance function consist arbitrary single btt degree approximable btt degrees bt degrees exactly function approximable step,53,3,1111.2480.txt
http://arxiv.org/abs/1111.3048,On a Connection Between Small Set Expansions and Modularity Clustering   in Social Networks,"  In this paper we explore a connection between two seemingly different problems from two different domains: the small-set expansion problem studied in unique games conjecture, and a popular community finding approach for social networks known as the modularity clustering approach. We show that a sub-exponential time algorithm for the small-set expansion problem leads to a sub-exponential time constant factor approximation for some hard input instances of the modularity clustering problem. ","Computer Science - Social and Information Networks ; Computer Science - Computational Complexity ; Physics - Physics and Society ; 68Q25, 68W25 ; F.2.2 ; J.4 ; ","DasGupta, Bhaskar ; Desai, Devendra ; ","On a Connection Between Small Set Expansions and Modularity Clustering   in Social Networks  In this paper we explore a connection between two seemingly different problems from two different domains: the small-set expansion problem studied in unique games conjecture, and a popular community finding approach for social networks known as the modularity clustering approach. We show that a sub-exponential time algorithm for the small-set expansion problem leads to a sub-exponential time constant factor approximation for some hard input instances of the modularity clustering problem. ",connection small set expansions modularity cluster social network paper explore connection two seemingly different problems two different domains small set expansion problem study unique game conjecture popular community find approach social network know modularity cluster approach show sub exponential time algorithm small set expansion problem lead sub exponential time constant factor approximation hard input instance modularity cluster problem,58,6,1111.3048.txt
http://arxiv.org/abs/1111.4662,Traffic distributions and independence: permutation invariant random   matrices and the three notions of independence,"  Voiculescu's notion of asymptotic free independence is known for a large class of random matrices including independent unitary invariant matrices. This notion is extended for independent random matrices invariant in law by conjugation by permutation matrices. This fact leads naturally to an extension of free probability, formalized under the notions of traffic probability. We first establish this construction for random matrices. We define the traffic distribution of random matrices, which is richer than the *-distribution of free probability. The knowledge of the individual traffic distributions of independent permutation invariant families of matrices is sufficient to compute the limiting distribution of the join family. Under a factorization assumption, we call traffic independence the asymptotic rule that plays the role of independence with respect to traffic distributions. Wigner matrices, Haar unitary matrices and uniform permutation matrices converge in traffic distributions, a fact which yields new results on the limiting *-distributions of several matrices we can construct from them. Then we define the abstract traffic spaces as non commutative probability spaces with more structure. We prove that at an algebraic level, traffic independence in some sense unifies the three canonical notions of tensor, free and Boolean independence. A central limiting theorem is stated in this context, interpolating between the tensor, free and Boolean central limit theorems. ",Mathematics - Probability ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; Mathematics - Operator Algebras ; ,"Male, Camille ; ","Traffic distributions and independence: permutation invariant random   matrices and the three notions of independence  Voiculescu's notion of asymptotic free independence is known for a large class of random matrices including independent unitary invariant matrices. This notion is extended for independent random matrices invariant in law by conjugation by permutation matrices. This fact leads naturally to an extension of free probability, formalized under the notions of traffic probability. We first establish this construction for random matrices. We define the traffic distribution of random matrices, which is richer than the *-distribution of free probability. The knowledge of the individual traffic distributions of independent permutation invariant families of matrices is sufficient to compute the limiting distribution of the join family. Under a factorization assumption, we call traffic independence the asymptotic rule that plays the role of independence with respect to traffic distributions. Wigner matrices, Haar unitary matrices and uniform permutation matrices converge in traffic distributions, a fact which yields new results on the limiting *-distributions of several matrices we can construct from them. Then we define the abstract traffic spaces as non commutative probability spaces with more structure. We prove that at an algebraic level, traffic independence in some sense unifies the three canonical notions of tensor, free and Boolean independence. A central limiting theorem is stated in this context, interpolating between the tensor, free and Boolean central limit theorems. ",traffic distributions independence permutation invariant random matrices three notions independence voiculescu notion asymptotic free independence know large class random matrices include independent unitary invariant matrices notion extend independent random matrices invariant law conjugation permutation matrices fact lead naturally extension free probability formalize notions traffic probability first establish construction random matrices define traffic distribution random matrices richer distribution free probability knowledge individual traffic distributions independent permutation invariant families matrices sufficient compute limit distribution join family factorization assumption call traffic independence asymptotic rule play role independence respect traffic distributions wigner matrices haar unitary matrices uniform permutation matrices converge traffic distributions fact yield new result limit distributions several matrices construct define abstract traffic space non commutative probability space structure prove algebraic level traffic independence sense unify three canonical notions tensor free boolean independence central limit theorem state context interpolate tensor free boolean central limit theorems,142,7,1111.4662.txt
http://arxiv.org/abs/1111.7013,A Taxation Policy for Maximizing Social Welfare in Networks: A General   Framework,"  We present a simple tatonnement process based on a decomposition method which is simple to implement and achieves the maximal social welfare, under the assumption that the utility function of each [price-taking] individual will be his own private information and need not be known by the designer. At each iteration, very little information needs to be exchanged among the individuals in order to achieve the optimal allocation. Furthermore, the given tatonnement process is always balanced at equilibrium and off equilibrium. ",Mathematics - Optimization and Control ; Computer Science - Computer Science and Game Theory ; ,"Kakhbod, Ali ; Koo, Joseph ; Teneketzis, Demosthenis ; ","A Taxation Policy for Maximizing Social Welfare in Networks: A General   Framework  We present a simple tatonnement process based on a decomposition method which is simple to implement and achieves the maximal social welfare, under the assumption that the utility function of each [price-taking] individual will be his own private information and need not be known by the designer. At each iteration, very little information needs to be exchanged among the individuals in order to achieve the optimal allocation. Furthermore, the given tatonnement process is always balanced at equilibrium and off equilibrium. ",taxation policy maximize social welfare network general framework present simple tatonnement process base decomposition method simple implement achieve maximal social welfare assumption utility function price take individual private information need know designer iteration little information need exchange among individuals order achieve optimal allocation furthermore give tatonnement process always balance equilibrium equilibrium,51,0,1111.7013.txt
http://arxiv.org/abs/1112.0857,I/O efficient bisimulation partitioning on very large directed acyclic   graphs,"  In this paper we introduce the first efficient external-memory algorithm to compute the bisimilarity equivalence classes of a directed acyclic graph (DAG). DAGs are commonly used to model data in a wide variety of practical applications, ranging from XML documents and data provenance models, to web taxonomies and scientific workflows. In the study of efficient reasoning over massive graphs, the notion of node bisimilarity plays a central role. For example, grouping together bisimilar nodes in an XML data set is the first step in many sophisticated approaches to building indexing data structures for efficient XPath query evaluation. To date, however, only internal-memory bisimulation algorithms have been investigated. As the size of real-world DAG data sets often exceeds available main memory, storage in external memory becomes necessary. Hence, there is a practical need for an efficient approach to computing bisimulation in external memory.   Our general algorithm has a worst-case IO-complexity of O(Sort(|N| + |E|)), where |N| and |E| are the numbers of nodes and edges, resp., in the data graph and Sort(n) is the number of accesses to external memory needed to sort an input of size n. We also study specializations of this algorithm to common variations of bisimulation for tree-structured XML data sets. We empirically verify efficient performance of the algorithms on graphs and XML documents having billions of nodes and edges, and find that the algorithms can process such graphs efficiently even when very limited internal memory is available. The proposed algorithms are simple enough for practical implementation and use, and open the door for further study of external-memory bisimulation algorithms. To this end, the full open-source C++ implementation has been made freely available. ",Computer Science - Data Structures and Algorithms ; Computer Science - Databases ; ,"Hellings, Jelle ; Fletcher, George H. L. ; Haverkort, Herman ; ","I/O efficient bisimulation partitioning on very large directed acyclic   graphs  In this paper we introduce the first efficient external-memory algorithm to compute the bisimilarity equivalence classes of a directed acyclic graph (DAG). DAGs are commonly used to model data in a wide variety of practical applications, ranging from XML documents and data provenance models, to web taxonomies and scientific workflows. In the study of efficient reasoning over massive graphs, the notion of node bisimilarity plays a central role. For example, grouping together bisimilar nodes in an XML data set is the first step in many sophisticated approaches to building indexing data structures for efficient XPath query evaluation. To date, however, only internal-memory bisimulation algorithms have been investigated. As the size of real-world DAG data sets often exceeds available main memory, storage in external memory becomes necessary. Hence, there is a practical need for an efficient approach to computing bisimulation in external memory.   Our general algorithm has a worst-case IO-complexity of O(Sort(|N| + |E|)), where |N| and |E| are the numbers of nodes and edges, resp., in the data graph and Sort(n) is the number of accesses to external memory needed to sort an input of size n. We also study specializations of this algorithm to common variations of bisimulation for tree-structured XML data sets. We empirically verify efficient performance of the algorithms on graphs and XML documents having billions of nodes and edges, and find that the algorithms can process such graphs efficiently even when very limited internal memory is available. The proposed algorithms are simple enough for practical implementation and use, and open the door for further study of external-memory bisimulation algorithms. To this end, the full open-source C++ implementation has been made freely available. ",efficient bisimulation partition large direct acyclic graph paper introduce first efficient external memory algorithm compute bisimilarity equivalence class direct acyclic graph dag dags commonly use model data wide variety practical applications range xml document data provenance model web taxonomies scientific workflows study efficient reason massive graph notion node bisimilarity play central role example group together bisimilar nod xml data set first step many sophisticate approach build index data structure efficient xpath query evaluation date however internal memory bisimulation algorithms investigate size real world dag data set often exceed available main memory storage external memory become necessary hence practical need efficient approach compute bisimulation external memory general algorithm worst case io complexity sort number nod edge resp data graph sort number access external memory need sort input size also study specializations algorithm common variations bisimulation tree structure xml data set empirically verify efficient performance algorithms graph xml document billions nod edge find algorithms process graph efficiently even limit internal memory available propose algorithms simple enough practical implementation use open door study external memory bisimulation algorithms end full open source implementation make freely available,182,3,1112.0857.txt
http://arxiv.org/abs/1112.2275,On Problems as Hard as CNFSAT,"  The field of exact exponential time algorithms for NP-hard problems has thrived over the last decade. While exhaustive search remains asymptotically the fastest known algorithm for some basic problems, difficult and non-trivial exponential time algorithms have been found for a myriad of problems, including Graph Coloring, Hamiltonian Path, Dominating Set and 3-CNF-Sat. In some instances, improving these algorithms further seems to be out of reach. The CNF-Sat problem is the canonical example of a problem for which the trivial exhaustive search algorithm runs in time O(2^n), where n is the number of variables in the input formula. While there exist non-trivial algorithms for CNF-Sat that run in time o(2^n), no algorithm was able to improve the growth rate 2 to a smaller constant, and hence it is natural to conjecture that 2 is the optimal growth rate. The strong exponential time hypothesis (SETH) by Impagliazzo and Paturi [JCSS 2001] goes a little bit further and asserts that, for every epsilon<1, there is a (large) integer k such that that k-CNF-Sat cannot be computed in time 2^{epsilon n}.   In this paper, we show that, for every epsilon < 1, the problems Hitting Set, Set Splitting, and NAE-Sat cannot be computed in time O(2^{epsilon n}) unless SETH fails. Here n is the number of elements or variables in the input. For these problems, we actually get an equivalence to SETH in a certain sense. We conjecture that SETH implies a similar statement for Set Cover, and prove that, under this assumption, the fastest known algorithms for Steinter Tree, Connected Vertex Cover, Set Partitioning, and the pseudo-polynomial time algorithm for Subset Sum cannot be significantly improved. Finally, we justify our assumption about the hardness of Set Cover by showing that the parity of the number of set covers cannot be computed in time O(2^{epsilon n}) for any epsilon<1 unless SETH fails. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; ,"Cygan, Marek ; Dell, Holger ; Lokshtanov, Daniel ; Marx, Daniel ; Nederlof, Jesper ; Okamoto, Yoshio ; Paturi, Ramamohan ; Saurabh, Saket ; Wahlstrom, Magnus ; ","On Problems as Hard as CNFSAT  The field of exact exponential time algorithms for NP-hard problems has thrived over the last decade. While exhaustive search remains asymptotically the fastest known algorithm for some basic problems, difficult and non-trivial exponential time algorithms have been found for a myriad of problems, including Graph Coloring, Hamiltonian Path, Dominating Set and 3-CNF-Sat. In some instances, improving these algorithms further seems to be out of reach. The CNF-Sat problem is the canonical example of a problem for which the trivial exhaustive search algorithm runs in time O(2^n), where n is the number of variables in the input formula. While there exist non-trivial algorithms for CNF-Sat that run in time o(2^n), no algorithm was able to improve the growth rate 2 to a smaller constant, and hence it is natural to conjecture that 2 is the optimal growth rate. The strong exponential time hypothesis (SETH) by Impagliazzo and Paturi [JCSS 2001] goes a little bit further and asserts that, for every epsilon<1, there is a (large) integer k such that that k-CNF-Sat cannot be computed in time 2^{epsilon n}.   In this paper, we show that, for every epsilon < 1, the problems Hitting Set, Set Splitting, and NAE-Sat cannot be computed in time O(2^{epsilon n}) unless SETH fails. Here n is the number of elements or variables in the input. For these problems, we actually get an equivalence to SETH in a certain sense. We conjecture that SETH implies a similar statement for Set Cover, and prove that, under this assumption, the fastest known algorithms for Steinter Tree, Connected Vertex Cover, Set Partitioning, and the pseudo-polynomial time algorithm for Subset Sum cannot be significantly improved. Finally, we justify our assumption about the hardness of Set Cover by showing that the parity of the number of set covers cannot be computed in time O(2^{epsilon n}) for any epsilon<1 unless SETH fails. ",problems hard cnfsat field exact exponential time algorithms np hard problems thrive last decade exhaustive search remain asymptotically fastest know algorithm basic problems difficult non trivial exponential time algorithms find myriad problems include graph color hamiltonian path dominate set cnf sit instance improve algorithms seem reach cnf sit problem canonical example problem trivial exhaustive search algorithm run time number variables input formula exist non trivial algorithms cnf sit run time algorithm able improve growth rate smaller constant hence natural conjecture optimal growth rate strong exponential time hypothesis seth impagliazzo paturi jcss go little bite assert every epsilon large integer cnf sit cannot compute time epsilon paper show every epsilon problems hit set set split nae sit cannot compute time epsilon unless seth fail number elements variables input problems actually get equivalence seth certain sense conjecture seth imply similar statement set cover prove assumption fastest know algorithms steinter tree connect vertex cover set partition pseudo polynomial time algorithm subset sum cannot significantly improve finally justify assumption hardness set cover show parity number set cover cannot compute time epsilon epsilon unless seth fail,181,1,1112.2275.txt
http://arxiv.org/abs/1201.0490,Scikit-learn: Machine Learning in Python,"  Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org. ",Computer Science - Machine Learning ; Computer Science - Mathematical Software ; ,"Pedregosa, Fabian ; Varoquaux, Gaël ; Gramfort, Alexandre ; Michel, Vincent ; Thirion, Bertrand ; Grisel, Olivier ; Blondel, Mathieu ; Müller, Andreas ; Nothman, Joel ; Louppe, Gilles ; Prettenhofer, Peter ; Weiss, Ron ; Dubourg, Vincent ; Vanderplas, Jake ; Passos, Alexandre ; Cournapeau, David ; Brucher, Matthieu ; Perrot, Matthieu ; Duchesnay, Édouard ; ","Scikit-learn: Machine Learning in Python  Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org. ",scikit learn machine learn python scikit learn python module integrate wide range state art machine learn algorithms medium scale supervise unsupervised problems package focus bring machine learn non specialists use general purpose high level language emphasis put ease use performance documentation api consistency minimal dependencies distribute simplify bsd license encourage use academic commercial settings source code binaries documentation download http scikit learn org,63,10,1201.0490.txt
http://arxiv.org/abs/1201.2845,Competition through selective inhibitory synchrony,"  Models of cortical neuronal circuits commonly depend on inhibitory feedback to control gain, provide signal normalization, and to selectively amplify signals using winner-take-all (WTA) dynamics. Such models generally assume that excitatory and inhibitory neurons are able to interact easily, because their axons and dendrites are co-localized in the same small volume. However, quantitative neuroanatomical studies of the dimensions of axonal and dendritic trees of neurons in the neocortex show that this co-localization assumption is not valid. In this paper we describe a simple modification to the WTA circuit design that permits the effects of distributed inhibitory neurons to be coupled through synchronization, and so allows a single WTA to be distributed widely in cortical space, well beyond the arborization of any single inhibitory neuron, and even across different cortical areas. We prove by non-linear contraction analysis, and demonstrate by simulation that distributed WTA sub-systems combined by such inhibitory synchrony are inherently stable. We show analytically that synchronization is substantially faster than winner selection. This circuit mechanism allows networks of independent WTAs to fully or partially compete with each other. ",Quantitative Biology - Neurons and Cognition ; Computer Science - Neural and Evolutionary Computing ; ,"Rutishauser, Ueli ; Slotine, Jean-Jacques ; Douglas, Rodney J. ; ","Competition through selective inhibitory synchrony  Models of cortical neuronal circuits commonly depend on inhibitory feedback to control gain, provide signal normalization, and to selectively amplify signals using winner-take-all (WTA) dynamics. Such models generally assume that excitatory and inhibitory neurons are able to interact easily, because their axons and dendrites are co-localized in the same small volume. However, quantitative neuroanatomical studies of the dimensions of axonal and dendritic trees of neurons in the neocortex show that this co-localization assumption is not valid. In this paper we describe a simple modification to the WTA circuit design that permits the effects of distributed inhibitory neurons to be coupled through synchronization, and so allows a single WTA to be distributed widely in cortical space, well beyond the arborization of any single inhibitory neuron, and even across different cortical areas. We prove by non-linear contraction analysis, and demonstrate by simulation that distributed WTA sub-systems combined by such inhibitory synchrony are inherently stable. We show analytically that synchronization is substantially faster than winner selection. This circuit mechanism allows networks of independent WTAs to fully or partially compete with each other. ",competition selective inhibitory synchrony model cortical neuronal circuit commonly depend inhibitory feedback control gain provide signal normalization selectively amplify signal use winner take wta dynamics model generally assume excitatory inhibitory neurons able interact easily axons dendrites co localize small volume however quantitative neuroanatomical study dimension axonal dendritic tree neurons neocortex show co localization assumption valid paper describe simple modification wta circuit design permit effect distribute inhibitory neurons couple synchronization allow single wta distribute widely cortical space well beyond arborization single inhibitory neuron even across different cortical areas prove non linear contraction analysis demonstrate simulation distribute wta sub systems combine inhibitory synchrony inherently stable show analytically synchronization substantially faster winner selection circuit mechanism allow network independent wtas fully partially compete,119,9,1201.2845.txt
http://arxiv.org/abs/1201.3416,Verifying Real-time Commit Protocols Using Dense-time Model Checking   Technology,"  The timed-based automata model, introduced by Alur and Dill, provides a useful formalism for describing real-time systems. Over the last two decades, several dense-time model checking tools have been developed based on that model. The paper considers the verification of real-time distributed commit protocols using dense-time model checking technology. More precisely, we model and verify the well-known timed two phase commit protocol in three different state-of-the-art real-time model checkers: UPPAAL, Rabbit, and RED, and compare the results. ",Computer Science - Software Engineering ; ,"Al-Bataineh, Omar I. ; Reynolds, Mark ; ","Verifying Real-time Commit Protocols Using Dense-time Model Checking   Technology  The timed-based automata model, introduced by Alur and Dill, provides a useful formalism for describing real-time systems. Over the last two decades, several dense-time model checking tools have been developed based on that model. The paper considers the verification of real-time distributed commit protocols using dense-time model checking technology. More precisely, we model and verify the well-known timed two phase commit protocol in three different state-of-the-art real-time model checkers: UPPAAL, Rabbit, and RED, and compare the results. ",verify real time commit protocols use dense time model check technology time base automata model introduce alur dill provide useful formalism describe real time systems last two decades several dense time model check tool develop base model paper consider verification real time distribute commit protocols use dense time model check technology precisely model verify well know time two phase commit protocol three different state art real time model checker uppaal rabbit red compare result,74,11,1201.3416.txt
http://arxiv.org/abs/1201.5921,An iterative algorithm for parametrization of shortest length shift   registers over finite rings,"  The construction of shortest feedback shift registers for a finite sequence S_1,...,S_N is considered over the finite ring Z_{p^r}. A novel algorithm is presented that yields a parametrization of all shortest feedback shift registers for the sequence of numbers S_1,...,S_N, thus solving an open problem in the literature. The algorithm iteratively processes each number, starting with S_1, and constructs at each step a particular type of minimal Gr\""obner basis. The construction involves a simple update rule at each step which leads to computational efficiency. It is shown that the algorithm simultaneously computes a similar parametrization for the reciprocal sequence S_N,...,S_1. ","Computer Science - Information Theory ; Computer Science - Symbolic Computation ; 94A55, 11T71 ; ","Kuijper, M. ; Pinto, R. ; ","An iterative algorithm for parametrization of shortest length shift   registers over finite rings  The construction of shortest feedback shift registers for a finite sequence S_1,...,S_N is considered over the finite ring Z_{p^r}. A novel algorithm is presented that yields a parametrization of all shortest feedback shift registers for the sequence of numbers S_1,...,S_N, thus solving an open problem in the literature. The algorithm iteratively processes each number, starting with S_1, and constructs at each step a particular type of minimal Gr\""obner basis. The construction involves a simple update rule at each step which leads to computational efficiency. It is shown that the algorithm simultaneously computes a similar parametrization for the reciprocal sequence S_N,...,S_1. ",iterative algorithm parametrization shortest length shift register finite ring construction shortest feedback shift register finite sequence consider finite ring novel algorithm present yield parametrization shortest feedback shift register sequence number thus solve open problem literature algorithm iteratively process number start construct step particular type minimal gr obner basis construction involve simple update rule step lead computational efficiency show algorithm simultaneously compute similar parametrization reciprocal sequence,65,4,1201.5921.txt
http://arxiv.org/abs/1201.6371,Standard decomposition of expansive ergodically supported dynamics,"  In this work we introduce the notion of weak quasigroups, that are quasigroup operations defined almost everywhere on some set. Then we prove that the topological entropy and the ergodic period of an invertible expansive ergodically supported dynamical system $(X,T)$ with the shadowing property establishes a sufficient criterion for the existence of quasigroup operations defined almost everywhere outside of universally null sets and for which $T$ is an automorphism. Furthermore, we find a decomposition of the dynamics of $T$ in terms of $T$-invariant weak topological subquasigroups. ","Mathematics - Dynamical Systems ; Computer Science - Information Theory ; Mathematics - Group Theory ; 20N05, 17C10, 94A55, 68P30 ; ","Sobottka, Marcelo ; ","Standard decomposition of expansive ergodically supported dynamics  In this work we introduce the notion of weak quasigroups, that are quasigroup operations defined almost everywhere on some set. Then we prove that the topological entropy and the ergodic period of an invertible expansive ergodically supported dynamical system $(X,T)$ with the shadowing property establishes a sufficient criterion for the existence of quasigroup operations defined almost everywhere outside of universally null sets and for which $T$ is an automorphism. Furthermore, we find a decomposition of the dynamics of $T$ in terms of $T$-invariant weak topological subquasigroups. ",standard decomposition expansive ergodically support dynamics work introduce notion weak quasigroups quasigroup operations define almost everywhere set prove topological entropy ergodic period invertible expansive ergodically support dynamical system shadow property establish sufficient criterion existence quasigroup operations define almost everywhere outside universally null set automorphism furthermore find decomposition dynamics term invariant weak topological subquasigroups,53,8,1201.6371.txt
http://arxiv.org/abs/1202.3538,Refinement Modal Logic,"  In this paper we present {\em refinement modal logic}. A refinement is like a bisimulation, except that from the three relational requirements only `atoms' and `back' need to be satisfied. Our logic contains a new operator 'all' in addition to the standard modalities 'box' for each agent. The operator 'all' acts as a quantifier over the set of all refinements of a given model. As a variation on a bisimulation quantifier, this refinement operator or refinement quantifier 'all' can be seen as quantifying over a variable not occurring in the formula bound by it. The logic combines the simplicity of multi-agent modal logic with some powers of monadic second-order quantification. We present a sound and complete axiomatization of multi-agent refinement modal logic. We also present an extension of the logic to the modal mu-calculus, and an axiomatization for the single-agent version of this logic. Examples and applications are also discussed: to software verification and design (the set of agents can also be seen as a set of actions), and to dynamic epistemic logic. We further give detailed results on the complexity of satisfiability, and on succinctness. ",Computer Science - Logic in Computer Science ; Computer Science - Artificial Intelligence ; ,"Bozzelli, Laura ; van Ditmarsch, Hans ; French, Tim ; Hales, James ; Pinchinat, Sophie ; ","Refinement Modal Logic  In this paper we present {\em refinement modal logic}. A refinement is like a bisimulation, except that from the three relational requirements only `atoms' and `back' need to be satisfied. Our logic contains a new operator 'all' in addition to the standard modalities 'box' for each agent. The operator 'all' acts as a quantifier over the set of all refinements of a given model. As a variation on a bisimulation quantifier, this refinement operator or refinement quantifier 'all' can be seen as quantifying over a variable not occurring in the formula bound by it. The logic combines the simplicity of multi-agent modal logic with some powers of monadic second-order quantification. We present a sound and complete axiomatization of multi-agent refinement modal logic. We also present an extension of the logic to the modal mu-calculus, and an axiomatization for the single-agent version of this logic. Examples and applications are also discussed: to software verification and design (the set of agents can also be seen as a set of actions), and to dynamic epistemic logic. We further give detailed results on the complexity of satisfiability, and on succinctness. ",refinement modal logic paper present em refinement modal logic refinement like bisimulation except three relational requirements atoms back need satisfy logic contain new operator addition standard modalities box agent operator act quantifier set refinements give model variation bisimulation quantifier refinement operator refinement quantifier see quantify variable occur formula bind logic combine simplicity multi agent modal logic power monadic second order quantification present sound complete axiomatization multi agent refinement modal logic also present extension logic modal mu calculus axiomatization single agent version logic examples applications also discuss software verification design set agents also see set action dynamic epistemic logic give detail result complexity satisfiability succinctness,104,8,1202.3538.txt
http://arxiv.org/abs/1202.3733,Lipschitz Parametrization of Probabilistic Graphical Models,"  We show that the log-likelihood of several probabilistic graphical models is Lipschitz continuous with respect to the lp-norm of the parameters. We discuss several implications of Lipschitz parametrization. We present an upper bound of the Kullback-Leibler divergence that allows understanding methods that penalize the lp-norm of differences of parameters as the minimization of that upper bound. The expected log-likelihood is lower bounded by the negative lp-norm, which allows understanding the generalization ability of probabilistic models. The exponential of the negative lp-norm is involved in the lower bound of the Bayes error rate, which shows that it is reasonable to use parameters as features in algorithms that rely on metric spaces (e.g. classification, dimensionality reduction, clustering). Our results do not rely on specific algorithms for learning the structure or parameters. We show preliminary results for activity recognition and temporal segmentation. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Honorio, Jean ; ","Lipschitz Parametrization of Probabilistic Graphical Models  We show that the log-likelihood of several probabilistic graphical models is Lipschitz continuous with respect to the lp-norm of the parameters. We discuss several implications of Lipschitz parametrization. We present an upper bound of the Kullback-Leibler divergence that allows understanding methods that penalize the lp-norm of differences of parameters as the minimization of that upper bound. The expected log-likelihood is lower bounded by the negative lp-norm, which allows understanding the generalization ability of probabilistic models. The exponential of the negative lp-norm is involved in the lower bound of the Bayes error rate, which shows that it is reasonable to use parameters as features in algorithms that rely on metric spaces (e.g. classification, dimensionality reduction, clustering). Our results do not rely on specific algorithms for learning the structure or parameters. We show preliminary results for activity recognition and temporal segmentation. ",lipschitz parametrization probabilistic graphical model show log likelihood several probabilistic graphical model lipschitz continuous respect lp norm parameters discuss several implications lipschitz parametrization present upper bind kullback leibler divergence allow understand methods penalize lp norm differences parameters minimization upper bind expect log likelihood lower bound negative lp norm allow understand generalization ability probabilistic model exponential negative lp norm involve lower bind bay error rate show reasonable use parameters feature algorithms rely metric space classification dimensionality reduction cluster result rely specific algorithms learn structure parameters show preliminary result activity recognition temporal segmentation,91,7,1202.3733.txt
http://arxiv.org/abs/1202.4707,A para-model agent for dynamical systems,"  Consider a dynamical system $u \mapsto x, \dot{x} = f_{nl}(x,u)$ where $f_{nl}$ is a nonlinear (convex or nonconvex) function, or a combination of nonlinear functions that can eventually switch. We present, in this preliminary work, a generalization of the standard model-free control, that can either control the dynamical system, given an output reference trajectory, or optimize the dynamical system as a derivative-free optimization based ""extremum-seeking"" procedure. Multiple applications are presented and the robustness of the proposed method is studied in simulation. ",Mathematics - Optimization and Control ; Computer Science - Systems and Control ; ,"Michel, Loïc ; ","A para-model agent for dynamical systems  Consider a dynamical system $u \mapsto x, \dot{x} = f_{nl}(x,u)$ where $f_{nl}$ is a nonlinear (convex or nonconvex) function, or a combination of nonlinear functions that can eventually switch. We present, in this preliminary work, a generalization of the standard model-free control, that can either control the dynamical system, given an output reference trajectory, or optimize the dynamical system as a derivative-free optimization based ""extremum-seeking"" procedure. Multiple applications are presented and the robustness of the proposed method is studied in simulation. ",para model agent dynamical systems consider dynamical system mapsto dot nl nl nonlinear convex nonconvex function combination nonlinear function eventually switch present preliminary work generalization standard model free control either control dynamical system give output reference trajectory optimize dynamical system derivative free optimization base extremum seek procedure multiple applications present robustness propose method study simulation,55,7,1202.4707.txt
http://arxiv.org/abs/1202.4910,Distributed Private Heavy Hitters,"  In this paper, we give efficient algorithms and lower bounds for solving the heavy hitters problem while preserving differential privacy in the fully distributed local model. In this model, there are n parties, each of which possesses a single element from a universe of size N. The heavy hitters problem is to find the identity of the most common element shared amongst the n parties. In the local model, there is no trusted database administrator, and so the algorithm must interact with each of the $n$ parties separately, using a differentially private protocol. We give tight information-theoretic upper and lower bounds on the accuracy to which this problem can be solved in the local model (giving a separation between the local model and the more common centralized model of privacy), as well as computationally efficient algorithms even in the case where the data universe N may be exponentially large. ",Computer Science - Data Structures and Algorithms ; Computer Science - Cryptography and Security ; Computer Science - Databases ; ,"Hsu, Justin ; Khanna, Sanjeev ; Roth, Aaron ; ","Distributed Private Heavy Hitters  In this paper, we give efficient algorithms and lower bounds for solving the heavy hitters problem while preserving differential privacy in the fully distributed local model. In this model, there are n parties, each of which possesses a single element from a universe of size N. The heavy hitters problem is to find the identity of the most common element shared amongst the n parties. In the local model, there is no trusted database administrator, and so the algorithm must interact with each of the $n$ parties separately, using a differentially private protocol. We give tight information-theoretic upper and lower bounds on the accuracy to which this problem can be solved in the local model (giving a separation between the local model and the more common centralized model of privacy), as well as computationally efficient algorithms even in the case where the data universe N may be exponentially large. ",distribute private heavy hitters paper give efficient algorithms lower bound solve heavy hitters problem preserve differential privacy fully distribute local model model party possess single element universe size heavy hitters problem find identity common element share amongst party local model trust database administrator algorithm must interact party separately use differentially private protocol give tight information theoretic upper lower bound accuracy problem solve local model give separation local model common centralize model privacy well computationally efficient algorithms even case data universe may exponentially large,83,12,1202.4910.txt
http://arxiv.org/abs/1202.4961,Strongly universal string hashing is fast,"  We present fast strongly universal string hashing families: they can process data at a rate of 0.2 CPU cycle per byte. Maybe surprisingly, we find that these families---though they require a large buffer of random numbers---are often faster than popular hash functions with weaker theoretical guarantees. Moreover, conventional wisdom is that hash functions with fewer multiplications are faster. Yet we find that they may fail to be faster due to operation pipelining. We present experimental results on several processors including low-powered processors. Our tests include hash functions designed for processors with the Carry-Less Multiplication (CLMUL) instruction set. We also prove, using accessible proofs, the strong universality of our families. ",Computer Science - Databases ; Computer Science - Data Structures and Algorithms ; ,"Kaser, Owen ; Lemire, Daniel ; ","Strongly universal string hashing is fast  We present fast strongly universal string hashing families: they can process data at a rate of 0.2 CPU cycle per byte. Maybe surprisingly, we find that these families---though they require a large buffer of random numbers---are often faster than popular hash functions with weaker theoretical guarantees. Moreover, conventional wisdom is that hash functions with fewer multiplications are faster. Yet we find that they may fail to be faster due to operation pipelining. We present experimental results on several processors including low-powered processors. Our tests include hash functions designed for processors with the Carry-Less Multiplication (CLMUL) instruction set. We also prove, using accessible proofs, the strong universality of our families. ",strongly universal string hash fast present fast strongly universal string hash families process data rate cpu cycle per byte maybe surprisingly find families though require large buffer random number often faster popular hash function weaker theoretical guarantee moreover conventional wisdom hash function fewer multiplications faster yet find may fail faster due operation pipelining present experimental result several processors include low power processors test include hash function design processors carry less multiplication clmul instruction set also prove use accessible proof strong universality families,82,7,1202.4961.txt
http://arxiv.org/abs/1203.3341,"A Comparison of the Embedding Method to Multi-Parametric Programming,   Mixed-Integer Programming, Gradient-Descent, and Hybrid Minimum Principle   Based Methods","  In recent years, the embedding approach for solving switched optimal control problems has been developed in a series of papers. However, the embedding approach, which advantageously converts the hybrid optimal control problem to a classical nonlinear optimization, has not been extensively compared to alternative solution approaches. The goal of this paper is thus to compare the embedding approach to multi-parametric programming, mixed-integer programming (e.g., CPLEX), and gradient-descent based methods in the context of five recently published examples: a spring-mass system, moving-target tracking for a mobile robot, two-tank filling, DC-DC boost converter, and skid-steered vehicle. A sixth example, an autonomous switched 11-region linear system, is used to compare a hybrid minimum principle method and traditional numerical programming. For a given performance index for each case, cost and solution times are presented. It is shown that there are numerical advantages of the embedding approach: lower performance index cost (except in some instances when autonomous switches are present), generally faster solution time, and convergence to a solution when other methods may fail. In addition, the embedding method requires no ad hoc assumptions (e.g., predetermined mode sequences) or specialized control models. Theoretical advantages of the embedding approach over the other methods are also described: guaranteed existence of a solution under mild conditions, convexity of the embedded hybrid optimization problem (under the customary conditions on the performance index), solvability with traditional techniques (e.g., sequential quadratic programming) avoiding the combinatorial complexity in the number of modes/discrete variables of mixed-integer programming, applicability to affine nonlinear systems, and no need to explicitly assign discrete/mode variables to autonomous switches. ",Mathematics - Optimization and Control ; Computer Science - Systems and Control ; ,"Meyer, Richard ; Žefran, Miloš ; DeCarlo, Raymond A. ; ","A Comparison of the Embedding Method to Multi-Parametric Programming,   Mixed-Integer Programming, Gradient-Descent, and Hybrid Minimum Principle   Based Methods  In recent years, the embedding approach for solving switched optimal control problems has been developed in a series of papers. However, the embedding approach, which advantageously converts the hybrid optimal control problem to a classical nonlinear optimization, has not been extensively compared to alternative solution approaches. The goal of this paper is thus to compare the embedding approach to multi-parametric programming, mixed-integer programming (e.g., CPLEX), and gradient-descent based methods in the context of five recently published examples: a spring-mass system, moving-target tracking for a mobile robot, two-tank filling, DC-DC boost converter, and skid-steered vehicle. A sixth example, an autonomous switched 11-region linear system, is used to compare a hybrid minimum principle method and traditional numerical programming. For a given performance index for each case, cost and solution times are presented. It is shown that there are numerical advantages of the embedding approach: lower performance index cost (except in some instances when autonomous switches are present), generally faster solution time, and convergence to a solution when other methods may fail. In addition, the embedding method requires no ad hoc assumptions (e.g., predetermined mode sequences) or specialized control models. Theoretical advantages of the embedding approach over the other methods are also described: guaranteed existence of a solution under mild conditions, convexity of the embedded hybrid optimization problem (under the customary conditions on the performance index), solvability with traditional techniques (e.g., sequential quadratic programming) avoiding the combinatorial complexity in the number of modes/discrete variables of mixed-integer programming, applicability to affine nonlinear systems, and no need to explicitly assign discrete/mode variables to autonomous switches. ",comparison embed method multi parametric program mix integer program gradient descent hybrid minimum principle base methods recent years embed approach solve switch optimal control problems develop series paper however embed approach advantageously convert hybrid optimal control problem classical nonlinear optimization extensively compare alternative solution approach goal paper thus compare embed approach multi parametric program mix integer program cplex gradient descent base methods context five recently publish examples spring mass system move target track mobile robot two tank fill dc dc boost converter skid steer vehicle sixth example autonomous switch region linear system use compare hybrid minimum principle method traditional numerical program give performance index case cost solution time present show numerical advantage embed approach lower performance index cost except instance autonomous switch present generally faster solution time convergence solution methods may fail addition embed method require ad hoc assumptions predetermine mode sequence specialize control model theoretical advantage embed approach methods also describe guarantee existence solution mild condition convexity embed hybrid optimization problem customary condition performance index solvability traditional techniques sequential quadratic program avoid combinatorial complexity number modes discrete variables mix integer program applicability affine nonlinear systems need explicitly assign discrete mode variables autonomous switch,194,11,1203.3341.txt
http://arxiv.org/abs/1203.4600,A Szemeredi-Trotter type theorem in $\mathbb{R}^4$,"  We show that $m$ points and $n$ two-dimensional algebraic surfaces in $\mathbb{R}^4$ can have at most $O(m^{\frac{k}{2k-1}}n^{\frac{2k-2}{2k-1}}+m+n)$ incidences, provided that the algebraic surfaces behave like pseudoflats with $k$ degrees of freedom, and that $m\leq n^{\frac{2k+2}{3k}}$. As a special case, we obtain a Szemer\'edi-Trotter type theorem for 2--planes in $\mathbb{R}^4$, provided $m\leq n$ and the planes intersect transversely. As a further special case, we obtain a Szemer\'edi-Trotter type theorem for complex lines in $\mathbb{C}^2$ with no restrictions on $m$ and $n$ (this theorem was originally proved by T\'oth using a different method). As a third special case, we obtain a Szemer\'edi-Trotter type theorem for complex unit circles in $\mathbb{C}^2$. We obtain our results by combining several tools, including a two-level analogue of the discrete polynomial partitioning theorem and the crossing lemma. ",Mathematics - Combinatorics ; Computer Science - Computational Geometry ; ,"Zahl, Joshua ; ","A Szemeredi-Trotter type theorem in $\mathbb{R}^4$  We show that $m$ points and $n$ two-dimensional algebraic surfaces in $\mathbb{R}^4$ can have at most $O(m^{\frac{k}{2k-1}}n^{\frac{2k-2}{2k-1}}+m+n)$ incidences, provided that the algebraic surfaces behave like pseudoflats with $k$ degrees of freedom, and that $m\leq n^{\frac{2k+2}{3k}}$. As a special case, we obtain a Szemer\'edi-Trotter type theorem for 2--planes in $\mathbb{R}^4$, provided $m\leq n$ and the planes intersect transversely. As a further special case, we obtain a Szemer\'edi-Trotter type theorem for complex lines in $\mathbb{C}^2$ with no restrictions on $m$ and $n$ (this theorem was originally proved by T\'oth using a different method). As a third special case, we obtain a Szemer\'edi-Trotter type theorem for complex unit circles in $\mathbb{C}^2$. We obtain our results by combining several tools, including a two-level analogue of the discrete polynomial partitioning theorem and the crossing lemma. ",szemeredi trotter type theorem mathbb show point two dimensional algebraic surface mathbb frac frac incidences provide algebraic surface behave like pseudoflats degrees freedom leq frac special case obtain szemer edi trotter type theorem plan mathbb provide leq plan intersect transversely special case obtain szemer edi trotter type theorem complex line mathbb restrictions theorem originally prove oth use different method third special case obtain szemer edi trotter type theorem complex unit circle mathbb obtain result combine several tool include two level analogue discrete polynomial partition theorem cross lemma,87,8,1203.4600.txt
http://arxiv.org/abs/1203.5184,A Universal Model of Commuting Networks,"  We test a recently proposed model of commuting networks on 80 case studies from different regions of the world (Europe and United-States) and with geographic units of different sizes (municipality, county, region). The model takes as input the number of commuters coming in and out of each geographic unit and generates the matrix of commuting flows betwen the geographic units. We show that the single parameter of the model, which rules the compromise between the influence of the distance and job opportunities, follows a universal law that depends only on the average surface of the geographic units. We verified that the law derived from a part of the case studies yields accurate results on other case studies. We also show that our model significantly outperforms the two other approaches proposing a universal commuting model (Balcan et al. (2009); Simini et al. (2012)), particularly when the geographic units are small (e.g. municipalities). ",Mathematics - Statistics Theory ; Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Lenormand, Maxime ; Huet, Sylvie ; Gargiulo, Floriana ; Deffuant, Guillaume ; ","A Universal Model of Commuting Networks  We test a recently proposed model of commuting networks on 80 case studies from different regions of the world (Europe and United-States) and with geographic units of different sizes (municipality, county, region). The model takes as input the number of commuters coming in and out of each geographic unit and generates the matrix of commuting flows betwen the geographic units. We show that the single parameter of the model, which rules the compromise between the influence of the distance and job opportunities, follows a universal law that depends only on the average surface of the geographic units. We verified that the law derived from a part of the case studies yields accurate results on other case studies. We also show that our model significantly outperforms the two other approaches proposing a universal commuting model (Balcan et al. (2009); Simini et al. (2012)), particularly when the geographic units are small (e.g. municipalities). ",universal model commute network test recently propose model commute network case study different regions world europe unite state geographic units different size municipality county region model take input number commuters come geographic unit generate matrix commute flow betwen geographic units show single parameter model rule compromise influence distance job opportunities follow universal law depend average surface geographic units verify law derive part case study yield accurate result case study also show model significantly outperform two approach propose universal commute model balcan et al simini et al particularly geographic units small municipalities,91,11,1203.5184.txt
http://arxiv.org/abs/1203.5188,Semi-Automatically Extracting FAQs to Improve Accessibility of Software   Development Knowledge,"  Frequently asked questions (FAQs) are a popular way to document software development knowledge. As creating such documents is expensive, this paper presents an approach for automatically extracting FAQs from sources of software development discussion, such as mailing lists and Internet forums, by combining techniques of text mining and natural language processing. We apply the approach to popular mailing lists and carry out a survey among software developers to show that it is able to extract high-quality FAQs that may be further improved by experts. ",Computer Science - Software Engineering ; Computer Science - Computation and Language ; Computer Science - Information Retrieval ; ,"Henß, Stefan ; Monperrus, Martin ; Mezini, Mira ; ","Semi-Automatically Extracting FAQs to Improve Accessibility of Software   Development Knowledge  Frequently asked questions (FAQs) are a popular way to document software development knowledge. As creating such documents is expensive, this paper presents an approach for automatically extracting FAQs from sources of software development discussion, such as mailing lists and Internet forums, by combining techniques of text mining and natural language processing. We apply the approach to popular mailing lists and carry out a survey among software developers to show that it is able to extract high-quality FAQs that may be further improved by experts. ",semi automatically extract faqs improve accessibility software development knowledge frequently ask question faqs popular way document software development knowledge create document expensive paper present approach automatically extract faqs source software development discussion mail list internet forums combine techniques text mine natural language process apply approach popular mail list carry survey among software developers show able extract high quality faqs may improve experts,62,10,1203.5188.txt
http://arxiv.org/abs/1203.5414,"Clique problem, cutting plane proofs and communication complexity","  Motivated by its relation to the length of cutting plane proofs for the Maximum Biclique problem, we consider the following communication game on a given graph G, known to both players. Let K be the maximal number of vertices in a complete bipartite subgraph of G, which is not necessarily an induced subgraph if G is not bipartite. Alice gets a set A of vertices, and Bob gets a disjoint set B of vertices such that |A|+|B|>K. The goal is to find a nonedge of G between A and B. We show that O(\log n) bits of communication are enough for every n-vertex graph. ",Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; ,"Jukna, S. ; ","Clique problem, cutting plane proofs and communication complexity  Motivated by its relation to the length of cutting plane proofs for the Maximum Biclique problem, we consider the following communication game on a given graph G, known to both players. Let K be the maximal number of vertices in a complete bipartite subgraph of G, which is not necessarily an induced subgraph if G is not bipartite. Alice gets a set A of vertices, and Bob gets a disjoint set B of vertices such that |A|+|B|>K. The goal is to find a nonedge of G between A and B. We show that O(\log n) bits of communication are enough for every n-vertex graph. ",clique problem cut plane proof communication complexity motivate relation length cut plane proof maximum biclique problem consider follow communication game give graph know players let maximal number vertices complete bipartite subgraph necessarily induce subgraph bipartite alice get set vertices bob get disjoint set vertices goal find nonedge show log bits communication enough every vertex graph,55,3,1203.5414.txt
http://arxiv.org/abs/1203.5706,Effective de Rham Cohomology - The General Case,"  Grothendieck has proved that each class in the de Rham cohomology of a smooth complex affine variety can be represented by a differential form with polynomial coefficients. After having proved a single exponential bound for the degrees of these forms in the case of a hypersurface, here we generalize this result to arbitrary codimension. More precisely, we show that the p-th de Rham cohomology of a smooth affine variety of dimension m and degree D can be represented by differential forms of degree (pD)^{O(pm)}. This result is relevant for the algorithmic computation of the cohomology, but is also motivated by questions in the theory of ordinary differential equations related to the infinitesimal Hilbert 16th problem. ","Mathematics - Algebraic Geometry ; Computer Science - Computational Complexity ; Mathematics - Commutative Algebra ; 14Q20, 14Q15, 68W30, 34C07 ; ","Scheiblechner, Peter ; ","Effective de Rham Cohomology - The General Case  Grothendieck has proved that each class in the de Rham cohomology of a smooth complex affine variety can be represented by a differential form with polynomial coefficients. After having proved a single exponential bound for the degrees of these forms in the case of a hypersurface, here we generalize this result to arbitrary codimension. More precisely, we show that the p-th de Rham cohomology of a smooth affine variety of dimension m and degree D can be represented by differential forms of degree (pD)^{O(pm)}. This result is relevant for the algorithmic computation of the cohomology, but is also motivated by questions in the theory of ordinary differential equations related to the infinitesimal Hilbert 16th problem. ",effective de rham cohomology general case grothendieck prove class de rham cohomology smooth complex affine variety represent differential form polynomial coefficients prove single exponential bind degrees form case hypersurface generalize result arbitrary codimension precisely show th de rham cohomology smooth affine variety dimension degree represent differential form degree pd pm result relevant algorithmic computation cohomology also motivate question theory ordinary differential equations relate infinitesimal hilbert th problem,67,8,1203.5706.txt
http://arxiv.org/abs/1203.6152,The FO^2 alternation hierarchy is decidable,"  We consider the two-variable fragment FO^2[<] of first-order logic over finite words. Numerous characterizations of this class are known. Th\'erien and Wilke have shown that it is decidable whether a given regular language is definable in FO^2[<]. From a practical point of view, as shown by Weis, FO^2[<] is interesting since its satisfiability problem is in NP. Restricting the number of quantifier alternations yields an infinite hierarchy inside the class of FO^2[<]-definable languages. We show that each level of this hierarchy is decidable. For this purpose, we relate each level of the hierarchy with a decidable variety of finite monoids. Our result implies that there are many different ways of climbing up the FO^2[<]-quantifier alternation hierarchy: deterministic and co-deterministic products, Mal'cev products with definite and reverse definite semigroups, iterated block products with J-trivial monoids, and some inductively defined omega-term identities. A combinatorial tool in the process of ascension is that of condensed rankers, a refinement of the rankers of Weis and Immerman and the turtle programs of Schwentick, Th\'erien, and Vollmer. ",Computer Science - Logic in Computer Science ; Computer Science - Formal Languages and Automata Theory ; ,"Kufleitner, Manfred ; Weil, Pascal ; ","The FO^2 alternation hierarchy is decidable  We consider the two-variable fragment FO^2[<] of first-order logic over finite words. Numerous characterizations of this class are known. Th\'erien and Wilke have shown that it is decidable whether a given regular language is definable in FO^2[<]. From a practical point of view, as shown by Weis, FO^2[<] is interesting since its satisfiability problem is in NP. Restricting the number of quantifier alternations yields an infinite hierarchy inside the class of FO^2[<]-definable languages. We show that each level of this hierarchy is decidable. For this purpose, we relate each level of the hierarchy with a decidable variety of finite monoids. Our result implies that there are many different ways of climbing up the FO^2[<]-quantifier alternation hierarchy: deterministic and co-deterministic products, Mal'cev products with definite and reverse definite semigroups, iterated block products with J-trivial monoids, and some inductively defined omega-term identities. A combinatorial tool in the process of ascension is that of condensed rankers, a refinement of the rankers of Weis and Immerman and the turtle programs of Schwentick, Th\'erien, and Vollmer. ",fo alternation hierarchy decidable consider two variable fragment fo first order logic finite word numerous characterizations class know th erien wilke show decidable whether give regular language definable fo practical point view show weis fo interest since satisfiability problem np restrict number quantifier alternations yield infinite hierarchy inside class fo definable languages show level hierarchy decidable purpose relate level hierarchy decidable variety finite monoids result imply many different ways climb fo quantifier alternation hierarchy deterministic co deterministic products mal cev products definite reverse definite semigroups iterate block products trivial monoids inductively define omega term identities combinatorial tool process ascension condense rankers refinement rankers weis immerman turtle program schwentick th erien vollmer,111,8,1203.6152.txt
http://arxiv.org/abs/1203.6286,On the Easiest and Hardest Fitness Functions,"  The hardness of fitness functions is an important research topic in the field of evolutionary computation. In theory, the study can help understanding the ability of evolutionary algorithms. In practice, the study may provide a guideline to the design of benchmarks. The aim of this paper is to answer the following research questions: Given a fitness function class, which functions are the easiest with respect to an evolutionary algorithm? Which are the hardest? How are these functions constructed? The paper provides theoretical answers to these questions. The easiest and hardest fitness functions are constructed for an elitist (1+1) evolutionary algorithm to maximise a class of fitness functions with the same optima. It is demonstrated that the unimodal functions are the easiest and deceptive functions are the hardest in terms of the time-fitness landscape. The paper also reveals that the easiest fitness function to one algorithm may become the hardest to another algorithm, and vice versa. ",Computer Science - Neural and Evolutionary Computing ; ,"He, Jun ; Chen, Tianshi ; Yao, Xin ; ","On the Easiest and Hardest Fitness Functions  The hardness of fitness functions is an important research topic in the field of evolutionary computation. In theory, the study can help understanding the ability of evolutionary algorithms. In practice, the study may provide a guideline to the design of benchmarks. The aim of this paper is to answer the following research questions: Given a fitness function class, which functions are the easiest with respect to an evolutionary algorithm? Which are the hardest? How are these functions constructed? The paper provides theoretical answers to these questions. The easiest and hardest fitness functions are constructed for an elitist (1+1) evolutionary algorithm to maximise a class of fitness functions with the same optima. It is demonstrated that the unimodal functions are the easiest and deceptive functions are the hardest in terms of the time-fitness landscape. The paper also reveals that the easiest fitness function to one algorithm may become the hardest to another algorithm, and vice versa. ",easiest hardest fitness function hardness fitness function important research topic field evolutionary computation theory study help understand ability evolutionary algorithms practice study may provide guideline design benchmarks aim paper answer follow research question give fitness function class function easiest respect evolutionary algorithm hardest function construct paper provide theoretical answer question easiest hardest fitness function construct elitist evolutionary algorithm maximise class fitness function optima demonstrate unimodal function easiest deceptive function hardest term time fitness landscape paper also reveal easiest fitness function one algorithm may become hardest another algorithm vice versa,89,7,1203.6286.txt
http://arxiv.org/abs/1203.6566,New Combinatorial Construction Techniques for Low-Density Parity-Check   Codes and Systematic Repeat-Accumulate Codes,"  This paper presents several new construction techniques for low-density parity-check (LDPC) and systematic repeat-accumulate (RA) codes. Based on specific classes of combinatorial designs, the improved code design focuses on high-rate structured codes with constant column weights 3 and higher. The proposed codes are efficiently encodable and exhibit good structural properties. Experimental results on decoding performance with the sum-product algorithm show that the novel codes offer substantial practical application potential, for instance, in high-speed applications in magnetic recording and optical communications channels. ",Computer Science - Information Theory ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Gruner, Alexander ; Huber, Michael ; ","New Combinatorial Construction Techniques for Low-Density Parity-Check   Codes and Systematic Repeat-Accumulate Codes  This paper presents several new construction techniques for low-density parity-check (LDPC) and systematic repeat-accumulate (RA) codes. Based on specific classes of combinatorial designs, the improved code design focuses on high-rate structured codes with constant column weights 3 and higher. The proposed codes are efficiently encodable and exhibit good structural properties. Experimental results on decoding performance with the sum-product algorithm show that the novel codes offer substantial practical application potential, for instance, in high-speed applications in magnetic recording and optical communications channels. ",new combinatorial construction techniques low density parity check cod systematic repeat accumulate cod paper present several new construction techniques low density parity check ldpc systematic repeat accumulate ra cod base specific class combinatorial design improve code design focus high rate structure cod constant column weight higher propose cod efficiently encodable exhibit good structural properties experimental result decode performance sum product algorithm show novel cod offer substantial practical application potential instance high speed applications magnetic record optical communications channel,78,5,1203.6566.txt
http://arxiv.org/abs/1204.0480,Deducing Security Goals From Shape Analysis Sentences,"  Guttman presented a model-theoretic approach to establishing security goals in the context of Strand Space theory. In his approach, a run of the Cryptographic Protocol Shapes Analyzer (CPSA) produces models that determine if a goal is satisfied. This paper presents a method for extracting a sentence that completely characterizes a run of CPSA. Logical deduction can then be used to determine if a goal is satisfied. This method has been implemented and is available to all. ",Computer Science - Cryptography and Security ; Computer Science - Logic in Computer Science ; ,"Ramsdell, John D. ; ","Deducing Security Goals From Shape Analysis Sentences  Guttman presented a model-theoretic approach to establishing security goals in the context of Strand Space theory. In his approach, a run of the Cryptographic Protocol Shapes Analyzer (CPSA) produces models that determine if a goal is satisfied. This paper presents a method for extracting a sentence that completely characterizes a run of CPSA. Logical deduction can then be used to determine if a goal is satisfied. This method has been implemented and is available to all. ",deduce security goals shape analysis sentence guttman present model theoretic approach establish security goals context strand space theory approach run cryptographic protocol shape analyzer cpsa produce model determine goal satisfy paper present method extract sentence completely characterize run cpsa logical deduction use determine goal satisfy method implement available,48,4,1204.0480.txt
http://arxiv.org/abs/1204.0839,A Constrained Random Demodulator for Sub-Nyquist Sampling,"  This paper presents a significant modification to the Random Demodulator (RD) of Tropp et al. for sub-Nyquist sampling of frequency-sparse signals. The modification, termed constrained random demodulator, involves replacing the random waveform, essential to the operation of the RD, with a constrained random waveform that has limits on its switching rate because fast switching waveforms may be hard to generate cleanly. The result is a relaxation on the hardware requirements with a slight, but manageable, decrease in the recovery guarantees. The paper also establishes the importance of properly choosing the statistics of the constrained random waveform. If the power spectrum of the random waveform matches the distribution on the tones of the input signal (i.e., the distribution is proportional to the power spectrum), then recovery of the input signal tones is improved. The theoretical guarantees provided in the paper are validated through extensive numerical simulations and phase transition plots. ",Computer Science - Information Theory ; ,"Harms, Andrew ; Bajwa, Waheed U. ; Calderbank, Robert ; ","A Constrained Random Demodulator for Sub-Nyquist Sampling  This paper presents a significant modification to the Random Demodulator (RD) of Tropp et al. for sub-Nyquist sampling of frequency-sparse signals. The modification, termed constrained random demodulator, involves replacing the random waveform, essential to the operation of the RD, with a constrained random waveform that has limits on its switching rate because fast switching waveforms may be hard to generate cleanly. The result is a relaxation on the hardware requirements with a slight, but manageable, decrease in the recovery guarantees. The paper also establishes the importance of properly choosing the statistics of the constrained random waveform. If the power spectrum of the random waveform matches the distribution on the tones of the input signal (i.e., the distribution is proportional to the power spectrum), then recovery of the input signal tones is improved. The theoretical guarantees provided in the paper are validated through extensive numerical simulations and phase transition plots. ",constrain random demodulator sub nyquist sample paper present significant modification random demodulator rd tropp et al sub nyquist sample frequency sparse signal modification term constrain random demodulator involve replace random waveform essential operation rd constrain random waveform limit switch rate fast switch waveforms may hard generate cleanly result relaxation hardware requirements slight manageable decrease recovery guarantee paper also establish importance properly choose statistics constrain random waveform power spectrum random waveform match distribution tone input signal distribution proportional power spectrum recovery input signal tone improve theoretical guarantee provide paper validate extensive numerical simulations phase transition plot,95,9,1204.0839.txt
http://arxiv.org/abs/1204.1160,Opinion formation in time-varying social networks: The case of the   naming game,"  We study the dynamics of the naming game as an opinion formation model on time-varying social networks. This agent-based model captures the essential features of the agreement dynamics by means of a memory-based negotiation process. Our study focuses on the impact of time-varying properties of the social network of the agents on the naming game dynamics. In particular, we perform a computational exploration of this model using simulations on top of real networks. We investigate the outcomes of the dynamics on two different types of time-varying data - (i) the networks vary on a day-to-day basis and (ii) the networks vary within very short intervals of time (20 seconds). In the first case, we find that networks with strong community structure hinder the system from reaching global agreement; the evolution of the naming game in these networks maintains clusters of coexisting opinions indefinitely leading to metastability. In the second case, we investigate the evolution of the naming game in perfect synchronization with the time evolution of the underlying social network shedding new light on the traditional emergent properties of the game that differ largely from what has been reported in the existing literature. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Maity, Suman Kalyan ; Manoj, T. Venkat ; Mukherjee, Animesh ; ","Opinion formation in time-varying social networks: The case of the   naming game  We study the dynamics of the naming game as an opinion formation model on time-varying social networks. This agent-based model captures the essential features of the agreement dynamics by means of a memory-based negotiation process. Our study focuses on the impact of time-varying properties of the social network of the agents on the naming game dynamics. In particular, we perform a computational exploration of this model using simulations on top of real networks. We investigate the outcomes of the dynamics on two different types of time-varying data - (i) the networks vary on a day-to-day basis and (ii) the networks vary within very short intervals of time (20 seconds). In the first case, we find that networks with strong community structure hinder the system from reaching global agreement; the evolution of the naming game in these networks maintains clusters of coexisting opinions indefinitely leading to metastability. In the second case, we investigate the evolution of the naming game in perfect synchronization with the time evolution of the underlying social network shedding new light on the traditional emergent properties of the game that differ largely from what has been reported in the existing literature. ",opinion formation time vary social network case name game study dynamics name game opinion formation model time vary social network agent base model capture essential feature agreement dynamics mean memory base negotiation process study focus impact time vary properties social network agents name game dynamics particular perform computational exploration model use simulations top real network investigate outcomes dynamics two different type time vary data network vary day day basis ii network vary within short intervals time second first case find network strong community structure hinder system reach global agreement evolution name game network maintain cluster coexist opinions indefinitely lead metastability second case investigate evolution name game perfect synchronization time evolution underlie social network shed new light traditional emergent properties game differ largely report exist literature,125,6,1204.1160.txt
http://arxiv.org/abs/1204.1846,Approximate Revenue Maximization with Multiple Items,"  Maximizing the revenue from selling _more than one_ good (or item) to a single buyer is a notoriously difficult problem, in stark contrast to the one-good case. For two goods, we show that simple ""one-dimensional"" mechanisms, such as selling the goods separately, _guarantee_ at least 73% of the optimal revenue when the valuations of the two goods are independent and identically distributed, and at least $50\%$ when they are independent. For the case of $k>2$ independent goods, we show that selling them separately guarantees at least a $c/\log^2 k$ fraction of the optimal revenue; and, for independent and identically distributed goods, we show that selling them as one bundle guarantees at least a $c/\log k$ fraction of the optimal revenue. Additional results compare the revenues from the two simple mechanisms of selling the goods separately and bundled, identify situations where bundling is optimal, and extend the analysis to multiple buyers. ",Computer Science - Computer Science and Game Theory ; ,"Hart, Sergiu ; Nisan, Noam ; ","Approximate Revenue Maximization with Multiple Items  Maximizing the revenue from selling _more than one_ good (or item) to a single buyer is a notoriously difficult problem, in stark contrast to the one-good case. For two goods, we show that simple ""one-dimensional"" mechanisms, such as selling the goods separately, _guarantee_ at least 73% of the optimal revenue when the valuations of the two goods are independent and identically distributed, and at least $50\%$ when they are independent. For the case of $k>2$ independent goods, we show that selling them separately guarantees at least a $c/\log^2 k$ fraction of the optimal revenue; and, for independent and identically distributed goods, we show that selling them as one bundle guarantees at least a $c/\log k$ fraction of the optimal revenue. Additional results compare the revenues from the two simple mechanisms of selling the goods separately and bundled, identify situations where bundling is optimal, and extend the analysis to multiple buyers. ",approximate revenue maximization multiple items maximize revenue sell one good item single buyer notoriously difficult problem stark contrast one good case two goods show simple one dimensional mechanisms sell goods separately guarantee least optimal revenue valuations two goods independent identically distribute least independent case independent goods show sell separately guarantee least log fraction optimal revenue independent identically distribute goods show sell one bundle guarantee least log fraction optimal revenue additional result compare revenues two simple mechanisms sell goods separately bundle identify situations bundle optimal extend analysis multiple buyers,88,0,1204.1846.txt
http://arxiv.org/abs/1204.1868,User-based key frame detection in social web video,"  Video search results and suggested videos on web sites are represented with a video thumbnail, which is manually selected by the video up-loader among three randomly generated ones (e.g., YouTube). In contrast, we present a grounded user-based approach for automatically detecting interesting key-frames within a video through aggregated users' replay interactions with the video player. Previous research has focused on content-based systems that have the benefit of analyzing a video without user interactions, but they are monolithic, because the resulting video thumbnails are the same regardless of the user preferences. We constructed a user interest function, which is based on aggregate video replays, and analyzed hundreds of user interactions. We found that the local maximum of the replaying activity stands for the semantics of information rich videos, such as lecture, and how-to. The concept of user-based key-frame detection could be applied to any video on the web, in order to generate a user-based and dynamic video thumbnail in search results. ",Computer Science - Multimedia ; Computer Science - Human-Computer Interaction ; Computer Science - Information Retrieval ; ,"Chorianopoulos, Konstantinos ; ","User-based key frame detection in social web video  Video search results and suggested videos on web sites are represented with a video thumbnail, which is manually selected by the video up-loader among three randomly generated ones (e.g., YouTube). In contrast, we present a grounded user-based approach for automatically detecting interesting key-frames within a video through aggregated users' replay interactions with the video player. Previous research has focused on content-based systems that have the benefit of analyzing a video without user interactions, but they are monolithic, because the resulting video thumbnails are the same regardless of the user preferences. We constructed a user interest function, which is based on aggregate video replays, and analyzed hundreds of user interactions. We found that the local maximum of the replaying activity stands for the semantics of information rich videos, such as lecture, and how-to. The concept of user-based key-frame detection could be applied to any video on the web, in order to generate a user-based and dynamic video thumbnail in search results. ",user base key frame detection social web video video search result suggest videos web sit represent video thumbnail manually select video loader among three randomly generate ones youtube contrast present ground user base approach automatically detect interest key frame within video aggregate users replay interactions video player previous research focus content base systems benefit analyze video without user interactions monolithic result video thumbnails regardless user preferences construct user interest function base aggregate video replay analyze hundreds user interactions find local maximum replay activity stand semantics information rich videos lecture concept user base key frame detection could apply video web order generate user base dynamic video thumbnail search result,108,11,1204.1868.txt
http://arxiv.org/abs/1204.2606,Privacy via the Johnson-Lindenstrauss Transform,"  Suppose that party A collects private information about its users, where each user's data is represented as a bit vector. Suppose that party B has a proprietary data mining algorithm that requires estimating the distance between users, such as clustering or nearest neighbors. We ask if it is possible for party A to publish some information about each user so that B can estimate the distance between users without being able to infer any private bit of a user. Our method involves projecting each user's representation into a random, lower-dimensional space via a sparse Johnson-Lindenstrauss transform and then adding Gaussian noise to each entry of the lower-dimensional representation. We show that the method preserves differential privacy---where the more privacy is desired, the larger the variance of the Gaussian noise. Further, we show how to approximate the true distances between users via only the lower-dimensional, perturbed data. Finally, we consider other perturbation methods such as randomized response and draw comparisons to sketch-based methods. While the goal of releasing user-specific data to third parties is more broad than preserving distances, this work shows that distance computations with privacy is an achievable goal. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computers and Society ; Computer Science - Databases ; Computer Science - Social and Information Networks ; K.4.1 ; F.2 ; H.3.5 ; G.3 ; I.5.3 ; H.3.3 ; H.2.8 ; E.1 ; G.1.3 ; ,"Kenthapadi, Krishnaram ; Korolova, Aleksandra ; Mironov, Ilya ; Mishra, Nina ; ","Privacy via the Johnson-Lindenstrauss Transform  Suppose that party A collects private information about its users, where each user's data is represented as a bit vector. Suppose that party B has a proprietary data mining algorithm that requires estimating the distance between users, such as clustering or nearest neighbors. We ask if it is possible for party A to publish some information about each user so that B can estimate the distance between users without being able to infer any private bit of a user. Our method involves projecting each user's representation into a random, lower-dimensional space via a sparse Johnson-Lindenstrauss transform and then adding Gaussian noise to each entry of the lower-dimensional representation. We show that the method preserves differential privacy---where the more privacy is desired, the larger the variance of the Gaussian noise. Further, we show how to approximate the true distances between users via only the lower-dimensional, perturbed data. Finally, we consider other perturbation methods such as randomized response and draw comparisons to sketch-based methods. While the goal of releasing user-specific data to third parties is more broad than preserving distances, this work shows that distance computations with privacy is an achievable goal. ",privacy via johnson lindenstrauss transform suppose party collect private information users user data represent bite vector suppose party proprietary data mine algorithm require estimate distance users cluster nearest neighbor ask possible party publish information user estimate distance users without able infer private bite user method involve project user representation random lower dimensional space via sparse johnson lindenstrauss transform add gaussian noise entry lower dimensional representation show method preserve differential privacy privacy desire larger variance gaussian noise show approximate true distance users via lower dimensional perturb data finally consider perturbation methods randomize response draw comparisons sketch base methods goal release user specific data third party broad preserve distance work show distance computations privacy achievable goal,114,10,1204.2606.txt
http://arxiv.org/abs/1204.2727,The Cost of Perfection for Matchings in Graphs,"  Perfect matchings and maximum weight matchings are two fundamental combinatorial structures. We consider the ratio between the maximum weight of a perfect matching and the maximum weight of a general matching. Motivated by the computer graphics application in triangle meshes, where we seek to convert a triangulation into a quadrangulation by merging pairs of adjacent triangles, we focus mainly on bridgeless cubic graphs. First, we characterize graphs that attain the extreme ratios. Second, we present a lower bound for all bridgeless cubic graphs. Third, we present upper bounds for subclasses of bridgeless cubic graphs, most of which are shown to be tight. Additionally, we present tight bounds for the class of regular bipartite graphs. ",Computer Science - Discrete Mathematics ; ,"Brazil, Emilio Vital ; da Fonseca, Guilherme D. ; de Figueiredo, Celina ; Sasaki, Diana ; ","The Cost of Perfection for Matchings in Graphs  Perfect matchings and maximum weight matchings are two fundamental combinatorial structures. We consider the ratio between the maximum weight of a perfect matching and the maximum weight of a general matching. Motivated by the computer graphics application in triangle meshes, where we seek to convert a triangulation into a quadrangulation by merging pairs of adjacent triangles, we focus mainly on bridgeless cubic graphs. First, we characterize graphs that attain the extreme ratios. Second, we present a lower bound for all bridgeless cubic graphs. Third, we present upper bounds for subclasses of bridgeless cubic graphs, most of which are shown to be tight. Additionally, we present tight bounds for the class of regular bipartite graphs. ",cost perfection match graph perfect match maximum weight match two fundamental combinatorial structure consider ratio maximum weight perfect match maximum weight general match motivate computer graphics application triangle mesh seek convert triangulation quadrangulation merge pair adjacent triangles focus mainly bridgeless cubic graph first characterize graph attain extreme ratios second present lower bind bridgeless cubic graph third present upper bound subclasses bridgeless cubic graph show tight additionally present tight bound class regular bipartite graph,73,3,1204.2727.txt
http://arxiv.org/abs/1204.3850,Simple Agents Learn to Find Their Way: An Introduction on Mapping   Polygons,"  This paper gives an introduction to the problem of mapping simple polygons with autonomous agents. We focus on minimalistic agents that move from vertex to vertex along straight lines inside a polygon, using their sensors to gather local observations at each vertex. Our attention revolves around the question whether a given configuration of sensors and movement capabilities of the agents allows them to capture enough data in order to draw conclusions regarding the global layout of the polygon. In particular, we study the problem of reconstructing the visibility graph of a simple polygon by an agent moving either inside or on the boundary of the polygon. Our aim is to provide insight about the algorithmic challenges faced by an agent trying to map a polygon. We present an overview of techniques for solving this problem with agents that are equipped with simple sensorial capabilities. We illustrate these techniques on examples with sensors that mea- sure angles between lines of sight or identify the previous location. We give an overview over related problems in combinatorial geometry as well as graph exploration. ",Computer Science - Computational Geometry ; ,"Chalopin, Jérémie ; Das, Shantanu ; Disser, Yann ; Mihalák, Matúš ; Widmayer, Peter ; ","Simple Agents Learn to Find Their Way: An Introduction on Mapping   Polygons  This paper gives an introduction to the problem of mapping simple polygons with autonomous agents. We focus on minimalistic agents that move from vertex to vertex along straight lines inside a polygon, using their sensors to gather local observations at each vertex. Our attention revolves around the question whether a given configuration of sensors and movement capabilities of the agents allows them to capture enough data in order to draw conclusions regarding the global layout of the polygon. In particular, we study the problem of reconstructing the visibility graph of a simple polygon by an agent moving either inside or on the boundary of the polygon. Our aim is to provide insight about the algorithmic challenges faced by an agent trying to map a polygon. We present an overview of techniques for solving this problem with agents that are equipped with simple sensorial capabilities. We illustrate these techniques on examples with sensors that mea- sure angles between lines of sight or identify the previous location. We give an overview over related problems in combinatorial geometry as well as graph exploration. ",simple agents learn find way introduction map polygons paper give introduction problem map simple polygons autonomous agents focus minimalistic agents move vertex vertex along straight line inside polygon use sensors gather local observations vertex attention revolve around question whether give configuration sensors movement capabilities agents allow capture enough data order draw conclusions regard global layout polygon particular study problem reconstruct visibility graph simple polygon agent move either inside boundary polygon aim provide insight algorithmic challenge face agent try map polygon present overview techniques solve problem agents equip simple sensorial capabilities illustrate techniques examples sensors mea sure angle line sight identify previous location give overview relate problems combinatorial geometry well graph exploration,111,4,1204.3850.txt
http://arxiv.org/abs/1204.6321,Efficient Video Indexing on the Web: A System that Leverages User   Interactions with a Video Player,"  In this paper, we propose a user-based video indexing method, that automatically generates thumbnails of the most important scenes of an online video stream, by analyzing users' interactions with a web video player. As a test bench to verify our idea we have extended the YouTube video player into the VideoSkip system. In addition, VideoSkip uses a web-database (Google Application Engine) to keep a record of some important parameters, such as the timing of basic user actions (play, pause, skip). Moreover, we implemented an algorithm that selects representative thumbnails. Finally, we populated the system with data from an experiment with nine users. We found that the VideoSkip system indexes video content by leveraging implicit users interactions, such as pause and thirty seconds skip. Our early findings point toward improvements of the web video player and its thumbnail generation technique. The VideSkip system could compliment content-based algorithms, in order to achieve efficient video-indexing in difficult videos, such as lectures or sports. ",Computer Science - Multimedia ; Computer Science - Digital Libraries ; Computer Science - Human-Computer Interaction ; Computer Science - Information Retrieval ; ,"Leftheriotis, Ioannis ; Gkonela, Chrysoula ; Chorianopoulos, Konstantinos ; ","Efficient Video Indexing on the Web: A System that Leverages User   Interactions with a Video Player  In this paper, we propose a user-based video indexing method, that automatically generates thumbnails of the most important scenes of an online video stream, by analyzing users' interactions with a web video player. As a test bench to verify our idea we have extended the YouTube video player into the VideoSkip system. In addition, VideoSkip uses a web-database (Google Application Engine) to keep a record of some important parameters, such as the timing of basic user actions (play, pause, skip). Moreover, we implemented an algorithm that selects representative thumbnails. Finally, we populated the system with data from an experiment with nine users. We found that the VideoSkip system indexes video content by leveraging implicit users interactions, such as pause and thirty seconds skip. Our early findings point toward improvements of the web video player and its thumbnail generation technique. The VideSkip system could compliment content-based algorithms, in order to achieve efficient video-indexing in difficult videos, such as lectures or sports. ",efficient video index web system leverage user interactions video player paper propose user base video index method automatically generate thumbnails important scenes online video stream analyze users interactions web video player test bench verify idea extend youtube video player videoskip system addition videoskip use web database google application engine keep record important parameters time basic user action play pause skip moreover implement algorithm select representative thumbnails finally populate system data experiment nine users find videoskip system index video content leverage implicit users interactions pause thirty second skip early find point toward improvements web video player thumbnail generation technique videskip system could compliment content base algorithms order achieve efficient video index difficult videos lecture sport,114,11,1204.6321.txt
http://arxiv.org/abs/1204.6445,A Complete Dichotomy Rises from the Capture of Vanishing Signatures,"  We prove a complexity dichotomy theorem for Holant problems over an arbitrary set of complex-valued symmetric constraint functions F on Boolean variables. This extends and unifies all previous dichotomies for Holant problems on symmetric constraint functions (taking values without a finite modulus). We define and characterize all symmetric vanishing signatures. They turned out to be essential to the complete classification of Holant problems. The dichotomy theorem has an explicit tractability criterion expressible in terms of holographic transformations. A Holant problem defined by a set of constraint functions F is solvable in polynomial time if it satisfies this tractability criterion, and is #P-hard otherwise. The tractability criterion can be intuitively stated as follows: A set F is tractable if (1) every function in F has arity at most two, or (2) F is transformable to an affine type, or (3) F is transformable to a product type, or (4) F is vanishing, combined with the right type of binary functions, or (5) F belongs to a special category of vanishing type Fibonacci gates. The proof of this theorem utilizes many previous dichotomy theorems on Holant problems and Boolean #CSP. Holographic transformations play an indispensable role as both a proof technique and in the statement of the tractability criterion. ",Computer Science - Computational Complexity ; 68Q17 ; F.1.3 ; G.2.1 ; ,"Cai, Jin-Yi ; Guo, Heng ; Williams, Tyson ; ","A Complete Dichotomy Rises from the Capture of Vanishing Signatures  We prove a complexity dichotomy theorem for Holant problems over an arbitrary set of complex-valued symmetric constraint functions F on Boolean variables. This extends and unifies all previous dichotomies for Holant problems on symmetric constraint functions (taking values without a finite modulus). We define and characterize all symmetric vanishing signatures. They turned out to be essential to the complete classification of Holant problems. The dichotomy theorem has an explicit tractability criterion expressible in terms of holographic transformations. A Holant problem defined by a set of constraint functions F is solvable in polynomial time if it satisfies this tractability criterion, and is #P-hard otherwise. The tractability criterion can be intuitively stated as follows: A set F is tractable if (1) every function in F has arity at most two, or (2) F is transformable to an affine type, or (3) F is transformable to a product type, or (4) F is vanishing, combined with the right type of binary functions, or (5) F belongs to a special category of vanishing type Fibonacci gates. The proof of this theorem utilizes many previous dichotomy theorems on Holant problems and Boolean #CSP. Holographic transformations play an indispensable role as both a proof technique and in the statement of the tractability criterion. ",complete dichotomy rise capture vanish signatures prove complexity dichotomy theorem holant problems arbitrary set complex value symmetric constraint function boolean variables extend unify previous dichotomies holant problems symmetric constraint function take value without finite modulus define characterize symmetric vanish signatures turn essential complete classification holant problems dichotomy theorem explicit tractability criterion expressible term holographic transformations holant problem define set constraint function solvable polynomial time satisfy tractability criterion hard otherwise tractability criterion intuitively state follow set tractable every function arity two transformable affine type transformable product type vanish combine right type binary function belong special category vanish type fibonacci gate proof theorem utilize many previous dichotomy theorems holant problems boolean csp holographic transformations play indispensable role proof technique statement tractability criterion,120,8,1204.6445.txt
http://arxiv.org/abs/1205.3576,Dexpler: Converting Android Dalvik Bytecode to Jimple for Static   Analysis with Soot,"  This paper introduces Dexpler, a software package which converts Dalvik bytecode to Jimple. Dexpler is built on top of Dedexer and Soot. As Jimple is Soot's main internal rep- resentation of code, the Dalvik bytecode can be manipu- lated with any Jimple based tool, for instance for performing point-to or flow analysis. ",Computer Science - Software Engineering ; ,"Bartel, Alexandre ; Klein, Jacques ; Monperrus, Martin ; Traon, Yves Le ; ","Dexpler: Converting Android Dalvik Bytecode to Jimple for Static   Analysis with Soot  This paper introduces Dexpler, a software package which converts Dalvik bytecode to Jimple. Dexpler is built on top of Dedexer and Soot. As Jimple is Soot's main internal rep- resentation of code, the Dalvik bytecode can be manipu- lated with any Jimple based tool, for instance for performing point-to or flow analysis. ",dexpler convert android dalvik bytecode jimple static analysis soot paper introduce dexpler software package convert dalvik bytecode jimple dexpler build top dedexer soot jimple soot main internal rep resentation code dalvik bytecode manipu lated jimple base tool instance perform point flow analysis,42,8,1205.3576.txt
http://arxiv.org/abs/1205.4874,Perfect Secrecy Systems Immune to Spoofing Attacks,"  We present novel perfect secrecy systems that provide immunity to spoofing attacks under equiprobable source probability distributions. On the theoretical side, relying on an existence result for $t$-designs by Teirlinck, our construction method constructively generates systems that can reach an arbitrary high level of security. On the practical side, we obtain, via cyclic difference families, very efficient constructions of new optimal systems that are onefold secure against spoofing. Moreover, we construct, by means of $t$-designs for large values of $t$, the first near-optimal systems that are 5- and 6-fold secure as well as further systems with a feasible number of keys that are 7-fold secure against spoofing. We apply our results furthermore to a recently extended authentication model, where the opponent has access to a verification oracle. We obtain this way novel perfect secrecy systems with immunity to spoofing in the verification oracle model. ",Computer Science - Cryptography and Security ; Computer Science - Information Theory ; ,"Huber, Michael ; ","Perfect Secrecy Systems Immune to Spoofing Attacks  We present novel perfect secrecy systems that provide immunity to spoofing attacks under equiprobable source probability distributions. On the theoretical side, relying on an existence result for $t$-designs by Teirlinck, our construction method constructively generates systems that can reach an arbitrary high level of security. On the practical side, we obtain, via cyclic difference families, very efficient constructions of new optimal systems that are onefold secure against spoofing. Moreover, we construct, by means of $t$-designs for large values of $t$, the first near-optimal systems that are 5- and 6-fold secure as well as further systems with a feasible number of keys that are 7-fold secure against spoofing. We apply our results furthermore to a recently extended authentication model, where the opponent has access to a verification oracle. We obtain this way novel perfect secrecy systems with immunity to spoofing in the verification oracle model. ",perfect secrecy systems immune spoof attack present novel perfect secrecy systems provide immunity spoof attack equiprobable source probability distributions theoretical side rely existence result design teirlinck construction method constructively generate systems reach arbitrary high level security practical side obtain via cyclic difference families efficient constructions new optimal systems onefold secure spoof moreover construct mean design large value first near optimal systems fold secure well systems feasible number key fold secure spoof apply result furthermore recently extend authentication model opponent access verification oracle obtain way novel perfect secrecy systems immunity spoof verification oracle model,93,8,1205.4874.txt
http://arxiv.org/abs/1205.5770,Randomized Extended Kaczmarz for Solving Least-Squares,  We present a randomized iterative algorithm that exponentially converges in expectation to the minimum Euclidean norm least squares solution of a given linear system of equations. The expected number of arithmetic operations required to obtain an estimate of given accuracy is proportional to the square condition number of the system multiplied by the number of non-zeros entries of the input matrix. The proposed algorithm is an extension of the randomized Kaczmarz method that was analyzed by Strohmer and Vershynin. ,Mathematics - Numerical Analysis ; Computer Science - Data Structures and Algorithms ; ,"Zouzias, Anastasios ; Freris, Nikolaos ; ",Randomized Extended Kaczmarz for Solving Least-Squares  We present a randomized iterative algorithm that exponentially converges in expectation to the minimum Euclidean norm least squares solution of a given linear system of equations. The expected number of arithmetic operations required to obtain an estimate of given accuracy is proportional to the square condition number of the system multiplied by the number of non-zeros entries of the input matrix. The proposed algorithm is an extension of the randomized Kaczmarz method that was analyzed by Strohmer and Vershynin. ,randomize extend kaczmarz solve least square present randomize iterative algorithm exponentially converge expectation minimum euclidean norm least square solution give linear system equations expect number arithmetic operations require obtain estimate give accuracy proportional square condition number system multiply number non zero entries input matrix propose algorithm extension randomize kaczmarz method analyze strohmer vershynin,53,4,1205.5770.txt
http://arxiv.org/abs/1205.6363,What Should Developers Be Aware Of? An Empirical Study on the Directives   of API Documentation,  Application Programming Interfaces (API) are exposed to developers in order to reuse software libraries. API directives are natural-language statements in API documentation that make developers aware of constraints and guidelines related to the usage of an API. This paper presents the design and the results of an empirical study on the directives of API documentation of object-oriented libraries. Its main contribution is to propose and extensively discuss a taxonomy of 23 kinds of API directives. ,Computer Science - Software Engineering ; ,"Monperrus, Martin ; Eichberg, Michael ; Tekes, Elif ; Mezini, Mira ; ",What Should Developers Be Aware Of? An Empirical Study on the Directives   of API Documentation  Application Programming Interfaces (API) are exposed to developers in order to reuse software libraries. API directives are natural-language statements in API documentation that make developers aware of constraints and guidelines related to the usage of an API. This paper presents the design and the results of an empirical study on the directives of API documentation of object-oriented libraries. Its main contribution is to propose and extensively discuss a taxonomy of 23 kinds of API directives. ,developers aware empirical study directives api documentation application program interfaces api expose developers order reuse software libraries api directives natural language statements api documentation make developers aware constraints guidelines relate usage api paper present design result empirical study directives api documentation object orient libraries main contribution propose extensively discuss taxonomy kinds api directives,53,4,1205.6363.txt
http://arxiv.org/abs/1206.1775,Exponential Time Complexity of the Permanent and the Tutte Polynomial,"  We show conditional lower bounds for well-studied #P-hard problems:   (a) The number of satisfying assignments of a 2-CNF formula with n variables cannot be counted in time exp(o(n)), and the same is true for computing the number of all independent sets in an n-vertex graph.   (b) The permanent of an n x n matrix with entries 0 and 1 cannot be computed in time exp(o(n)).   (c) The Tutte polynomial of an n-vertex multigraph cannot be computed in time exp(o(n)) at most evaluation points (x,y) in the case of multigraphs, and it cannot be computed in time exp(o(n/polylog n)) in the case of simple graphs.   Our lower bounds are relative to (variants of) the Exponential Time Hypothesis (ETH), which says that the satisfiability of n-variable 3-CNF formulas cannot be decided in time exp(o(n)). We relax this hypothesis by introducing its counting version #ETH, namely that the satisfying assignments cannot be counted in time exp(o(n)). In order to use #ETH for our lower bounds, we transfer the sparsification lemma for d-CNF formulas to the counting setting. ",Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; F.2.1 ; G.2.1 ; ,"Dell, Holger ; Husfeldt, Thore ; Marx, Dániel ; Taslaman, Nina ; Wáhlen, Martin ; ","Exponential Time Complexity of the Permanent and the Tutte Polynomial  We show conditional lower bounds for well-studied #P-hard problems:   (a) The number of satisfying assignments of a 2-CNF formula with n variables cannot be counted in time exp(o(n)), and the same is true for computing the number of all independent sets in an n-vertex graph.   (b) The permanent of an n x n matrix with entries 0 and 1 cannot be computed in time exp(o(n)).   (c) The Tutte polynomial of an n-vertex multigraph cannot be computed in time exp(o(n)) at most evaluation points (x,y) in the case of multigraphs, and it cannot be computed in time exp(o(n/polylog n)) in the case of simple graphs.   Our lower bounds are relative to (variants of) the Exponential Time Hypothesis (ETH), which says that the satisfiability of n-variable 3-CNF formulas cannot be decided in time exp(o(n)). We relax this hypothesis by introducing its counting version #ETH, namely that the satisfying assignments cannot be counted in time exp(o(n)). In order to use #ETH for our lower bounds, we transfer the sparsification lemma for d-CNF formulas to the counting setting. ",exponential time complexity permanent tutte polynomial show conditional lower bound well study hard problems number satisfy assignments cnf formula variables cannot count time exp true compute number independent set vertex graph permanent matrix entries cannot compute time exp tutte polynomial vertex multigraph cannot compute time exp evaluation point case multigraphs cannot compute time exp polylog case simple graph lower bound relative variants exponential time hypothesis eth say satisfiability variable cnf formulas cannot decide time exp relax hypothesis introduce count version eth namely satisfy assignments cannot count time exp order use eth lower bound transfer sparsification lemma cnf formulas count set,100,1,1206.1775.txt
http://arxiv.org/abs/1206.3431,Computability and analysis: the legacy of Alan Turing,  We discuss the legacy of Alan Turing and his impact on computability and analysis. ,Mathematics - Logic ; Computer Science - Logic in Computer Science ; Mathematics - History and Overview ; ,"Avigad, Jeremy ; Brattka, Vasco ; ",Computability and analysis: the legacy of Alan Turing  We discuss the legacy of Alan Turing and his impact on computability and analysis. ,computability analysis legacy alan turing discuss legacy alan turing impact computability analysis,12,8,1206.3431.txt
http://arxiv.org/abs/1206.3862,Total coloring of 1-toroidal graphs of maximum degree at least 11 and no   adjacent triangles,"  A {\em total coloring} of a graph $G$ is an assignment of colors to the vertices and the edges of $G$ such that every pair of adjacent/incident elements receive distinct colors. The {\em total chromatic number} of a graph $G$, denoted by $\chiup''(G)$, is the minimum number of colors in a total coloring of $G$. The well-known Total Coloring Conjecture (TCC) says that every graph with maximum degree $\Delta$ admits a total coloring with at most $\Delta + 2$ colors. A graph is {\em $1$-toroidal} if it can be drawn in torus such that every edge crosses at most one other edge. In this paper, we investigate the total coloring of $1$-toroidal graphs, and prove that the TCC holds for the $1$-toroidal graphs with maximum degree at least~$11$ and some restrictions on the triangles. Consequently, if $G$ is a $1$-toroidal graph with maximum degree $\Delta$ at least~$11$ and without adjacent triangles, then $G$ admits a total coloring with at most $\Delta + 2$ colors. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C15 ; ,"Wang, Tao ; ","Total coloring of 1-toroidal graphs of maximum degree at least 11 and no   adjacent triangles  A {\em total coloring} of a graph $G$ is an assignment of colors to the vertices and the edges of $G$ such that every pair of adjacent/incident elements receive distinct colors. The {\em total chromatic number} of a graph $G$, denoted by $\chiup''(G)$, is the minimum number of colors in a total coloring of $G$. The well-known Total Coloring Conjecture (TCC) says that every graph with maximum degree $\Delta$ admits a total coloring with at most $\Delta + 2$ colors. A graph is {\em $1$-toroidal} if it can be drawn in torus such that every edge crosses at most one other edge. In this paper, we investigate the total coloring of $1$-toroidal graphs, and prove that the TCC holds for the $1$-toroidal graphs with maximum degree at least~$11$ and some restrictions on the triangles. Consequently, if $G$ is a $1$-toroidal graph with maximum degree $\Delta$ at least~$11$ and without adjacent triangles, then $G$ admits a total coloring with at most $\Delta + 2$ colors. ",total color toroidal graph maximum degree least adjacent triangles em total color graph assignment color vertices edge every pair adjacent incident elements receive distinct color em total chromatic number graph denote chiup minimum number color total color well know total color conjecture tcc say every graph maximum degree delta admit total color delta color graph em toroidal draw torus every edge cross one edge paper investigate total color toroidal graph prove tcc hold toroidal graph maximum degree least restrictions triangles consequently toroidal graph maximum degree delta least without adjacent triangles admit total color delta color,95,13,1206.3862.txt
http://arxiv.org/abs/1206.4627,Convergence Rates of Biased Stochastic Optimization for Learning Sparse   Ising Models,"  We study the convergence rate of stochastic optimization of exact (NP-hard) objectives, for which only biased estimates of the gradient are available. We motivate this problem in the context of learning the structure and parameters of Ising models. We first provide a convergence-rate analysis of deterministic errors for forward-backward splitting (FBS). We then extend our analysis to biased stochastic errors, by first characterizing a family of samplers and providing a high probability bound that allows understanding not only FBS, but also proximal gradient (PG) methods. We derive some interesting conclusions: FBS requires only a logarithmically increasing number of random samples in order to converge (although at a very low rate); the required number of random samples is the same for the deterministic and the biased stochastic setting for FBS and basic PG; accelerated PG is not guaranteed to converge in the biased stochastic setting. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Honorio, Jean ; ","Convergence Rates of Biased Stochastic Optimization for Learning Sparse   Ising Models  We study the convergence rate of stochastic optimization of exact (NP-hard) objectives, for which only biased estimates of the gradient are available. We motivate this problem in the context of learning the structure and parameters of Ising models. We first provide a convergence-rate analysis of deterministic errors for forward-backward splitting (FBS). We then extend our analysis to biased stochastic errors, by first characterizing a family of samplers and providing a high probability bound that allows understanding not only FBS, but also proximal gradient (PG) methods. We derive some interesting conclusions: FBS requires only a logarithmically increasing number of random samples in order to converge (although at a very low rate); the required number of random samples is the same for the deterministic and the biased stochastic setting for FBS and basic PG; accelerated PG is not guaranteed to converge in the biased stochastic setting. ",convergence rat bias stochastic optimization learn sparse ising model study convergence rate stochastic optimization exact np hard objectives bias estimate gradient available motivate problem context learn structure parameters ising model first provide convergence rate analysis deterministic errors forward backward split fbs extend analysis bias stochastic errors first characterize family samplers provide high probability bind allow understand fbs also proximal gradient pg methods derive interest conclusions fbs require logarithmically increase number random sample order converge although low rate require number random sample deterministic bias stochastic set fbs basic pg accelerate pg guarantee converge bias stochastic set,95,9,1206.4627.txt
http://arxiv.org/abs/1206.4656,Machine Learning that Matters,"  Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field?s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters. ",Computer Science - Machine Learning ; Computer Science - Artificial Intelligence ; Statistics - Machine Learning ; ,"Wagstaff, Kiri ; ","Machine Learning that Matters  Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field?s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters. ",machine learn matter much current machine learn ml research lose connection problems import larger world science society perspective exist glare limitations data set investigate metrics employ evaluation degree result communicate back originate domains change need conduct research increase impact ml present six impact challenge explicitly focus field energy attention discuss exist obstacles must address aim inspire ongoing discussion focus ml matter,61,10,1206.4656.txt
http://arxiv.org/abs/1206.4809,Connected Choice and the Brouwer Fixed Point Theorem,"  We study the computational content of the Brouwer Fixed Point Theorem in the Weihrauch lattice. Connected choice is the operation that finds a point in a non-empty connected closed set given by negative information. One of our main results is that for any fixed dimension the Brouwer Fixed Point Theorem of that dimension is computably equivalent to connected choice of the Euclidean unit cube of the same dimension. Another main result is that connected choice is complete for dimension greater than or equal to two in the sense that it is computably equivalent to Weak K\H{o}nig's Lemma. While we can present two independent proofs for dimension three and upwards that are either based on a simple geometric construction or a combinatorial argument, the proof for dimension two is based on a more involved inverse limit construction. The connected choice operation in dimension one is known to be equivalent to the Intermediate Value Theorem; we prove that this problem is not idempotent in contrast to the case of dimension two and upwards. We also prove that Lipschitz continuity with Lipschitz constants strictly larger than one does not simplify finding fixed points. Finally, we prove that finding a connectedness component of a closed subset of the Euclidean unit cube of any dimension greater or equal to one is equivalent to Weak K\H{o}nig's Lemma. In order to describe these results, we introduce a representation of closed subsets of the unit cube by trees of rational complexes. ","Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03D30, 03D78, 03F60, 37C25 ; ","Brattka, Vasco ; Roux, Stéphane Le ; Miller, Joseph S. ; Pauly, Arno ; ","Connected Choice and the Brouwer Fixed Point Theorem  We study the computational content of the Brouwer Fixed Point Theorem in the Weihrauch lattice. Connected choice is the operation that finds a point in a non-empty connected closed set given by negative information. One of our main results is that for any fixed dimension the Brouwer Fixed Point Theorem of that dimension is computably equivalent to connected choice of the Euclidean unit cube of the same dimension. Another main result is that connected choice is complete for dimension greater than or equal to two in the sense that it is computably equivalent to Weak K\H{o}nig's Lemma. While we can present two independent proofs for dimension three and upwards that are either based on a simple geometric construction or a combinatorial argument, the proof for dimension two is based on a more involved inverse limit construction. The connected choice operation in dimension one is known to be equivalent to the Intermediate Value Theorem; we prove that this problem is not idempotent in contrast to the case of dimension two and upwards. We also prove that Lipschitz continuity with Lipschitz constants strictly larger than one does not simplify finding fixed points. Finally, we prove that finding a connectedness component of a closed subset of the Euclidean unit cube of any dimension greater or equal to one is equivalent to Weak K\H{o}nig's Lemma. In order to describe these results, we introduce a representation of closed subsets of the unit cube by trees of rational complexes. ",connect choice brouwer fix point theorem study computational content brouwer fix point theorem weihrauch lattice connect choice operation find point non empty connect close set give negative information one main result fix dimension brouwer fix point theorem dimension computably equivalent connect choice euclidean unit cube dimension another main result connect choice complete dimension greater equal two sense computably equivalent weak nig lemma present two independent proof dimension three upwards either base simple geometric construction combinatorial argument proof dimension two base involve inverse limit construction connect choice operation dimension one know equivalent intermediate value theorem prove problem idempotent contrast case dimension two upwards also prove lipschitz continuity lipschitz constants strictly larger one simplify find fix point finally prove find connectedness component close subset euclidean unit cube dimension greater equal one equivalent weak nig lemma order describe result introduce representation close subsets unit cube tree rational complexes,145,4,1206.4809.txt
http://arxiv.org/abs/1206.5336,Near-Optimal Online Multiselection in Internal and External Memory,"  We introduce an online version of the multiselection problem, in which q selection queries are requested on an unsorted array of n elements. We provide the first online algorithm that is 1-competitive with Kaligosi et al. [ICALP 2005] in terms of comparison complexity. Our algorithm also supports online search queries efficiently.   We then extend our algorithm to the dynamic setting, while retaining online functionality, by supporting arbitrary insertions and deletions on the array. Assuming that the insertion of an element is immediately preceded by a search for that element, we show that our dynamic online algorithm performs an optimal number of comparisons, up to lower order terms and an additive O(n) term.   For the external memory model, we describe the first online multiselection algorithm that is O(1)-competitive. This result improves upon the work of Sibeyn [Journal of Algorithms 2006] when q > m, where m is the number of blocks that can be stored in main memory. We also extend it to support searches, insertions, and deletions of elements efficiently. ",Computer Science - Data Structures and Algorithms ; ,"Barbay, Jérémy ; Gupta, Ankur ; Rao, S. Srinivasa ; Sorenson, Jonathan ; ","Near-Optimal Online Multiselection in Internal and External Memory  We introduce an online version of the multiselection problem, in which q selection queries are requested on an unsorted array of n elements. We provide the first online algorithm that is 1-competitive with Kaligosi et al. [ICALP 2005] in terms of comparison complexity. Our algorithm also supports online search queries efficiently.   We then extend our algorithm to the dynamic setting, while retaining online functionality, by supporting arbitrary insertions and deletions on the array. Assuming that the insertion of an element is immediately preceded by a search for that element, we show that our dynamic online algorithm performs an optimal number of comparisons, up to lower order terms and an additive O(n) term.   For the external memory model, we describe the first online multiselection algorithm that is O(1)-competitive. This result improves upon the work of Sibeyn [Journal of Algorithms 2006] when q > m, where m is the number of blocks that can be stored in main memory. We also extend it to support searches, insertions, and deletions of elements efficiently. ",near optimal online multiselection internal external memory introduce online version multiselection problem selection query request unsorted array elements provide first online algorithm competitive kaligosi et al icalp term comparison complexity algorithm also support online search query efficiently extend algorithm dynamic set retain online functionality support arbitrary insertions deletions array assume insertion element immediately precede search element show dynamic online algorithm perform optimal number comparisons lower order term additive term external memory model describe first online multiselection algorithm competitive result improve upon work sibeyn journal algorithms number block store main memory also extend support search insertions deletions elements efficiently,98,1,1206.5336.txt
http://arxiv.org/abs/1208.0713,On logical hierarchies within FO^2-definable languages,"  We consider the class of languages defined in the 2-variable fragment of the first-order logic of the linear order. Many interesting characterizations of this class are known, as well as the fact that restricting the number of quantifier alternations yields an infinite hierarchy whose levels are varieties of languages (and hence admit an algebraic characterization). Using this algebraic approach, we show that the quantifier alternation hierarchy inside FO^{2}[<] is decidable within one unit. For this purpose, we relate each level of the hierarchy with decidable varieties of languages, which can be defined in terms of iterated deterministic and co-deterministic products. A crucial notion in this process is that of condensed rankers, a refinement of the rankers of Weis and Immerman and the turtle languages of Schwentick, Th\'erien and Vollmer. ",Computer Science - Logic in Computer Science ; Computer Science - Formal Languages and Automata Theory ; F.4.3 ; F.4.1 ; ,"Kufleitner, Manfred ; Weil, Pascal ; ","On logical hierarchies within FO^2-definable languages  We consider the class of languages defined in the 2-variable fragment of the first-order logic of the linear order. Many interesting characterizations of this class are known, as well as the fact that restricting the number of quantifier alternations yields an infinite hierarchy whose levels are varieties of languages (and hence admit an algebraic characterization). Using this algebraic approach, we show that the quantifier alternation hierarchy inside FO^{2}[<] is decidable within one unit. For this purpose, we relate each level of the hierarchy with decidable varieties of languages, which can be defined in terms of iterated deterministic and co-deterministic products. A crucial notion in this process is that of condensed rankers, a refinement of the rankers of Weis and Immerman and the turtle languages of Schwentick, Th\'erien and Vollmer. ",logical hierarchies within fo definable languages consider class languages define variable fragment first order logic linear order many interest characterizations class know well fact restrict number quantifier alternations yield infinite hierarchy whose level varieties languages hence admit algebraic characterization use algebraic approach show quantifier alternation hierarchy inside fo decidable within one unit purpose relate level hierarchy decidable varieties languages define term iterate deterministic co deterministic products crucial notion process condense rankers refinement rankers weis immerman turtle languages schwentick th erien vollmer,81,8,1208.0713.txt
http://arxiv.org/abs/1208.3124,On the computation of zone and double zone diagrams,"  Classical objects in computational geometry are defined by explicit relations. Several years ago the pioneering works of T. Asano, J. Matousek and T. Tokuyama introduced ""implicit computational geometry"", in which the geometric objects are defined by implicit relations involving sets. An important member in this family is called ""a zone diagram"". The implicit nature of zone diagrams implies, as already observed in the original works, that their computation is a challenging task. In a continuous setting this task has been addressed (briefly) only by these authors in the Euclidean plane with point sites. We discuss the possibility to compute zone diagrams in a wide class of spaces and also shed new light on their computation in the original setting. The class of spaces, which is introduced here, includes, in particular, Euclidean spheres and finite dimensional strictly convex normed spaces. Sites of a general form are allowed and it is shown that a generalization of the iterative method suggested by Asano, Matousek and Tokuyama converges to a double zone diagram, another implicit geometric object whose existence is known in general. Occasionally a zone diagram can be obtained from this procedure. The actual (approximate) computation of the iterations is based on a simple algorithm which enables the approximate computation of Voronoi diagrams in a general setting. Our analysis also yields a few byproducts of independent interest, such as certain topological properties of Voronoi cells (e.g., that in the considered setting their boundaries cannot be ""fat""). ","Computer Science - Computational Geometry ; Mathematics - Functional Analysis ; Mathematics - Metric Geometry ; 68U05, 47H10, 51M05, 53C22, 46B20, 65D18, 51N05 ; F.2.2 ; G.0 ; I.3.5 ; ","Reem, Daniel ; ","On the computation of zone and double zone diagrams  Classical objects in computational geometry are defined by explicit relations. Several years ago the pioneering works of T. Asano, J. Matousek and T. Tokuyama introduced ""implicit computational geometry"", in which the geometric objects are defined by implicit relations involving sets. An important member in this family is called ""a zone diagram"". The implicit nature of zone diagrams implies, as already observed in the original works, that their computation is a challenging task. In a continuous setting this task has been addressed (briefly) only by these authors in the Euclidean plane with point sites. We discuss the possibility to compute zone diagrams in a wide class of spaces and also shed new light on their computation in the original setting. The class of spaces, which is introduced here, includes, in particular, Euclidean spheres and finite dimensional strictly convex normed spaces. Sites of a general form are allowed and it is shown that a generalization of the iterative method suggested by Asano, Matousek and Tokuyama converges to a double zone diagram, another implicit geometric object whose existence is known in general. Occasionally a zone diagram can be obtained from this procedure. The actual (approximate) computation of the iterations is based on a simple algorithm which enables the approximate computation of Voronoi diagrams in a general setting. Our analysis also yields a few byproducts of independent interest, such as certain topological properties of Voronoi cells (e.g., that in the considered setting their boundaries cannot be ""fat""). ",computation zone double zone diagram classical object computational geometry define explicit relations several years ago pioneer work asano matousek tokuyama introduce implicit computational geometry geometric object define implicit relations involve set important member family call zone diagram implicit nature zone diagram imply already observe original work computation challenge task continuous set task address briefly author euclidean plane point sit discuss possibility compute zone diagram wide class space also shed new light computation original set class space introduce include particular euclidean spheres finite dimensional strictly convex normed space sit general form allow show generalization iterative method suggest asano matousek tokuyama converge double zone diagram another implicit geometric object whose existence know general occasionally zone diagram obtain procedure actual approximate computation iterations base simple algorithm enable approximate computation voronoi diagram general set analysis also yield byproducts independent interest certain topological properties voronoi cells consider set boundaries cannot fat,146,4,1208.3124.txt
http://arxiv.org/abs/1208.3251,Toward Resource-Optimal Consensus over the Wireless Medium,"  We carry out a comprehensive study of the resource cost of averaging consensus in wireless networks. Most previous approaches suppose a graphical network, which abstracts away crucial features of the wireless medium, and measure resource consumption only in terms of the total number of transmissions required to achieve consensus. Under a path-loss dominated model, we study the resource requirements of consensus with respect to three wireless-appropriate metrics: total transmit energy, elapsed time, and time-bandwidth product. First we characterize the performance of several popular gossip algorithms, showing that they may be order-optimal with respect to transmit energy but are strictly suboptimal with respect to elapsed time and time-bandwidth product. Further, we propose a new consensus scheme, termed hierarchical averaging, and show that it is nearly order-optimal with respect to all three metrics. Finally, we examine the effects of quantization, showing that hierarchical averaging provides a nearly order-optimal tradeoff between resource consumption and quantization error. ",Computer Science - Information Theory ; ,"Nokleby, Matthew ; Bajwa, Waheed U. ; Calderbank, Robert ; Aazhang, Behnaam ; ","Toward Resource-Optimal Consensus over the Wireless Medium  We carry out a comprehensive study of the resource cost of averaging consensus in wireless networks. Most previous approaches suppose a graphical network, which abstracts away crucial features of the wireless medium, and measure resource consumption only in terms of the total number of transmissions required to achieve consensus. Under a path-loss dominated model, we study the resource requirements of consensus with respect to three wireless-appropriate metrics: total transmit energy, elapsed time, and time-bandwidth product. First we characterize the performance of several popular gossip algorithms, showing that they may be order-optimal with respect to transmit energy but are strictly suboptimal with respect to elapsed time and time-bandwidth product. Further, we propose a new consensus scheme, termed hierarchical averaging, and show that it is nearly order-optimal with respect to all three metrics. Finally, we examine the effects of quantization, showing that hierarchical averaging provides a nearly order-optimal tradeoff between resource consumption and quantization error. ",toward resource optimal consensus wireless medium carry comprehensive study resource cost average consensus wireless network previous approach suppose graphical network abstract away crucial feature wireless medium measure resource consumption term total number transmissions require achieve consensus path loss dominate model study resource requirements consensus respect three wireless appropriate metrics total transmit energy elapse time time bandwidth product first characterize performance several popular gossip algorithms show may order optimal respect transmit energy strictly suboptimal respect elapse time time bandwidth product propose new consensus scheme term hierarchical average show nearly order optimal respect three metrics finally examine effect quantization show hierarchical average provide nearly order optimal tradeoff resource consumption quantization error,109,11,1208.3251.txt
http://arxiv.org/abs/1208.6408,Java Source-code Clustering: Unifying Syntactic and Semantic Features,"  This is a companion draft to paper 'Software Clustering: Unifying Syntactic and Semantic Features', in proceedings of the 19th Working Conference on Reverse Engineering (WCRE 2012). It discusses the clustering process in detail, which appeared in the paper in an abridged form. It also contains certain additional process steps which were not covered in the WCRE paper. The clustering process is described for applications with Java source-code. However, as argued in the WCRE paper, it can be seamlessly adapted to many other programming paradigms. ",Computer Science - Software Engineering ; D.2.7 ; ,"Misra, Janardan ; Kaulgud, Vikrant ; Titus, Gary ; KM, Annervaz ; Sengupta, Shubhashis ; ","Java Source-code Clustering: Unifying Syntactic and Semantic Features  This is a companion draft to paper 'Software Clustering: Unifying Syntactic and Semantic Features', in proceedings of the 19th Working Conference on Reverse Engineering (WCRE 2012). It discusses the clustering process in detail, which appeared in the paper in an abridged form. It also contains certain additional process steps which were not covered in the WCRE paper. The clustering process is described for applications with Java source-code. However, as argued in the WCRE paper, it can be seamlessly adapted to many other programming paradigms. ",java source code cluster unify syntactic semantic feature companion draft paper software cluster unify syntactic semantic feature proceed th work conference reverse engineer wcre discuss cluster process detail appear paper abridge form also contain certain additional process step cover wcre paper cluster process describe applications java source code however argue wcre paper seamlessly adapt many program paradigms,57,0,1208.6408.txt
http://arxiv.org/abs/1209.0521,Efficient EM Training of Gaussian Mixtures with Missing Data,"  In data-mining applications, we are frequently faced with a large fraction of missing entries in the data matrix, which is problematic for most discriminant machine learning algorithms. A solution that we explore in this paper is the use of a generative model (a mixture of Gaussians) to compute the conditional expectation of the missing variables given the observed variables. Since training a Gaussian mixture with many different patterns of missing values can be computationally very expensive, we introduce a spanning-tree based algorithm that significantly speeds up training in these conditions. We also observe that good results can be obtained by using the generative model to fill-in the missing values for a separate discriminant learning algorithm. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Delalleau, Olivier ; Courville, Aaron ; Bengio, Yoshua ; ","Efficient EM Training of Gaussian Mixtures with Missing Data  In data-mining applications, we are frequently faced with a large fraction of missing entries in the data matrix, which is problematic for most discriminant machine learning algorithms. A solution that we explore in this paper is the use of a generative model (a mixture of Gaussians) to compute the conditional expectation of the missing variables given the observed variables. Since training a Gaussian mixture with many different patterns of missing values can be computationally very expensive, we introduce a spanning-tree based algorithm that significantly speeds up training in these conditions. We also observe that good results can be obtained by using the generative model to fill-in the missing values for a separate discriminant learning algorithm. ",efficient em train gaussian mixtures miss data data mine applications frequently face large fraction miss entries data matrix problematic discriminant machine learn algorithms solution explore paper use generative model mixture gaussians compute conditional expectation miss variables give observe variables since train gaussian mixture many different pattern miss value computationally expensive introduce span tree base algorithm significantly speed train condition also observe good result obtain use generative model fill miss value separate discriminant learn algorithm,74,10,1209.0521.txt
http://arxiv.org/abs/1209.0735,Lambert W Function for Applications in Physics,"  The Lambert W(x) function and its possible applications in physics are presented. The actual numerical implementation in C++ consists of Halley's and Fritsch's iterations with initial approximations based on branch-point expansion, asymptotic series, rational fits, and continued-logarithm recursion. ",Computer Science - Mathematical Software ; Computer Science - Numerical Analysis ; Physics - Computational Physics ; ,"Veberic, Darko ; ","Lambert W Function for Applications in Physics  The Lambert W(x) function and its possible applications in physics are presented. The actual numerical implementation in C++ consists of Halley's and Fritsch's iterations with initial approximations based on branch-point expansion, asymptotic series, rational fits, and continued-logarithm recursion. ",lambert function applications physics lambert function possible applications physics present actual numerical implementation consist halley fritsch iterations initial approximations base branch point expansion asymptotic series rational fit continue logarithm recursion,30,11,1209.0735.txt
http://arxiv.org/abs/1209.1711,Programming Languages for Scientific Computing,"  Scientific computation is a discipline that combines numerical analysis, physical understanding, algorithm development, and structured programming. Several yottacycles per year on the world's largest computers are spent simulating problems as diverse as weather prediction, the properties of material composites, the behavior of biomolecules in solution, and the quantum nature of chemical compounds. This article is intended to review specfic languages features and their use in computational science. We will review the strengths and weaknesses of different programming styles, with examples taken from widely used scientific codes. ","Computer Science - Programming Languages ; Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Mathematical Software ; ","Knepley, Matthew G. ; ","Programming Languages for Scientific Computing  Scientific computation is a discipline that combines numerical analysis, physical understanding, algorithm development, and structured programming. Several yottacycles per year on the world's largest computers are spent simulating problems as diverse as weather prediction, the properties of material composites, the behavior of biomolecules in solution, and the quantum nature of chemical compounds. This article is intended to review specfic languages features and their use in computational science. We will review the strengths and weaknesses of different programming styles, with examples taken from widely used scientific codes. ",program languages scientific compute scientific computation discipline combine numerical analysis physical understand algorithm development structure program several yottacycles per year world largest computers spend simulate problems diverse weather prediction properties material composites behavior biomolecules solution quantum nature chemical compound article intend review specfic languages feature use computational science review strengths weaknesses different program style examples take widely use scientific cod,60,4,1209.1711.txt
http://arxiv.org/abs/1209.5527,Strategic Learning and the Topology of Social Networks,"  We consider a group of strategic agents who must each repeatedly take one of two possible actions. They learn which of the two actions is preferable from initial private signals, and by observing the actions of their neighbors in a social network.   We show that the question of whether or not the agents learn efficiently depends on the topology of the social network. In particular, we identify a geometric ""egalitarianism"" condition on the social network that guarantees learning in infinite networks, or learning with high probability in large finite networks, in any equilibrium. We also give examples of non-egalitarian networks with equilibria in which learning fails. ",Computer Science - Computer Science and Game Theory ; Economics - Theoretical Economics ; Mathematics - Probability ; ,"Mossel, Elchanan ; Sly, Allan ; Tamuz, Omer ; ","Strategic Learning and the Topology of Social Networks  We consider a group of strategic agents who must each repeatedly take one of two possible actions. They learn which of the two actions is preferable from initial private signals, and by observing the actions of their neighbors in a social network.   We show that the question of whether or not the agents learn efficiently depends on the topology of the social network. In particular, we identify a geometric ""egalitarianism"" condition on the social network that guarantees learning in infinite networks, or learning with high probability in large finite networks, in any equilibrium. We also give examples of non-egalitarian networks with equilibria in which learning fails. ",strategic learn topology social network consider group strategic agents must repeatedly take one two possible action learn two action preferable initial private signal observe action neighbor social network show question whether agents learn efficiently depend topology social network particular identify geometric egalitarianism condition social network guarantee learn infinite network learn high probability large finite network equilibrium also give examples non egalitarian network equilibria learn fail,65,6,1209.5527.txt
http://arxiv.org/abs/1209.5785,Coupling Data Transmission for Multiple-Access Communications,"  We consider a signaling format where the information to be communicated from one or multiple transmitters to a receiver is modulated via a superposition of independent data streams. Each data stream is formed by error-correction encoding, constellation mapping, replication and permutation of symbols, and application of signature sequences. The relations between the data bits and modulation symbols transmitted over the channel can be represented by a sparse graph. In the case where the modulated data streams are transmitted with time offsets the receiver observes spatial coupling of the individual graphs into a graph chain enabling efficient demodulation/decoding. We prove that a two-stage demodulation/decoding method, in which iterative demodulation based on symbol estimation and interference cancellation is followed by parallel error correction decoding, achieves capacity on the additive white Gaussian noise (AWGN) channel asymptotically. We compare the performance of the two-stage receiver to the receiver which utilizes hard feedback between the error-correction encoders and the iterative demodulator. ",Computer Science - Information Theory ; ,"Truhachev, Dmitri ; Schlegel, Christian ; ","Coupling Data Transmission for Multiple-Access Communications  We consider a signaling format where the information to be communicated from one or multiple transmitters to a receiver is modulated via a superposition of independent data streams. Each data stream is formed by error-correction encoding, constellation mapping, replication and permutation of symbols, and application of signature sequences. The relations between the data bits and modulation symbols transmitted over the channel can be represented by a sparse graph. In the case where the modulated data streams are transmitted with time offsets the receiver observes spatial coupling of the individual graphs into a graph chain enabling efficient demodulation/decoding. We prove that a two-stage demodulation/decoding method, in which iterative demodulation based on symbol estimation and interference cancellation is followed by parallel error correction decoding, achieves capacity on the additive white Gaussian noise (AWGN) channel asymptotically. We compare the performance of the two-stage receiver to the receiver which utilizes hard feedback between the error-correction encoders and the iterative demodulator. ",couple data transmission multiple access communications consider signal format information communicate one multiple transmitters receiver modulate via superposition independent data stream data stream form error correction encode constellation map replication permutation symbols application signature sequence relations data bits modulation symbols transmit channel represent sparse graph case modulate data stream transmit time offset receiver observe spatial couple individual graph graph chain enable efficient demodulation decode prove two stage demodulation decode method iterative demodulation base symbol estimation interference cancellation follow parallel error correction decode achieve capacity additive white gaussian noise awgn channel asymptotically compare performance two stage receiver receiver utilize hard feedback error correction encoders iterative demodulator,105,9,1209.5785.txt
http://arxiv.org/abs/1209.6626,On Newton-Raphson iteration for multiplicative inverses modulo prime   powers,"  We study algorithms for the fast computation of modular inverses. Newton-Raphson iteration over $p$-adic numbers gives a recurrence relation computing modular inverse modulo $p^m$, that is logarithmic in $m$. We solve the recurrence to obtain an explicit formula for the inverse. Then we study different implementation variants of this iteration and show that our explicit formula is interesting for small exponent values but slower or large exponent, say of more than $700$ bits. Overall we thus propose a hybrid combination of our explicit formula and the best asymptotic variants. This hybrid combination yields then a constant factor improvement, also for large exponents. ",Computer Science - Symbolic Computation ; Computer Science - Mathematical Software ; ,"Dumas, Jean-Guillaume ; ","On Newton-Raphson iteration for multiplicative inverses modulo prime   powers  We study algorithms for the fast computation of modular inverses. Newton-Raphson iteration over $p$-adic numbers gives a recurrence relation computing modular inverse modulo $p^m$, that is logarithmic in $m$. We solve the recurrence to obtain an explicit formula for the inverse. Then we study different implementation variants of this iteration and show that our explicit formula is interesting for small exponent values but slower or large exponent, say of more than $700$ bits. Overall we thus propose a hybrid combination of our explicit formula and the best asymptotic variants. This hybrid combination yields then a constant factor improvement, also for large exponents. ",newton raphson iteration multiplicative inverses modulo prime power study algorithms fast computation modular inverses newton raphson iteration adic number give recurrence relation compute modular inverse modulo logarithmic solve recurrence obtain explicit formula inverse study different implementation variants iteration show explicit formula interest small exponent value slower large exponent say bits overall thus propose hybrid combination explicit formula best asymptotic variants hybrid combination yield constant factor improvement also large exponents,69,4,1209.6626.txt
http://arxiv.org/abs/1210.2246,An empirical study to order citation statistics between subject fields,"  An empirical study is conducted to compare citations per publication, statistics and observed Hirsch indexes between subject fields using summary statistics of countries. No distributional assumptions are made and ratios are calculated. These ratios can be used to make approximate comparisons between researchers of different subject fields with respect to the Hirsch index. ",Computer Science - Digital Libraries ; Physics - Physics and Society ; 62P99 ; ,"van Zyl, J. Martin ; van der Merwe, Sean ; ","An empirical study to order citation statistics between subject fields  An empirical study is conducted to compare citations per publication, statistics and observed Hirsch indexes between subject fields using summary statistics of countries. No distributional assumptions are made and ratios are calculated. These ratios can be used to make approximate comparisons between researchers of different subject fields with respect to the Hirsch index. ",empirical study order citation statistics subject field empirical study conduct compare citations per publication statistics observe hirsch index subject field use summary statistics countries distributional assumptions make ratios calculate ratios use make approximate comparisons researchers different subject field respect hirsch index,41,12,1210.2246.txt
http://arxiv.org/abs/1210.2440,"Group Model Selection Using Marginal Correlations: The Good, the Bad and   the Ugly","  Group model selection is the problem of determining a small subset of groups of predictors (e.g., the expression data of genes) that are responsible for majority of the variation in a response variable (e.g., the malignancy of a tumor). This paper focuses on group model selection in high-dimensional linear models, in which the number of predictors far exceeds the number of samples of the response variable. Existing works on high-dimensional group model selection either require the number of samples of the response variable to be significantly larger than the total number of predictors contributing to the response or impose restrictive statistical priors on the predictors and/or nonzero regression coefficients. This paper provides comprehensive understanding of a low-complexity approach to group model selection that avoids some of these limitations. The proposed approach, termed Group Thresholding (GroTh), is based on thresholding of marginal correlations of groups of predictors with the response variable and is reminiscent of existing thresholding-based approaches in the literature. The most important contribution of the paper in this regard is relating the performance of GroTh to a polynomial-time verifiable property of the predictors for the general case of arbitrary (random or deterministic) predictors and arbitrary nonzero regression coefficients. ",Mathematics - Statistics Theory ; Computer Science - Information Theory ; Statistics - Machine Learning ; ,"Bajwa, Waheed U. ; Mixon, Dustin G. ; ","Group Model Selection Using Marginal Correlations: The Good, the Bad and   the Ugly  Group model selection is the problem of determining a small subset of groups of predictors (e.g., the expression data of genes) that are responsible for majority of the variation in a response variable (e.g., the malignancy of a tumor). This paper focuses on group model selection in high-dimensional linear models, in which the number of predictors far exceeds the number of samples of the response variable. Existing works on high-dimensional group model selection either require the number of samples of the response variable to be significantly larger than the total number of predictors contributing to the response or impose restrictive statistical priors on the predictors and/or nonzero regression coefficients. This paper provides comprehensive understanding of a low-complexity approach to group model selection that avoids some of these limitations. The proposed approach, termed Group Thresholding (GroTh), is based on thresholding of marginal correlations of groups of predictors with the response variable and is reminiscent of existing thresholding-based approaches in the literature. The most important contribution of the paper in this regard is relating the performance of GroTh to a polynomial-time verifiable property of the predictors for the general case of arbitrary (random or deterministic) predictors and arbitrary nonzero regression coefficients. ",group model selection use marginal correlations good bad ugly group model selection problem determine small subset group predictors expression data genes responsible majority variation response variable malignancy tumor paper focus group model selection high dimensional linear model number predictors far exceed number sample response variable exist work high dimensional group model selection either require number sample response variable significantly larger total number predictors contribute response impose restrictive statistical priors predictors nonzero regression coefficients paper provide comprehensive understand low complexity approach group model selection avoid limitations propose approach term group thresholding groth base thresholding marginal correlations group predictors response variable reminiscent exist thresholding base approach literature important contribution paper regard relate performance groth polynomial time verifiable property predictors general case arbitrary random deterministic predictors arbitrary nonzero regression coefficients,127,9,1210.2440.txt
http://arxiv.org/abs/1210.2540,"On the Automorphism Group of a Binary Self-dual [120, 60, 24] Code","  We prove that an automorphism of order 3 of a putative binary self-dual [120, 60, 24] code C has no fixed points. Moreover, the order of the automorphism group of C divides 2^a.3.5.7.19.23.29 where a is a nonegative integer. Automorphisms of odd composite order r may occur only for r=15, 57 or r=115 with corresponding cycle structures 15-(0,0,8;0), 57-(2,0,2;0) or 115-(1,0,1;0), respectively. In case that all involutions act fixed point freely we have |Aut(C)|<=920, and Aut(C) is solvable if it contains an element of prime order p>=7. Moreover, the alternating group A_5 is the only non-abelian composition factor which may occur. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; Mathematics - Group Theory ; ,"Bouyuklieva, Stefka ; de la Cruz, Javier ; Willems, Wolfgang ; ","On the Automorphism Group of a Binary Self-dual [120, 60, 24] Code  We prove that an automorphism of order 3 of a putative binary self-dual [120, 60, 24] code C has no fixed points. Moreover, the order of the automorphism group of C divides 2^a.3.5.7.19.23.29 where a is a nonegative integer. Automorphisms of odd composite order r may occur only for r=15, 57 or r=115 with corresponding cycle structures 15-(0,0,8;0), 57-(2,0,2;0) or 115-(1,0,1;0), respectively. In case that all involutions act fixed point freely we have |Aut(C)|<=920, and Aut(C) is solvable if it contains an element of prime order p>=7. Moreover, the alternating group A_5 is the only non-abelian composition factor which may occur. ",automorphism group binary self dual code prove automorphism order putative binary self dual code fix point moreover order automorphism group divide nonegative integer automorphisms odd composite order may occur correspond cycle structure respectively case involutions act fix point freely aut aut solvable contain element prime order moreover alternate group non abelian composition factor may occur,55,4,1210.2540.txt
http://arxiv.org/abs/1210.4663,A PRQ Search Method for Probabilistic Objects,"  This article proposes an PQR search method for probabilistic objects. The main idea of our method is to use a strategy called \textit{pre-approximation} that can reduce the initial problem to a highly simplified version, implying that it makes the rest of steps easy to tackle. In particular, this strategy itself is pretty simple and easy to implement. Furthermore, motivated by the cost analysis, we further optimize our solution. The optimizations are mainly based on two insights: (\romannumeral 1) the number of \textit{effective subdivision}s is no more than 1; and (\romannumeral 2) an entity with the larger \textit{span} is more likely to subdivide a single region. We demonstrate the effectiveness and efficiency of our proposed approaches through extensive experiments under various experimental settings. ",Computer Science - Databases ; Computer Science - Computational Geometry ; Computer Science - Data Structures and Algorithms ; H.2.8 ; H.3.3 ; G.3 ; ,"Wang, Jack ; ","A PRQ Search Method for Probabilistic Objects  This article proposes an PQR search method for probabilistic objects. The main idea of our method is to use a strategy called \textit{pre-approximation} that can reduce the initial problem to a highly simplified version, implying that it makes the rest of steps easy to tackle. In particular, this strategy itself is pretty simple and easy to implement. Furthermore, motivated by the cost analysis, we further optimize our solution. The optimizations are mainly based on two insights: (\romannumeral 1) the number of \textit{effective subdivision}s is no more than 1; and (\romannumeral 2) an entity with the larger \textit{span} is more likely to subdivide a single region. We demonstrate the effectiveness and efficiency of our proposed approaches through extensive experiments under various experimental settings. ",prq search method probabilistic object article propose pqr search method probabilistic object main idea method use strategy call textit pre approximation reduce initial problem highly simplify version imply make rest step easy tackle particular strategy pretty simple easy implement furthermore motivate cost analysis optimize solution optimizations mainly base two insights romannumeral number textit effective subdivision romannumeral entity larger textit span likely subdivide single region demonstrate effectiveness efficiency propose approach extensive experiment various experimental settings,74,11,1210.4663.txt
http://arxiv.org/abs/1210.4959,Halving Lines and Their Underlying Graphs,"  In this paper we study underlying graphs corresponding to a set of halving lines. We establish many properties of such graphs. In addition, we tighten the upper bound for the number of halving lines. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C30 ; ,"Khovanova, Tanya ; Yang, Dai ; ","Halving Lines and Their Underlying Graphs  In this paper we study underlying graphs corresponding to a set of halving lines. We establish many properties of such graphs. In addition, we tighten the upper bound for the number of halving lines. ",halve line underlie graph paper study underlie graph correspond set halve line establish many properties graph addition tighten upper bind number halve line,23,3,1210.4959.txt
http://arxiv.org/abs/1210.5065,Realizability algebras III: some examples,"  We use the technique of ""classical realizability"" to build new models of ZF + DC in which R is not well ordered. This gives new relative consistency results, probably not obtainable by forcing. This gives also a new method to get programs from proofs of arithmetical formulas with dependent choice. ",Computer Science - Logic in Computer Science ; Mathematics - Logic ; 03E35 ; F.4.1 ; ,"Krivine, Jean-Louis ; ","Realizability algebras III: some examples  We use the technique of ""classical realizability"" to build new models of ZF + DC in which R is not well ordered. This gives new relative consistency results, probably not obtainable by forcing. This gives also a new method to get programs from proofs of arithmetical formulas with dependent choice. ",realizability algebras iii examples use technique classical realizability build new model zf dc well order give new relative consistency result probably obtainable force give also new method get program proof arithmetical formulas dependent choice,34,8,1210.5065.txt
http://arxiv.org/abs/1210.7341,Subset Codes for Packet Networks,"  In this paper, we present a coding-theoretic framework for message transmission over packet-switched networks. Network is modeled as a channel which can induce packet errors, deletions, insertions, and out of order delivery of packets. The proposed approach can be viewed as an extension of the one introduced by Koetter and Kschischang for networks based on random linear network coding. Namely, while their framework is based on subspace codes and designed for networks in which network nodes perform random linear combining of the packets, ours is based on the so-called subset codes, and is designed for networks employing routing in network nodes. ",Computer Science - Information Theory ; 94B60 ; ,"Kovačević, Mladen ; Vukobratović, Dejan ; ","Subset Codes for Packet Networks  In this paper, we present a coding-theoretic framework for message transmission over packet-switched networks. Network is modeled as a channel which can induce packet errors, deletions, insertions, and out of order delivery of packets. The proposed approach can be viewed as an extension of the one introduced by Koetter and Kschischang for networks based on random linear network coding. Namely, while their framework is based on subspace codes and designed for networks in which network nodes perform random linear combining of the packets, ours is based on the so-called subset codes, and is designed for networks employing routing in network nodes. ",subset cod packet network paper present cod theoretic framework message transmission packet switch network network model channel induce packet errors deletions insertions order delivery packets propose approach view extension one introduce koetter kschischang network base random linear network cod namely framework base subspace cod design network network nod perform random linear combine packets base call subset cod design network employ rout network nod,63,6,1210.7341.txt
http://arxiv.org/abs/1210.7638,Finding Efficient Region in The Plane with Line segments,"  Let $\mathscr O$ be a set of $n$ disjoint obstacles in $\mathbb{R}^2$, $\mathscr M$ be a moving object. Let $s$ and $l$ denote the starting point and maximum path length of the moving object $\mathscr M$, respectively. Given a point $p$ in ${R}^2$, we say the point $p$ is achievable for $\mathscr M$ such that $\pi(s,p)\leq l$, where $\pi(\cdot)$ denotes the shortest path length in the presence of obstacles. One is to find a region $\mathscr R$ such that, for any point $p\in \mathbb{R}^2$, if it is achievable for $\mathscr M$, then $p\in \mathscr R$; otherwise, $p\notin \mathscr R$. In this paper, we restrict our attention to the case of line-segment obstacles. To tackle this problem, we develop three algorithms. We first present a simpler-version algorithm for the sake of intuition. Its basic idea is to reduce our problem to computing the union of a set of circular visibility regions (CVRs). This algorithm takes $O(n^3)$ time. By analysing its dominant steps, we break through its bottleneck by using the short path map (SPM) technique to obtain those circles (unavailable beforehand), yielding an $O(n^2\log n)$ algorithm. Owing to the finding above, the third algorithm also uses the SPM technique. It however, does not continue to construct the CVRs. Instead, it directly traverses each region of the SPM to trace the boundaries, the final algorithm obtains $O(n\log n)$ complexity. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Geometry ; F.2 ; I.1.2 ; ,"Wang, Jack ; ","Finding Efficient Region in The Plane with Line segments  Let $\mathscr O$ be a set of $n$ disjoint obstacles in $\mathbb{R}^2$, $\mathscr M$ be a moving object. Let $s$ and $l$ denote the starting point and maximum path length of the moving object $\mathscr M$, respectively. Given a point $p$ in ${R}^2$, we say the point $p$ is achievable for $\mathscr M$ such that $\pi(s,p)\leq l$, where $\pi(\cdot)$ denotes the shortest path length in the presence of obstacles. One is to find a region $\mathscr R$ such that, for any point $p\in \mathbb{R}^2$, if it is achievable for $\mathscr M$, then $p\in \mathscr R$; otherwise, $p\notin \mathscr R$. In this paper, we restrict our attention to the case of line-segment obstacles. To tackle this problem, we develop three algorithms. We first present a simpler-version algorithm for the sake of intuition. Its basic idea is to reduce our problem to computing the union of a set of circular visibility regions (CVRs). This algorithm takes $O(n^3)$ time. By analysing its dominant steps, we break through its bottleneck by using the short path map (SPM) technique to obtain those circles (unavailable beforehand), yielding an $O(n^2\log n)$ algorithm. Owing to the finding above, the third algorithm also uses the SPM technique. It however, does not continue to construct the CVRs. Instead, it directly traverses each region of the SPM to trace the boundaries, the final algorithm obtains $O(n\log n)$ complexity. ",find efficient region plane line segment let mathscr set disjoint obstacles mathbb mathscr move object let denote start point maximum path length move object mathscr respectively give point say point achievable mathscr pi leq pi cdot denote shortest path length presence obstacles one find region mathscr point mathbb achievable mathscr mathscr otherwise notin mathscr paper restrict attention case line segment obstacles tackle problem develop three algorithms first present simpler version algorithm sake intuition basic idea reduce problem compute union set circular visibility regions cvrs algorithm take time analyse dominant step break bottleneck use short path map spm technique obtain circle unavailable beforehand yield log algorithm owe find third algorithm also use spm technique however continue construct cvrs instead directly traverse region spm trace boundaries final algorithm obtain log complexity,129,4,1210.7638.txt
http://arxiv.org/abs/1211.0071,Randomness and Non-determinism,"  Exponentiation makes the difference between the bit-size of this line and the number (<< 2^{300}) of particles in the known Universe. The expulsion of exponential time algorithms from Computer Theory in the 60's broke its umbilical cord from Mathematical Logic. It created a deep gap between deterministic computation and -- formerly its unremarkable tools -- randomness and non-determinism. Little did we learn in the past decades about the power of either of these two basic ""freedoms"" of computation, but some vague pattern is emerging in relationships between them. The pattern of similar techniques instrumental for quite different results in this area seems even more interesting. Ideas like multilinear and low-degree multivariate polynomials, Fourier transformation over low-periodic groups seem very illuminating. The talk surveyed some recent results. One of them, given in a stronger form than previously published, is described below. ",Computer Science - Computational Complexity ; Computer Science - Cryptography and Security ; Computer Science - Information Theory ; ,"Levin, Leonid A. ; ","Randomness and Non-determinism  Exponentiation makes the difference between the bit-size of this line and the number (<< 2^{300}) of particles in the known Universe. The expulsion of exponential time algorithms from Computer Theory in the 60's broke its umbilical cord from Mathematical Logic. It created a deep gap between deterministic computation and -- formerly its unremarkable tools -- randomness and non-determinism. Little did we learn in the past decades about the power of either of these two basic ""freedoms"" of computation, but some vague pattern is emerging in relationships between them. The pattern of similar techniques instrumental for quite different results in this area seems even more interesting. Ideas like multilinear and low-degree multivariate polynomials, Fourier transformation over low-periodic groups seem very illuminating. The talk surveyed some recent results. One of them, given in a stronger form than previously published, is described below. ",randomness non determinism exponentiation make difference bite size line number particles know universe expulsion exponential time algorithms computer theory break umbilical cord mathematical logic create deep gap deterministic computation formerly unremarkable tool randomness non determinism little learn past decades power either two basic freedoms computation vague pattern emerge relationships pattern similar techniques instrumental quite different result area seem even interest ideas like multilinear low degree multivariate polynomials fourier transformation low periodic group seem illuminate talk survey recent result one give stronger form previously publish describe,85,4,1211.0071.txt
http://arxiv.org/abs/1211.0589,Sharp Bounds on Random Walk Eigenvalues via Spectral Embedding,"  Spectral embedding of graphs uses the top k non-trivial eigenvectors of the random walk matrix to embed the graph into R^k. The primary use of this embedding has been for practical spectral clustering algorithms [SM00,NJW02]. Recently, spectral embedding was studied from a theoretical perspective to prove higher order variants of Cheeger's inequality [LOT12,LRTV12].   We use spectral embedding to provide a unifying framework for bounding all the eigenvalues of graphs. For example, we show that for any finite graph with n vertices and all k >= 2, the k-th largest eigenvalue is at most 1-Omega(k^3/n^3), which extends the only other such result known, which is for k=2 only and is due to [LO81]. This upper bound improves to 1-Omega(k^2/n^2) if the graph is regular. We generalize these results, and we provide sharp bounds on the spectral measure of various classes of graphs, including vertex-transitive graphs and infinite graphs, in terms of specific graph parameters like the volume growth.   As a consequence, using the entire spectrum, we provide (improved) upper bounds on the return probabilities and mixing time of random walks with considerably shorter and more direct proofs. Our work introduces spectral embedding as a new tool in analyzing reversible Markov chains. Furthermore, building on [Lyo05], we design a local algorithm to approximate the number of spanning trees of massive graphs. ",Mathematics - Probability ; Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Mathematics - Spectral Theory ; ,"Lyons, Russell ; Gharan, Shayan Oveis ; ","Sharp Bounds on Random Walk Eigenvalues via Spectral Embedding  Spectral embedding of graphs uses the top k non-trivial eigenvectors of the random walk matrix to embed the graph into R^k. The primary use of this embedding has been for practical spectral clustering algorithms [SM00,NJW02]. Recently, spectral embedding was studied from a theoretical perspective to prove higher order variants of Cheeger's inequality [LOT12,LRTV12].   We use spectral embedding to provide a unifying framework for bounding all the eigenvalues of graphs. For example, we show that for any finite graph with n vertices and all k >= 2, the k-th largest eigenvalue is at most 1-Omega(k^3/n^3), which extends the only other such result known, which is for k=2 only and is due to [LO81]. This upper bound improves to 1-Omega(k^2/n^2) if the graph is regular. We generalize these results, and we provide sharp bounds on the spectral measure of various classes of graphs, including vertex-transitive graphs and infinite graphs, in terms of specific graph parameters like the volume growth.   As a consequence, using the entire spectrum, we provide (improved) upper bounds on the return probabilities and mixing time of random walks with considerably shorter and more direct proofs. Our work introduces spectral embedding as a new tool in analyzing reversible Markov chains. Furthermore, building on [Lyo05], we design a local algorithm to approximate the number of spanning trees of massive graphs. ",sharp bound random walk eigenvalues via spectral embed spectral embed graph use top non trivial eigenvectors random walk matrix embed graph primary use embed practical spectral cluster algorithms sm njw recently spectral embed study theoretical perspective prove higher order variants cheeger inequality lot lrtv use spectral embed provide unify framework bound eigenvalues graph example show finite graph vertices th largest eigenvalue omega extend result know due lo upper bind improve omega graph regular generalize result provide sharp bound spectral measure various class graph include vertex transitive graph infinite graph term specific graph parameters like volume growth consequence use entire spectrum provide improve upper bound return probabilities mix time random walk considerably shorter direct proof work introduce spectral embed new tool analyze reversible markov chain furthermore build lyo design local algorithm approximate number span tree massive graph,136,3,1211.0589.txt
http://arxiv.org/abs/1211.0729,A Simple Algorithm for Computing BOCP,"  In this article, we devise a concise algorithm for computing BOCP. Our method is simple, easy-to-implement but without loss of efficiency. Given two circular-arc polygons with $m$ and $n$ edges respectively, our method runs in $O(m+n+(l+k)\log l)$ time, using $O(m+n+k)$ space, where $k$ is the number of intersections, and $l$ is the number of {edge}s. Our algorithm has the power to approximate to linear complexity when $k$ and $l$ are small. The superiority of the proposed algorithm is also validated through empirical study. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Geometry ; Computer Science - Graphics ; E.1 ; I.3.5 ; I.3.6 ; ,"Wang, Jack ; ","A Simple Algorithm for Computing BOCP  In this article, we devise a concise algorithm for computing BOCP. Our method is simple, easy-to-implement but without loss of efficiency. Given two circular-arc polygons with $m$ and $n$ edges respectively, our method runs in $O(m+n+(l+k)\log l)$ time, using $O(m+n+k)$ space, where $k$ is the number of intersections, and $l$ is the number of {edge}s. Our algorithm has the power to approximate to linear complexity when $k$ and $l$ are small. The superiority of the proposed algorithm is also validated through empirical study. ",simple algorithm compute bocp article devise concise algorithm compute bocp method simple easy implement without loss efficiency give two circular arc polygons edge respectively method run log time use space number intersections number edge algorithm power approximate linear complexity small superiority propose algorithm also validate empirical study,47,4,1211.0729.txt
http://arxiv.org/abs/1211.0877,Differential Privacy for the Analyst via Private Equilibrium Computation,"  We give new mechanisms for answering exponentially many queries from multiple analysts on a private database, while protecting differential privacy both for the individuals in the database and for the analysts. That is, our mechanism's answer to each query is nearly insensitive to changes in the queries asked by other analysts. Our mechanism is the first to offer differential privacy on the joint distribution over analysts' answers, providing privacy for data analysts even if the other data analysts collude or register multiple accounts. In some settings, we are able to achieve nearly optimal error rates (even compared to mechanisms which do not offer analyst privacy), and we are able to extend our techniques to handle non-linear queries. Our analysis is based on a novel view of the private query-release problem as a two-player zero-sum game, which may be of independent interest. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computer Science and Game Theory ; ,"Hsu, Justin ; Roth, Aaron ; Ullman, Jonathan ; ","Differential Privacy for the Analyst via Private Equilibrium Computation  We give new mechanisms for answering exponentially many queries from multiple analysts on a private database, while protecting differential privacy both for the individuals in the database and for the analysts. That is, our mechanism's answer to each query is nearly insensitive to changes in the queries asked by other analysts. Our mechanism is the first to offer differential privacy on the joint distribution over analysts' answers, providing privacy for data analysts even if the other data analysts collude or register multiple accounts. In some settings, we are able to achieve nearly optimal error rates (even compared to mechanisms which do not offer analyst privacy), and we are able to extend our techniques to handle non-linear queries. Our analysis is based on a novel view of the private query-release problem as a two-player zero-sum game, which may be of independent interest. ",differential privacy analyst via private equilibrium computation give new mechanisms answer exponentially many query multiple analysts private database protect differential privacy individuals database analysts mechanism answer query nearly insensitive change query ask analysts mechanism first offer differential privacy joint distribution analysts answer provide privacy data analysts even data analysts collude register multiple account settings able achieve nearly optimal error rat even compare mechanisms offer analyst privacy able extend techniques handle non linear query analysis base novel view private query release problem two player zero sum game may independent interest,89,1,1211.0877.txt
http://arxiv.org/abs/1211.2384,Strong Bounds for Evolution in Undirected Graphs,"  This work studies the generalized Moran process, as introduced by Lieberman et al. [Nature, 433:312-316, 2005]. We introduce the parameterized notions of selective amplifiers and selective suppressors of evolution, i.e. of networks (graphs) with many ""strong starts"" and many ""weak starts"" for the mutant, respectively. We first prove the existence of strong selective amplifiers and of (quite) strong selective suppressors. Furthermore we provide strong upper bounds and almost tight lower bounds (by proving the ""Thermal Theorem"") for the traditional notion of fixation probability of Lieberman et al., i.e. assuming a random initial placement of the mutant. ",Computer Science - Data Structures and Algorithms ; Mathematics - Probability ; G.3 ; E.1 ; ,"Mertzios, George B. ; Spirakis, Paul G. ; ","Strong Bounds for Evolution in Undirected Graphs  This work studies the generalized Moran process, as introduced by Lieberman et al. [Nature, 433:312-316, 2005]. We introduce the parameterized notions of selective amplifiers and selective suppressors of evolution, i.e. of networks (graphs) with many ""strong starts"" and many ""weak starts"" for the mutant, respectively. We first prove the existence of strong selective amplifiers and of (quite) strong selective suppressors. Furthermore we provide strong upper bounds and almost tight lower bounds (by proving the ""Thermal Theorem"") for the traditional notion of fixation probability of Lieberman et al., i.e. assuming a random initial placement of the mutant. ",strong bound evolution undirected graph work study generalize moran process introduce lieberman et al nature introduce parameterized notions selective amplifiers selective suppressors evolution network graph many strong start many weak start mutant respectively first prove existence strong selective amplifiers quite strong selective suppressors furthermore provide strong upper bound almost tight lower bound prove thermal theorem traditional notion fixation probability lieberman et al assume random initial placement mutant,67,3,1211.2384.txt
http://arxiv.org/abs/1211.2662,Recognizing Interval Bigraphs by Forbidden Patterns,  Let H be a connected bipartite graph with n nodes and m edges. We give an O(nm) time algorithm to decide whether H is an interval bigraph. The best known algorithm has time complexity O(nm^6(m + n) \log n) and it was developed in 1997 [18]. Our approach is based on an ordering characterization of interval bigraphs introduced by Hell and Huang [13]. We transform the problem of finding the desired ordering to choosing strong components of a pair-digraph without creating conflicts. We make use of the structure of the pair-digraph as well as decomposition of bigraph H based on the special components of the pair-digraph. This way we make explicit what the difficult cases are and gain efficiency by isolating such situations. ,Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Rafiey, Arash ; ",Recognizing Interval Bigraphs by Forbidden Patterns  Let H be a connected bipartite graph with n nodes and m edges. We give an O(nm) time algorithm to decide whether H is an interval bigraph. The best known algorithm has time complexity O(nm^6(m + n) \log n) and it was developed in 1997 [18]. Our approach is based on an ordering characterization of interval bigraphs introduced by Hell and Huang [13]. We transform the problem of finding the desired ordering to choosing strong components of a pair-digraph without creating conflicts. We make use of the structure of the pair-digraph as well as decomposition of bigraph H based on the special components of the pair-digraph. This way we make explicit what the difficult cases are and gain efficiency by isolating such situations. ,recognize interval bigraphs forbid pattern let connect bipartite graph nod edge give nm time algorithm decide whether interval bigraph best know algorithm time complexity nm log develop approach base order characterization interval bigraphs introduce hell huang transform problem find desire order choose strong components pair digraph without create conflict make use structure pair digraph well decomposition bigraph base special components pair digraph way make explicit difficult case gain efficiency isolate situations,71,1,1211.2662.txt
http://arxiv.org/abs/1211.3234,Computational topology and normal surfaces: Theoretical and experimental   complexity bounds,"  In three-dimensional computational topology, the theory of normal surfaces is a tool of great theoretical and practical significance. Although this theory typically leads to exponential time algorithms, very little is known about how these algorithms perform in ""typical"" scenarios, or how far the best known theoretical bounds are from the real worst-case scenarios. Here we study the combinatorial and algebraic complexity of normal surfaces from both the theoretical and experimental viewpoints. Theoretically, we obtain new exponential lower bounds on the worst-case complexities in a variety of settings that are important for practical computation. Experimentally, we study the worst-case and average-case complexities over a comprehensive body of roughly three billion input triangulations. Many of our lower bounds are the first known exponential lower bounds in these settings, and experimental evidence suggests that many of our theoretical lower bounds on worst-case growth rates may indeed be asymptotically tight. ","Mathematics - Geometric Topology ; Computer Science - Computational Geometry ; Mathematics - Combinatorics ; 68Q17 (Primary) 68Q15, 68Q15, 57Q35, 57Q35 (Secondary) ; ","Burton, Benjamin A. ; Paixão, João ; Spreer, Jonathan ; ","Computational topology and normal surfaces: Theoretical and experimental   complexity bounds  In three-dimensional computational topology, the theory of normal surfaces is a tool of great theoretical and practical significance. Although this theory typically leads to exponential time algorithms, very little is known about how these algorithms perform in ""typical"" scenarios, or how far the best known theoretical bounds are from the real worst-case scenarios. Here we study the combinatorial and algebraic complexity of normal surfaces from both the theoretical and experimental viewpoints. Theoretically, we obtain new exponential lower bounds on the worst-case complexities in a variety of settings that are important for practical computation. Experimentally, we study the worst-case and average-case complexities over a comprehensive body of roughly three billion input triangulations. Many of our lower bounds are the first known exponential lower bounds in these settings, and experimental evidence suggests that many of our theoretical lower bounds on worst-case growth rates may indeed be asymptotically tight. ",computational topology normal surface theoretical experimental complexity bound three dimensional computational topology theory normal surface tool great theoretical practical significance although theory typically lead exponential time algorithms little know algorithms perform typical scenarios far best know theoretical bound real worst case scenarios study combinatorial algebraic complexity normal surface theoretical experimental viewpoints theoretically obtain new exponential lower bound worst case complexities variety settings important practical computation experimentally study worst case average case complexities comprehensive body roughly three billion input triangulations many lower bound first know exponential lower bound settings experimental evidence suggest many theoretical lower bound worst case growth rat may indeed asymptotically tight,103,8,1211.3234.txt
http://arxiv.org/abs/1211.4892,Confusion of Tagged Perturbations in Forward Automatic Differentiation   of Higher-Order Functions,"  Forward Automatic Differentiation (AD) is a technique for augmenting programs to compute derivatives. The essence of Forward AD is to attach perturbations to each number, and propagate these through the computation. When derivatives are nested, the distinct derivative calculations, and their associated perturbations, must be distinguished. This is typically accomplished by creating a unique tag for each derivative calculation, tagging the perturbations, and overloading the arithmetic operators. We exhibit a subtle bug, present in fielded implementations, in which perturbations are confused despite the tagging machinery. The essence of the bug is this: each invocation of a derivative creates a unique tag but a unique tag is needed for each derivative calculation. When taking derivatives of higher-order functions, these need not correspond! The derivative of a higher-order function $f$ that returns a function $g$ will be a function $f'$ that returns a function $\bar{g}$ that performs a derivative calculation. A single invocation of $f'$ will create a single fresh tag but that same tag will be used for each derivative calculation resulting from an invocation of $\bar{g}$. This situation arises when taking derivatives of curried functions. Two potential solutions are presented, and their serious deficiencies discussed. One requires eta expansion to delay the creation of fresh tags from the invocation of $f'$ to the invocation of $\bar{g}$, which can be difficult or even impossible in some circumstances. The other requires $f'$ to wrap $\bar{g}$ with tag renaming, which is difficult to implement without violating the desirable complexity properties of forward AD. ",Computer Science - Symbolic Computation ; Computer Science - Mathematical Software ; Mathematics - Differential Geometry ; ,"Manzyuk, Oleksandr ; Pearlmutter, Barak A. ; Radul, Alexey Andreyevich ; Rush, David R. ; Siskind, Jeffrey Mark ; ","Confusion of Tagged Perturbations in Forward Automatic Differentiation   of Higher-Order Functions  Forward Automatic Differentiation (AD) is a technique for augmenting programs to compute derivatives. The essence of Forward AD is to attach perturbations to each number, and propagate these through the computation. When derivatives are nested, the distinct derivative calculations, and their associated perturbations, must be distinguished. This is typically accomplished by creating a unique tag for each derivative calculation, tagging the perturbations, and overloading the arithmetic operators. We exhibit a subtle bug, present in fielded implementations, in which perturbations are confused despite the tagging machinery. The essence of the bug is this: each invocation of a derivative creates a unique tag but a unique tag is needed for each derivative calculation. When taking derivatives of higher-order functions, these need not correspond! The derivative of a higher-order function $f$ that returns a function $g$ will be a function $f'$ that returns a function $\bar{g}$ that performs a derivative calculation. A single invocation of $f'$ will create a single fresh tag but that same tag will be used for each derivative calculation resulting from an invocation of $\bar{g}$. This situation arises when taking derivatives of curried functions. Two potential solutions are presented, and their serious deficiencies discussed. One requires eta expansion to delay the creation of fresh tags from the invocation of $f'$ to the invocation of $\bar{g}$, which can be difficult or even impossible in some circumstances. The other requires $f'$ to wrap $\bar{g}$ with tag renaming, which is difficult to implement without violating the desirable complexity properties of forward AD. ",confusion tag perturbations forward automatic differentiation higher order function forward automatic differentiation ad technique augment program compute derivatives essence forward ad attach perturbations number propagate computation derivatives nest distinct derivative calculations associate perturbations must distinguish typically accomplish create unique tag derivative calculation tag perturbations overload arithmetic operators exhibit subtle bug present field implementations perturbations confuse despite tag machinery essence bug invocation derivative create unique tag unique tag need derivative calculation take derivatives higher order function need correspond derivative higher order function return function function return function bar perform derivative calculation single invocation create single fresh tag tag use derivative calculation result invocation bar situation arise take derivatives curry function two potential solutions present serious deficiencies discuss one require eta expansion delay creation fresh tag invocation invocation bar difficult even impossible circumstances require wrap bar tag rename difficult implement without violate desirable complexity properties forward ad,145,7,1211.4892.txt
http://arxiv.org/abs/1211.5608,Blind Deconvolution using Convex Programming,"  We consider the problem of recovering two unknown vectors, $\boldsymbol{w}$ and $\boldsymbol{x}$, of length $L$ from their circular convolution. We make the structural assumption that the two vectors are members of known subspaces, one with dimension $N$ and the other with dimension $K$. Although the observed convolution is nonlinear in both $\boldsymbol{w}$ and $\boldsymbol{x}$, it is linear in the rank-1 matrix formed by their outer product $\boldsymbol{w}\boldsymbol{x}^*$. This observation allows us to recast the deconvolution problem as low-rank matrix recovery problem from linear measurements, whose natural convex relaxation is a nuclear norm minimization program.   We prove the effectiveness of this relaxation by showing that for ""generic"" signals, the program can deconvolve $\boldsymbol{w}$ and $\boldsymbol{x}$ exactly when the maximum of $N$ and $K$ is almost on the order of $L$. That is, we show that if $\boldsymbol{x}$ is drawn from a random subspace of dimension $N$, and $\boldsymbol{w}$ is a vector in a subspace of dimension $K$ whose basis vectors are ""spread out"" in the frequency domain, then nuclear norm minimization recovers $\boldsymbol{w}\boldsymbol{x}^*$ without error.   We discuss this result in the context of blind channel estimation in communications. If we have a message of length $N$ which we code using a random $L\times N$ coding matrix, and the encoded message travels through an unknown linear time-invariant channel of maximum length $K$, then the receiver can recover both the channel response and the message when $L\gtrsim N+K$, to within constant and log factors. ",Computer Science - Information Theory ; ,"Ahmed, Ali ; Recht, Benjamin ; Romberg, Justin ; ","Blind Deconvolution using Convex Programming  We consider the problem of recovering two unknown vectors, $\boldsymbol{w}$ and $\boldsymbol{x}$, of length $L$ from their circular convolution. We make the structural assumption that the two vectors are members of known subspaces, one with dimension $N$ and the other with dimension $K$. Although the observed convolution is nonlinear in both $\boldsymbol{w}$ and $\boldsymbol{x}$, it is linear in the rank-1 matrix formed by their outer product $\boldsymbol{w}\boldsymbol{x}^*$. This observation allows us to recast the deconvolution problem as low-rank matrix recovery problem from linear measurements, whose natural convex relaxation is a nuclear norm minimization program.   We prove the effectiveness of this relaxation by showing that for ""generic"" signals, the program can deconvolve $\boldsymbol{w}$ and $\boldsymbol{x}$ exactly when the maximum of $N$ and $K$ is almost on the order of $L$. That is, we show that if $\boldsymbol{x}$ is drawn from a random subspace of dimension $N$, and $\boldsymbol{w}$ is a vector in a subspace of dimension $K$ whose basis vectors are ""spread out"" in the frequency domain, then nuclear norm minimization recovers $\boldsymbol{w}\boldsymbol{x}^*$ without error.   We discuss this result in the context of blind channel estimation in communications. If we have a message of length $N$ which we code using a random $L\times N$ coding matrix, and the encoded message travels through an unknown linear time-invariant channel of maximum length $K$, then the receiver can recover both the channel response and the message when $L\gtrsim N+K$, to within constant and log factors. ",blind deconvolution use convex program consider problem recover two unknown vectors boldsymbol boldsymbol length circular convolution make structural assumption two vectors members know subspaces one dimension dimension although observe convolution nonlinear boldsymbol boldsymbol linear rank matrix form outer product boldsymbol boldsymbol observation allow us recast deconvolution problem low rank matrix recovery problem linear measurements whose natural convex relaxation nuclear norm minimization program prove effectiveness relaxation show generic signal program deconvolve boldsymbol boldsymbol exactly maximum almost order show boldsymbol draw random subspace dimension boldsymbol vector subspace dimension whose basis vectors spread frequency domain nuclear norm minimization recover boldsymbol boldsymbol without error discuss result context blind channel estimation communications message length code use random time cod matrix encode message travel unknown linear time invariant channel maximum length receiver recover channel response message gtrsim within constant log factor,135,9,1211.5608.txt
http://arxiv.org/abs/1211.5773,Circuit complexity and Problem structure in Hamming space,"  This paper describes about relation between circuit complexity and accept inputs structure in Hamming space by using almost all monotone circuit that emulate deterministic Turing machine (DTM).   Circuit family that emulate DTM are almost all monotone circuit family except some NOT-gate which connect input variables (like negation normal form (NNF)). Therefore, we can analyze DTM limitation by using this NNF Circuit family.   NNF circuit have symmetry of OR-gate input line, so NNF circuit cannot identify from OR-gate output line which of OR-gate input line is 1. So NNF circuit family cannot compute sandwich structure effectively (Sandwich structure is two accept inputs that sandwich reject inputs in Hamming space). NNF circuit have to use unique AND-gate to identify each different vector of sandwich structure. That is, we can measure problem complexity by counting different vectors.   Some decision problem have characteristic in sandwich structure. Different vectors of Negate HornSAT problem are at most constant length because we can delete constant part of each negative literal in Horn clauses by using definite clauses. Therefore, number of these different vector is at most polynomial size. The other hand, we can design high complexity problem with almost perfct nonlinear (APN) function. ",Computer Science - Computational Complexity ; ,"Kobayashi, Koji ; ","Circuit complexity and Problem structure in Hamming space  This paper describes about relation between circuit complexity and accept inputs structure in Hamming space by using almost all monotone circuit that emulate deterministic Turing machine (DTM).   Circuit family that emulate DTM are almost all monotone circuit family except some NOT-gate which connect input variables (like negation normal form (NNF)). Therefore, we can analyze DTM limitation by using this NNF Circuit family.   NNF circuit have symmetry of OR-gate input line, so NNF circuit cannot identify from OR-gate output line which of OR-gate input line is 1. So NNF circuit family cannot compute sandwich structure effectively (Sandwich structure is two accept inputs that sandwich reject inputs in Hamming space). NNF circuit have to use unique AND-gate to identify each different vector of sandwich structure. That is, we can measure problem complexity by counting different vectors.   Some decision problem have characteristic in sandwich structure. Different vectors of Negate HornSAT problem are at most constant length because we can delete constant part of each negative literal in Horn clauses by using definite clauses. Therefore, number of these different vector is at most polynomial size. The other hand, we can design high complexity problem with almost perfct nonlinear (APN) function. ",circuit complexity problem structure ham space paper describe relation circuit complexity accept input structure ham space use almost monotone circuit emulate deterministic turing machine dtm circuit family emulate dtm almost monotone circuit family except gate connect input variables like negation normal form nnf therefore analyze dtm limitation use nnf circuit family nnf circuit symmetry gate input line nnf circuit cannot identify gate output line gate input line nnf circuit family cannot compute sandwich structure effectively sandwich structure two accept input sandwich reject input ham space nnf circuit use unique gate identify different vector sandwich structure measure problem complexity count different vectors decision problem characteristic sandwich structure different vectors negate hornsat problem constant length delete constant part negative literal horn clauses use definite clauses therefore number different vector polynomial size hand design high complexity problem almost perfct nonlinear apn function,139,8,1211.5773.txt
http://arxiv.org/abs/1211.6468,"Using Isabelle to verify special relativity, with application to   hypercomputation theory","  Logicians at the R\'enyi Mathematical Institute in Budapest have spent several years developing versions of relativity theory (special, general, and other variants) based wholly on first order logic, and have argued in favour of the physical decidability, via exploitation of cosmological phenomena, of formally undecidable questions such as the Halting Problem and the consistency of set theory.   The Hungarian theories are very extensive, and their associated proofs are intuitively very satisfying, but this brings its own risks since intuition can sometimes be misleading. As part of a joint project, researchers at Sheffield have recently started generating rigorous machine-verified versions of the Hungarian proofs, so as to demonstrate the soundness of their work. In this paper, we explain the background to the project and demonstrate an Isabelle proof of the theorem ""No inertial observer can travel faster than light"".   This approach to physical theories and physical computability has several pay-offs: (a) we can be certain our intuition hasn't led us astray (or if it has, we can identify where this has happened); (b) we can identify which axioms are specifically required in the proof of each theorem and to what extent those axioms can be weakened (the fewer assumptions we make up-front, the stronger the results); and (c) we can identify whether new formal proof techniques and tactics are needed when tackling physical as opposed to mathematical theories. ",Computer Science - Logic in Computer Science ; General Relativity and Quantum Cosmology ; F.4.1 ; J.2 ; ,"Stannett, Mike ; Németi, István ; ","Using Isabelle to verify special relativity, with application to   hypercomputation theory  Logicians at the R\'enyi Mathematical Institute in Budapest have spent several years developing versions of relativity theory (special, general, and other variants) based wholly on first order logic, and have argued in favour of the physical decidability, via exploitation of cosmological phenomena, of formally undecidable questions such as the Halting Problem and the consistency of set theory.   The Hungarian theories are very extensive, and their associated proofs are intuitively very satisfying, but this brings its own risks since intuition can sometimes be misleading. As part of a joint project, researchers at Sheffield have recently started generating rigorous machine-verified versions of the Hungarian proofs, so as to demonstrate the soundness of their work. In this paper, we explain the background to the project and demonstrate an Isabelle proof of the theorem ""No inertial observer can travel faster than light"".   This approach to physical theories and physical computability has several pay-offs: (a) we can be certain our intuition hasn't led us astray (or if it has, we can identify where this has happened); (b) we can identify which axioms are specifically required in the proof of each theorem and to what extent those axioms can be weakened (the fewer assumptions we make up-front, the stronger the results); and (c) we can identify whether new formal proof techniques and tactics are needed when tackling physical as opposed to mathematical theories. ",use isabelle verify special relativity application hypercomputation theory logicians enyi mathematical institute budapest spend several years develop versions relativity theory special general variants base wholly first order logic argue favour physical decidability via exploitation cosmological phenomena formally undecidable question halt problem consistency set theory hungarian theories extensive associate proof intuitively satisfy bring risk since intuition sometimes mislead part joint project researchers sheffield recently start generate rigorous machine verify versions hungarian proof demonstrate soundness work paper explain background project demonstrate isabelle proof theorem inertial observer travel faster light approach physical theories physical computability several pay off certain intuition lead us astray identify happen identify axioms specifically require proof theorem extent axioms weaken fewer assumptions make front stronger result identify whether new formal proof techniques tactics need tackle physical oppose mathematical theories,130,8,1211.6468.txt
http://arxiv.org/abs/1211.6470,A new class of SETI beacons that contain information (22-aug-2010),"  In the cm-wavelength range, an extraterrestrial electromagnetic narrow band (sine wave) beacon is an excellent choice to get alien attention across interstellar distances because 1) it is not strongly affected by interstellar / interplanetary dispersion or scattering, and 2) searching for narrowband signals is computationally efficient (scales as Ns log(Ns) where Ns = number of voltage samples). Here we consider a special case wideband signal where two or more delayed copies of the same signal are transmitted over the same frequency and bandwidth, with the result that ISM dispersion and scattering cancel out during the detection stage. Such a signal is both a good beacon (easy to find) and carries arbitrarily large information rate (limited only by the atmospheric transparency to about 10 GHz). The discovery process uses an autocorrelation algorithm, and we outline a compute scheme where the beacon discovery search can be accomplished with only 2x the processing of a conventional sine wave search, and discuss signal to background response for sighting the beacon. Once the beacon is discovered, the focus turns to information extraction. Information extraction requires similar processing as for generic wideband signal searches, but since we have already identified the beacon, the efficiency of information extraction is negligible. ",Astrophysics - Instrumentation and Methods for Astrophysics ; Computer Science - Other Computer Science ; ,"Harp, G. R. ; Ackermann, R. F. ; Blair, Samantha K. ; Arbunich, J. ; Backus, P. R. ; Tarter, J. C. ; Team, the ATA ; ","A new class of SETI beacons that contain information (22-aug-2010)  In the cm-wavelength range, an extraterrestrial electromagnetic narrow band (sine wave) beacon is an excellent choice to get alien attention across interstellar distances because 1) it is not strongly affected by interstellar / interplanetary dispersion or scattering, and 2) searching for narrowband signals is computationally efficient (scales as Ns log(Ns) where Ns = number of voltage samples). Here we consider a special case wideband signal where two or more delayed copies of the same signal are transmitted over the same frequency and bandwidth, with the result that ISM dispersion and scattering cancel out during the detection stage. Such a signal is both a good beacon (easy to find) and carries arbitrarily large information rate (limited only by the atmospheric transparency to about 10 GHz). The discovery process uses an autocorrelation algorithm, and we outline a compute scheme where the beacon discovery search can be accomplished with only 2x the processing of a conventional sine wave search, and discuss signal to background response for sighting the beacon. Once the beacon is discovered, the focus turns to information extraction. Information extraction requires similar processing as for generic wideband signal searches, but since we have already identified the beacon, the efficiency of information extraction is negligible. ",new class seti beacon contain information aug cm wavelength range extraterrestrial electromagnetic narrow band sine wave beacon excellent choice get alien attention across interstellar distance strongly affect interstellar interplanetary dispersion scatter search narrowband signal computationally efficient scale ns log ns ns number voltage sample consider special case wideband signal two delay copy signal transmit frequency bandwidth result ism dispersion scatter cancel detection stage signal good beacon easy find carry arbitrarily large information rate limit atmospheric transparency ghz discovery process use autocorrelation algorithm outline compute scheme beacon discovery search accomplish process conventional sine wave search discuss signal background response sight beacon beacon discover focus turn information extraction information extraction require similar process generic wideband signal search since already identify beacon efficiency information extraction negligible,123,9,1211.6470.txt
http://arxiv.org/abs/1212.1095,The projector algorithm: a simple parallel algorithm for computing   Voronoi diagrams and Delaunay graphs,"  The Voronoi diagram is a certain geometric data structure which has numerous applications in various scientific and technological fields. The theory of algorithms for computing 2D Euclidean Voronoi diagrams of point sites is rich and useful, with several different and important algorithms. However, this theory has been quite steady during the last few decades in the sense that no essentially new algorithms have entered the game. In addition, most of the known algorithms are serial in nature and hence cast inherent difficulties on the possibility to compute the diagram in parallel. In this paper we present the projector algorithm: a new and simple algorithm which enables the (combinatorial) computation of 2D Voronoi diagrams. The algorithm is significantly different from previous ones and some of the involved concepts in it are in the spirit of linear programming and optics. Parallel implementation is naturally supported since each Voronoi cell can be computed independently of the other cells. A new combinatorial structure for representing the cells (and any convex polytope) is described along the way and the computation of the induced Delaunay graph is obtained almost automatically. ","Computer Science - Computational Geometry ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Data Structures and Algorithms ; 68U05, 68W10, 65D18, 68W40, 52B05 ; D.1.3 ; D.3.2 ; F.1.2 ; F.2.2 ; G.1.0 ; G.2.1 ; G.4 ; I.1.2 ; I.3.5 ; ","Reem, Daniel ; ","The projector algorithm: a simple parallel algorithm for computing   Voronoi diagrams and Delaunay graphs  The Voronoi diagram is a certain geometric data structure which has numerous applications in various scientific and technological fields. The theory of algorithms for computing 2D Euclidean Voronoi diagrams of point sites is rich and useful, with several different and important algorithms. However, this theory has been quite steady during the last few decades in the sense that no essentially new algorithms have entered the game. In addition, most of the known algorithms are serial in nature and hence cast inherent difficulties on the possibility to compute the diagram in parallel. In this paper we present the projector algorithm: a new and simple algorithm which enables the (combinatorial) computation of 2D Voronoi diagrams. The algorithm is significantly different from previous ones and some of the involved concepts in it are in the spirit of linear programming and optics. Parallel implementation is naturally supported since each Voronoi cell can be computed independently of the other cells. A new combinatorial structure for representing the cells (and any convex polytope) is described along the way and the computation of the induced Delaunay graph is obtained almost automatically. ",projector algorithm simple parallel algorithm compute voronoi diagram delaunay graph voronoi diagram certain geometric data structure numerous applications various scientific technological field theory algorithms compute euclidean voronoi diagram point sit rich useful several different important algorithms however theory quite steady last decades sense essentially new algorithms enter game addition know algorithms serial nature hence cast inherent difficulties possibility compute diagram parallel paper present projector algorithm new simple algorithm enable combinatorial computation voronoi diagram algorithm significantly different previous ones involve concepts spirit linear program optics parallel implementation naturally support since voronoi cell compute independently cells new combinatorial structure represent cells convex polytope describe along way computation induce delaunay graph obtain almost automatically,111,4,1212.1095.txt
http://arxiv.org/abs/1212.1149,Threshold Digraphs,"  A digraph whose degree sequence has a unique vertex labeled realization is called threshold. In this paper we present several characterizations of threshold digraphs and their degree sequences, and show these characterizations to be equivalent. One of the characterizations is new, and allows for a shorter proof of the equivalence of the two known characterizations as well as proving the final characterization which appears without proof in the literature. Using this result, we obtain a new, short proof of the Fulkerson-Chen theorem on degree sequences of general digraphs. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C20 ; ,"Cloteaux, Brian ; LaMar, M. Drew ; Moseman, Elizabeth ; Shook, James ; ","Threshold Digraphs  A digraph whose degree sequence has a unique vertex labeled realization is called threshold. In this paper we present several characterizations of threshold digraphs and their degree sequences, and show these characterizations to be equivalent. One of the characterizations is new, and allows for a shorter proof of the equivalence of the two known characterizations as well as proving the final characterization which appears without proof in the literature. Using this result, we obtain a new, short proof of the Fulkerson-Chen theorem on degree sequences of general digraphs. ",threshold digraphs digraph whose degree sequence unique vertex label realization call threshold paper present several characterizations threshold digraphs degree sequence show characterizations equivalent one characterizations new allow shorter proof equivalence two know characterizations well prove final characterization appear without proof literature use result obtain new short proof fulkerson chen theorem degree sequence general digraphs,54,8,1212.1149.txt
http://arxiv.org/abs/1212.1710,"The information and its observer: external and internal information   processes, information cooperation, and the origin of the observer intellect","  In observing interactive processes, conversion of observed uncertainty to observer certainty is natural phenomenon creating Yes-No actions of information Bit and its information observer. Observer emerges from interacting random field of Kolmogorov probabilities which link Kolmogorov 0-1 law probabilities and Bayesian probabilities observing Markov diffusion process by probabilistic 0-1 impulses. Each No-0 action cuts maximum of impulse minimal entropy while following Yes-1 action transfers maxim between impulses performing dual principle of converting process entropy to information. Merging Yes-No actions generate microprocess within bordered impulse producing Bit with free information when the microprocess probability approaches 1. Interacting bits memorizes each impulse free information which attracts multiple Bits moving macroprocess that self-joins triplet macrounits. Each memorized information binds reversible microprocess within impulse with irreversible macroprocess. The observation automatically converts cutting entropy to information macrounits. Macrounits logically self-organize information networks IN encoding the units in geometrical structures enclosing triplet code. Multiple IN binds their ending triplets which encloses observer information cognition and intelligence. The observer cognition assembles common units through multiple attraction and resonances at forming IN triplet hierarchy which accept only units that recognizes each IN node. Maximal number of accepted triplet levels in multiple IN measures the observer maximum comparative information intelligence. The observation process carries probabilistic and certain wave function which self-organizes the space hierarchical structures.These information regularities create integral logic and intelligence self-operating multiple IN up to physical reality matter. ","Nonlinear Sciences - Adaptation and Self-Organizing Systems ; Computer Science - Information Theory ; 58J65, 60J65, 93B52, 93E02, 93E15, 93E30 ; H.1.1 ; ","Lerner, Vladimir S. ; ","The information and its observer: external and internal information   processes, information cooperation, and the origin of the observer intellect  In observing interactive processes, conversion of observed uncertainty to observer certainty is natural phenomenon creating Yes-No actions of information Bit and its information observer. Observer emerges from interacting random field of Kolmogorov probabilities which link Kolmogorov 0-1 law probabilities and Bayesian probabilities observing Markov diffusion process by probabilistic 0-1 impulses. Each No-0 action cuts maximum of impulse minimal entropy while following Yes-1 action transfers maxim between impulses performing dual principle of converting process entropy to information. Merging Yes-No actions generate microprocess within bordered impulse producing Bit with free information when the microprocess probability approaches 1. Interacting bits memorizes each impulse free information which attracts multiple Bits moving macroprocess that self-joins triplet macrounits. Each memorized information binds reversible microprocess within impulse with irreversible macroprocess. The observation automatically converts cutting entropy to information macrounits. Macrounits logically self-organize information networks IN encoding the units in geometrical structures enclosing triplet code. Multiple IN binds their ending triplets which encloses observer information cognition and intelligence. The observer cognition assembles common units through multiple attraction and resonances at forming IN triplet hierarchy which accept only units that recognizes each IN node. Maximal number of accepted triplet levels in multiple IN measures the observer maximum comparative information intelligence. The observation process carries probabilistic and certain wave function which self-organizes the space hierarchical structures.These information regularities create integral logic and intelligence self-operating multiple IN up to physical reality matter. ",information observer external internal information process information cooperation origin observer intellect observe interactive process conversion observe uncertainty observer certainty natural phenomenon create yes action information bite information observer observer emerge interact random field kolmogorov probabilities link kolmogorov law probabilities bayesian probabilities observe markov diffusion process probabilistic impulses action cut maximum impulse minimal entropy follow yes action transfer maxim impulses perform dual principle convert process entropy information merge yes action generate microprocess within border impulse produce bite free information microprocess probability approach interact bits memorize impulse free information attract multiple bits move macroprocess self join triplet macrounits memorize information bind reversible microprocess within impulse irreversible macroprocess observation automatically convert cut entropy information macrounits macrounits logically self organize information network encode units geometrical structure enclose triplet code multiple bind end triplets enclose observer information cognition intelligence observer cognition assemble common units multiple attraction resonances form triplet hierarchy accept units recognize node maximal number accept triplet level multiple measure observer maximum comparative information intelligence observation process carry probabilistic certain wave function self organize space hierarchical structure information regularities create integral logic intelligence self operate multiple physical reality matter,185,11,1212.1710.txt
http://arxiv.org/abs/1212.6751,Computably Categorical Fields via Fermat's Last Theorem,"  We construct a computable, computably categorical field of infinite transcendence degree over the rational numbers, using the Fermat polynomials and assorted results from algebraic geometry. We also show that this field has an intrinsically computable (infinite) transcendence basis. ","Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03C57 (Primary), 03D45, 12L05 (Secondary) ; ","Miller, Russell ; Schoutens, Hans ; ","Computably Categorical Fields via Fermat's Last Theorem  We construct a computable, computably categorical field of infinite transcendence degree over the rational numbers, using the Fermat polynomials and assorted results from algebraic geometry. We also show that this field has an intrinsically computable (infinite) transcendence basis. ",computably categorical field via fermat last theorem construct computable computably categorical field infinite transcendence degree rational number use fermat polynomials assort result algebraic geometry also show field intrinsically computable infinite transcendence basis,32,4,1212.6751.txt
http://arxiv.org/abs/1212.6879,On two conjectures of Maurer concerning basis graphs of matroids,"  We characterize 2-dimensional complexes associated canonically with basis graphs of matroids as simply connected triangle-square complexes satisfying some local conditions. This proves a version of a (disproved) conjecture by Stephen Maurer (Conjecture 3 of S. Maurer, Matroid basis graphs I, JCTB 14 (1973), 216-240). We also establish Conjecture 1 from the same paper about the redundancy of the conditions in the characterization of basis graphs. We indicate positive-curvature-like aspects of the local properties of the studied complexes. We characterize similarly the corresponding 2-dimensional complexes of even $\Delta$-matroids. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05B35, 05C12, 57M10, 57M20 ; ","Chalopin, Jérémie ; Chepoi, Victor ; Osajda, Damian ; ","On two conjectures of Maurer concerning basis graphs of matroids  We characterize 2-dimensional complexes associated canonically with basis graphs of matroids as simply connected triangle-square complexes satisfying some local conditions. This proves a version of a (disproved) conjecture by Stephen Maurer (Conjecture 3 of S. Maurer, Matroid basis graphs I, JCTB 14 (1973), 216-240). We also establish Conjecture 1 from the same paper about the redundancy of the conditions in the characterization of basis graphs. We indicate positive-curvature-like aspects of the local properties of the studied complexes. We characterize similarly the corresponding 2-dimensional complexes of even $\Delta$-matroids. ",two conjecture maurer concern basis graph matroids characterize dimensional complexes associate canonically basis graph matroids simply connect triangle square complexes satisfy local condition prove version disprove conjecture stephen maurer conjecture maurer matroid basis graph jctb also establish conjecture paper redundancy condition characterization basis graph indicate positive curvature like aspects local properties study complexes characterize similarly correspond dimensional complexes even delta matroids,61,3,1212.6879.txt
http://arxiv.org/abs/1301.1027,On online energy harvesting in multiple access communication systems,"  We investigate performance limits of a multiple access communication system with energy harvesting nodes where the utility function is taken to be the long-term average sum-throughput. We assume a causal structure for energy arrivals and study the problem in the continuous time regime. For this setting, we first characterize a storage dam model that captures the dynamics of a battery with energy harvesting and variable transmission power. Using this model, we next establish an upper bound on the throughput problem as a function of battery capacity. We also formulate a non-linear optimization problem to determine optimal achievable power policies for transmitters. Applying a calculus of variation technique, we then derive Euler-Lagrange equations as necessary conditions for optimum power policies in terms of a system of coupled partial integro-differential equations (PIDEs). Based on a Gauss-Seidel algorithm, we devise an iterative algorithm to solve these equations. We also propose a fixed-point algorithm for the symmetric multiple access setting in which the statistical descriptions of energy harvesters are identical. Along with the analysis and to support our iterative algorithms, comprehensive numerical results are also obtained. ",Computer Science - Information Theory ; ,"Khuzani, Masoud Badiei ; Mitran, Patrick ; ","On online energy harvesting in multiple access communication systems  We investigate performance limits of a multiple access communication system with energy harvesting nodes where the utility function is taken to be the long-term average sum-throughput. We assume a causal structure for energy arrivals and study the problem in the continuous time regime. For this setting, we first characterize a storage dam model that captures the dynamics of a battery with energy harvesting and variable transmission power. Using this model, we next establish an upper bound on the throughput problem as a function of battery capacity. We also formulate a non-linear optimization problem to determine optimal achievable power policies for transmitters. Applying a calculus of variation technique, we then derive Euler-Lagrange equations as necessary conditions for optimum power policies in terms of a system of coupled partial integro-differential equations (PIDEs). Based on a Gauss-Seidel algorithm, we devise an iterative algorithm to solve these equations. We also propose a fixed-point algorithm for the symmetric multiple access setting in which the statistical descriptions of energy harvesters are identical. Along with the analysis and to support our iterative algorithms, comprehensive numerical results are also obtained. ",online energy harvest multiple access communication systems investigate performance limit multiple access communication system energy harvest nod utility function take long term average sum throughput assume causal structure energy arrivals study problem continuous time regime set first characterize storage dam model capture dynamics battery energy harvest variable transmission power use model next establish upper bind throughput problem function battery capacity also formulate non linear optimization problem determine optimal achievable power policies transmitters apply calculus variation technique derive euler lagrange equations necessary condition optimum power policies term system couple partial integro differential equations pides base gauss seidel algorithm devise iterative algorithm solve equations also propose fix point algorithm symmetric multiple access set statistical descriptions energy harvesters identical along analysis support iterative algorithms comprehensive numerical result also obtain,126,0,1301.1027.txt
http://arxiv.org/abs/1301.1071,Direct QR factorizations for tall-and-skinny matrices in MapReduce   architectures,"  The QR factorization and the SVD are two fundamental matrix decompositions with applications throughout scientific computing and data analysis. For matrices with many more rows than columns, so-called ""tall-and-skinny matrices,"" there is a numerically stable, efficient, communication-avoiding algorithm for computing the QR factorization. It has been used in traditional high performance computing and grid computing environments. For MapReduce environments, existing methods to compute the QR decomposition use a numerically unstable approach that relies on indirectly computing the Q factor. In the best case, these methods require only two passes over the data. In this paper, we describe how to compute a stable tall-and-skinny QR factorization on a MapReduce architecture in only slightly more than 2 passes over the data. We can compute the SVD with only a small change and no difference in performance. We present a performance comparison between our new direct TSQR method, a standard unstable implementation for MapReduce (Cholesky QR), and the classic stable algorithm implemented for MapReduce (Householder QR). We find that our new stable method has a large performance advantage over the Householder QR method. This holds both in a theoretical performance model as well as in an actual implementation. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Numerical Analysis ; ","Benson, Austin R. ; Gleich, David F. ; Demmel, James ; ","Direct QR factorizations for tall-and-skinny matrices in MapReduce   architectures  The QR factorization and the SVD are two fundamental matrix decompositions with applications throughout scientific computing and data analysis. For matrices with many more rows than columns, so-called ""tall-and-skinny matrices,"" there is a numerically stable, efficient, communication-avoiding algorithm for computing the QR factorization. It has been used in traditional high performance computing and grid computing environments. For MapReduce environments, existing methods to compute the QR decomposition use a numerically unstable approach that relies on indirectly computing the Q factor. In the best case, these methods require only two passes over the data. In this paper, we describe how to compute a stable tall-and-skinny QR factorization on a MapReduce architecture in only slightly more than 2 passes over the data. We can compute the SVD with only a small change and no difference in performance. We present a performance comparison between our new direct TSQR method, a standard unstable implementation for MapReduce (Cholesky QR), and the classic stable algorithm implemented for MapReduce (Householder QR). We find that our new stable method has a large performance advantage over the Householder QR method. This holds both in a theoretical performance model as well as in an actual implementation. ",direct qr factorizations tall skinny matrices mapreduce architectures qr factorization svd two fundamental matrix decompositions applications throughout scientific compute data analysis matrices many row columns call tall skinny matrices numerically stable efficient communication avoid algorithm compute qr factorization use traditional high performance compute grid compute environments mapreduce environments exist methods compute qr decomposition use numerically unstable approach rely indirectly compute factor best case methods require two pass data paper describe compute stable tall skinny qr factorization mapreduce architecture slightly pass data compute svd small change difference performance present performance comparison new direct tsqr method standard unstable implementation mapreduce cholesky qr classic stable algorithm implement mapreduce householder qr find new stable method large performance advantage householder qr method hold theoretical performance model well actual implementation,124,4,1301.1071.txt
http://arxiv.org/abs/1301.1107,Spectral Condition-Number Estimation of Large Sparse Matrices,"  We describe a randomized Krylov-subspace method for estimating the spectral condition number of a real matrix A or indicating that it is numerically rank deficient. The main difficulty in estimating the condition number is the estimation of the smallest singular value \sigma_{\min} of A. Our method estimates this value by solving a consistent linear least-squares problem with a known solution using a specific Krylov-subspace method called LSQR. In this method, the forward error tends to concentrate in the direction of a right singular vector corresponding to \sigma_{\min}. Extensive experiments show that the method is able to estimate well the condition number of a wide array of matrices. It can sometimes estimate the condition number when running a dense SVD would be impractical due to the computational cost or the memory requirements. The method uses very little memory (it inherits this property from LSQR) and it works equally well on square and rectangular matrices. ",Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; ,"Avron, Haim ; Druinsky, Alex ; Toledo, Sivan ; ","Spectral Condition-Number Estimation of Large Sparse Matrices  We describe a randomized Krylov-subspace method for estimating the spectral condition number of a real matrix A or indicating that it is numerically rank deficient. The main difficulty in estimating the condition number is the estimation of the smallest singular value \sigma_{\min} of A. Our method estimates this value by solving a consistent linear least-squares problem with a known solution using a specific Krylov-subspace method called LSQR. In this method, the forward error tends to concentrate in the direction of a right singular vector corresponding to \sigma_{\min}. Extensive experiments show that the method is able to estimate well the condition number of a wide array of matrices. It can sometimes estimate the condition number when running a dense SVD would be impractical due to the computational cost or the memory requirements. The method uses very little memory (it inherits this property from LSQR) and it works equally well on square and rectangular matrices. ",spectral condition number estimation large sparse matrices describe randomize krylov subspace method estimate spectral condition number real matrix indicate numerically rank deficient main difficulty estimate condition number estimation smallest singular value sigma min method estimate value solve consistent linear least square problem know solution use specific krylov subspace method call lsqr method forward error tend concentrate direction right singular vector correspond sigma min extensive experiment show method able estimate well condition number wide array matrices sometimes estimate condition number run dense svd would impractical due computational cost memory requirements method use little memory inherit property lsqr work equally well square rectangular matrices,102,7,1301.1107.txt
http://arxiv.org/abs/1301.2158,Artificial Intelligence Framework for Simulating Clinical   Decision-Making: A Markov Decision Process Approach,"  In the modern healthcare system, rapidly expanding costs/complexity, the growing myriad of treatment options, and exploding information streams that often do not effectively reach the front lines hinder the ability to choose optimal treatment decisions over time. The goal in this paper is to develop a general purpose (non-disease-specific) computational/artificial intelligence (AI) framework to address these challenges. This serves two potential functions: 1) a simulation environment for exploring various healthcare policies, payment methodologies, etc., and 2) the basis for clinical artificial intelligence - an AI that can think like a doctor. This approach combines Markov decision processes and dynamic decision networks to learn from clinical data and develop complex plans via simulation of alternative sequential decision paths while capturing the sometimes conflicting, sometimes synergistic interactions of various components in the healthcare system. It can operate in partially observable environments (in the case of missing observations or data) by maintaining belief states about patient health status and functions as an online agent that plans and re-plans. This framework was evaluated using real patient data from an electronic health record. Such an AI framework easily outperforms the current treatment-as-usual (TAU) case-rate/fee-for-service models of healthcare (Cost per Unit Change: $189 vs. $497) while obtaining a 30-35% increase in patient outcomes. Tweaking certain model parameters further enhances this advantage, obtaining roughly 50% more improvement for roughly half the costs. Given careful design and problem formulation, an AI simulation framework can approximate optimal decisions even in complex and uncertain environments. Future work is described that outlines potential lines of research and integration of machine learning algorithms for personalized medicine. ",Computer Science - Artificial Intelligence ; Statistics - Machine Learning ; ,"Bennett, Casey C. ; Hauser, Kris ; ","Artificial Intelligence Framework for Simulating Clinical   Decision-Making: A Markov Decision Process Approach  In the modern healthcare system, rapidly expanding costs/complexity, the growing myriad of treatment options, and exploding information streams that often do not effectively reach the front lines hinder the ability to choose optimal treatment decisions over time. The goal in this paper is to develop a general purpose (non-disease-specific) computational/artificial intelligence (AI) framework to address these challenges. This serves two potential functions: 1) a simulation environment for exploring various healthcare policies, payment methodologies, etc., and 2) the basis for clinical artificial intelligence - an AI that can think like a doctor. This approach combines Markov decision processes and dynamic decision networks to learn from clinical data and develop complex plans via simulation of alternative sequential decision paths while capturing the sometimes conflicting, sometimes synergistic interactions of various components in the healthcare system. It can operate in partially observable environments (in the case of missing observations or data) by maintaining belief states about patient health status and functions as an online agent that plans and re-plans. This framework was evaluated using real patient data from an electronic health record. Such an AI framework easily outperforms the current treatment-as-usual (TAU) case-rate/fee-for-service models of healthcare (Cost per Unit Change: $189 vs. $497) while obtaining a 30-35% increase in patient outcomes. Tweaking certain model parameters further enhances this advantage, obtaining roughly 50% more improvement for roughly half the costs. Given careful design and problem formulation, an AI simulation framework can approximate optimal decisions even in complex and uncertain environments. Future work is described that outlines potential lines of research and integration of machine learning algorithms for personalized medicine. ",artificial intelligence framework simulate clinical decision make markov decision process approach modern healthcare system rapidly expand cost complexity grow myriad treatment options explode information stream often effectively reach front line hinder ability choose optimal treatment decisions time goal paper develop general purpose non disease specific computational artificial intelligence ai framework address challenge serve two potential function simulation environment explore various healthcare policies payment methodologies etc basis clinical artificial intelligence ai think like doctor approach combine markov decision process dynamic decision network learn clinical data develop complex plan via simulation alternative sequential decision paths capture sometimes conflict sometimes synergistic interactions various components healthcare system operate partially observable environments case miss observations data maintain belief state patient health status function online agent plan plan framework evaluate use real patient data electronic health record ai framework easily outperform current treatment usual tau case rate fee service model healthcare cost per unit change vs obtain increase patient outcomes tweak certain model parameters enhance advantage obtain roughly improvement roughly half cost give careful design problem formulation ai simulation framework approximate optimal decisions even complex uncertain environments future work describe outline potential line research integration machine learn algorithms personalize medicine,194,11,1301.2158.txt
http://arxiv.org/abs/1301.2959,New elements for a network (including brain) general theory during   learning period,"  This study deals with the evolution of the so called 'intelligent' networks (insect society without leader, cells of an organism, brain,...) during their learning period. First we summarize briefly the Version 2 (published in French), whose the main characteristics are: 1) A network connected to its environment is considered as immersed into an information field created by this environment which so dictates to it the learning constraints. 2) The used formalism draws one's inspiration from the one of the Quantum field theory (Principle of stationary action, gauge fields, invariance by symmetry transformations,...). 3) We obtain Lagrange equations whose solutions describe the network evolution during the whole learning period. 4) Then, while proceeding with the same formalism inspiration, we suggest other study ways capable of evolving the knowledge in the considered scope. In a second part, after a reminder of the points to be improved, we exhibit the Version 5 which brings, we think, relevant improvements. Indeed: 5) We consider the weighted averages of the variables; this introduces probabilities. 6) We define two observables (L average of information flux and A activity of the network) which could be measured and so be compared with experimental results. 7) We find that L , weighted average of information flows, is an invariant. 8) Finally, we propose two expressions for the conactance, from which we deduce the corresponding Lagrange equations which have to be solved to know the evolution of the considered weighted averages. But, at the present stage, we think that we can progress only by carrying out experiments (see projects like Human brain project) and discovering invariants, symmetries which would allow us, like in Physics, to classify networks and above all to understand better the connections between them. Indeed, and that is what we propose among the future research ways, the underlying problem is to understand how, after their learning period, several networks can connect together to produce, in the brain case for instance, what we call mental states. ",Nonlinear Sciences - Adaptation and Self-Organizing Systems ; Computer Science - Neural and Evolutionary Computing ; Nonlinear Sciences - Chaotic Dynamics ; ,"Piniello, Jean ; ","New elements for a network (including brain) general theory during   learning period  This study deals with the evolution of the so called 'intelligent' networks (insect society without leader, cells of an organism, brain,...) during their learning period. First we summarize briefly the Version 2 (published in French), whose the main characteristics are: 1) A network connected to its environment is considered as immersed into an information field created by this environment which so dictates to it the learning constraints. 2) The used formalism draws one's inspiration from the one of the Quantum field theory (Principle of stationary action, gauge fields, invariance by symmetry transformations,...). 3) We obtain Lagrange equations whose solutions describe the network evolution during the whole learning period. 4) Then, while proceeding with the same formalism inspiration, we suggest other study ways capable of evolving the knowledge in the considered scope. In a second part, after a reminder of the points to be improved, we exhibit the Version 5 which brings, we think, relevant improvements. Indeed: 5) We consider the weighted averages of the variables; this introduces probabilities. 6) We define two observables (L average of information flux and A activity of the network) which could be measured and so be compared with experimental results. 7) We find that L , weighted average of information flows, is an invariant. 8) Finally, we propose two expressions for the conactance, from which we deduce the corresponding Lagrange equations which have to be solved to know the evolution of the considered weighted averages. But, at the present stage, we think that we can progress only by carrying out experiments (see projects like Human brain project) and discovering invariants, symmetries which would allow us, like in Physics, to classify networks and above all to understand better the connections between them. Indeed, and that is what we propose among the future research ways, the underlying problem is to understand how, after their learning period, several networks can connect together to produce, in the brain case for instance, what we call mental states. ",new elements network include brain general theory learn period study deal evolution call intelligent network insect society without leader cells organism brain learn period first summarize briefly version publish french whose main characteristics network connect environment consider immerse information field create environment dictate learn constraints use formalism draw one inspiration one quantum field theory principle stationary action gauge field invariance symmetry transformations obtain lagrange equations whose solutions describe network evolution whole learn period proceed formalism inspiration suggest study ways capable evolve knowledge consider scope second part reminder point improve exhibit version bring think relevant improvements indeed consider weight average variables introduce probabilities define two observables average information flux activity network could measure compare experimental result find weight average information flow invariant finally propose two expressions conactance deduce correspond lagrange equations solve know evolution consider weight average present stage think progress carry experiment see project like human brain project discover invariants symmetries would allow us like physics classify network understand better connections indeed propose among future research ways underlie problem understand learn period several network connect together produce brain case instance call mental state,183,6,1301.2959.txt
http://arxiv.org/abs/1301.3605,Feature Learning in Deep Neural Networks - Studies on Speech Recognition   Tasks,"  Recent studies have shown that deep neural networks (DNNs) perform significantly better than shallow networks and Gaussian mixture models (GMMs) on large vocabulary speech recognition tasks. In this paper, we argue that the improved accuracy achieved by the DNNs is the result of their ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals. We show that these representations become increasingly insensitive to small perturbations in the input with increasing network depth, which leads to better speech recognition performance with deeper networks. We also show that DNNs cannot extrapolate to test samples that are substantially different from the training examples. If the training data are sufficiently representative, however, internal features learned by the DNN are relatively stable with respect to speaker differences, bandwidth differences, and environment distortion. This enables DNN-based recognizers to perform as well or better than state-of-the-art systems based on GMMs or shallow networks without the need for explicit model adaptation or feature normalization. ",Computer Science - Machine Learning ; Computer Science - Computation and Language ; Computer Science - Neural and Evolutionary Computing ; Electrical Engineering and Systems Science - Audio and Speech Processing ; ,"Yu, Dong ; Seltzer, Michael L. ; Li, Jinyu ; Huang, Jui-Ting ; Seide, Frank ; ","Feature Learning in Deep Neural Networks - Studies on Speech Recognition   Tasks  Recent studies have shown that deep neural networks (DNNs) perform significantly better than shallow networks and Gaussian mixture models (GMMs) on large vocabulary speech recognition tasks. In this paper, we argue that the improved accuracy achieved by the DNNs is the result of their ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals. We show that these representations become increasingly insensitive to small perturbations in the input with increasing network depth, which leads to better speech recognition performance with deeper networks. We also show that DNNs cannot extrapolate to test samples that are substantially different from the training examples. If the training data are sufficiently representative, however, internal features learned by the DNN are relatively stable with respect to speaker differences, bandwidth differences, and environment distortion. This enables DNN-based recognizers to perform as well or better than state-of-the-art systems based on GMMs or shallow networks without the need for explicit model adaptation or feature normalization. ",feature learn deep neural network study speech recognition task recent study show deep neural network dnns perform significantly better shallow network gaussian mixture model gmms large vocabulary speech recognition task paper argue improve accuracy achieve dnns result ability extract discriminative internal representations robust many source variability speech signal show representations become increasingly insensitive small perturbations input increase network depth lead better speech recognition performance deeper network also show dnns cannot extrapolate test sample substantially different train examples train data sufficiently representative however internal feature learn dnn relatively stable respect speaker differences bandwidth differences environment distortion enable dnn base recognizers perform well better state art systems base gmms shallow network without need explicit model adaptation feature normalization,116,6,1301.3605.txt
http://arxiv.org/abs/1301.5055,"Nested Recursions, Simultaneous Parameters and Tree Superpositions","  We apply a tree-based methodology to solve new, very broadly defined families of nested recursions of the general form R(n)=sum_{i=1}^k R(n-a_i-sum_{j=1}^p R(n-b_{ij})), where a_i are integers, b_{ij} are natural numbers, and k,p are natural numbers that we use to denote ""arity"" and ""order,"" respectively, and with some specified initial conditions. The key idea of the tree-based solution method is to associate such recursions with infinite labelled trees in a natural way so that the solution to the recursions solves a counting question relating to the corresponding trees. We characterize certain recursion families within R(n) by introducing ""simultaneous parameters"" that appear both within the recursion itself and that also specify structural properties of the corresponding tree. First, we extend and unify recently discovered results concerning two families of arity k=2, order p=1 recursions. Next, we investigate the solution of nested recursion families by taking linear combinations of solution sequence frequencies for simpler nested recursions, which correspond to superpositions of the associated trees; this leads us to identify and solve two new recursion families for arity k=2 and general order p. Finally, we extend these results to general arity k>2. We conclude with several related open problems. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 11B37, 05C05 (Primary) 05A15, 05A19 (Secondary) ; ","Isgur, Abraham ; Kuznetsov, Vitaly ; Rahman, Mustazee ; Tanny, Stephen ; ","Nested Recursions, Simultaneous Parameters and Tree Superpositions  We apply a tree-based methodology to solve new, very broadly defined families of nested recursions of the general form R(n)=sum_{i=1}^k R(n-a_i-sum_{j=1}^p R(n-b_{ij})), where a_i are integers, b_{ij} are natural numbers, and k,p are natural numbers that we use to denote ""arity"" and ""order,"" respectively, and with some specified initial conditions. The key idea of the tree-based solution method is to associate such recursions with infinite labelled trees in a natural way so that the solution to the recursions solves a counting question relating to the corresponding trees. We characterize certain recursion families within R(n) by introducing ""simultaneous parameters"" that appear both within the recursion itself and that also specify structural properties of the corresponding tree. First, we extend and unify recently discovered results concerning two families of arity k=2, order p=1 recursions. Next, we investigate the solution of nested recursion families by taking linear combinations of solution sequence frequencies for simpler nested recursions, which correspond to superpositions of the associated trees; this leads us to identify and solve two new recursion families for arity k=2 and general order p. Finally, we extend these results to general arity k>2. We conclude with several related open problems. ",nest recursions simultaneous parameters tree superpositions apply tree base methodology solve new broadly define families nest recursions general form sum sum ij integers ij natural number natural number use denote arity order respectively specify initial condition key idea tree base solution method associate recursions infinite label tree natural way solution recursions solve count question relate correspond tree characterize certain recursion families within introduce simultaneous parameters appear within recursion also specify structural properties correspond tree first extend unify recently discover result concern two families arity order recursions next investigate solution nest recursion families take linear combinations solution sequence frequencies simpler nest recursions correspond superpositions associate tree lead us identify solve two new recursion families arity general order finally extend result general arity conclude several relate open problems,126,8,1301.5055.txt
http://arxiv.org/abs/1301.5293,Approximately counting semismooth integers,"  An integer $n$ is $(y,z)$-semismooth if $n=pm$ where $m$ is an integer with all prime divisors $\le y$ and $p$ is 1 or a prime $\le z$. arge quantities of semismooth integers are utilized in modern integer factoring algorithms, such as the number field sieve, that incorporate the so-called large prime variant. Thus, it is useful for factoring practitioners to be able to estimate the value of $\Psi(x,y,z)$, the number of $(y,z)$-semismooth integers up to $x$, so that they can better set algorithm parameters and minimize running times, which could be weeks or months on a cluster supercomputer. In this paper, we explore several algorithms to approximate $\Psi(x,y,z)$ using a generalization of Buchstab's identity with numeric integration. ","Computer Science - Data Structures and Algorithms ; Mathematics - Number Theory ; 11y16, 11y05, 11a51 ; F.2.1 ; I.1.2 ; ","Bach, Eric ; Sorenson, Jonathan ; ","Approximately counting semismooth integers  An integer $n$ is $(y,z)$-semismooth if $n=pm$ where $m$ is an integer with all prime divisors $\le y$ and $p$ is 1 or a prime $\le z$. arge quantities of semismooth integers are utilized in modern integer factoring algorithms, such as the number field sieve, that incorporate the so-called large prime variant. Thus, it is useful for factoring practitioners to be able to estimate the value of $\Psi(x,y,z)$, the number of $(y,z)$-semismooth integers up to $x$, so that they can better set algorithm parameters and minimize running times, which could be weeks or months on a cluster supercomputer. In this paper, we explore several algorithms to approximate $\Psi(x,y,z)$ using a generalization of Buchstab's identity with numeric integration. ",approximately count semismooth integers integer semismooth pm integer prime divisors le prime le arge quantities semismooth integers utilize modern integer factor algorithms number field sieve incorporate call large prime variant thus useful factor practitioners able estimate value psi number semismooth integers better set algorithm parameters minimize run time could weeks months cluster supercomputer paper explore several algorithms approximate psi use generalization buchstab identity numeric integration,65,4,1301.5293.txt
http://arxiv.org/abs/1301.5522,On Gaussian Half-Duplex Relay Networks,"  This paper considers Gaussian relay networks where a source transmits a message to a sink terminal with the help of one or more relay nodes. The relays work in half-duplex mode, in the sense that they can not transmit and receive at the same time. For the case of one relay, the generalized Degrees-of-Freedom is characterized first and then it is shown that capacity can be achieved to within a constant gap regardless of the actual value of the channel parameters. Different achievable schemes are presented with either deterministic or random switch for the relay node. It is shown that random switch in general achieves higher rates than deterministic switch. For the case of K relays, it is shown that the generalized Degrees-of-Freedom can be obtained by solving a linear program and that capacity can be achieved to within a constant gap of K/2log(4K). This gap may be further decreased by considering more structured networks such as, for example, the diamond network. ",Computer Science - Information Theory ; ,"Cardone, Martina ; Tuninetti, Daniela ; Knopp, Raymond ; Salim, Umer ; ","On Gaussian Half-Duplex Relay Networks  This paper considers Gaussian relay networks where a source transmits a message to a sink terminal with the help of one or more relay nodes. The relays work in half-duplex mode, in the sense that they can not transmit and receive at the same time. For the case of one relay, the generalized Degrees-of-Freedom is characterized first and then it is shown that capacity can be achieved to within a constant gap regardless of the actual value of the channel parameters. Different achievable schemes are presented with either deterministic or random switch for the relay node. It is shown that random switch in general achieves higher rates than deterministic switch. For the case of K relays, it is shown that the generalized Degrees-of-Freedom can be obtained by solving a linear program and that capacity can be achieved to within a constant gap of K/2log(4K). This gap may be further decreased by considering more structured networks such as, for example, the diamond network. ",gaussian half duplex relay network paper consider gaussian relay network source transmit message sink terminal help one relay nod relay work half duplex mode sense transmit receive time case one relay generalize degrees freedom characterize first show capacity achieve within constant gap regardless actual value channel parameters different achievable scheme present either deterministic random switch relay node show random switch general achieve higher rat deterministic switch case relay show generalize degrees freedom obtain solve linear program capacity achieve within constant gap log gap may decrease consider structure network example diamond network,91,6,1301.5522.txt
http://arxiv.org/abs/1301.7023,The Capacity of Adaptive Group Testing,"  We define capacity for group testing problems and deduce bounds for the capacity of a variety of noisy models, based on the capacity of equivalent noisy communication channels. For noiseless adaptive group testing we prove an information-theoretic lower bound which tightens a bound of Chan et al. This can be combined with a performance analysis of a version of Hwang's adaptive group testing algorithm, in order to deduce the capacity of noiseless and erasure group testing models. ",Computer Science - Information Theory ; ,"Baldassini, Leonardo ; Johnson, Oliver ; Aldridge, Matthew ; ","The Capacity of Adaptive Group Testing  We define capacity for group testing problems and deduce bounds for the capacity of a variety of noisy models, based on the capacity of equivalent noisy communication channels. For noiseless adaptive group testing we prove an information-theoretic lower bound which tightens a bound of Chan et al. This can be combined with a performance analysis of a version of Hwang's adaptive group testing algorithm, in order to deduce the capacity of noiseless and erasure group testing models. ",capacity adaptive group test define capacity group test problems deduce bound capacity variety noisy model base capacity equivalent noisy communication channel noiseless adaptive group test prove information theoretic lower bind tighten bind chan et al combine performance analysis version hwang adaptive group test algorithm order deduce capacity noiseless erasure group test model,52,12,1301.7023.txt
http://arxiv.org/abs/1302.1211,Quantum Lyapunov Control Based on the Average Value of an Imaginary   Mechanical Quantity,"  The convergence of closed quantum systems in the degenerate cases to the desired target state by using the quantum Lyapunov control based on the average value of an imaginary mechanical quantity is studied. On the basis of the existing methods which can only ensure the single-control Hamiltonian systems converge toward a set, we design the control laws to make the multi-control Hamiltonian systems converge to the desired target state. The convergence of the control system is proved, and the convergence to the desired target state is analyzed. How to make these conditions of convergence to the target state to be satisfied is proved or analyzed. Finally, numerical simulations for a three level system in the degenrate case transfering form an initial eigenstate to a target superposition state are studied to verify the effectiveness of the proposed control method. ",Computer Science - Systems and Control ; Mathematical Physics ; ,"Cong, Shuang ; Meng, Fangfang ; Kuang, Sen ; ","Quantum Lyapunov Control Based on the Average Value of an Imaginary   Mechanical Quantity  The convergence of closed quantum systems in the degenerate cases to the desired target state by using the quantum Lyapunov control based on the average value of an imaginary mechanical quantity is studied. On the basis of the existing methods which can only ensure the single-control Hamiltonian systems converge toward a set, we design the control laws to make the multi-control Hamiltonian systems converge to the desired target state. The convergence of the control system is proved, and the convergence to the desired target state is analyzed. How to make these conditions of convergence to the target state to be satisfied is proved or analyzed. Finally, numerical simulations for a three level system in the degenrate case transfering form an initial eigenstate to a target superposition state are studied to verify the effectiveness of the proposed control method. ",quantum lyapunov control base average value imaginary mechanical quantity convergence close quantum systems degenerate case desire target state use quantum lyapunov control base average value imaginary mechanical quantity study basis exist methods ensure single control hamiltonian systems converge toward set design control laws make multi control hamiltonian systems converge desire target state convergence control system prove convergence desire target state analyze make condition convergence target state satisfy prove analyze finally numerical simulations three level system degenrate case transfer form initial eigenstate target superposition state study verify effectiveness propose control method,90,2,1302.1211.txt
http://arxiv.org/abs/1302.2279,Expressing Second-order Sentences in Intuitionistic Dependence Logic,"  Intuitionistic dependence logic was introduced by Abramsky and Vaananen (2009) as a variant of dependence logic under a general construction of Hodges' (trump) team semantics. It was proven that there is a translation from intuitionistic dependence logic sentences into second order logic sentences. In this paper, we prove that the other direction is also true, therefore intuitionistic dependence logic is equivalent to second order logic on the level of sentences. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; ,"Yang, Fan ; ","Expressing Second-order Sentences in Intuitionistic Dependence Logic  Intuitionistic dependence logic was introduced by Abramsky and Vaananen (2009) as a variant of dependence logic under a general construction of Hodges' (trump) team semantics. It was proven that there is a translation from intuitionistic dependence logic sentences into second order logic sentences. In this paper, we prove that the other direction is also true, therefore intuitionistic dependence logic is equivalent to second order logic on the level of sentences. ",express second order sentence intuitionistic dependence logic intuitionistic dependence logic introduce abramsky vaananen variant dependence logic general construction hodges trump team semantics prove translation intuitionistic dependence logic sentence second order logic sentence paper prove direction also true therefore intuitionistic dependence logic equivalent second order logic level sentence,47,8,1302.2279.txt
http://arxiv.org/abs/1302.4118,Target Estimation in Colocated MIMO Radar via Matrix Completion,"  We consider a colocated MIMO radar scenario, in which the receive antennas forward their measurements to a fusion center. Based on the received data, the fusion center formulates a matrix which is then used for target parameter estimation. When the receive antennas sample the target returns at Nyquist rate, and assuming that there are more receive antennas than targets, the data matrix at the fusion center is low-rank. When each receive antenna sends to the fusion center only a small number of samples, along with the sample index, the receive data matrix has missing elements, corresponding to the samples that were not forwarded. Under certain conditions, matrix completion techniques can be applied to recover the full receive data matrix, which can then be used in conjunction with array processing techniques, e.g., MUSIC, to obtain target information. Numerical results indicate that good target recovery can be achieved with occupancy of the receive data matrix as low as 50%. ",Computer Science - Information Theory ; Statistics - Applications ; ,"Sun, Shunqiao ; Petropulu, Athina P. ; Bajwa, Waheed U. ; ","Target Estimation in Colocated MIMO Radar via Matrix Completion  We consider a colocated MIMO radar scenario, in which the receive antennas forward their measurements to a fusion center. Based on the received data, the fusion center formulates a matrix which is then used for target parameter estimation. When the receive antennas sample the target returns at Nyquist rate, and assuming that there are more receive antennas than targets, the data matrix at the fusion center is low-rank. When each receive antenna sends to the fusion center only a small number of samples, along with the sample index, the receive data matrix has missing elements, corresponding to the samples that were not forwarded. Under certain conditions, matrix completion techniques can be applied to recover the full receive data matrix, which can then be used in conjunction with array processing techniques, e.g., MUSIC, to obtain target information. Numerical results indicate that good target recovery can be achieved with occupancy of the receive data matrix as low as 50%. ",target estimation colocated mimo radar via matrix completion consider colocated mimo radar scenario receive antennas forward measurements fusion center base receive data fusion center formulate matrix use target parameter estimation receive antennas sample target return nyquist rate assume receive antennas target data matrix fusion center low rank receive antenna send fusion center small number sample along sample index receive data matrix miss elements correspond sample forward certain condition matrix completion techniques apply recover full receive data matrix use conjunction array process techniques music obtain target information numerical result indicate good target recovery achieve occupancy receive data matrix low,98,12,1302.4118.txt
http://arxiv.org/abs/1302.4808,Verifying the Consistency of Remote Untrusted Services with   Conflict-Free Operations,"  A group of mutually trusting clients outsources a computation service to a remote server, which they do not fully trust and that may be subject to attacks. The clients do not communicate with each other and would like to verify the correctness of the remote computation and the consistency of the server's responses. This paper presents the Conflict-free Operation verification Protocol (COP) that ensures linearizability when the server is correct and preserves fork-linearizability in any other case. All clients that observe each other's operations are consistent, in the sense that their own operations and those operations of other clients that they see are linearizable. If the server forks two clients by hiding an operation, these clients never again see operations of each other. COP supports wait-free client operations in the sense that when executed with a correct server, non-conflicting operations can run without waiting for other clients, allowing more parallelism than earlier protocols. A conflict arises when an operation causes a subsequent operation to produce a different output value for the client who runs it. The paper gives a precise model for the guarantees of COP and includes a formal analysis that these are achieved. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Cachin, Christian ; Ohrimenko, Olga ; ","Verifying the Consistency of Remote Untrusted Services with   Conflict-Free Operations  A group of mutually trusting clients outsources a computation service to a remote server, which they do not fully trust and that may be subject to attacks. The clients do not communicate with each other and would like to verify the correctness of the remote computation and the consistency of the server's responses. This paper presents the Conflict-free Operation verification Protocol (COP) that ensures linearizability when the server is correct and preserves fork-linearizability in any other case. All clients that observe each other's operations are consistent, in the sense that their own operations and those operations of other clients that they see are linearizable. If the server forks two clients by hiding an operation, these clients never again see operations of each other. COP supports wait-free client operations in the sense that when executed with a correct server, non-conflicting operations can run without waiting for other clients, allowing more parallelism than earlier protocols. A conflict arises when an operation causes a subsequent operation to produce a different output value for the client who runs it. The paper gives a precise model for the guarantees of COP and includes a formal analysis that these are achieved. ",verify consistency remote untrusted service conflict free operations group mutually trust clients outsource computation service remote server fully trust may subject attack clients communicate would like verify correctness remote computation consistency server responses paper present conflict free operation verification protocol cop ensure linearizability server correct preserve fork linearizability case clients observe operations consistent sense operations operations clients see linearizable server fork two clients hide operation clients never see operations cop support wait free client operations sense execute correct server non conflict operations run without wait clients allow parallelism earlier protocols conflict arise operation cause subsequent operation produce different output value client run paper give precise model guarantee cop include formal analysis achieve,112,7,1302.4808.txt
http://arxiv.org/abs/1302.5906,Achieving AWGN Channel Capacity With Lattice Gaussian Coding,"  We propose a new coding scheme using only one lattice that achieves the $\frac{1}{2}\log(1+\SNR)$ capacity of the additive white Gaussian noise (AWGN) channel with lattice decoding, when the signal-to-noise ratio $\SNR>e-1$. The scheme applies a discrete Gaussian distribution over an AWGN-good lattice, but otherwise does not require a shaping lattice or dither. Thus, it significantly simplifies the default lattice coding scheme of Erez and Zamir which involves a quantization-good lattice as well as an AWGN-good lattice. Using the flatness factor, we show that the error probability of the proposed scheme under minimum mean-square error (MMSE) lattice decoding is almost the same as that of Erez and Zamir, for any rate up to the AWGN channel capacity. We introduce the notion of good constellations, which carry almost the same mutual information as that of continuous Gaussian inputs. We also address the implementation of Gaussian shaping for the proposed lattice Gaussian coding scheme. ",Computer Science - Information Theory ; ,"Ling, Cong ; Belfiore, Jean-Claude ; ","Achieving AWGN Channel Capacity With Lattice Gaussian Coding  We propose a new coding scheme using only one lattice that achieves the $\frac{1}{2}\log(1+\SNR)$ capacity of the additive white Gaussian noise (AWGN) channel with lattice decoding, when the signal-to-noise ratio $\SNR>e-1$. The scheme applies a discrete Gaussian distribution over an AWGN-good lattice, but otherwise does not require a shaping lattice or dither. Thus, it significantly simplifies the default lattice coding scheme of Erez and Zamir which involves a quantization-good lattice as well as an AWGN-good lattice. Using the flatness factor, we show that the error probability of the proposed scheme under minimum mean-square error (MMSE) lattice decoding is almost the same as that of Erez and Zamir, for any rate up to the AWGN channel capacity. We introduce the notion of good constellations, which carry almost the same mutual information as that of continuous Gaussian inputs. We also address the implementation of Gaussian shaping for the proposed lattice Gaussian coding scheme. ",achieve awgn channel capacity lattice gaussian cod propose new cod scheme use one lattice achieve frac log snr capacity additive white gaussian noise awgn channel lattice decode signal noise ratio snr scheme apply discrete gaussian distribution awgn good lattice otherwise require shape lattice dither thus significantly simplify default lattice cod scheme erez zamir involve quantization good lattice well awgn good lattice use flatness factor show error probability propose scheme minimum mean square error mmse lattice decode almost erez zamir rate awgn channel capacity introduce notion good constellations carry almost mutual information continuous gaussian input also address implementation gaussian shape propose lattice gaussian cod scheme,104,5,1302.5906.txt
http://arxiv.org/abs/1302.6325,"A Note on ""A polynomial-time algorithm for global value numbering""","  Global Value Numbering(GVN) is a popular method for detecting redundant computations. A polynomial time algorithm for GVN is presented by Gulwani and Necula(2006). Here we present two limitations of this GVN algorithm due to which detection of certain kinds of redundancies can not be done using this algorithm. The first one is concerning the use of this algorithm in detecting some instances of the classical global common subexpressions, and the second is concerning its use in the detection of some redundancies that a local value numbering algorithm will detect. We suggest improvements that enable the algorithm to detect these kinds of redundancies as well. ",Computer Science - Programming Languages ; Computer Science - Logic in Computer Science ; ,"Nabeezath, Saleena ; Paleri, Vineeth ; ","A Note on ""A polynomial-time algorithm for global value numbering""  Global Value Numbering(GVN) is a popular method for detecting redundant computations. A polynomial time algorithm for GVN is presented by Gulwani and Necula(2006). Here we present two limitations of this GVN algorithm due to which detection of certain kinds of redundancies can not be done using this algorithm. The first one is concerning the use of this algorithm in detecting some instances of the classical global common subexpressions, and the second is concerning its use in the detection of some redundancies that a local value numbering algorithm will detect. We suggest improvements that enable the algorithm to detect these kinds of redundancies as well. ",note polynomial time algorithm global value number global value number gvn popular method detect redundant computations polynomial time algorithm gvn present gulwani necula present two limitations gvn algorithm due detection certain kinds redundancies do use algorithm first one concern use algorithm detect instance classical global common subexpressions second concern use detection redundancies local value number algorithm detect suggest improvements enable algorithm detect kinds redundancies well,65,4,1302.6325.txt
http://arxiv.org/abs/1303.0730,Diagonalizing by Fixed-Points,"  A universal schema for diagonalization was popularized by N. S. Yanofsky (2003) in which the existence of a (diagonolized-out and contradictory) object implies the existence of a fixed-point for a certain function. It was shown that many self-referential paradoxes and diagonally proved theorems can fit in that schema. Here, we fit more theorems in the universal schema of diagonalization, such as Euclid's theorem on the infinitude of the primes and new proofs of Boolos (1997) for Cantor's theorem on the non-equinumerosity of a set with its powerset. Then, in Linear Temporal Logic, we show the non-existence of a fixed-point in this logic whose proof resembles the argument of Yablo's paradox. Thus, Yablo's paradox turns for the first time into a genuine mathematico-logical theorem in the framework of Linear Temporal Logic. Again the diagonal schema of the paper is used in this proof, and also it is shown that G. Priest's inclosure schema (1997) can fit in our universal diagonal/fixed-point schema. We also show the existence of dominating (Ackermann-like) functions (which dominate a given countable set of functions---like primitive recursives) using the schema. ","Mathematics - Logic ; Computer Science - Logic in Computer Science ; 18A10, 18A15, 03B44, 03A05 ; ","Karimi, Ahmad ; Salehi, Saeed ; ","Diagonalizing by Fixed-Points  A universal schema for diagonalization was popularized by N. S. Yanofsky (2003) in which the existence of a (diagonolized-out and contradictory) object implies the existence of a fixed-point for a certain function. It was shown that many self-referential paradoxes and diagonally proved theorems can fit in that schema. Here, we fit more theorems in the universal schema of diagonalization, such as Euclid's theorem on the infinitude of the primes and new proofs of Boolos (1997) for Cantor's theorem on the non-equinumerosity of a set with its powerset. Then, in Linear Temporal Logic, we show the non-existence of a fixed-point in this logic whose proof resembles the argument of Yablo's paradox. Thus, Yablo's paradox turns for the first time into a genuine mathematico-logical theorem in the framework of Linear Temporal Logic. Again the diagonal schema of the paper is used in this proof, and also it is shown that G. Priest's inclosure schema (1997) can fit in our universal diagonal/fixed-point schema. We also show the existence of dominating (Ackermann-like) functions (which dominate a given countable set of functions---like primitive recursives) using the schema. ",diagonalize fix point universal schema diagonalization popularize yanofsky existence diagonolized contradictory object imply existence fix point certain function show many self referential paradoxes diagonally prove theorems fit schema fit theorems universal schema diagonalization euclid theorem infinitude prim new proof boolos cantor theorem non equinumerosity set powerset linear temporal logic show non existence fix point logic whose proof resemble argument yablo paradox thus yablo paradox turn first time genuine mathematico logical theorem framework linear temporal logic diagonal schema paper use proof also show priest inclosure schema fit universal diagonal fix point schema also show existence dominate ackermann like function dominate give countable set function like primitive recursives use schema,108,8,1303.0730.txt
http://arxiv.org/abs/1303.0926,Injectivity w.r.t. Distribution of Elements in the Compressed Sequences   Derived from Primitive Sequences over $Z/p^eZ$,"  Let $p\geq3$ be a prime and $e\geq2$ an integer. Let $\sigma(x)$ be a primitive polynomial of degree $n$ over $Z/p^eZ$ and $G'(\sigma(x),p^e)$ the set of primitive linear recurring sequences generated by $\sigma(x)$. A compressing map $\varphi$ on $Z/p^eZ$ naturally induces a map $\hat{\varphi}$ on $G'(\sigma(x),p^e)$. For a subset $D$ of the image of $\varphi$,$\hat{\varphi}$ is called to be injective w.r.t. $D$-uniformity if the distribution of elements of $D$ in the compressed sequence implies all information of the original primitive sequence. In this correspondence, for at least $1-2(p-1)/(p^n-1)$ of primitive polynomials of degree $n$, a clear criterion on $\varphi$ is obtained to decide whether $\hat{\varphi}$ is injective w.r.t. $D$-uniformity, and the majority of maps on $Z/p^eZ$ induce injective maps on $G'(\sigma(x),p^e)$. Furthermore, a sufficient condition on $\varphi$ is given to ensure injectivity of $\hat{\varphi}$ w.r.t. $D$-uniformity. It follows from the sufficient condition that if $\sigma(x)$ is strongly primitive and the compressing map $\varphi(x)=f(x_{e-1})$, where $f(x_{e-1})$ is a permutation polynomial over $\mathbb{F}_{p}$, then $\hat{\varphi}$ is injective w.r.t. $D$-uniformity for $\emptyset\neq D\subset\mathbb{F}_{p}$. Moreover, we give three specific families of compressing maps which induce injective maps on $G'(\sigma(x),p^e)$. ","Computer Science - Information Theory ; 11T71, 11B50 ; ","Wang, Lin ; Hu, Zhi ; ","Injectivity w.r.t. Distribution of Elements in the Compressed Sequences   Derived from Primitive Sequences over $Z/p^eZ$  Let $p\geq3$ be a prime and $e\geq2$ an integer. Let $\sigma(x)$ be a primitive polynomial of degree $n$ over $Z/p^eZ$ and $G'(\sigma(x),p^e)$ the set of primitive linear recurring sequences generated by $\sigma(x)$. A compressing map $\varphi$ on $Z/p^eZ$ naturally induces a map $\hat{\varphi}$ on $G'(\sigma(x),p^e)$. For a subset $D$ of the image of $\varphi$,$\hat{\varphi}$ is called to be injective w.r.t. $D$-uniformity if the distribution of elements of $D$ in the compressed sequence implies all information of the original primitive sequence. In this correspondence, for at least $1-2(p-1)/(p^n-1)$ of primitive polynomials of degree $n$, a clear criterion on $\varphi$ is obtained to decide whether $\hat{\varphi}$ is injective w.r.t. $D$-uniformity, and the majority of maps on $Z/p^eZ$ induce injective maps on $G'(\sigma(x),p^e)$. Furthermore, a sufficient condition on $\varphi$ is given to ensure injectivity of $\hat{\varphi}$ w.r.t. $D$-uniformity. It follows from the sufficient condition that if $\sigma(x)$ is strongly primitive and the compressing map $\varphi(x)=f(x_{e-1})$, where $f(x_{e-1})$ is a permutation polynomial over $\mathbb{F}_{p}$, then $\hat{\varphi}$ is injective w.r.t. $D$-uniformity for $\emptyset\neq D\subset\mathbb{F}_{p}$. Moreover, we give three specific families of compressing maps which induce injective maps on $G'(\sigma(x),p^e)$. ",injectivity distribution elements compress sequence derive primitive sequence ez let geq prime geq integer let sigma primitive polynomial degree ez sigma set primitive linear recur sequence generate sigma compress map varphi ez naturally induce map hat varphi sigma subset image varphi hat varphi call injective uniformity distribution elements compress sequence imply information original primitive sequence correspondence least primitive polynomials degree clear criterion varphi obtain decide whether hat varphi injective uniformity majority map ez induce injective map sigma furthermore sufficient condition varphi give ensure injectivity hat varphi uniformity follow sufficient condition sigma strongly primitive compress map varphi permutation polynomial mathbb hat varphi injective uniformity emptyset neq subset mathbb moreover give three specific families compress map induce injective map sigma,118,4,1303.0926.txt
http://arxiv.org/abs/1303.2054,Mining Representative Unsubstituted Graph Patterns Using Prior   Similarity Matrix,"  One of the most powerful techniques to study protein structures is to look for recurrent fragments (also called substructures or spatial motifs), then use them as patterns to characterize the proteins under study. An emergent trend consists in parsing proteins three-dimensional (3D) structures into graphs of amino acids. Hence, the search of recurrent spatial motifs is formulated as a process of frequent subgraph discovery where each subgraph represents a spatial motif. In this scope, several efficient approaches for frequent subgraph discovery have been proposed in the literature. However, the set of discovered frequent subgraphs is too large to be efficiently analyzed and explored in any further process. In this paper, we propose a novel pattern selection approach that shrinks the large number of discovered frequent subgraphs by selecting the representative ones. Existing pattern selection approaches do not exploit the domain knowledge. Yet, in our approach we incorporate the evolutionary information of amino acids defined in the substitution matrices in order to select the representative subgraphs. We show the effectiveness of our approach on a number of real datasets. The results issued from our experiments show that our approach is able to considerably decrease the number of motifs while enhancing their interestingness. ","Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Machine Learning ; ","Dhifli, Wajdi ; Saidi, Rabie ; Nguifo, Engelbert Mephu ; ","Mining Representative Unsubstituted Graph Patterns Using Prior   Similarity Matrix  One of the most powerful techniques to study protein structures is to look for recurrent fragments (also called substructures or spatial motifs), then use them as patterns to characterize the proteins under study. An emergent trend consists in parsing proteins three-dimensional (3D) structures into graphs of amino acids. Hence, the search of recurrent spatial motifs is formulated as a process of frequent subgraph discovery where each subgraph represents a spatial motif. In this scope, several efficient approaches for frequent subgraph discovery have been proposed in the literature. However, the set of discovered frequent subgraphs is too large to be efficiently analyzed and explored in any further process. In this paper, we propose a novel pattern selection approach that shrinks the large number of discovered frequent subgraphs by selecting the representative ones. Existing pattern selection approaches do not exploit the domain knowledge. Yet, in our approach we incorporate the evolutionary information of amino acids defined in the substitution matrices in order to select the representative subgraphs. We show the effectiveness of our approach on a number of real datasets. The results issued from our experiments show that our approach is able to considerably decrease the number of motifs while enhancing their interestingness. ",mine representative unsubstituted graph pattern use prior similarity matrix one powerful techniques study protein structure look recurrent fragment also call substructures spatial motifs use pattern characterize proteins study emergent trend consist parse proteins three dimensional structure graph amino acids hence search recurrent spatial motifs formulate process frequent subgraph discovery subgraph represent spatial motif scope several efficient approach frequent subgraph discovery propose literature however set discover frequent subgraphs large efficiently analyze explore process paper propose novel pattern selection approach shrink large number discover frequent subgraphs select representative ones exist pattern selection approach exploit domain knowledge yet approach incorporate evolutionary information amino acids define substitution matrices order select representative subgraphs show effectiveness approach number real datasets result issue experiment show approach able considerably decrease number motifs enhance interestingness,126,10,1303.2054.txt
http://arxiv.org/abs/1303.3235,On the Entropy of Couplings,"  In this paper, some general properties of Shannon information measures are investigated over sets of probability distributions with restricted marginals. Certain optimization problems associated with these functionals are shown to be NP-hard, and their special cases are found to be essentially information-theoretic restatements of well-known computational problems, such as the SUBSET SUM and the 3-PARTITION. The notion of minimum entropy coupling is introduced and its relevance is demonstrated in information-theoretic, computational, and statistical contexts. Finally, a family of pseudometrics (on the space of discrete probability distributions) defined by these couplings is studied, in particular their relation to the total variation distance, and a new characterization of the conditional entropy is given. ","Computer Science - Information Theory ; 94A17, 60E99, 68Q17 ; ","Kovačević, Mladen ; Stanojević, Ivan ; Šenk, Vojin ; ","On the Entropy of Couplings  In this paper, some general properties of Shannon information measures are investigated over sets of probability distributions with restricted marginals. Certain optimization problems associated with these functionals are shown to be NP-hard, and their special cases are found to be essentially information-theoretic restatements of well-known computational problems, such as the SUBSET SUM and the 3-PARTITION. The notion of minimum entropy coupling is introduced and its relevance is demonstrated in information-theoretic, computational, and statistical contexts. Finally, a family of pseudometrics (on the space of discrete probability distributions) defined by these couplings is studied, in particular their relation to the total variation distance, and a new characterization of the conditional entropy is given. ",entropy couple paper general properties shannon information measure investigate set probability distributions restrict marginals certain optimization problems associate functionals show np hard special case find essentially information theoretic restatements well know computational problems subset sum partition notion minimum entropy couple introduce relevance demonstrate information theoretic computational statistical contexts finally family pseudometrics space discrete probability distributions define couple study particular relation total variation distance new characterization conditional entropy give,68,11,1303.3235.txt
http://arxiv.org/abs/1303.5613,Network Detection Theory and Performance,"  Network detection is an important capability in many areas of applied research in which data can be represented as a graph of entities and relationships. Oftentimes the object of interest is a relatively small subgraph in an enormous, potentially uninteresting background. This aspect characterizes network detection as a ""big data"" problem. Graph partitioning and network discovery have been major research areas over the last ten years, driven by interest in internet search, cyber security, social networks, and criminal or terrorist activities. The specific problem of network discovery is addressed as a special case of graph partitioning in which membership in a small subgraph of interest must be determined. Algebraic graph theory is used as the basis to analyze and compare different network detection methods. A new Bayesian network detection framework is introduced that partitions the graph based on prior information and direct observations. The new approach, called space-time threat propagation, is proved to maximize the probability of detection and is therefore optimum in the Neyman-Pearson sense. This optimality criterion is compared to spectral community detection approaches which divide the global graph into subsets or communities with optimal connectivity properties. We also explore a new generative stochastic model for covert networks and analyze using receiver operating characteristics the detection performance of both classes of optimal detection techniques. ",Computer Science - Social and Information Networks ; Computer Science - Machine Learning ; Mathematics - Statistics Theory ; Physics - Physics and Society ; Statistics - Machine Learning ; ,"Smith, Steven T. ; Senne, Kenneth D. ; Philips, Scott ; Kao, Edward K. ; Bernstein, Garrett ; ","Network Detection Theory and Performance  Network detection is an important capability in many areas of applied research in which data can be represented as a graph of entities and relationships. Oftentimes the object of interest is a relatively small subgraph in an enormous, potentially uninteresting background. This aspect characterizes network detection as a ""big data"" problem. Graph partitioning and network discovery have been major research areas over the last ten years, driven by interest in internet search, cyber security, social networks, and criminal or terrorist activities. The specific problem of network discovery is addressed as a special case of graph partitioning in which membership in a small subgraph of interest must be determined. Algebraic graph theory is used as the basis to analyze and compare different network detection methods. A new Bayesian network detection framework is introduced that partitions the graph based on prior information and direct observations. The new approach, called space-time threat propagation, is proved to maximize the probability of detection and is therefore optimum in the Neyman-Pearson sense. This optimality criterion is compared to spectral community detection approaches which divide the global graph into subsets or communities with optimal connectivity properties. We also explore a new generative stochastic model for covert networks and analyze using receiver operating characteristics the detection performance of both classes of optimal detection techniques. ",network detection theory performance network detection important capability many areas apply research data represent graph entities relationships oftentimes object interest relatively small subgraph enormous potentially uninteresting background aspect characterize network detection big data problem graph partition network discovery major research areas last ten years drive interest internet search cyber security social network criminal terrorist activities specific problem network discovery address special case graph partition membership small subgraph interest must determine algebraic graph theory use basis analyze compare different network detection methods new bayesian network detection framework introduce partition graph base prior information direct observations new approach call space time threat propagation prove maximize probability detection therefore optimum neyman pearson sense optimality criterion compare spectral community detection approach divide global graph subsets communities optimal connectivity properties also explore new generative stochastic model covert network analyze use receiver operate characteristics detection performance class optimal detection techniques,144,6,1303.5613.txt
http://arxiv.org/abs/1303.5678,Interference alignment for the MIMO interference channel,"  We study vector space interference alignment for the MIMO interference channel with no time or frequency diversity, and no symbol extensions. We prove both necessary and sufficient conditions for alignment. In particular, we characterize the feasibility of alignment for the symmetric three-user channel where all users transmit along d dimensions, all transmitters have M antennas and all receivers have N antennas, as well as feasibility of alignment for the fully symmetric (M=N) channel with an arbitrary number of users.   An implication of our results is that the total degrees of freedom available in a K-user interference channel, using only spatial diversity from the multiple antennas, is at most 2. This is in sharp contrast to the K/2 degrees of freedom shown to be possible by Cadambe and Jafar with arbitrarily large time or frequency diversity.   Moving beyond the question of feasibility, we additionally discuss computation of the number of solutions using Schubert calculus in cases where there are a finite number of solutions. ",Computer Science - Information Theory ; ,"Bresler, Guy ; Cartwright, Dustin ; Tse, David ; ","Interference alignment for the MIMO interference channel  We study vector space interference alignment for the MIMO interference channel with no time or frequency diversity, and no symbol extensions. We prove both necessary and sufficient conditions for alignment. In particular, we characterize the feasibility of alignment for the symmetric three-user channel where all users transmit along d dimensions, all transmitters have M antennas and all receivers have N antennas, as well as feasibility of alignment for the fully symmetric (M=N) channel with an arbitrary number of users.   An implication of our results is that the total degrees of freedom available in a K-user interference channel, using only spatial diversity from the multiple antennas, is at most 2. This is in sharp contrast to the K/2 degrees of freedom shown to be possible by Cadambe and Jafar with arbitrarily large time or frequency diversity.   Moving beyond the question of feasibility, we additionally discuss computation of the number of solutions using Schubert calculus in cases where there are a finite number of solutions. ",interference alignment mimo interference channel study vector space interference alignment mimo interference channel time frequency diversity symbol extensions prove necessary sufficient condition alignment particular characterize feasibility alignment symmetric three user channel users transmit along dimension transmitters antennas receivers antennas well feasibility alignment fully symmetric channel arbitrary number users implication result total degrees freedom available user interference channel use spatial diversity multiple antennas sharp contrast degrees freedom show possible cadambe jafar arbitrarily large time frequency diversity move beyond question feasibility additionally discuss computation number solutions use schubert calculus case finite number solutions,91,12,1303.5678.txt
http://arxiv.org/abs/1303.7037,Parameterized Complexity of Discrete Morse Theory,"  Optimal Morse matchings reveal essential structures of cell complexes which lead to powerful tools to study discrete geometrical objects, in particular discrete 3-manifolds. However, such matchings are known to be NP-hard to compute on 3-manifolds, through a reduction to the erasability problem.   Here, we refine the study of the complexity of problems related to discrete Morse theory in terms of parameterized complexity. On the one hand we prove that the erasability problem is W[P]-complete on the natural parameter. On the other hand we propose an algorithm for computing optimal Morse matchings on triangulations of 3-manifolds which is fixed-parameter tractable in the treewidth of the bipartite graph representing the adjacency of the 1- and 2-simplexes. This algorithm also shows fixed parameter tractability for problems such as erasability and maximum alternating cycle-free matching. We further show that these results are also true when the treewidth of the dual graph of the triangulated 3-manifold is bounded. Finally, we investigate the respective treewidths of simplicial and generalized triangulations of 3-manifolds. ","Computer Science - Computational Geometry ; Computer Science - Computational Complexity ; Mathematics - Geometric Topology ; 68Q17, 68Q15, 57Q15, 58E05, 68R01 ; ","Burton, Benjamin A. ; Lewiner, Thomas ; Paixão, João ; Spreer, Jonathan ; ","Parameterized Complexity of Discrete Morse Theory  Optimal Morse matchings reveal essential structures of cell complexes which lead to powerful tools to study discrete geometrical objects, in particular discrete 3-manifolds. However, such matchings are known to be NP-hard to compute on 3-manifolds, through a reduction to the erasability problem.   Here, we refine the study of the complexity of problems related to discrete Morse theory in terms of parameterized complexity. On the one hand we prove that the erasability problem is W[P]-complete on the natural parameter. On the other hand we propose an algorithm for computing optimal Morse matchings on triangulations of 3-manifolds which is fixed-parameter tractable in the treewidth of the bipartite graph representing the adjacency of the 1- and 2-simplexes. This algorithm also shows fixed parameter tractability for problems such as erasability and maximum alternating cycle-free matching. We further show that these results are also true when the treewidth of the dual graph of the triangulated 3-manifold is bounded. Finally, we investigate the respective treewidths of simplicial and generalized triangulations of 3-manifolds. ",parameterized complexity discrete morse theory optimal morse match reveal essential structure cell complexes lead powerful tool study discrete geometrical object particular discrete manifold however match know np hard compute manifold reduction erasability problem refine study complexity problems relate discrete morse theory term parameterized complexity one hand prove erasability problem complete natural parameter hand propose algorithm compute optimal morse match triangulations manifold fix parameter tractable treewidth bipartite graph represent adjacency simplexes algorithm also show fix parameter tractability problems erasability maximum alternate cycle free match show result also true treewidth dual graph triangulate manifold bound finally investigate respective treewidths simplicial generalize triangulations manifold,101,3,1303.7037.txt
http://arxiv.org/abs/1304.0912,Structures Without Scattered-Automatic Presentation,"  Bruyere and Carton lifted the notion of finite automata reading infinite words to finite automata reading words with shape an arbitrary linear order L. Automata on finite words can be used to represent infinite structures, the so-called word-automatic structures. Analogously, for a linear order L there is the class of L-automatic structures. In this paper we prove the following limitations on the class of L-automatic structures for a fixed L of finite condensation rank 1+\alpha. Firstly, no scattered linear order with finite condensation rank above \omega^(\alpha+1) is L-\alpha-automatic. In particular, every L-automatic ordinal is below \omega^\omega^(\alpha+1). Secondly, we provide bounds on the (ordinal) height of well-founded order trees that are L-automatic. If \alpha is finite or L is an ordinal, the height of such a tree is bounded by \omega^{\alpha+1}. Finally, we separate the class of tree-automatic structures from that of L-automatic structures for any ordinal L: the countable atomless boolean algebra is known to be tree-automatic, but we show that it is not L-automatic. ",Computer Science - Formal Languages and Automata Theory ; Mathematics - Logic ; ,"Kartzow, Alexander ; Schlicht, Philipp ; ","Structures Without Scattered-Automatic Presentation  Bruyere and Carton lifted the notion of finite automata reading infinite words to finite automata reading words with shape an arbitrary linear order L. Automata on finite words can be used to represent infinite structures, the so-called word-automatic structures. Analogously, for a linear order L there is the class of L-automatic structures. In this paper we prove the following limitations on the class of L-automatic structures for a fixed L of finite condensation rank 1+\alpha. Firstly, no scattered linear order with finite condensation rank above \omega^(\alpha+1) is L-\alpha-automatic. In particular, every L-automatic ordinal is below \omega^\omega^(\alpha+1). Secondly, we provide bounds on the (ordinal) height of well-founded order trees that are L-automatic. If \alpha is finite or L is an ordinal, the height of such a tree is bounded by \omega^{\alpha+1}. Finally, we separate the class of tree-automatic structures from that of L-automatic structures for any ordinal L: the countable atomless boolean algebra is known to be tree-automatic, but we show that it is not L-automatic. ",structure without scatter automatic presentation bruyere carton lift notion finite automata read infinite word finite automata read word shape arbitrary linear order automata finite word use represent infinite structure call word automatic structure analogously linear order class automatic structure paper prove follow limitations class automatic structure fix finite condensation rank alpha firstly scatter linear order finite condensation rank omega alpha alpha automatic particular every automatic ordinal omega omega alpha secondly provide bound ordinal height well found order tree automatic alpha finite ordinal height tree bound omega alpha finally separate class tree automatic structure automatic structure ordinal countable atomless boolean algebra know tree automatic show automatic,105,14,1304.0912.txt
http://arxiv.org/abs/1304.1572,Stable and Informative Spectral Signatures for Graph Matching,"  In this paper, we consider the approximate weighted graph matching problem and introduce stable and informative first and second order compatibility terms suitable for inclusion into the popular integer quadratic program formulation. Our approach relies on a rigorous analysis of stability of spectral signatures based on the graph Laplacian. In the case of the first order term, we derive an objective function that measures both the stability and informativeness of a given spectral signature. By optimizing this objective, we design new spectral node signatures tuned to a specific graph to be matched. We also introduce the pairwise heat kernel distance as a stable second order compatibility term; we justify its plausibility by showing that in a certain limiting case it converges to the classical adjacency matrix-based second order compatibility function. We have tested our approach on a set of synthetic graphs, the widely-used CMU house sequence, and a set of real images. These experiments show the superior performance of our first and second order compatibility terms as compared with the commonly used ones. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Hu, Nan ; Rustamov, Raif M. ; Guibas, Leonidas ; ","Stable and Informative Spectral Signatures for Graph Matching  In this paper, we consider the approximate weighted graph matching problem and introduce stable and informative first and second order compatibility terms suitable for inclusion into the popular integer quadratic program formulation. Our approach relies on a rigorous analysis of stability of spectral signatures based on the graph Laplacian. In the case of the first order term, we derive an objective function that measures both the stability and informativeness of a given spectral signature. By optimizing this objective, we design new spectral node signatures tuned to a specific graph to be matched. We also introduce the pairwise heat kernel distance as a stable second order compatibility term; we justify its plausibility by showing that in a certain limiting case it converges to the classical adjacency matrix-based second order compatibility function. We have tested our approach on a set of synthetic graphs, the widely-used CMU house sequence, and a set of real images. These experiments show the superior performance of our first and second order compatibility terms as compared with the commonly used ones. ",stable informative spectral signatures graph match paper consider approximate weight graph match problem introduce stable informative first second order compatibility term suitable inclusion popular integer quadratic program formulation approach rely rigorous analysis stability spectral signatures base graph laplacian case first order term derive objective function measure stability informativeness give spectral signature optimize objective design new spectral node signatures tune specific graph match also introduce pairwise heat kernel distance stable second order compatibility term justify plausibility show certain limit case converge classical adjacency matrix base second order compatibility function test approach set synthetic graph widely use cmu house sequence set real image experiment show superior performance first second order compatibility term compare commonly use ones,114,3,1304.1572.txt
http://arxiv.org/abs/1304.2503,Simulating the Smart Grid,"  Major challenges for the transition of power systems do not only tackle power electronics but also communication technology, power market economy and user acceptance studies. Simulation is an important research method therein, as it helps to avoid costly failures. A common smart grid simulation platform is still missing. We introduce a conceptual model of agents in multiple flow networks. Flow networks extend the depth of established power flow analysis through use of networks of information flow and financial transactions. We use this model as a basis for comparing different power system simulators. Furthermore, a quantitative comparison of simulators is done to facilitate the decision for a suitable tool in comprehensive smart grid simulation. ",Computer Science - Systems and Control ; ,"Pöchacker, Manfred ; Sobe, Anita ; Elmenreich, Wilfried ; ","Simulating the Smart Grid  Major challenges for the transition of power systems do not only tackle power electronics but also communication technology, power market economy and user acceptance studies. Simulation is an important research method therein, as it helps to avoid costly failures. A common smart grid simulation platform is still missing. We introduce a conceptual model of agents in multiple flow networks. Flow networks extend the depth of established power flow analysis through use of networks of information flow and financial transactions. We use this model as a basis for comparing different power system simulators. Furthermore, a quantitative comparison of simulators is done to facilitate the decision for a suitable tool in comprehensive smart grid simulation. ",simulate smart grid major challenge transition power systems tackle power electronics also communication technology power market economy user acceptance study simulation important research method therein help avoid costly failures common smart grid simulation platform still miss introduce conceptual model agents multiple flow network flow network extend depth establish power flow analysis use network information flow financial transactions use model basis compare different power system simulators furthermore quantitative comparison simulators do facilitate decision suitable tool comprehensive smart grid simulation,78,6,1304.2503.txt
http://arxiv.org/abs/1304.2816,Asymptotic Behaviour and Ratios of Complexity in Cellular Automata,"  We study the asymptotic behaviour of symbolic computing systems, notably one-dimensional cellular automata (CA), in order to ascertain whether and at what rate the number of complex versus simple rules dominate the rule space for increasing neighbourhood range and number of symbols (or colours), and how different behaviour is distributed in the spaces of different cellular automata formalisms. Using two different measures, Shannon's block entropy and Kolmogorov complexity, the latter approximated by two different methods (lossless compressibility and block decomposition), we arrive at the same trend of larger complex behavioural fractions. We also advance a notion of asymptotic and limit behaviour for individual rules, both over initial conditions and runtimes, and we provide a formalisation of Wolfram's classification as a limit function in terms of Kolmogorov complexity. ",Nonlinear Sciences - Cellular Automata and Lattice Gases ; Computer Science - Computational Complexity ; ,"Zenil, Hector ; ","Asymptotic Behaviour and Ratios of Complexity in Cellular Automata  We study the asymptotic behaviour of symbolic computing systems, notably one-dimensional cellular automata (CA), in order to ascertain whether and at what rate the number of complex versus simple rules dominate the rule space for increasing neighbourhood range and number of symbols (or colours), and how different behaviour is distributed in the spaces of different cellular automata formalisms. Using two different measures, Shannon's block entropy and Kolmogorov complexity, the latter approximated by two different methods (lossless compressibility and block decomposition), we arrive at the same trend of larger complex behavioural fractions. We also advance a notion of asymptotic and limit behaviour for individual rules, both over initial conditions and runtimes, and we provide a formalisation of Wolfram's classification as a limit function in terms of Kolmogorov complexity. ",asymptotic behaviour ratios complexity cellular automata study asymptotic behaviour symbolic compute systems notably one dimensional cellular automata ca order ascertain whether rate number complex versus simple rule dominate rule space increase neighbourhood range number symbols colour different behaviour distribute space different cellular automata formalisms use two different measure shannon block entropy kolmogorov complexity latter approximate two different methods lossless compressibility block decomposition arrive trend larger complex behavioural fraction also advance notion asymptotic limit behaviour individual rule initial condition runtimes provide formalisation wolfram classification limit function term kolmogorov complexity,88,14,1304.2816.txt
http://arxiv.org/abs/1304.3944,Smart Microgrids: Overview and Outlook,"  The idea of changing our energy system from a hierarchical design into a set of nearly independent microgrids becomes feasible with the availability of small renewable energy generators. The smart microgrid concept comes with several challenges in research and engineering targeting load balancing, pricing, consumer integration and home automation. In this paper we first provide an overview on these challenges and present approaches that target the problems identified. While there exist promising algorithms for the particular field, we see a missing integration which specifically targets smart microgrids. Therefore, we propose an architecture that integrates the presented approaches and defines interfaces between the identified components such as generators, storage, smart and \dq{dumb} devices. ",Computer Science - Emerging Technologies ; Computer Science - Computers and Society ; Computer Science - Systems and Control ; ,"Sobe, Anita ; Elmenreich, Wilfried ; ","Smart Microgrids: Overview and Outlook  The idea of changing our energy system from a hierarchical design into a set of nearly independent microgrids becomes feasible with the availability of small renewable energy generators. The smart microgrid concept comes with several challenges in research and engineering targeting load balancing, pricing, consumer integration and home automation. In this paper we first provide an overview on these challenges and present approaches that target the problems identified. While there exist promising algorithms for the particular field, we see a missing integration which specifically targets smart microgrids. Therefore, we propose an architecture that integrates the presented approaches and defines interfaces between the identified components such as generators, storage, smart and \dq{dumb} devices. ",smart microgrids overview outlook idea change energy system hierarchical design set nearly independent microgrids become feasible availability small renewable energy generators smart microgrid concept come several challenge research engineer target load balance price consumer integration home automation paper first provide overview challenge present approach target problems identify exist promise algorithms particular field see miss integration specifically target smart microgrids therefore propose architecture integrate present approach define interfaces identify components generators storage smart dq dumb devices,75,2,1304.3944.txt
http://arxiv.org/abs/1304.4964,Newton-Based Optimization for Kullback-Leibler Nonnegative Tensor   Factorizations,"  Tensor factorizations with nonnegative constraints have found application in analyzing data from cyber traffic, social networks, and other areas. We consider application data best described as being generated by a Poisson process (e.g., count data), which leads to sparse tensors that can be modeled by sparse factor matrices. In this paper we investigate efficient techniques for computing an appropriate canonical polyadic tensor factorization based on the Kullback-Leibler divergence function. We propose novel subproblem solvers within the standard alternating block variable approach. Our new methods exploit structure and reformulate the optimization problem as small independent subproblems. We employ bound-constrained Newton and quasi-Newton methods. We compare our algorithms against other codes, demonstrating superior speed for high accuracy results and the ability to quickly find sparse solutions. ",Mathematics - Numerical Analysis ; Computer Science - Numerical Analysis ; ,"Hansen, Samantha ; Plantenga, Todd ; Kolda, Tamara G. ; ","Newton-Based Optimization for Kullback-Leibler Nonnegative Tensor   Factorizations  Tensor factorizations with nonnegative constraints have found application in analyzing data from cyber traffic, social networks, and other areas. We consider application data best described as being generated by a Poisson process (e.g., count data), which leads to sparse tensors that can be modeled by sparse factor matrices. In this paper we investigate efficient techniques for computing an appropriate canonical polyadic tensor factorization based on the Kullback-Leibler divergence function. We propose novel subproblem solvers within the standard alternating block variable approach. Our new methods exploit structure and reformulate the optimization problem as small independent subproblems. We employ bound-constrained Newton and quasi-Newton methods. We compare our algorithms against other codes, demonstrating superior speed for high accuracy results and the ability to quickly find sparse solutions. ",newton base optimization kullback leibler nonnegative tensor factorizations tensor factorizations nonnegative constraints find application analyze data cyber traffic social network areas consider application data best describe generate poisson process count data lead sparse tensors model sparse factor matrices paper investigate efficient techniques compute appropriate canonical polyadic tensor factorization base kullback leibler divergence function propose novel subproblem solvers within standard alternate block variable approach new methods exploit structure reformulate optimization problem small independent subproblems employ bind constrain newton quasi newton methods compare algorithms cod demonstrate superior speed high accuracy result ability quickly find sparse solutions,94,9,1304.4964.txt
http://arxiv.org/abs/1304.5591,Parameterized Complexity of 1-Planarity,"  We consider the problem of finding a 1-planar drawing for a general graph, where a 1-planar drawing is a drawing in which each edge participates in at most one crossing. Since this problem is known to be NP-hard we investigate the parameterized complexity of the problem with respect to the vertex cover number, tree-depth, and cyclomatic number. For these parameters we construct fixed-parameter tractable algorithms. However, the problem remains NP-complete for graphs of bounded bandwidth, pathwidth, or treewidth. ",Computer Science - Data Structures and Algorithms ; ,"Bannister, Michael J. ; Cabello, Sergio ; Eppstein, David ; ","Parameterized Complexity of 1-Planarity  We consider the problem of finding a 1-planar drawing for a general graph, where a 1-planar drawing is a drawing in which each edge participates in at most one crossing. Since this problem is known to be NP-hard we investigate the parameterized complexity of the problem with respect to the vertex cover number, tree-depth, and cyclomatic number. For these parameters we construct fixed-parameter tractable algorithms. However, the problem remains NP-complete for graphs of bounded bandwidth, pathwidth, or treewidth. ",parameterized complexity planarity consider problem find planar draw general graph planar draw draw edge participate one cross since problem know np hard investigate parameterized complexity problem respect vertex cover number tree depth cyclomatic number parameters construct fix parameter tractable algorithms however problem remain np complete graph bound bandwidth pathwidth treewidth,50,3,1304.5591.txt
http://arxiv.org/abs/1304.6116,"Selling Multiple Correlated Goods: Revenue Maximization and Menu-Size   Complexity (old title: ""The Menu-Size Complexity of Auctions"")","  We consider the well known, and notoriously difficult, problem of a single revenue-maximizing seller selling two or more heterogeneous goods to a single buyer whose private values for the goods are drawn from a (possibly correlated) known distribution, and whose valuation is additive over the goods. We show that when there are two (or more) goods, _simple mechanisms_ -- such as selling the goods separately or as a bundle -- _may yield only a negligible fraction of the optimal revenue_. This resolves the open problem of Briest, Chawla, Kleinberg, and Weinberg (JET 2015) who prove the result for at least three goods in the related setup of a unit-demand buyer. We also introduce the menu size as a simple measure of the complexity of mechanisms, and show that the revenue may increase polynomially with _menu size_ and that no bounded menu size can ensure any positive fraction of the optimal revenue. The menu size also turns out to ""pin down"" the revenue properties of deterministic mechanisms. ",Computer Science - Computer Science and Game Theory ; ,"Hart, Sergiu ; Nisan, Noam ; ","Selling Multiple Correlated Goods: Revenue Maximization and Menu-Size   Complexity (old title: ""The Menu-Size Complexity of Auctions"")  We consider the well known, and notoriously difficult, problem of a single revenue-maximizing seller selling two or more heterogeneous goods to a single buyer whose private values for the goods are drawn from a (possibly correlated) known distribution, and whose valuation is additive over the goods. We show that when there are two (or more) goods, _simple mechanisms_ -- such as selling the goods separately or as a bundle -- _may yield only a negligible fraction of the optimal revenue_. This resolves the open problem of Briest, Chawla, Kleinberg, and Weinberg (JET 2015) who prove the result for at least three goods in the related setup of a unit-demand buyer. We also introduce the menu size as a simple measure of the complexity of mechanisms, and show that the revenue may increase polynomially with _menu size_ and that no bounded menu size can ensure any positive fraction of the optimal revenue. The menu size also turns out to ""pin down"" the revenue properties of deterministic mechanisms. ",sell multiple correlate goods revenue maximization menu size complexity old title menu size complexity auction consider well know notoriously difficult problem single revenue maximize seller sell two heterogeneous goods single buyer whose private value goods draw possibly correlate know distribution whose valuation additive goods show two goods simple mechanisms sell goods separately bundle may yield negligible fraction optimal revenue resolve open problem briest chawla kleinberg weinberg jet prove result least three goods relate setup unit demand buyer also introduce menu size simple measure complexity mechanisms show revenue may increase polynomially menu size bound menu size ensure positive fraction optimal revenue menu size also turn pin revenue properties deterministic mechanisms,109,0,1304.6116.txt
http://arxiv.org/abs/1304.6896,Strongly light subgraphs in the 1-planar graphs with minimum degree 7,"  A graph is {\em $1$-planar} if it can be drawn in the plane such that every edge crosses at most one other edge. A connected graph $H$ is {\em strongly light} in a family of graphs $\mathfrak{G}$, if there exists a constant $\lambda$, such that every graph $G$ in $\mathfrak{G}$ contains a subgraph $K$ isomorphic to $H$ with $\deg_{G}(v) \leq \lambda$ for all $v \in V(K)$. In this paper, we present some strongly light subgraphs in the family of $1$-planar graphs with minimum degree~$7$. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C10 ; ,"Wang, Tao ; ","Strongly light subgraphs in the 1-planar graphs with minimum degree 7  A graph is {\em $1$-planar} if it can be drawn in the plane such that every edge crosses at most one other edge. A connected graph $H$ is {\em strongly light} in a family of graphs $\mathfrak{G}$, if there exists a constant $\lambda$, such that every graph $G$ in $\mathfrak{G}$ contains a subgraph $K$ isomorphic to $H$ with $\deg_{G}(v) \leq \lambda$ for all $v \in V(K)$. In this paper, we present some strongly light subgraphs in the family of $1$-planar graphs with minimum degree~$7$. ",strongly light subgraphs planar graph minimum degree graph em planar draw plane every edge cross one edge connect graph em strongly light family graph mathfrak exist constant lambda every graph mathfrak contain subgraph isomorphic deg leq lambda paper present strongly light subgraphs family planar graph minimum degree,47,3,1304.6896.txt
http://arxiv.org/abs/1304.7480,The Ergodic Capacity of the Multiple Access Channel Under Distributed   Scheduling - Order Optimality of Linear Receivers,"  Consider the problem of a Multiple-Input Multiple-Output (MIMO) Multiple-Access Channel (MAC) at the limit of large number of users. Clearly, in practical scenarios, only a small subset of the users can be scheduled to utilize the channel simultaneously. Thus, a problem of user selection arises. However, since solutions which collect Channel State Information (CSI) from all users and decide on the best subset to transmit in each slot do not scale when the number of users is large, distributed algorithms for user selection are advantageous.   In this paper, we analyse a distributed user selection algorithm, which selects a group of users to transmit without coordinating between users and without all users sending CSI to the base station. This threshold-based algorithm is analysed for both Zero-Forcing (ZF) and Minimum Mean Square Error (MMSE) receivers, and its expected sum-rate in the limit of large number of users is investigated. It is shown that for large number of users it achieves the same scaling laws as the optimal centralized scheme. ",Computer Science - Information Theory ; ,"Kampeas, Joseph ; Cohen, Asaf ; Gurewitz, Omer ; ","The Ergodic Capacity of the Multiple Access Channel Under Distributed   Scheduling - Order Optimality of Linear Receivers  Consider the problem of a Multiple-Input Multiple-Output (MIMO) Multiple-Access Channel (MAC) at the limit of large number of users. Clearly, in practical scenarios, only a small subset of the users can be scheduled to utilize the channel simultaneously. Thus, a problem of user selection arises. However, since solutions which collect Channel State Information (CSI) from all users and decide on the best subset to transmit in each slot do not scale when the number of users is large, distributed algorithms for user selection are advantageous.   In this paper, we analyse a distributed user selection algorithm, which selects a group of users to transmit without coordinating between users and without all users sending CSI to the base station. This threshold-based algorithm is analysed for both Zero-Forcing (ZF) and Minimum Mean Square Error (MMSE) receivers, and its expected sum-rate in the limit of large number of users is investigated. It is shown that for large number of users it achieves the same scaling laws as the optimal centralized scheme. ",ergodic capacity multiple access channel distribute schedule order optimality linear receivers consider problem multiple input multiple output mimo multiple access channel mac limit large number users clearly practical scenarios small subset users schedule utilize channel simultaneously thus problem user selection arise however since solutions collect channel state information csi users decide best subset transmit slot scale number users large distribute algorithms user selection advantageous paper analyse distribute user selection algorithm select group users transmit without coordinate users without users send csi base station threshold base algorithm analyse zero force zf minimum mean square error mmse receivers expect sum rate limit large number users investigate show large number users achieve scale laws optimal centralize scheme,114,12,1304.7480.txt
http://arxiv.org/abs/1305.0750,Multi-Sided Boundary Labeling,"  In the Boundary Labeling problem, we are given a set of $n$ points, referred to as sites, inside an axis-parallel rectangle $R$, and a set of $n$ pairwise disjoint rectangular labels that are attached to $R$ from the outside. The task is to connect the sites to the labels by non-intersecting rectilinear paths, so-called leaders, with at most one bend.   In this paper, we study the Multi-Sided Boundary Labeling problem, with labels lying on at least two sides of the enclosing rectangle. We present a polynomial-time algorithm that computes a crossing-free leader layout if one exists. So far, such an algorithm has only been known for the cases in which labels lie on one side or on two opposite sides of $R$ (here a crossing-free solution always exists). The case where labels may lie on adjacent sides is more difficult. We present efficient algorithms for testing the existence of a crossing-free leader layout that labels all sites and also for maximizing the number of labeled sites in a crossing-free leader layout. For two-sided boundary labeling with adjacent sides, we further show how to minimize the total leader length in a crossing-free layout. ",Computer Science - Computational Geometry ; ,"Kindermann, Philipp ; Niedermann, Benjamin ; Rutter, Ignaz ; Schaefer, Marcus ; Schulz, André ; Wolff, Alexander ; ","Multi-Sided Boundary Labeling  In the Boundary Labeling problem, we are given a set of $n$ points, referred to as sites, inside an axis-parallel rectangle $R$, and a set of $n$ pairwise disjoint rectangular labels that are attached to $R$ from the outside. The task is to connect the sites to the labels by non-intersecting rectilinear paths, so-called leaders, with at most one bend.   In this paper, we study the Multi-Sided Boundary Labeling problem, with labels lying on at least two sides of the enclosing rectangle. We present a polynomial-time algorithm that computes a crossing-free leader layout if one exists. So far, such an algorithm has only been known for the cases in which labels lie on one side or on two opposite sides of $R$ (here a crossing-free solution always exists). The case where labels may lie on adjacent sides is more difficult. We present efficient algorithms for testing the existence of a crossing-free leader layout that labels all sites and also for maximizing the number of labeled sites in a crossing-free leader layout. For two-sided boundary labeling with adjacent sides, we further show how to minimize the total leader length in a crossing-free layout. ",multi side boundary label boundary label problem give set point refer sit inside axis parallel rectangle set pairwise disjoint rectangular label attach outside task connect sit label non intersect rectilinear paths call leaders one bend paper study multi side boundary label problem label lie least two side enclose rectangle present polynomial time algorithm compute cross free leader layout one exist far algorithm know case label lie one side two opposite side cross free solution always exist case label may lie adjacent side difficult present efficient algorithms test existence cross free leader layout label sit also maximize number label sit cross free leader layout two side boundary label adjacent side show minimize total leader length cross free layout,117,4,1305.0750.txt
http://arxiv.org/abs/1305.2386,Disappointment in Social Choice Protocols,"  Social choice theory is a theoretical framework for analysis of combining individual preferences, interests, or welfare to reach a collective decision or social welfare in some sense. We introduce a new criterion for social choice protocols called social disappointment. Social disappointment happens when the outcome of a voting system occurs for those alternatives which are at the end of at least half of individual preference profiles. Here we introduce some protocols that prevent social disappointment and prove an impossibility theorem based on this key concept. ",Computer Science - Multiagent Systems ; 91B14 ; ,"Javidian, Mohammad Ali ; Ramezanian, Rasoul ; ","Disappointment in Social Choice Protocols  Social choice theory is a theoretical framework for analysis of combining individual preferences, interests, or welfare to reach a collective decision or social welfare in some sense. We introduce a new criterion for social choice protocols called social disappointment. Social disappointment happens when the outcome of a voting system occurs for those alternatives which are at the end of at least half of individual preference profiles. Here we introduce some protocols that prevent social disappointment and prove an impossibility theorem based on this key concept. ",disappointment social choice protocols social choice theory theoretical framework analysis combine individual preferences interest welfare reach collective decision social welfare sense introduce new criterion social choice protocols call social disappointment social disappointment happen outcome vote system occur alternatives end least half individual preference profile introduce protocols prevent social disappointment prove impossibility theorem base key concept,55,0,1305.2386.txt
http://arxiv.org/abs/1305.2494,Computing Solution Operators of Boundary-value Problems for Some Linear   Hyperbolic Systems of PDEs,"  We discuss possibilities of application of Numerical Analysis methods to proving computability, in the sense of the TTE approach, of solution operators of boundary-value problems for systems of PDEs. We prove computability of the solution operator for a symmetric hyperbolic system with computable real coefficients and dissipative boundary conditions, and of the Cauchy problem for the same system (we also prove computable dependence on the coefficients) in a cube $Q\subseteq\mathbb R^m$. Such systems describe a wide variety of physical processes (e.g. elasticity, acoustics, Maxwell equations). Moreover, many boundary-value problems for the wave equation also can be reduced to this case, thus we partially answer a question raised in Weihrauch and Zhong (2002). Compared with most of other existing methods of proving computability for PDEs, this method does not require existence of explicit solution formulas and is thus applicable to a broader class of (systems of) equations. ","Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; 03D78, 58J45, 65M06, 65M25 ; F.1.1 ; G.1.8 ; ","Selivanova, Svetlana ; Selivanov, Victor ; ","Computing Solution Operators of Boundary-value Problems for Some Linear   Hyperbolic Systems of PDEs  We discuss possibilities of application of Numerical Analysis methods to proving computability, in the sense of the TTE approach, of solution operators of boundary-value problems for systems of PDEs. We prove computability of the solution operator for a symmetric hyperbolic system with computable real coefficients and dissipative boundary conditions, and of the Cauchy problem for the same system (we also prove computable dependence on the coefficients) in a cube $Q\subseteq\mathbb R^m$. Such systems describe a wide variety of physical processes (e.g. elasticity, acoustics, Maxwell equations). Moreover, many boundary-value problems for the wave equation also can be reduced to this case, thus we partially answer a question raised in Weihrauch and Zhong (2002). Compared with most of other existing methods of proving computability for PDEs, this method does not require existence of explicit solution formulas and is thus applicable to a broader class of (systems of) equations. ",compute solution operators boundary value problems linear hyperbolic systems pdes discuss possibilities application numerical analysis methods prove computability sense tte approach solution operators boundary value problems systems pdes prove computability solution operator symmetric hyperbolic system computable real coefficients dissipative boundary condition cauchy problem system also prove computable dependence coefficients cube subseteq mathbb systems describe wide variety physical process elasticity acoustics maxwell equations moreover many boundary value problems wave equation also reduce case thus partially answer question raise weihrauch zhong compare exist methods prove computability pdes method require existence explicit solution formulas thus applicable broader class systems equations,97,7,1305.2494.txt
http://arxiv.org/abs/1305.4732,Enabling Self-Powered Autonomous Wireless Sensors with New-Generation   I2C-RFID Chips,"  A self-powered autonomous RFID device with sensing and computing capabilities is presented in this paper. Powered by an RF energy-harvesting circuit enhanced by a DC-DC voltage booster in silicon-on-insulator (SOI) technology, the device relies on a microcontroller and a new generation I2C-RFID chip to wirelessly deliver sensor data to standard RFID EPC Class-1 Generation-2 (Gen2) readers. When the RF power received from the interrogating reader is -14 dBm or higher, the device, fabricated on an FR4 substrate using low-cost discrete components, is able to produce 2.4-V DC voltage to power its circuitry. The experimental results demonstrate the effectiveness of the device to perform reliable sensor data transmissions up to 5 meters in fully-passive mode. To the best of our knowledge, this represents the longest read range ever reported for passive UHF RFID sensors compliant with the EPC Gen2 standard. ",Computer Science - Other Computer Science ; ,"De Donno, D. ; Catarinucci, L. ; Tarricone, L. ; ","Enabling Self-Powered Autonomous Wireless Sensors with New-Generation   I2C-RFID Chips  A self-powered autonomous RFID device with sensing and computing capabilities is presented in this paper. Powered by an RF energy-harvesting circuit enhanced by a DC-DC voltage booster in silicon-on-insulator (SOI) technology, the device relies on a microcontroller and a new generation I2C-RFID chip to wirelessly deliver sensor data to standard RFID EPC Class-1 Generation-2 (Gen2) readers. When the RF power received from the interrogating reader is -14 dBm or higher, the device, fabricated on an FR4 substrate using low-cost discrete components, is able to produce 2.4-V DC voltage to power its circuitry. The experimental results demonstrate the effectiveness of the device to perform reliable sensor data transmissions up to 5 meters in fully-passive mode. To the best of our knowledge, this represents the longest read range ever reported for passive UHF RFID sensors compliant with the EPC Gen2 standard. ",enable self power autonomous wireless sensors new generation rfid chip self power autonomous rfid device sense compute capabilities present paper power rf energy harvest circuit enhance dc dc voltage booster silicon insulator soi technology device rely microcontroller new generation rfid chip wirelessly deliver sensor data standard rfid epc class generation gen readers rf power receive interrogate reader dbm higher device fabricate fr substrate use low cost discrete components able produce dc voltage power circuitry experimental result demonstrate effectiveness device perform reliable sensor data transmissions meter fully passive mode best knowledge represent longest read range ever report passive uhf rfid sensors compliant epc gen standard,104,10,1305.4732.txt
http://arxiv.org/abs/1305.4874,The Query Complexity of Correlated Equilibria,"  We consider the complexity of finding a correlated equilibrium of an $n$-player game in a model that allows the algorithm to make queries on players' payoffs at pure strategy profiles. Randomized regret-based dynamics are known to yield an approximate correlated equilibrium efficiently, namely, in time that is polynomial in the number of players $n$. Here we show that both randomization and approximation are necessary: no efficient deterministic algorithm can reach even an approximate correlated equilibrium, and no efficient randomized algorithm can reach an exact correlated equilibrium. The results are obtained by bounding from below the number of payoff queries that are needed. ",Computer Science - Computer Science and Game Theory ; Computer Science - Data Structures and Algorithms ; ,"Hart, Sergiu ; Nisan, Noam ; ","The Query Complexity of Correlated Equilibria  We consider the complexity of finding a correlated equilibrium of an $n$-player game in a model that allows the algorithm to make queries on players' payoffs at pure strategy profiles. Randomized regret-based dynamics are known to yield an approximate correlated equilibrium efficiently, namely, in time that is polynomial in the number of players $n$. Here we show that both randomization and approximation are necessary: no efficient deterministic algorithm can reach even an approximate correlated equilibrium, and no efficient randomized algorithm can reach an exact correlated equilibrium. The results are obtained by bounding from below the number of payoff queries that are needed. ",query complexity correlate equilibria consider complexity find correlate equilibrium player game model allow algorithm make query players payoffs pure strategy profile randomize regret base dynamics know yield approximate correlate equilibrium efficiently namely time polynomial number players show randomization approximation necessary efficient deterministic algorithm reach even approximate correlate equilibrium efficient randomize algorithm reach exact correlate equilibrium result obtain bound number payoff query need,62,1,1305.4874.txt
http://arxiv.org/abs/1305.5592,Finite-Length and Asymptotic Analysis of Correlogram for Undersampled   Data,"  This paper studies a spectrum estimation method for the case that the samples are obtained at a rate lower than the Nyquist rate. The method is referred to as the correlogram for undersampled data. The algorithm partitions the spectrum into a number of segments and estimates the average power within each spectral segment. This method is able to estimate the power spectrum density of a signal from undersampled data without essentially requiring the signal to be sparse. We derive the bias and the variance of the spectrum estimator, and show that there is a tradeoff between the accuracy of the estimation, the frequency resolution, and the complexity of the estimator. A closed-form approximation of the estimation variance is also derived, which clearly shows how the variance is related to different parameters. The asymptotic behavior of the estimator is also investigated, and it is proved that this spectrum estimator is consistent. Moreover, the estimation made for different spectral segments becomes uncorrelated as the signal length tends to infinity. Finally, numerical examples and simulation results are provided, which approve the theoretical conclusions. ",Computer Science - Information Theory ; ,"Shaghaghi, Mahdi ; Vorobyov, Sergiy A. ; ","Finite-Length and Asymptotic Analysis of Correlogram for Undersampled   Data  This paper studies a spectrum estimation method for the case that the samples are obtained at a rate lower than the Nyquist rate. The method is referred to as the correlogram for undersampled data. The algorithm partitions the spectrum into a number of segments and estimates the average power within each spectral segment. This method is able to estimate the power spectrum density of a signal from undersampled data without essentially requiring the signal to be sparse. We derive the bias and the variance of the spectrum estimator, and show that there is a tradeoff between the accuracy of the estimation, the frequency resolution, and the complexity of the estimator. A closed-form approximation of the estimation variance is also derived, which clearly shows how the variance is related to different parameters. The asymptotic behavior of the estimator is also investigated, and it is proved that this spectrum estimator is consistent. Moreover, the estimation made for different spectral segments becomes uncorrelated as the signal length tends to infinity. Finally, numerical examples and simulation results are provided, which approve the theoretical conclusions. ",finite length asymptotic analysis correlogram undersampled data paper study spectrum estimation method case sample obtain rate lower nyquist rate method refer correlogram undersampled data algorithm partition spectrum number segment estimate average power within spectral segment method able estimate power spectrum density signal undersampled data without essentially require signal sparse derive bias variance spectrum estimator show tradeoff accuracy estimation frequency resolution complexity estimator close form approximation estimation variance also derive clearly show variance relate different parameters asymptotic behavior estimator also investigate prove spectrum estimator consistent moreover estimation make different spectral segment become uncorrelated signal length tend infinity finally numerical examples simulation result provide approve theoretical conclusions,105,9,1305.5592.txt
http://arxiv.org/abs/1305.5670,What is Visualization Really for?,"  Whenever a visualization researcher is asked about the purpose of visualization, the phrase ""gaining insight"" by and large pops out instinctively. However, it is not absolutely factual that all uses of visualization are for gaining a deep understanding, unless the term insight is broadened to encompass all types of thought. Even when insight is the focus of a visualization task, it is rather difficult to know what insight is gained, how much, or how accurate. In this paper, we propose that ""saving time"" in accomplishing a user's task is the most fundamental objective. By giving emphasis to saving time, we can establish a concrete metric, alleviate unnecessary contention caused by different interpretations of insight, and stimulate new research efforts in some aspects of visualization, such as empirical studies, design optimisation and theories of visualization. ",Computer Science - Human-Computer Interaction ; ,"Chen, Min ; Floridi, Luciano ; Borgo, Rita ; ","What is Visualization Really for?  Whenever a visualization researcher is asked about the purpose of visualization, the phrase ""gaining insight"" by and large pops out instinctively. However, it is not absolutely factual that all uses of visualization are for gaining a deep understanding, unless the term insight is broadened to encompass all types of thought. Even when insight is the focus of a visualization task, it is rather difficult to know what insight is gained, how much, or how accurate. In this paper, we propose that ""saving time"" in accomplishing a user's task is the most fundamental objective. By giving emphasis to saving time, we can establish a concrete metric, alleviate unnecessary contention caused by different interpretations of insight, and stimulate new research efforts in some aspects of visualization, such as empirical studies, design optimisation and theories of visualization. ",visualization really whenever visualization researcher ask purpose visualization phrase gain insight large pop instinctively however absolutely factual use visualization gain deep understand unless term insight broaden encompass type think even insight focus visualization task rather difficult know insight gain much accurate paper propose save time accomplish user task fundamental objective give emphasis save time establish concrete metric alleviate unnecessary contention cause different interpretations insight stimulate new research efforts aspects visualization empirical study design optimisation theories visualization,76,11,1305.5670.txt
http://arxiv.org/abs/1305.6431,Certifying Machine Code Safe from Hardware Aliasing: RISC is not   necessarily risky,"  Sometimes machine code turns out to be a better target for verification than source code. RISC machine code is especially advantaged with respect to source code in this regard because it has only two instructions that access memory. That architecture forms the basis here for an inference system that can prove machine code safe against `hardware aliasing', an effect that occurs in embedded systems. There are programming memes that ensure code is safe from hardware aliasing, but we want to certify that a given machine code is provably safe. ",Computer Science - Logic in Computer Science ; Computer Science - Software Engineering ; D.2.4 ; ,"Breuer, Peter T. ; Bowen, Jonathan P. ; ","Certifying Machine Code Safe from Hardware Aliasing: RISC is not   necessarily risky  Sometimes machine code turns out to be a better target for verification than source code. RISC machine code is especially advantaged with respect to source code in this regard because it has only two instructions that access memory. That architecture forms the basis here for an inference system that can prove machine code safe against `hardware aliasing', an effect that occurs in embedded systems. There are programming memes that ensure code is safe from hardware aliasing, but we want to certify that a given machine code is provably safe. ",certify machine code safe hardware aliasing risc necessarily risky sometimes machine code turn better target verification source code risc machine code especially advantage respect source code regard two instructions access memory architecture form basis inference system prove machine code safe hardware aliasing effect occur embed systems program memes ensure code safe hardware aliasing want certify give machine code provably safe,60,4,1305.6431.txt
http://arxiv.org/abs/1305.7514,Studying new classes of graph metrics,"  In data analysis, there is a strong demand for graph metrics that differ from the classical shortest path and resistance distances. Recently, several new classes of graph metrics have been proposed. This paper presents some of them featuring the cutpoint additive distances. These include the path distances, the reliability distance, the walk distances, and the logarithmic forest distances among others. We discuss a number of connections between these and other distances. ",Mathematics - Metric Geometry ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; 05C12 05C50 05C05 51K05 15A48 15A51 ; ,"Chebotarev, Pavel ; ","Studying new classes of graph metrics  In data analysis, there is a strong demand for graph metrics that differ from the classical shortest path and resistance distances. Recently, several new classes of graph metrics have been proposed. This paper presents some of them featuring the cutpoint additive distances. These include the path distances, the reliability distance, the walk distances, and the logarithmic forest distances among others. We discuss a number of connections between these and other distances. ",study new class graph metrics data analysis strong demand graph metrics differ classical shortest path resistance distance recently several new class graph metrics propose paper present feature cutpoint additive distance include path distance reliability distance walk distance logarithmic forest distance among others discuss number connections distance,46,3,1305.7514.txt
http://arxiv.org/abs/1306.0760,Mashup of Meta-Languages and its Implementation in the Kermeta Language   Workbench,"  With the growing use of domain-specific languages (DSL) in industry, DSL design and implementation goes far beyond an activity for a few experts only and becomes a challenging task for thousands of software engineers. DSL implementation indeed requires engineers to care for various concerns, from abstract syntax, static semantics, behavioral semantics, to extra-functional issues such as run-time performance. This paper presents an approach that uses one meta-language per language implementation concern. We show that the usage and combination of those meta-languages is simple and intuitive enough to deserve the term ""mashup"". We evaluate the approach by completely implementing the non trivial fUML modeling language, a semantically sound and executable subset of the Unified Modeling Language (UML). ",Computer Science - Software Engineering ; ,"Jézéquel, Jean-Marc ; Combemale, Benoit ; Barais, Olivier ; Monperrus, Martin ; Fouquet, François ; ","Mashup of Meta-Languages and its Implementation in the Kermeta Language   Workbench  With the growing use of domain-specific languages (DSL) in industry, DSL design and implementation goes far beyond an activity for a few experts only and becomes a challenging task for thousands of software engineers. DSL implementation indeed requires engineers to care for various concerns, from abstract syntax, static semantics, behavioral semantics, to extra-functional issues such as run-time performance. This paper presents an approach that uses one meta-language per language implementation concern. We show that the usage and combination of those meta-languages is simple and intuitive enough to deserve the term ""mashup"". We evaluate the approach by completely implementing the non trivial fUML modeling language, a semantically sound and executable subset of the Unified Modeling Language (UML). ",mashup meta languages implementation kermeta language workbench grow use domain specific languages dsl industry dsl design implementation go far beyond activity experts become challenge task thousands software engineer dsl implementation indeed require engineer care various concern abstract syntax static semantics behavioral semantics extra functional issue run time performance paper present approach use one meta language per language implementation concern show usage combination meta languages simple intuitive enough deserve term mashup evaluate approach completely implement non trivial fuml model language semantically sound executable subset unify model language uml,87,4,1306.0760.txt
http://arxiv.org/abs/1306.1167,A Graphical Transformation for Belief Propagation: Maximum Weight   Matchings and Odd-Sized Cycles,"  We study the Maximum Weight Matching (MWM) problem for general graphs through the max-product Belief Propagation (BP) and related Linear Programming (LP). The BP approach provides distributed heuristics for finding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM) and respective LPs can be considered as continuous relaxations of the discrete MAP problem. It was recently shown that a BP algorithm converges to the correct MWM assignment under a simple GM formulation of MAP/MWM as long as the corresponding LP relaxation is tight. First, under the motivation for forcing the tightness condition, we consider a new GM formulation of MWM, say C-GM, using non-intersecting odd-sized cycles in the graph: the new corresponding LP relaxation, say C-LP, becomes tight for more MWM instances. However, the tightness of C-LP now does not guarantee such convergence and correctness of the new BP on C-GM. To address the issue, we introduce a novel graph transformation applied to C-GM, which results in another GM formulation of MWM, and prove that the respective BP on it converges to the correct MAP/MWM assignment as long as C-LP is tight. Finally, we also show that C-LP always has half-integral solutions, which leads to an efficient BP-based MWM heuristic consisting of making sequential, `cutting plane', modifications to the underlying GM. Our experiments show that this BP-based cutting plane heuristic performs as well as that based on traditional LP solvers. ",Computer Science - Data Structures and Algorithms ; ,"Ahn, Sungsoo ; Chertkov, Michael ; Gelfand, Andrew E. ; Park, Sejun ; Shin, Jinwoo ; ","A Graphical Transformation for Belief Propagation: Maximum Weight   Matchings and Odd-Sized Cycles  We study the Maximum Weight Matching (MWM) problem for general graphs through the max-product Belief Propagation (BP) and related Linear Programming (LP). The BP approach provides distributed heuristics for finding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM) and respective LPs can be considered as continuous relaxations of the discrete MAP problem. It was recently shown that a BP algorithm converges to the correct MWM assignment under a simple GM formulation of MAP/MWM as long as the corresponding LP relaxation is tight. First, under the motivation for forcing the tightness condition, we consider a new GM formulation of MWM, say C-GM, using non-intersecting odd-sized cycles in the graph: the new corresponding LP relaxation, say C-LP, becomes tight for more MWM instances. However, the tightness of C-LP now does not guarantee such convergence and correctness of the new BP on C-GM. To address the issue, we introduce a novel graph transformation applied to C-GM, which results in another GM formulation of MWM, and prove that the respective BP on it converges to the correct MAP/MWM assignment as long as C-LP is tight. Finally, we also show that C-LP always has half-integral solutions, which leads to an efficient BP-based MWM heuristic consisting of making sequential, `cutting plane', modifications to the underlying GM. Our experiments show that this BP-based cutting plane heuristic performs as well as that based on traditional LP solvers. ",graphical transformation belief propagation maximum weight match odd size cycle study maximum weight match mwm problem general graph max product belief propagation bp relate linear program lp bp approach provide distribute heuristics find maximum posteriori map assignment joint probability distribution represent graphical model gm respective lps consider continuous relaxations discrete map problem recently show bp algorithm converge correct mwm assignment simple gm formulation map mwm long correspond lp relaxation tight first motivation force tightness condition consider new gm formulation mwm say gm use non intersect odd size cycle graph new correspond lp relaxation say lp become tight mwm instance however tightness lp guarantee convergence correctness new bp gm address issue introduce novel graph transformation apply gm result another gm formulation mwm prove respective bp converge correct map mwm assignment long lp tight finally also show lp always half integral solutions lead efficient bp base mwm heuristic consist make sequential cut plane modifications underlie gm experiment show bp base cut plane heuristic perform well base traditional lp solvers,167,8,1306.1167.txt
http://arxiv.org/abs/1306.1595,Layered Separators in Minor-Closed Graph Classes with Applications,"  Graph separators are a ubiquitous tool in graph theory and computer science. However, in some applications, their usefulness is limited by the fact that the separator can be as large as $\Omega(\sqrt{n})$ in graphs with $n$ vertices. This is the case for planar graphs, and more generally, for proper minor-closed classes. We study a special type of graph separator, called a ""layered separator"", which may have linear size in $n$, but has bounded size with respect to a different measure, called the ""width"". We prove, for example, that planar graphs and graphs of bounded Euler genus admit layered separators of bounded width. More generally, we characterise the minor-closed classes that admit layered separators of bounded width as those that exclude a fixed apex graph as a minor.   We use layered separators to prove $\mathcal{O}(\log n)$ bounds for a number of problems where $\mathcal{O}(\sqrt{n})$ was a long-standing previous best bound. This includes the nonrepetitive chromatic number and queue-number of graphs with bounded Euler genus. We extend these results with a $\mathcal{O}(\log n)$ bound on the nonrepetitive chromatic number of graphs excluding a fixed topological minor, and a $\log^{ \mathcal{O}(1)}n$ bound on the queue-number of graphs excluding a fixed minor. Only for planar graphs were $\log^{ \mathcal{O}(1)}n$ bounds previously known. Our results imply that every $n$-vertex graph excluding a fixed minor has a 3-dimensional grid drawing with $n\log^{ \mathcal{O}(1)}n$ volume, whereas the previous best bound was $\mathcal{O}(n^{3/2})$. ",Mathematics - Combinatorics ; Computer Science - Computational Geometry ; Computer Science - Discrete Mathematics ; ,"Dujmović, Vida ; Morin, Pat ; Wood, David R. ; ","Layered Separators in Minor-Closed Graph Classes with Applications  Graph separators are a ubiquitous tool in graph theory and computer science. However, in some applications, their usefulness is limited by the fact that the separator can be as large as $\Omega(\sqrt{n})$ in graphs with $n$ vertices. This is the case for planar graphs, and more generally, for proper minor-closed classes. We study a special type of graph separator, called a ""layered separator"", which may have linear size in $n$, but has bounded size with respect to a different measure, called the ""width"". We prove, for example, that planar graphs and graphs of bounded Euler genus admit layered separators of bounded width. More generally, we characterise the minor-closed classes that admit layered separators of bounded width as those that exclude a fixed apex graph as a minor.   We use layered separators to prove $\mathcal{O}(\log n)$ bounds for a number of problems where $\mathcal{O}(\sqrt{n})$ was a long-standing previous best bound. This includes the nonrepetitive chromatic number and queue-number of graphs with bounded Euler genus. We extend these results with a $\mathcal{O}(\log n)$ bound on the nonrepetitive chromatic number of graphs excluding a fixed topological minor, and a $\log^{ \mathcal{O}(1)}n$ bound on the queue-number of graphs excluding a fixed minor. Only for planar graphs were $\log^{ \mathcal{O}(1)}n$ bounds previously known. Our results imply that every $n$-vertex graph excluding a fixed minor has a 3-dimensional grid drawing with $n\log^{ \mathcal{O}(1)}n$ volume, whereas the previous best bound was $\mathcal{O}(n^{3/2})$. ",layer separators minor close graph class applications graph separators ubiquitous tool graph theory computer science however applications usefulness limit fact separator large omega sqrt graph vertices case planar graph generally proper minor close class study special type graph separator call layer separator may linear size bound size respect different measure call width prove example planar graph graph bound euler genus admit layer separators bound width generally characterise minor close class admit layer separators bound width exclude fix apex graph minor use layer separators prove mathcal log bound number problems mathcal sqrt long stand previous best bind include nonrepetitive chromatic number queue number graph bound euler genus extend result mathcal log bind nonrepetitive chromatic number graph exclude fix topological minor log mathcal bind queue number graph exclude fix minor planar graph log mathcal bound previously know result imply every vertex graph exclude fix minor dimensional grid draw log mathcal volume whereas previous best bind mathcal,154,3,1306.1595.txt
http://arxiv.org/abs/1306.2476,A Systematically Empirical Evaluation of Vulnerability Discovery Models:   a Study on Browsers' Vulnerabilities,"  A precise vulnerability discovery model (VDM) will provide a useful insight to assess software security, and could be a good prediction instrument for both software vendors and users to understand security trends and plan ahead patching schedule accordingly. Thus far, several models have been proposed and validated. Yet, no systematically independent validation by somebody other than the author exists. Furthermore, there are a number of issues that might bias previous studies in the field. In this work, we fill in the gap by introducing an empirical methodology that systematically evaluates the performance of a VDM in two aspects: quality and predictability. We further apply this methodology to assess existing VDMs. The results show that some models should be rejected outright, while some others might be adequate to capture the discovery process of vulnerabilities. We also consider different usage scenarios of VDMs and find that the simplest linear model is the most appropriate choice in terms of both quality and predictability when browsers are young. Otherwise, logistics-based models are better choices. ",Computer Science - Cryptography and Security ; ,"Nguyen, Viet Hung ; Massacci, Fabio ; ","A Systematically Empirical Evaluation of Vulnerability Discovery Models:   a Study on Browsers' Vulnerabilities  A precise vulnerability discovery model (VDM) will provide a useful insight to assess software security, and could be a good prediction instrument for both software vendors and users to understand security trends and plan ahead patching schedule accordingly. Thus far, several models have been proposed and validated. Yet, no systematically independent validation by somebody other than the author exists. Furthermore, there are a number of issues that might bias previous studies in the field. In this work, we fill in the gap by introducing an empirical methodology that systematically evaluates the performance of a VDM in two aspects: quality and predictability. We further apply this methodology to assess existing VDMs. The results show that some models should be rejected outright, while some others might be adequate to capture the discovery process of vulnerabilities. We also consider different usage scenarios of VDMs and find that the simplest linear model is the most appropriate choice in terms of both quality and predictability when browsers are young. Otherwise, logistics-based models are better choices. ",systematically empirical evaluation vulnerability discovery model study browsers vulnerabilities precise vulnerability discovery model vdm provide useful insight assess software security could good prediction instrument software vendors users understand security trend plan ahead patch schedule accordingly thus far several model propose validate yet systematically independent validation somebody author exist furthermore number issue might bias previous study field work fill gap introduce empirical methodology systematically evaluate performance vdm two aspects quality predictability apply methodology assess exist vdms result show model reject outright others might adequate capture discovery process vulnerabilities also consider different usage scenarios vdms find simplest linear model appropriate choice term quality predictability browsers young otherwise logistics base model better choices,110,10,1306.2476.txt
http://arxiv.org/abs/1306.2595,Capacity Scaling in MIMO Systems with General Unitarily Invariant Random   Matrices,"  We investigate the capacity scaling of MIMO systems with the system dimensions. To that end, we quantify how the mutual information varies when the number of antennas (at either the receiver or transmitter side) is altered. For a system comprising $R$ receive and $T$ transmit antennas with $R>T$, we find the following: By removing as many receive antennas as needed to obtain a square system (provided the channel matrices before and after the removal have full rank) the maximum resulting loss of mutual information over all signal-to-noise ratios (SNRs) depends only on $R$, $T$ and the matrix of left-singular vectors of the initial channel matrix, but not on its singular values. In particular, if the latter matrix is Haar distributed the ergodic rate loss is given by $\sum_{t=1}^{T}\sum_{r=T+1}^{R}\frac{1}{r-t}$ nats. Under the same assumption, if $T,R\to \infty$ with the ratio $\phi\triangleq T/R$ fixed, the rate loss normalized by $R$ converges almost surely to $H(\phi)$ bits with $H(\cdot)$ denoting the binary entropy function. We also quantify and study how the mutual information as a function of the system dimensions deviates from the traditionally assumed linear growth in the minimum of the system dimensions at high SNR. ",Computer Science - Information Theory ; ,"Çakmak, Burak ; Müller, Ralf R. ; Fleury, Bernard H. ; ","Capacity Scaling in MIMO Systems with General Unitarily Invariant Random   Matrices  We investigate the capacity scaling of MIMO systems with the system dimensions. To that end, we quantify how the mutual information varies when the number of antennas (at either the receiver or transmitter side) is altered. For a system comprising $R$ receive and $T$ transmit antennas with $R>T$, we find the following: By removing as many receive antennas as needed to obtain a square system (provided the channel matrices before and after the removal have full rank) the maximum resulting loss of mutual information over all signal-to-noise ratios (SNRs) depends only on $R$, $T$ and the matrix of left-singular vectors of the initial channel matrix, but not on its singular values. In particular, if the latter matrix is Haar distributed the ergodic rate loss is given by $\sum_{t=1}^{T}\sum_{r=T+1}^{R}\frac{1}{r-t}$ nats. Under the same assumption, if $T,R\to \infty$ with the ratio $\phi\triangleq T/R$ fixed, the rate loss normalized by $R$ converges almost surely to $H(\phi)$ bits with $H(\cdot)$ denoting the binary entropy function. We also quantify and study how the mutual information as a function of the system dimensions deviates from the traditionally assumed linear growth in the minimum of the system dimensions at high SNR. ",capacity scale mimo systems general unitarily invariant random matrices investigate capacity scale mimo systems system dimension end quantify mutual information vary number antennas either receiver transmitter side alter system comprise receive transmit antennas find follow remove many receive antennas need obtain square system provide channel matrices removal full rank maximum result loss mutual information signal noise ratios snrs depend matrix leave singular vectors initial channel matrix singular value particular latter matrix haar distribute ergodic rate loss give sum sum frac nats assumption infty ratio phi triangleq fix rate loss normalize converge almost surely phi bits cdot denote binary entropy function also quantify study mutual information function system dimension deviate traditionally assume linear growth minimum system dimension high snr,118,9,1306.2595.txt
http://arxiv.org/abs/1306.3261,arXiv e-prints and the journal of record: An analysis of roles and   relationships,"  Since its creation in 1991, arXiv has become central to the diffusion of research in a number of fields. Combining data from the entirety of arXiv and the Web of Science (WoS), this paper investigates (a) the proportion of papers across all disciplines that are on arXiv and the proportion of arXiv papers that are in the WoS, (b) elapsed time between arXiv submission and journal publication, and (c) the aging characteristics and scientific impact of arXiv e-prints and their published version. It shows that the proportion of WoS papers found on arXiv varies across the specialties of physics and mathematics, and that only a few specialties make extensive use of the repository. Elapsed time between arXiv submission and journal publication has shortened but remains longer in mathematics than in physics. In physics, mathematics, as well as in astronomy and astrophysics, arXiv versions are cited more promptly and decay faster than WoS papers. The arXiv versions of papers - both published and unpublished - have lower citation rates than published papers, although there is almost no difference in the impact of the arXiv versions of both published and unpublished papers. ",Computer Science - Digital Libraries ; ,"Lariviere, Vincent ; Sugimoto, Cassidy R. ; Macaluso, Benoit ; Milojevic, Stasa ; Cronin, Blaise ; Thelwall, Mike ; ","arXiv e-prints and the journal of record: An analysis of roles and   relationships  Since its creation in 1991, arXiv has become central to the diffusion of research in a number of fields. Combining data from the entirety of arXiv and the Web of Science (WoS), this paper investigates (a) the proportion of papers across all disciplines that are on arXiv and the proportion of arXiv papers that are in the WoS, (b) elapsed time between arXiv submission and journal publication, and (c) the aging characteristics and scientific impact of arXiv e-prints and their published version. It shows that the proportion of WoS papers found on arXiv varies across the specialties of physics and mathematics, and that only a few specialties make extensive use of the repository. Elapsed time between arXiv submission and journal publication has shortened but remains longer in mathematics than in physics. In physics, mathematics, as well as in astronomy and astrophysics, arXiv versions are cited more promptly and decay faster than WoS papers. The arXiv versions of papers - both published and unpublished - have lower citation rates than published papers, although there is almost no difference in the impact of the arXiv versions of both published and unpublished papers. ",arxiv print journal record analysis roles relationships since creation arxiv become central diffusion research number field combine data entirety arxiv web science wos paper investigate proportion paper across discipline arxiv proportion arxiv paper wos elapse time arxiv submission journal publication age characteristics scientific impact arxiv print publish version show proportion wos paper find arxiv vary across specialties physics mathematics specialties make extensive use repository elapse time arxiv submission journal publication shorten remain longer mathematics physics physics mathematics well astronomy astrophysics arxiv versions cite promptly decay faster wos paper arxiv versions paper publish unpublished lower citation rat publish paper although almost difference impact arxiv versions publish unpublished paper,107,10,1306.3261.txt
http://arxiv.org/abs/1306.3726,"Automatic functions, linear time and learning","  The present work determines the exact nature of {\em linear time computable} notions which characterise automatic functions (those whose graphs are recognised by a finite automaton). The paper also determines which type of linear time notions permit full learnability for learning in the limit of automatic classes (families of languages which are uniformly recognised by a finite automaton). In particular it is shown that a function is automatic iff there is a one-tape Turing machine with a left end which computes the function in linear time where the input before the computation and the output after the computation both start at the left end. It is known that learners realised as automatic update functions are restrictive for learning. In the present work it is shown that one can overcome the problem by providing work tapes additional to a resource-bounded base tape while keeping the update-time to be linear in the length of the largest datum seen so far. In this model, one additional such work tape provides additional learning power over the automatic learner model and two additional work tapes give full learning power. Furthermore, one can also consider additional queues or additional stacks in place of additional work tapes and for these devices, one queue or two stacks are sufficient for full learning power while one stack is insufficient. ",Computer Science - Formal Languages and Automata Theory ; ,"Case, John ; Jain, Sanjay ; Seah, Samuel ; Stephan, Frank ; ","Automatic functions, linear time and learning  The present work determines the exact nature of {\em linear time computable} notions which characterise automatic functions (those whose graphs are recognised by a finite automaton). The paper also determines which type of linear time notions permit full learnability for learning in the limit of automatic classes (families of languages which are uniformly recognised by a finite automaton). In particular it is shown that a function is automatic iff there is a one-tape Turing machine with a left end which computes the function in linear time where the input before the computation and the output after the computation both start at the left end. It is known that learners realised as automatic update functions are restrictive for learning. In the present work it is shown that one can overcome the problem by providing work tapes additional to a resource-bounded base tape while keeping the update-time to be linear in the length of the largest datum seen so far. In this model, one additional such work tape provides additional learning power over the automatic learner model and two additional work tapes give full learning power. Furthermore, one can also consider additional queues or additional stacks in place of additional work tapes and for these devices, one queue or two stacks are sufficient for full learning power while one stack is insufficient. ",automatic function linear time learn present work determine exact nature em linear time computable notions characterise automatic function whose graph recognise finite automaton paper also determine type linear time notions permit full learnability learn limit automatic class families languages uniformly recognise finite automaton particular show function automatic iff one tape turing machine leave end compute function linear time input computation output computation start leave end know learners realise automatic update function restrictive learn present work show one overcome problem provide work tap additional resource bound base tape keep update time linear length largest datum see far model one additional work tape provide additional learn power automatic learner model two additional work tap give full learn power furthermore one also consider additional queue additional stack place additional work tap devices one queue two stack sufficient full learn power one stack insufficient,140,11,1306.3726.txt
http://arxiv.org/abs/1306.3875,Roughening Methods to Prevent Sample Impoverishment in the Particle PHD   Filter,"  Mahler's PHD (Probability Hypothesis Density) filter and its particle implementation (as called the particle PHD filter) have gained popularity to solve general MTT (Multi-target Tracking) problems. However, the resampling procedure used in the particle PHD filter can cause sample impoverishment. To rejuvenate the diversity of particles, two easy-to-implement roughening approaches are presented to enhance the particle PHD filter. One termed as ""separate-roughening"" is inspired by Gordon's roughening procedure that is applied on the resampled particles. Another termed as ""direct-roughening"" is implemented by increasing the simulation noise of the state propagation of particles. Four proposals are presented to customize the roughening approach. Simulations are presented showing that the roughening approach can benefit the particle PHD filter, especially when the sample size is small. ",Computer Science - Other Computer Science ; ,"Li, Tiancheng ; Sattar, Tariq P. ; Han, Qing ; Sun, Shudong ; ","Roughening Methods to Prevent Sample Impoverishment in the Particle PHD   Filter  Mahler's PHD (Probability Hypothesis Density) filter and its particle implementation (as called the particle PHD filter) have gained popularity to solve general MTT (Multi-target Tracking) problems. However, the resampling procedure used in the particle PHD filter can cause sample impoverishment. To rejuvenate the diversity of particles, two easy-to-implement roughening approaches are presented to enhance the particle PHD filter. One termed as ""separate-roughening"" is inspired by Gordon's roughening procedure that is applied on the resampled particles. Another termed as ""direct-roughening"" is implemented by increasing the simulation noise of the state propagation of particles. Four proposals are presented to customize the roughening approach. Simulations are presented showing that the roughening approach can benefit the particle PHD filter, especially when the sample size is small. ",roughen methods prevent sample impoverishment particle phd filter mahler phd probability hypothesis density filter particle implementation call particle phd filter gain popularity solve general mtt multi target track problems however resampling procedure use particle phd filter cause sample impoverishment rejuvenate diversity particles two easy implement roughen approach present enhance particle phd filter one term separate roughen inspire gordon roughen procedure apply resampled particles another term direct roughen implement increase simulation noise state propagation particles four proposals present customize roughen approach simulations present show roughen approach benefit particle phd filter especially sample size small,93,2,1306.3875.txt
http://arxiv.org/abs/1306.4664,Efficient Two-Stage Group Testing Algorithms for Genetic Screening,"  Efficient two-stage group testing algorithms that are particularly suited for rapid and less-expensive DNA library screening and other large scale biological group testing efforts are investigated in this paper. The main focus is on novel combinatorial constructions in order to minimize the number of individual tests at the second stage of a two-stage disjunctive testing procedure. Building on recent work by Levenshtein (2003) and Tonchev (2008), several new infinite classes of such combinatorial designs are presented. ",Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; Quantitative Biology - Quantitative Methods ; ,"Huber, Michael ; ","Efficient Two-Stage Group Testing Algorithms for Genetic Screening  Efficient two-stage group testing algorithms that are particularly suited for rapid and less-expensive DNA library screening and other large scale biological group testing efforts are investigated in this paper. The main focus is on novel combinatorial constructions in order to minimize the number of individual tests at the second stage of a two-stage disjunctive testing procedure. Building on recent work by Levenshtein (2003) and Tonchev (2008), several new infinite classes of such combinatorial designs are presented. ",efficient two stage group test algorithms genetic screen efficient two stage group test algorithms particularly suit rapid less expensive dna library screen large scale biological group test efforts investigate paper main focus novel combinatorial constructions order minimize number individual test second stage two stage disjunctive test procedure build recent work levenshtein tonchev several new infinite class combinatorial design present,59,12,1306.4664.txt
http://arxiv.org/abs/1306.5111,Low-Density Parity-Check Codes From Transversal Designs With Improved   Stopping Set Distributions,"  This paper examines the construction of low-density parity-check (LDPC) codes from transversal designs based on sets of mutually orthogonal Latin squares (MOLS). By transferring the concept of configurations in combinatorial designs to the level of Latin squares, we thoroughly investigate the occurrence and avoidance of stopping sets for the arising codes. Stopping sets are known to determine the decoding performance over the binary erasure channel and should be avoided for small sizes. Based on large sets of simple-structured MOLS, we derive powerful constraints for the choice of suitable subsets, leading to improved stopping set distributions for the corresponding codes. We focus on LDPC codes with column weight 4, but the results are also applicable for the construction of codes with higher column weights. Finally, we show that a subclass of the presented codes has quasi-cyclic structure which allows low-complexity encoding. ",Computer Science - Information Theory ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Gruner, Alexander ; Huber, Michael ; ","Low-Density Parity-Check Codes From Transversal Designs With Improved   Stopping Set Distributions  This paper examines the construction of low-density parity-check (LDPC) codes from transversal designs based on sets of mutually orthogonal Latin squares (MOLS). By transferring the concept of configurations in combinatorial designs to the level of Latin squares, we thoroughly investigate the occurrence and avoidance of stopping sets for the arising codes. Stopping sets are known to determine the decoding performance over the binary erasure channel and should be avoided for small sizes. Based on large sets of simple-structured MOLS, we derive powerful constraints for the choice of suitable subsets, leading to improved stopping set distributions for the corresponding codes. We focus on LDPC codes with column weight 4, but the results are also applicable for the construction of codes with higher column weights. Finally, we show that a subclass of the presented codes has quasi-cyclic structure which allows low-complexity encoding. ",low density parity check cod transversal design improve stop set distributions paper examine construction low density parity check ldpc cod transversal design base set mutually orthogonal latin square mols transfer concept configurations combinatorial design level latin square thoroughly investigate occurrence avoidance stop set arise cod stop set know determine decode performance binary erasure channel avoid small size base large set simple structure mols derive powerful constraints choice suitable subsets lead improve stop set distributions correspond cod focus ldpc cod column weight result also applicable construction cod higher column weight finally show subclass present cod quasi cyclic structure allow low complexity encode,101,5,1306.5111.txt
http://arxiv.org/abs/1306.5585,Soundness and Completeness of the NRB Verification Logic,"  This short paper gives a model for and a proof of completeness of the NRB verification logic for deterministic imperative programs, the logic having been used in the past as the basis for automated semantic checks of large, fast-changing, open source C code archives, such as that of the Linux kernel source. The model is a colored state transitions model that approximates from above the set of transitions possible for a program. Correspondingly, the logic catches all traces that may trigger a particular defect at a given point in the program, but may also flag false positives. ",Computer Science - Logic in Computer Science ; B.1.2 ; D.2.4 ; ,"Breuer, Peter T. ; Pickin, Simon J. ; ","Soundness and Completeness of the NRB Verification Logic  This short paper gives a model for and a proof of completeness of the NRB verification logic for deterministic imperative programs, the logic having been used in the past as the basis for automated semantic checks of large, fast-changing, open source C code archives, such as that of the Linux kernel source. The model is a colored state transitions model that approximates from above the set of transitions possible for a program. Correspondingly, the logic catches all traces that may trigger a particular defect at a given point in the program, but may also flag false positives. ",soundness completeness nrb verification logic short paper give model proof completeness nrb verification logic deterministic imperative program logic use past basis automate semantic check large fast change open source code archive linux kernel source model color state transition model approximate set transition possible program correspondingly logic catch trace may trigger particular defect give point program may also flag false positives,60,8,1306.5585.txt
http://arxiv.org/abs/1306.5720,On the Resilience of Bipartite Networks,"  Motivated by problems modeling the spread of infections in networks, in this paper we explore which bipartite graphs are most resilient to widespread infections under various parameter settings. Namely, we study bipartite networks with a requirement of a minimum degree $d$ on one side under an independent infection, independent transmission model. We completely characterize the optimal graphs in the case $d=1$, which already produces non-trivial behavior, and we give extremal results for the more general cases. We show that in the case $d=2$, surprisingly, the optimally resilient set of graphs includes a graph that is not one of the two ""extremes"" found in the case $d=1$.   Then, we briefly examine the case where we force a connectivity requirement instead of a one-sided degree requirement and again, we find that the set of the most resilient graphs contains more than the two ""extremes."" We also show that determining the subgraph of an arbitrary bipartite graph most resilient to infection is NP-hard for any one-sided minimal degree $d \ge 1$. ",Computer Science - Data Structures and Algorithms ; Computer Science - Social and Information Networks ; ,"Heinecke, Shelby ; Perkins, Will ; Reyzin, Lev ; ","On the Resilience of Bipartite Networks  Motivated by problems modeling the spread of infections in networks, in this paper we explore which bipartite graphs are most resilient to widespread infections under various parameter settings. Namely, we study bipartite networks with a requirement of a minimum degree $d$ on one side under an independent infection, independent transmission model. We completely characterize the optimal graphs in the case $d=1$, which already produces non-trivial behavior, and we give extremal results for the more general cases. We show that in the case $d=2$, surprisingly, the optimally resilient set of graphs includes a graph that is not one of the two ""extremes"" found in the case $d=1$.   Then, we briefly examine the case where we force a connectivity requirement instead of a one-sided degree requirement and again, we find that the set of the most resilient graphs contains more than the two ""extremes."" We also show that determining the subgraph of an arbitrary bipartite graph most resilient to infection is NP-hard for any one-sided minimal degree $d \ge 1$. ",resilience bipartite network motivate problems model spread infections network paper explore bipartite graph resilient widespread infections various parameter settings namely study bipartite network requirement minimum degree one side independent infection independent transmission model completely characterize optimal graph case already produce non trivial behavior give extremal result general case show case surprisingly optimally resilient set graph include graph one two extremes find case briefly examine case force connectivity requirement instead one side degree requirement find set resilient graph contain two extremes also show determine subgraph arbitrary bipartite graph resilient infection np hard one side minimal degree ge,96,3,1306.5720.txt
http://arxiv.org/abs/1306.6109,Broadcasting in Ad Hoc Multiple Access Channels,"  We study broadcast in multiple access channels in dynamic adversarial settings. There is an unbounded supply of anonymous stations attached to a synchronous channel. There is an adversary who injects packets into stations to be broadcast on the channel. The adversary is restricted by injection rate, burstiness, and by how many passive stations can be simultaneously activated by providing them with packets. We consider deterministic distributed broadcast algorithms, which are further categorized by their properties. We investigate for which injection rates can algorithms attain bounded packet latency, when adversaries are restricted to be able to activate at most one station per round. The rates of algorithms we present make the increasing sequence consisting of $\frac{1}{3}$, $\frac{3}{8}$ and $\frac{1}{2}$, reflecting the additional features of algorithms. We show that injection rate $\frac{3}{4}$ cannot be handled with bounded packet latency. ",Computer Science - Networking and Internet Architecture ; ,"Anantharamu, Lakshmi ; Chlebus, Bogdan S. ; ","Broadcasting in Ad Hoc Multiple Access Channels  We study broadcast in multiple access channels in dynamic adversarial settings. There is an unbounded supply of anonymous stations attached to a synchronous channel. There is an adversary who injects packets into stations to be broadcast on the channel. The adversary is restricted by injection rate, burstiness, and by how many passive stations can be simultaneously activated by providing them with packets. We consider deterministic distributed broadcast algorithms, which are further categorized by their properties. We investigate for which injection rates can algorithms attain bounded packet latency, when adversaries are restricted to be able to activate at most one station per round. The rates of algorithms we present make the increasing sequence consisting of $\frac{1}{3}$, $\frac{3}{8}$ and $\frac{1}{2}$, reflecting the additional features of algorithms. We show that injection rate $\frac{3}{4}$ cannot be handled with bounded packet latency. ",broadcast ad hoc multiple access channel study broadcast multiple access channel dynamic adversarial settings unbounded supply anonymous station attach synchronous channel adversary inject packets station broadcast channel adversary restrict injection rate burstiness many passive station simultaneously activate provide packets consider deterministic distribute broadcast algorithms categorize properties investigate injection rat algorithms attain bound packet latency adversaries restrict able activate one station per round rat algorithms present make increase sequence consist frac frac frac reflect additional feature algorithms show injection rate frac cannot handle bound packet latency,85,12,1306.6109.txt
http://arxiv.org/abs/1306.6458,Harmony Perception by Periodicity Detection,"  The perception of consonance/dissonance of musical harmonies is strongly correlated with their periodicity. This is shown in this article by consistently applying recent results from psychophysics and neuroacoustics, namely that the just noticeable difference between pitches for humans is about 1% for the musically important low frequency range and that periodicities of complex chords can be detected in the human brain. Based thereon, the concepts of relative and logarithmic periodicity with smoothing are introduced as powerful measures of harmoniousness. The presented results correlate significantly with empirical investigations on the perception of chords. Even for scales, plausible results are obtained. For example, all classical church modes appear in the front ranks of all theoretically possible seven-tone scales. ",Computer Science - Sound ; ,"Stolzenburg, Frieder ; ","Harmony Perception by Periodicity Detection  The perception of consonance/dissonance of musical harmonies is strongly correlated with their periodicity. This is shown in this article by consistently applying recent results from psychophysics and neuroacoustics, namely that the just noticeable difference between pitches for humans is about 1% for the musically important low frequency range and that periodicities of complex chords can be detected in the human brain. Based thereon, the concepts of relative and logarithmic periodicity with smoothing are introduced as powerful measures of harmoniousness. The presented results correlate significantly with empirical investigations on the perception of chords. Even for scales, plausible results are obtained. For example, all classical church modes appear in the front ranks of all theoretically possible seven-tone scales. ",harmony perception periodicity detection perception consonance dissonance musical harmonies strongly correlate periodicity show article consistently apply recent result psychophysics neuroacoustics namely noticeable difference pitch humans musically important low frequency range periodicities complex chord detect human brain base thereon concepts relative logarithmic periodicity smooth introduce powerful measure harmoniousness present result correlate significantly empirical investigations perception chord even scale plausible result obtain example classical church modes appear front rank theoretically possible seven tone scale,72,4,1306.6458.txt
http://arxiv.org/abs/1307.0426,"An Empirical Study into Annotator Agreement, Ground Truth Estimation,   and Algorithm Evaluation","  Although agreement between annotators has been studied in the past from a statistical viewpoint, little work has attempted to quantify the extent to which this phenomenon affects the evaluation of computer vision (CV) object detection algorithms. Many researchers utilise ground truth (GT) in experiments and more often than not this GT is derived from one annotator's opinion. How does the difference in opinion affect an algorithm's evaluation? Four examples of typical CV problems are chosen, and a methodology is applied to each to quantify the inter-annotator variance and to offer insight into the mechanisms behind agreement and the use of GT. It is found that when detecting linear objects annotator agreement is very low. The agreement in object position, linear or otherwise, can be partially explained through basic image properties. Automatic object detectors are compared to annotator agreement and it is found that a clear relationship exists. Several methods for calculating GTs from a number of annotations are applied and the resulting differences in the performance of the object detectors are quantified. It is found that the rank of a detector is highly dependent upon the method used to form the GT. It is also found that although the STAPLE and LSML GT estimation methods appear to represent the mean of the performance measured using the individual annotations, when there are few annotations, or there is a large variance in them, these estimates tend to degrade. Furthermore, one of the most commonly adopted annotation combination methods--consensus voting--accentuates more obvious features, which results in an overestimation of the algorithm's performance. Finally, it is concluded that in some datasets it may not be possible to state with any confidence that one algorithm outperforms another when evaluating upon one GT and a method for calculating confidence bounds is discussed. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; I.4.6 ; I.5.4 ; ,"Lampert, Thomas A. ; Stumpf, André ; Gançarski, Pierre ; ","An Empirical Study into Annotator Agreement, Ground Truth Estimation,   and Algorithm Evaluation  Although agreement between annotators has been studied in the past from a statistical viewpoint, little work has attempted to quantify the extent to which this phenomenon affects the evaluation of computer vision (CV) object detection algorithms. Many researchers utilise ground truth (GT) in experiments and more often than not this GT is derived from one annotator's opinion. How does the difference in opinion affect an algorithm's evaluation? Four examples of typical CV problems are chosen, and a methodology is applied to each to quantify the inter-annotator variance and to offer insight into the mechanisms behind agreement and the use of GT. It is found that when detecting linear objects annotator agreement is very low. The agreement in object position, linear or otherwise, can be partially explained through basic image properties. Automatic object detectors are compared to annotator agreement and it is found that a clear relationship exists. Several methods for calculating GTs from a number of annotations are applied and the resulting differences in the performance of the object detectors are quantified. It is found that the rank of a detector is highly dependent upon the method used to form the GT. It is also found that although the STAPLE and LSML GT estimation methods appear to represent the mean of the performance measured using the individual annotations, when there are few annotations, or there is a large variance in them, these estimates tend to degrade. Furthermore, one of the most commonly adopted annotation combination methods--consensus voting--accentuates more obvious features, which results in an overestimation of the algorithm's performance. Finally, it is concluded that in some datasets it may not be possible to state with any confidence that one algorithm outperforms another when evaluating upon one GT and a method for calculating confidence bounds is discussed. ",empirical study annotator agreement grind truth estimation algorithm evaluation although agreement annotators study past statistical viewpoint little work attempt quantify extent phenomenon affect evaluation computer vision cv object detection algorithms many researchers utilise grind truth gt experiment often gt derive one annotator opinion difference opinion affect algorithm evaluation four examples typical cv problems choose methodology apply quantify inter annotator variance offer insight mechanisms behind agreement use gt find detect linear object annotator agreement low agreement object position linear otherwise partially explain basic image properties automatic object detectors compare annotator agreement find clear relationship exist several methods calculate gts number annotations apply result differences performance object detectors quantify find rank detector highly dependent upon method use form gt also find although staple lsml gt estimation methods appear represent mean performance measure use individual annotations annotations large variance estimate tend degrade furthermore one commonly adopt annotation combination methods consensus vote accentuate obvious feature result overestimation algorithm performance finally conclude datasets may possible state confidence one algorithm outperform another evaluate upon one gt method calculate confidence bound discuss,175,4,1307.0426.txt
http://arxiv.org/abs/1307.0449,Arising information regularities in an observer,"  The approach defines information process from probabilistic observation, emerging microprocess,qubit, encoding bits, evolving macroprocess, and extends to Observer information self-organization, cognition, intelligence and understanding communicating information. Studying information originating in quantum process focuses not on particle physics but on natural interactive impulse modeling Bit composing information observer. Information emerges from Kolmogorov probabilities field when sequences of 1-0 probabilities link Markov probabilities modeling arising observer. These objective yes-no probabilities virtually cuts observing entropy hidden in cutting correlation decreasing Markov process entropy and increasing entropy of cutting impulse running minimax principle. Merging impulse curves and rotates yes-no conjugated entropies in microprocess. The entropies entangle within impulse time interval ending with beginning space. The opposite curvature lowers potential energy converting entropy to memorized bit. The memorized information binds reversible microprocess with irreversible information macroprocess. Multiple interacting Bits self-organize information process encoding causality, logic and complexity. Trajectory of observation process carries probabilistic and certain wave function self-building structural macrounits. Macrounits logically self-organize information networks encoding in triplet code. Multiple IN enclose observer information cognition and intelligence. Observer cognition assembles attracting common units in resonances forming IN hierarchy accepting only units recognizing IN node. Maximal number of accepted triplets measures the observer information intelligence. Intelligent observer recognizes and encodes digital images in message transmission enables understanding the message meaning. Cognitive logic self-controls encoding the intelligence in double helix code. ","Nonlinear Sciences - Adaptation and Self-Organizing Systems ; Computer Science - Information Theory ; 58J65, 60J65, 93B52, 93E02, 93E15, 93E30 ; H.1.1 ; ","Lerner, Vladimir S. ; ","Arising information regularities in an observer  The approach defines information process from probabilistic observation, emerging microprocess,qubit, encoding bits, evolving macroprocess, and extends to Observer information self-organization, cognition, intelligence and understanding communicating information. Studying information originating in quantum process focuses not on particle physics but on natural interactive impulse modeling Bit composing information observer. Information emerges from Kolmogorov probabilities field when sequences of 1-0 probabilities link Markov probabilities modeling arising observer. These objective yes-no probabilities virtually cuts observing entropy hidden in cutting correlation decreasing Markov process entropy and increasing entropy of cutting impulse running minimax principle. Merging impulse curves and rotates yes-no conjugated entropies in microprocess. The entropies entangle within impulse time interval ending with beginning space. The opposite curvature lowers potential energy converting entropy to memorized bit. The memorized information binds reversible microprocess with irreversible information macroprocess. Multiple interacting Bits self-organize information process encoding causality, logic and complexity. Trajectory of observation process carries probabilistic and certain wave function self-building structural macrounits. Macrounits logically self-organize information networks encoding in triplet code. Multiple IN enclose observer information cognition and intelligence. Observer cognition assembles attracting common units in resonances forming IN hierarchy accepting only units recognizing IN node. Maximal number of accepted triplets measures the observer information intelligence. Intelligent observer recognizes and encodes digital images in message transmission enables understanding the message meaning. Cognitive logic self-controls encoding the intelligence in double helix code. ",arise information regularities observer approach define information process probabilistic observation emerge microprocess qubit encode bits evolve macroprocess extend observer information self organization cognition intelligence understand communicate information study information originate quantum process focus particle physics natural interactive impulse model bite compose information observer information emerge kolmogorov probabilities field sequence probabilities link markov probabilities model arise observer objective yes probabilities virtually cut observe entropy hide cut correlation decrease markov process entropy increase entropy cut impulse run minimax principle merge impulse curve rotate yes conjugate entropies microprocess entropies entangle within impulse time interval end begin space opposite curvature lower potential energy convert entropy memorize bite memorize information bind reversible microprocess irreversible information macroprocess multiple interact bits self organize information process encode causality logic complexity trajectory observation process carry probabilistic certain wave function self build structural macrounits macrounits logically self organize information network encode triplet code multiple enclose observer information cognition intelligence observer cognition assemble attract common units resonances form hierarchy accept units recognize node maximal number accept triplets measure observer information intelligence intelligent observer recognize encode digital image message transmission enable understand message mean cognitive logic self control encode intelligence double helix code,191,11,1307.0449.txt
http://arxiv.org/abs/1307.2035,Periodic Strategies: A New Solution Concept and an Algorithm for   NonTrivial Strategic Form Games,"  We introduce a new solution concept, called periodicity, for selecting optimal strategies in strategic form games. This periodicity solution concept yields new insight into non-trivial games. In mixed strategy strategic form games, periodic solutions yield values for the utility function of each player that are equal to the Nash equilibrium ones. In contrast to the Nash strategies, here the payoffs of each player are robust against what the opponent plays. Sometimes, periodicity strategies yield higher utilities, and sometimes the Nash strategies do, but often the utilities of these two strategies coincide. We formally define and study periodic strategies in two player perfect information strategic form games with pure strategies and we prove that every non-trivial finite game has at least one periodic strategy, with non-trivial meaning non-degenerate payoffs. In some classes of games where mixed strategies are used, we identify quantitative features. Particularly interesting are the implications for collective action games, since there the collective action strategy can be incorporated in a purely non-cooperative context. Moreover, we address the periodicity issue when the players have a continuum set of strategies available. ",Computer Science - Computer Science and Game Theory ; ,"Oikonomou, V. K. ; Jost, J. ; ","Periodic Strategies: A New Solution Concept and an Algorithm for   NonTrivial Strategic Form Games  We introduce a new solution concept, called periodicity, for selecting optimal strategies in strategic form games. This periodicity solution concept yields new insight into non-trivial games. In mixed strategy strategic form games, periodic solutions yield values for the utility function of each player that are equal to the Nash equilibrium ones. In contrast to the Nash strategies, here the payoffs of each player are robust against what the opponent plays. Sometimes, periodicity strategies yield higher utilities, and sometimes the Nash strategies do, but often the utilities of these two strategies coincide. We formally define and study periodic strategies in two player perfect information strategic form games with pure strategies and we prove that every non-trivial finite game has at least one periodic strategy, with non-trivial meaning non-degenerate payoffs. In some classes of games where mixed strategies are used, we identify quantitative features. Particularly interesting are the implications for collective action games, since there the collective action strategy can be incorporated in a purely non-cooperative context. Moreover, we address the periodicity issue when the players have a continuum set of strategies available. ",periodic strategies new solution concept algorithm nontrivial strategic form game introduce new solution concept call periodicity select optimal strategies strategic form game periodicity solution concept yield new insight non trivial game mix strategy strategic form game periodic solutions yield value utility function player equal nash equilibrium ones contrast nash strategies payoffs player robust opponent play sometimes periodicity strategies yield higher utilities sometimes nash strategies often utilities two strategies coincide formally define study periodic strategies two player perfect information strategic form game pure strategies prove every non trivial finite game least one periodic strategy non trivial mean non degenerate payoffs class game mix strategies use identify quantitative feature particularly interest implications collective action game since collective action strategy incorporate purely non cooperative context moreover address periodicity issue players continuum set strategies available,131,8,1307.2035.txt
http://arxiv.org/abs/1307.2559,General Drift Analysis with Tail Bounds,"  Drift analysis is one of the state-of-the-art techniques for the runtime analysis of randomized search heuristics (RSHs) such as evolutionary algorithms (EAs), simulated annealing etc. The vast majority of existing drift theorems yield bounds on the expected value of the hitting time for a target state, e.g., the set of optimal solutions, without making additional statements on the distribution of this time. We address this lack by providing a general drift theorem that includes bounds on the upper and lower tail of the hitting time distribution. The new tail bounds are applied to prove very precise sharp-concentration results on the running time of a simple EA on standard benchmark problems, including the class of general linear functions. Surprisingly, the probability of deviating by an $r$-factor in lower order terms of the expected time decreases exponentially with $r$ on all these problems. The usefulness of the theorem outside the theory of RSHs is demonstrated by deriving tail bounds on the number of cycles in random permutations. All these results handle a position-dependent (variable) drift that was not covered by previous drift theorems with tail bounds. Moreover, our theorem can be specialized into virtually all existing drift theorems with drift towards the target from the literature. Finally, user-friendly specializations of the general drift theorem are given. ",Computer Science - Neural and Evolutionary Computing ; 68W20 ; ,"Lehre, Per Kristian ; Witt, Carsten ; ","General Drift Analysis with Tail Bounds  Drift analysis is one of the state-of-the-art techniques for the runtime analysis of randomized search heuristics (RSHs) such as evolutionary algorithms (EAs), simulated annealing etc. The vast majority of existing drift theorems yield bounds on the expected value of the hitting time for a target state, e.g., the set of optimal solutions, without making additional statements on the distribution of this time. We address this lack by providing a general drift theorem that includes bounds on the upper and lower tail of the hitting time distribution. The new tail bounds are applied to prove very precise sharp-concentration results on the running time of a simple EA on standard benchmark problems, including the class of general linear functions. Surprisingly, the probability of deviating by an $r$-factor in lower order terms of the expected time decreases exponentially with $r$ on all these problems. The usefulness of the theorem outside the theory of RSHs is demonstrated by deriving tail bounds on the number of cycles in random permutations. All these results handle a position-dependent (variable) drift that was not covered by previous drift theorems with tail bounds. Moreover, our theorem can be specialized into virtually all existing drift theorems with drift towards the target from the literature. Finally, user-friendly specializations of the general drift theorem are given. ",general drift analysis tail bound drift analysis one state art techniques runtime analysis randomize search heuristics rshs evolutionary algorithms eas simulate anneal etc vast majority exist drift theorems yield bound expect value hit time target state set optimal solutions without make additional statements distribution time address lack provide general drift theorem include bound upper lower tail hit time distribution new tail bound apply prove precise sharp concentration result run time simple ea standard benchmark problems include class general linear function surprisingly probability deviate factor lower order term expect time decrease exponentially problems usefulness theorem outside theory rshs demonstrate derive tail bound number cycle random permutations result handle position dependent variable drift cover previous drift theorems tail bound moreover theorem specialize virtually exist drift theorems drift towards target literature finally user friendly specializations general drift theorem give,136,11,1307.2559.txt
http://arxiv.org/abs/1307.2783,Coping with Unreliable Workers in Internet-based Computing: An   Evaluation of Reputation Mechanisms,"  We present reputation-based mechanisms for building reliable task computing systems over the Internet. The most characteristic examples of such systems are the volunteer computing and the crowdsourcing platforms. In both examples end users are offering over the Internet their computing power or their human intelligence to solve tasks either voluntarily or under payment. While the main advantage of these systems is the inexpensive computational power provided, the main drawback is the untrustworthy nature of the end users. Generally, this type of systems are modeled under the ""master-worker"" setting. A ""master"" has a set of tasks to compute and instead of computing them locally she sends these tasks to available ""workers"" that compute and report back the task results. We categorize these workers in three generic types: altruistic, malicious and rational. Altruistic workers that always return the correct result, malicious workers that always return an incorrect result, and rational workers that decide to reply or not truthfully depending on what increases their benefit. We design a reinforcement learning mechanism to induce a correct behavior to rational workers, while the mechanism is complemented by four reputation schemes that cope with malice. The goal of the mechanism is to reach a state of eventual correctness, that is, a stable state of the system in which the master always obtains the correct task results. Analysis of the system gives provable guarantees under which truthful behavior can be ensured. Finally, we observe the behavior of the mechanism through simulations that use realistic system parameters values. Simulations not only agree with the analysis but also reveal interesting trade-offs between various metrics and parameters. Finally, the four reputation schemes are assessed against the tolerance to cheaters. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Computer Science and Game Theory ; 68Q85 ; ","Christoforou, Evgenia ; Anta, Antonio Fernandez ; Georgiou, Chryssis ; Mosteiro, Miguel A. ; Sanchez, Angel ; ","Coping with Unreliable Workers in Internet-based Computing: An   Evaluation of Reputation Mechanisms  We present reputation-based mechanisms for building reliable task computing systems over the Internet. The most characteristic examples of such systems are the volunteer computing and the crowdsourcing platforms. In both examples end users are offering over the Internet their computing power or their human intelligence to solve tasks either voluntarily or under payment. While the main advantage of these systems is the inexpensive computational power provided, the main drawback is the untrustworthy nature of the end users. Generally, this type of systems are modeled under the ""master-worker"" setting. A ""master"" has a set of tasks to compute and instead of computing them locally she sends these tasks to available ""workers"" that compute and report back the task results. We categorize these workers in three generic types: altruistic, malicious and rational. Altruistic workers that always return the correct result, malicious workers that always return an incorrect result, and rational workers that decide to reply or not truthfully depending on what increases their benefit. We design a reinforcement learning mechanism to induce a correct behavior to rational workers, while the mechanism is complemented by four reputation schemes that cope with malice. The goal of the mechanism is to reach a state of eventual correctness, that is, a stable state of the system in which the master always obtains the correct task results. Analysis of the system gives provable guarantees under which truthful behavior can be ensured. Finally, we observe the behavior of the mechanism through simulations that use realistic system parameters values. Simulations not only agree with the analysis but also reveal interesting trade-offs between various metrics and parameters. Finally, the four reputation schemes are assessed against the tolerance to cheaters. ",cop unreliable workers internet base compute evaluation reputation mechanisms present reputation base mechanisms build reliable task compute systems internet characteristic examples systems volunteer compute crowdsourcing platforms examples end users offer internet compute power human intelligence solve task either voluntarily payment main advantage systems inexpensive computational power provide main drawback untrustworthy nature end users generally type systems model master worker set master set task compute instead compute locally send task available workers compute report back task result categorize workers three generic type altruistic malicious rational altruistic workers always return correct result malicious workers always return incorrect result rational workers decide reply truthfully depend increase benefit design reinforcement learn mechanism induce correct behavior rational workers mechanism complement four reputation scheme cope malice goal mechanism reach state eventual correctness stable state system master always obtain correct task result analysis system give provable guarantee truthful behavior ensure finally observe behavior mechanism simulations use realistic system parameters value simulations agree analysis also reveal interest trade off various metrics parameters finally four reputation scheme assess tolerance cheaters,171,4,1307.2783.txt
http://arxiv.org/abs/1307.2968,Introduction to Queueing Theory and Stochastic Teletraffic Models,"  The aim of this textbook is to provide students with basic knowledge of stochastic models that may apply to telecommunications research areas, such as traffic modelling, resource provisioning and traffic management. These study areas are often collectively called teletraffic. This book assumes prior knowledge of a programming language, mathematics, probability and stochastic processes normally taught in an electrical engineering course. For students who have some but not sufficiently strong background in probability and stochastic processes, we provide, in the first few chapters, background on the relevant concepts in these areas. ",Mathematics - Probability ; Computer Science - Information Theory ; ,"Zukerman, Moshe ; ","Introduction to Queueing Theory and Stochastic Teletraffic Models  The aim of this textbook is to provide students with basic knowledge of stochastic models that may apply to telecommunications research areas, such as traffic modelling, resource provisioning and traffic management. These study areas are often collectively called teletraffic. This book assumes prior knowledge of a programming language, mathematics, probability and stochastic processes normally taught in an electrical engineering course. For students who have some but not sufficiently strong background in probability and stochastic processes, we provide, in the first few chapters, background on the relevant concepts in these areas. ",introduction queue theory stochastic teletraffic model aim textbook provide students basic knowledge stochastic model may apply telecommunications research areas traffic model resource provision traffic management study areas often collectively call teletraffic book assume prior knowledge program language mathematics probability stochastic process normally teach electrical engineer course students sufficiently strong background probability stochastic process provide first chapters background relevant concepts areas,60,10,1307.2968.txt
http://arxiv.org/abs/1307.3142,Perfect Codes in the Discrete Simplex,"  We study the problem of existence of (nontrivial) perfect codes in the discrete $ n $-simplex $ \Delta_{\ell}^n := \left\{ \begin{pmatrix} x_0, \ldots, x_n \end{pmatrix} : x_i \in \mathbb{Z}_{+}, \sum_i x_i = \ell \right\} $ under $ \ell_1 $ metric. The problem is motivated by the so-called multiset codes, which have recently been introduced by the authors as appropriate constructs for error correction in the permutation channels. It is shown that $ e $-perfect codes in the $ 1 $-simplex $ \Delta_{\ell}^1 $ exist for any $ \ell \geq 2e + 1 $, the $ 2 $-simplex $ \Delta_{\ell}^2 $ admits an $ e $-perfect code if and only if $ \ell = 3e + 1 $, while there are no perfect codes in higher-dimensional simplices. In other words, perfect multiset codes exist only over binary and ternary alphabets. ","Computer Science - Information Theory ; Computer Science - Discrete Mathematics ; 94B25, 05B40, 52C17, 05C12, 68R99 ; ","Kovačević, Mladen ; Vukobratović, Dejan ; ","Perfect Codes in the Discrete Simplex  We study the problem of existence of (nontrivial) perfect codes in the discrete $ n $-simplex $ \Delta_{\ell}^n := \left\{ \begin{pmatrix} x_0, \ldots, x_n \end{pmatrix} : x_i \in \mathbb{Z}_{+}, \sum_i x_i = \ell \right\} $ under $ \ell_1 $ metric. The problem is motivated by the so-called multiset codes, which have recently been introduced by the authors as appropriate constructs for error correction in the permutation channels. It is shown that $ e $-perfect codes in the $ 1 $-simplex $ \Delta_{\ell}^1 $ exist for any $ \ell \geq 2e + 1 $, the $ 2 $-simplex $ \Delta_{\ell}^2 $ admits an $ e $-perfect code if and only if $ \ell = 3e + 1 $, while there are no perfect codes in higher-dimensional simplices. In other words, perfect multiset codes exist only over binary and ternary alphabets. ",perfect cod discrete simplex study problem existence nontrivial perfect cod discrete simplex delta ell leave begin pmatrix ldots end pmatrix mathbb sum ell right ell metric problem motivate call multiset cod recently introduce author appropriate construct error correction permutation channel show perfect cod simplex delta ell exist ell geq simplex delta ell admit perfect code ell perfect cod higher dimensional simplices word perfect multiset cod exist binary ternary alphabets,69,5,1307.3142.txt
http://arxiv.org/abs/1307.3544,Distributed Bayesian Detection with Byzantine Data,"  In this paper, we consider the problem of distributed Bayesian detection in the presence of Byzantines in the network. It is assumed that a fraction of the nodes in the network are compromised and reprogrammed by an adversary to transmit false information to the fusion center (FC) to degrade detection performance. The problem of distributed detection is formulated as a binary hypothesis test at the FC based on 1-bit data sent by the sensors. The expression for minimum attacking power required by the Byzantines to blind the FC is obtained. More specifically, we show that above a certain fraction of Byzantine attackers in the network, the detection scheme becomes completely incapable of utilizing the sensor data for detection. We analyze the problem under different attacking scenarios and derive results for different non-asymptotic cases. It is found that existing asymptotics-based results do not hold under several non-asymptotic scenarios. When the fraction of Byzantines is not sufficient to blind the FC, we also provide closed form expressions for the optimal attacking strategies for the Byzantines that most degrade the detection performance. ","Computer Science - Information Theory ; Computer Science - Cryptography and Security ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Computer Science and Game Theory ; Statistics - Applications ; ","Kailkhura, Bhavya ; Han, Yunghsiang S. ; Brahma, Swastik ; Varshney, Pramod K. ; ","Distributed Bayesian Detection with Byzantine Data  In this paper, we consider the problem of distributed Bayesian detection in the presence of Byzantines in the network. It is assumed that a fraction of the nodes in the network are compromised and reprogrammed by an adversary to transmit false information to the fusion center (FC) to degrade detection performance. The problem of distributed detection is formulated as a binary hypothesis test at the FC based on 1-bit data sent by the sensors. The expression for minimum attacking power required by the Byzantines to blind the FC is obtained. More specifically, we show that above a certain fraction of Byzantine attackers in the network, the detection scheme becomes completely incapable of utilizing the sensor data for detection. We analyze the problem under different attacking scenarios and derive results for different non-asymptotic cases. It is found that existing asymptotics-based results do not hold under several non-asymptotic scenarios. When the fraction of Byzantines is not sufficient to blind the FC, we also provide closed form expressions for the optimal attacking strategies for the Byzantines that most degrade the detection performance. ",distribute bayesian detection byzantine data paper consider problem distribute bayesian detection presence byzantines network assume fraction nod network compromise reprogrammed adversary transmit false information fusion center fc degrade detection performance problem distribute detection formulate binary hypothesis test fc base bite data send sensors expression minimum attack power require byzantines blind fc obtain specifically show certain fraction byzantine attackers network detection scheme become completely incapable utilize sensor data detection analyze problem different attack scenarios derive result different non asymptotic case find exist asymptotics base result hold several non asymptotic scenarios fraction byzantines sufficient blind fc also provide close form expressions optimal attack strategies byzantines degrade detection performance,106,12,1307.3544.txt
http://arxiv.org/abs/1307.4062,Empirical Evidence of Large-Scale Diversity in API Usage of   Object-Oriented Software,"  In this paper, we study how object-oriented classes are used across thousands of software packages. We concentrate on ""usage diversity'"", defined as the different statically observable combinations of methods called on the same object. We present empirical evidence that there is a significant usage diversity for many classes. For instance, we observe in our dataset that Java's String is used in 2460 manners. We discuss the reasons of this observed diversity and the consequences on software engineering knowledge and research. ",Computer Science - Software Engineering ; ,"Mendez, Diego ; Baudry, Benoit ; Monperrus, Martin ; ","Empirical Evidence of Large-Scale Diversity in API Usage of   Object-Oriented Software  In this paper, we study how object-oriented classes are used across thousands of software packages. We concentrate on ""usage diversity'"", defined as the different statically observable combinations of methods called on the same object. We present empirical evidence that there is a significant usage diversity for many classes. For instance, we observe in our dataset that Java's String is used in 2460 manners. We discuss the reasons of this observed diversity and the consequences on software engineering knowledge and research. ",empirical evidence large scale diversity api usage object orient software paper study object orient class use across thousands software package concentrate usage diversity define different statically observable combinations methods call object present empirical evidence significant usage diversity many class instance observe dataset java string use manners discuss reason observe diversity consequences software engineer knowledge research,55,10,1307.4062.txt
http://arxiv.org/abs/1307.4355,Near Linear Time Approximation Schemes for Uncapacitated and Capacitated   b--Matching Problems in Nonbipartite Graphs,"  We present the first near optimal approximation schemes for the   maximum weighted (uncapacitated or capacitated) $b$--matching   problems for non-bipartite graphs that run in time (near) linear in   the number of edges. For any $\delta>3/\sqrt{n}$ the algorithm   produces a $(1-\delta)$ approximation in $O(m \poly(\delta^{-1},\log   n))$ time. We provide fractional solutions for the standard linear   programming formulations for these problems and subsequently also   provide (near) linear time approximation schemes   for rounding the fractional solutions.   Through these problems as a vehicle, we also present several ideas   in the context of solving linear programs approximately using fast   primal-dual algorithms. First, even though the dual of these   problems have exponentially many variables and an efficient exact   computation of dual weights is infeasible, we show that we can   efficiently compute and use a sparse approximation of the dual   weights using a combination of (i) adding perturbation to the   constraints of the polytope and (ii) amplification followed by   thresholding of the dual weights. Second, we show that   approximation algorithms can be used to reduce the width of the   formulation, and faster convergence. ",Computer Science - Data Structures and Algorithms ; ,"Ahn, Kook Jin ; Guha, Sudipto ; ","Near Linear Time Approximation Schemes for Uncapacitated and Capacitated   b--Matching Problems in Nonbipartite Graphs  We present the first near optimal approximation schemes for the   maximum weighted (uncapacitated or capacitated) $b$--matching   problems for non-bipartite graphs that run in time (near) linear in   the number of edges. For any $\delta>3/\sqrt{n}$ the algorithm   produces a $(1-\delta)$ approximation in $O(m \poly(\delta^{-1},\log   n))$ time. We provide fractional solutions for the standard linear   programming formulations for these problems and subsequently also   provide (near) linear time approximation schemes   for rounding the fractional solutions.   Through these problems as a vehicle, we also present several ideas   in the context of solving linear programs approximately using fast   primal-dual algorithms. First, even though the dual of these   problems have exponentially many variables and an efficient exact   computation of dual weights is infeasible, we show that we can   efficiently compute and use a sparse approximation of the dual   weights using a combination of (i) adding perturbation to the   constraints of the polytope and (ii) amplification followed by   thresholding of the dual weights. Second, we show that   approximation algorithms can be used to reduce the width of the   formulation, and faster convergence. ",near linear time approximation scheme uncapacitated capacitate match problems nonbipartite graph present first near optimal approximation scheme maximum weight uncapacitated capacitate match problems non bipartite graph run time near linear number edge delta sqrt algorithm produce delta approximation poly delta log time provide fractional solutions standard linear program formulations problems subsequently also provide near linear time approximation scheme round fractional solutions problems vehicle also present several ideas context solve linear program approximately use fast primal dual algorithms first even though dual problems exponentially many variables efficient exact computation dual weight infeasible show efficiently compute use sparse approximation dual weight use combination add perturbation constraints polytope ii amplification follow thresholding dual weight second show approximation algorithms use reduce width formulation faster convergence,121,1,1307.4355.txt
http://arxiv.org/abs/1307.5001,On Lower Complexity Bounds for Large-Scale Smooth Convex Optimization,"  We derive lower bounds on the black-box oracle complexity of large-scale smooth convex minimization problems, with emphasis on minimizing smooth (with Holder continuous, with a given exponent and constant, gradient) convex functions over high-dimensional ||.||_p-balls, 1<=p<=\infty. Our bounds turn out to be tight (up to logarithmic in the design dimension factors), and can be viewed as a substantial extension of the existing lower complexity bounds for large-scale convex minimization covering the nonsmooth case and the 'Euclidean' smooth case (minimization of convex functions with Lipschitz continuous gradients over Euclidean balls). As a byproduct of our results, we demonstrate that the classical Conditional Gradient algorithm is near-optimal, in the sense of Information-Based Complexity Theory, when minimizing smooth convex functions over high-dimensional ||.||_\infty-balls and their matrix analogies -- spectral norm balls in the spaces of square matrices. ",Mathematics - Optimization and Control ; Computer Science - Computational Complexity ; ,"Guzman, Cristobal ; Nemirovski, Arkadi ; ","On Lower Complexity Bounds for Large-Scale Smooth Convex Optimization  We derive lower bounds on the black-box oracle complexity of large-scale smooth convex minimization problems, with emphasis on minimizing smooth (with Holder continuous, with a given exponent and constant, gradient) convex functions over high-dimensional ||.||_p-balls, 1<=p<=\infty. Our bounds turn out to be tight (up to logarithmic in the design dimension factors), and can be viewed as a substantial extension of the existing lower complexity bounds for large-scale convex minimization covering the nonsmooth case and the 'Euclidean' smooth case (minimization of convex functions with Lipschitz continuous gradients over Euclidean balls). As a byproduct of our results, we demonstrate that the classical Conditional Gradient algorithm is near-optimal, in the sense of Information-Based Complexity Theory, when minimizing smooth convex functions over high-dimensional ||.||_\infty-balls and their matrix analogies -- spectral norm balls in the spaces of square matrices. ",lower complexity bound large scale smooth convex optimization derive lower bound black box oracle complexity large scale smooth convex minimization problems emphasis minimize smooth holder continuous give exponent constant gradient convex function high dimensional ball infty bound turn tight logarithmic design dimension factor view substantial extension exist lower complexity bound large scale convex minimization cover nonsmooth case euclidean smooth case minimization convex function lipschitz continuous gradients euclidean ball byproduct result demonstrate classical conditional gradient algorithm near optimal sense information base complexity theory minimize smooth convex function high dimensional infty ball matrix analogies spectral norm ball space square matrices,98,7,1307.5001.txt
http://arxiv.org/abs/1307.6033,Sparse Reconstruction-based Detection of Spatial Dimension Holes in   Cognitive Radio Networks,"  In this paper, we investigate a spectrum sensing algorithm for detecting spatial dimension holes in Multiple Inputs Multiple Outputs (MIMO) transmissions for OFDM systems using Compressive Sensing (CS) tools. This extends the energy detector to allow for detecting transmission opportunities even if the band is already energy filled. We show that the task described above is not performed efficiently by regular MIMO decoders (such as MMSE decoder) due to possible sparsity in the transmit signal. Since CS reconstruction tools take into account the sparsity order of the signal, they are more efficient in detecting the activity of the users. Building on successful activity detection by the CS detector, we show that the use of a CS-aided MMSE decoders yields better performance rather than using either CS-based or MMSE decoders separately. Simulations are conducted to verify the gains from using CS detector for Primary user activity detection and the performance gain in using CS-aided MMSE decoders for decoding the PU information for future relaying. ",Computer Science - Information Theory ; Computer Science - Networking and Internet Architecture ; Mathematics - Optimization and Control ; ,"Ezzeldin, Yahya H. ; Sultan, Radwa A. ; Seddik, Karim G. ; ","Sparse Reconstruction-based Detection of Spatial Dimension Holes in   Cognitive Radio Networks  In this paper, we investigate a spectrum sensing algorithm for detecting spatial dimension holes in Multiple Inputs Multiple Outputs (MIMO) transmissions for OFDM systems using Compressive Sensing (CS) tools. This extends the energy detector to allow for detecting transmission opportunities even if the band is already energy filled. We show that the task described above is not performed efficiently by regular MIMO decoders (such as MMSE decoder) due to possible sparsity in the transmit signal. Since CS reconstruction tools take into account the sparsity order of the signal, they are more efficient in detecting the activity of the users. Building on successful activity detection by the CS detector, we show that the use of a CS-aided MMSE decoders yields better performance rather than using either CS-based or MMSE decoders separately. Simulations are conducted to verify the gains from using CS detector for Primary user activity detection and the performance gain in using CS-aided MMSE decoders for decoding the PU information for future relaying. ",sparse reconstruction base detection spatial dimension hole cognitive radio network paper investigate spectrum sense algorithm detect spatial dimension hole multiple input multiple output mimo transmissions ofdm systems use compressive sense cs tool extend energy detector allow detect transmission opportunities even band already energy fill show task describe perform efficiently regular mimo decoders mmse decoder due possible sparsity transmit signal since cs reconstruction tool take account sparsity order signal efficient detect activity users build successful activity detection cs detector show use cs aid mmse decoders yield better performance rather use either cs base mmse decoders separately simulations conduct verify gain use cs detector primary user activity detection performance gain use cs aid mmse decoders decode pu information future relay,118,9,1307.6033.txt
http://arxiv.org/abs/1307.6864,Convex recovery from interferometric measurements,"  This note formulates a deterministic recovery result for vectors $x$ from quadratic measurements of the form $(Ax)_i \overline{(Ax)_j}$ for some left-invertible $A$. Recovery is exact, or stable in the noisy case, when the couples $(i,j)$ are chosen as edges of a well-connected graph. One possible way of obtaining the solution is as a feasible point of a simple semidefinite program. Furthermore, we show how the proportionality constant in the error estimate depends on the spectral gap of a data-weighted graph Laplacian. Such quadratic measurements have found applications in phase retrieval, angular synchronization, and more recently interferometric waveform inversion. ",Mathematics - Numerical Analysis ; Computer Science - Information Theory ; Mathematics - Optimization and Control ; ,"Demanet, Laurent ; Jugnon, Vincent ; ","Convex recovery from interferometric measurements  This note formulates a deterministic recovery result for vectors $x$ from quadratic measurements of the form $(Ax)_i \overline{(Ax)_j}$ for some left-invertible $A$. Recovery is exact, or stable in the noisy case, when the couples $(i,j)$ are chosen as edges of a well-connected graph. One possible way of obtaining the solution is as a feasible point of a simple semidefinite program. Furthermore, we show how the proportionality constant in the error estimate depends on the spectral gap of a data-weighted graph Laplacian. Such quadratic measurements have found applications in phase retrieval, angular synchronization, and more recently interferometric waveform inversion. ",convex recovery interferometric measurements note formulate deterministic recovery result vectors quadratic measurements form ax overline ax leave invertible recovery exact stable noisy case couple choose edge well connect graph one possible way obtain solution feasible point simple semidefinite program furthermore show proportionality constant error estimate depend spectral gap data weight graph laplacian quadratic measurements find applications phase retrieval angular synchronization recently interferometric waveform inversion,64,9,1307.6864.txt
http://arxiv.org/abs/1307.7050,A Comprehensive Evaluation of Machine Learning Techniques for Cancer   Class Prediction Based on Microarray Data,"  Prostate cancer is among the most common cancer in males and its heterogeneity is well known. Its early detection helps making therapeutic decision. There is no standard technique or procedure yet which is full-proof in predicting cancer class. The genomic level changes can be detected in gene expression data and those changes may serve as standard model for any random cancer data for class prediction. Various techniques were implied on prostate cancer data set in order to accurately predict cancer class including machine learning techniques. Huge number of attributes and few number of sample in microarray data leads to poor machine learning, therefore the most challenging part is attribute reduction or non significant gene reduction. In this work we have compared several machine learning techniques for their accuracy in predicting the cancer class. Machine learning is effective when number of attributes (genes) are larger than the number of samples which is rarely possible with gene expression data. Attribute reduction or gene filtering is absolutely required in order to make the data more meaningful as most of the genes do not participate in tumor development and are irrelevant for cancer prediction. Here we have applied combination of statistical techniques such as inter-quartile range and t-test, which has been effective in filtering significant genes and minimizing noise from data. Further we have done a comprehensive evaluation of ten state-of-the-art machine learning techniques for their accuracy in class prediction of prostate cancer. Out of these techniques, Bayes Network out performed with an accuracy of 94.11% followed by Navie Bayes with an accuracy of 91.17%. To cross validate our results, we modified our training dataset in six different way and found that average sensitivity, specificity, precision and accuracy of Bayes Network is highest among all other techniques used. ","Computer Science - Machine Learning ; Computer Science - Computational Engineering, Finance, and Science ; ","Raza, Khalid ; Hasan, Atif N ; ","A Comprehensive Evaluation of Machine Learning Techniques for Cancer   Class Prediction Based on Microarray Data  Prostate cancer is among the most common cancer in males and its heterogeneity is well known. Its early detection helps making therapeutic decision. There is no standard technique or procedure yet which is full-proof in predicting cancer class. The genomic level changes can be detected in gene expression data and those changes may serve as standard model for any random cancer data for class prediction. Various techniques were implied on prostate cancer data set in order to accurately predict cancer class including machine learning techniques. Huge number of attributes and few number of sample in microarray data leads to poor machine learning, therefore the most challenging part is attribute reduction or non significant gene reduction. In this work we have compared several machine learning techniques for their accuracy in predicting the cancer class. Machine learning is effective when number of attributes (genes) are larger than the number of samples which is rarely possible with gene expression data. Attribute reduction or gene filtering is absolutely required in order to make the data more meaningful as most of the genes do not participate in tumor development and are irrelevant for cancer prediction. Here we have applied combination of statistical techniques such as inter-quartile range and t-test, which has been effective in filtering significant genes and minimizing noise from data. Further we have done a comprehensive evaluation of ten state-of-the-art machine learning techniques for their accuracy in class prediction of prostate cancer. Out of these techniques, Bayes Network out performed with an accuracy of 94.11% followed by Navie Bayes with an accuracy of 91.17%. To cross validate our results, we modified our training dataset in six different way and found that average sensitivity, specificity, precision and accuracy of Bayes Network is highest among all other techniques used. ",comprehensive evaluation machine learn techniques cancer class prediction base microarray data prostate cancer among common cancer males heterogeneity well know early detection help make therapeutic decision standard technique procedure yet full proof predict cancer class genomic level change detect gene expression data change may serve standard model random cancer data class prediction various techniques imply prostate cancer data set order accurately predict cancer class include machine learn techniques huge number attribute number sample microarray data lead poor machine learn therefore challenge part attribute reduction non significant gene reduction work compare several machine learn techniques accuracy predict cancer class machine learn effective number attribute genes larger number sample rarely possible gene expression data attribute reduction gene filter absolutely require order make data meaningful genes participate tumor development irrelevant cancer prediction apply combination statistical techniques inter quartile range test effective filter significant genes minimize noise data do comprehensive evaluation ten state art machine learn techniques accuracy class prediction prostate cancer techniques bay network perform accuracy follow navie bay accuracy cross validate result modify train dataset six different way find average sensitivity specificity precision accuracy bay network highest among techniques use,188,10,1307.7050.txt
http://arxiv.org/abs/1307.7087,Correcting Grain-Errors in Magnetic Media,"  This paper studies new bounds and constructions that are applicable to the combinatorial granular channel model previously introduced by Sharov and Roth. We derive new bounds on the maximum cardinality of a grain-error-correcting code and propose constructions of codes that correct grain-errors. We demonstrate that a permutation of the classical group codes (e.g., Constantin-Rao codes) can correct a single grain-error. In many cases of interest, our results improve upon the currently best known bounds and constructions. Some of the approaches adopted in the context of grain-errors may have application to other channel models. ",Computer Science - Information Theory ; ,"Gabrys, Ryan ; Yaakobi, Eitan ; Dolecek, Lara ; ","Correcting Grain-Errors in Magnetic Media  This paper studies new bounds and constructions that are applicable to the combinatorial granular channel model previously introduced by Sharov and Roth. We derive new bounds on the maximum cardinality of a grain-error-correcting code and propose constructions of codes that correct grain-errors. We demonstrate that a permutation of the classical group codes (e.g., Constantin-Rao codes) can correct a single grain-error. In many cases of interest, our results improve upon the currently best known bounds and constructions. Some of the approaches adopted in the context of grain-errors may have application to other channel models. ",correct grain errors magnetic media paper study new bound constructions applicable combinatorial granular channel model previously introduce sharov roth derive new bound maximum cardinality grain error correct code propose constructions cod correct grain errors demonstrate permutation classical group cod constantin rao cod correct single grain error many case interest result improve upon currently best know bound constructions approach adopt context grain errors may application channel model,66,5,1307.7087.txt
http://arxiv.org/abs/1307.7430,Holographic Algorithms Beyond Matchgates,"  Holographic algorithms introduced by Valiant are composed of two ingredients: matchgates, which are gadgets realizing local constraint functions by weighted planar perfect matchings, and holographic reductions, which show equivalences among problems with different descriptions via certain basis transformations. In this paper, we replace matchgates in the paradigm above by the affine type and the product type constraint functions, which are known to be tractable in general (not necessarily planar) graphs. More specifically, we present polynomial-time algorithms to decide if a given counting problem has a holographic reduction to another problem defined by the affine or product-type functions. Our algorithms also find a holographic transformation when one exists. We further present polynomial-time algorithms of the same decision and search problems for symmetric functions, where the complexity is measured in terms of the (exponentially more) succinct representations. The algorithm for the symmetric case also shows that the recent dichotomy theorem for Holant problems with symmetric constraints is efficiently decidable. Our proof techniques are mainly algebraic, e.g., using stabilizers and orbits of group actions. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; 68Q25 ; F.2.1 ; G.2.1 ; ,"Cai, Jin-Yi ; Guo, Heng ; Williams, Tyson ; ","Holographic Algorithms Beyond Matchgates  Holographic algorithms introduced by Valiant are composed of two ingredients: matchgates, which are gadgets realizing local constraint functions by weighted planar perfect matchings, and holographic reductions, which show equivalences among problems with different descriptions via certain basis transformations. In this paper, we replace matchgates in the paradigm above by the affine type and the product type constraint functions, which are known to be tractable in general (not necessarily planar) graphs. More specifically, we present polynomial-time algorithms to decide if a given counting problem has a holographic reduction to another problem defined by the affine or product-type functions. Our algorithms also find a holographic transformation when one exists. We further present polynomial-time algorithms of the same decision and search problems for symmetric functions, where the complexity is measured in terms of the (exponentially more) succinct representations. The algorithm for the symmetric case also shows that the recent dichotomy theorem for Holant problems with symmetric constraints is efficiently decidable. Our proof techniques are mainly algebraic, e.g., using stabilizers and orbits of group actions. ",holographic algorithms beyond matchgates holographic algorithms introduce valiant compose two ingredients matchgates gadgets realize local constraint function weight planar perfect match holographic reductions show equivalences among problems different descriptions via certain basis transformations paper replace matchgates paradigm affine type product type constraint function know tractable general necessarily planar graph specifically present polynomial time algorithms decide give count problem holographic reduction another problem define affine product type function algorithms also find holographic transformation one exist present polynomial time algorithms decision search problems symmetric function complexity measure term exponentially succinct representations algorithm symmetric case also show recent dichotomy theorem holant problems symmetric constraints efficiently decidable proof techniques mainly algebraic use stabilizers orbit group action,112,8,1307.7430.txt
http://arxiv.org/abs/1307.8371,The Power of Localization for Efficiently Learning Linear Separators   with Noise,"  We introduce a new approach for designing computationally efficient learning algorithms that are tolerant to noise, and demonstrate its effectiveness by designing algorithms with improved noise tolerance guarantees for learning linear separators.   We consider both the malicious noise model and the adversarial label noise model. For malicious noise, where the adversary can corrupt both the label and the features, we provide a polynomial-time algorithm for learning linear separators in $\Re^d$ under isotropic log-concave distributions that can tolerate a nearly information-theoretically optimal noise rate of $\eta = \Omega(\epsilon)$. For the adversarial label noise model, where the distribution over the feature vectors is unchanged, and the overall probability of a noisy label is constrained to be at most $\eta$, we also give a polynomial-time algorithm for learning linear separators in $\Re^d$ under isotropic log-concave distributions that can handle a noise rate of $\eta = \Omega\left(\epsilon\right)$.   We show that, in the active learning model, our algorithms achieve a label complexity whose dependence on the error parameter $\epsilon$ is polylogarithmic. This provides the first polynomial-time active learning algorithm for learning linear separators in the presence of malicious noise or adversarial label noise. ",Computer Science - Machine Learning ; Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; Statistics - Machine Learning ; F.2 ; ,"Awasthi, Pranjal ; Balcan, Maria Florina ; Long, Philip M. ; ","The Power of Localization for Efficiently Learning Linear Separators   with Noise  We introduce a new approach for designing computationally efficient learning algorithms that are tolerant to noise, and demonstrate its effectiveness by designing algorithms with improved noise tolerance guarantees for learning linear separators.   We consider both the malicious noise model and the adversarial label noise model. For malicious noise, where the adversary can corrupt both the label and the features, we provide a polynomial-time algorithm for learning linear separators in $\Re^d$ under isotropic log-concave distributions that can tolerate a nearly information-theoretically optimal noise rate of $\eta = \Omega(\epsilon)$. For the adversarial label noise model, where the distribution over the feature vectors is unchanged, and the overall probability of a noisy label is constrained to be at most $\eta$, we also give a polynomial-time algorithm for learning linear separators in $\Re^d$ under isotropic log-concave distributions that can handle a noise rate of $\eta = \Omega\left(\epsilon\right)$.   We show that, in the active learning model, our algorithms achieve a label complexity whose dependence on the error parameter $\epsilon$ is polylogarithmic. This provides the first polynomial-time active learning algorithm for learning linear separators in the presence of malicious noise or adversarial label noise. ",power localization efficiently learn linear separators noise introduce new approach design computationally efficient learn algorithms tolerant noise demonstrate effectiveness design algorithms improve noise tolerance guarantee learn linear separators consider malicious noise model adversarial label noise model malicious noise adversary corrupt label feature provide polynomial time algorithm learn linear separators isotropic log concave distributions tolerate nearly information theoretically optimal noise rate eta omega epsilon adversarial label noise model distribution feature vectors unchanged overall probability noisy label constrain eta also give polynomial time algorithm learn linear separators isotropic log concave distributions handle noise rate eta omega leave epsilon right show active learn model algorithms achieve label complexity whose dependence error parameter epsilon polylogarithmic provide first polynomial time active learn algorithm learn linear separators presence malicious noise adversarial label noise,127,1,1307.8371.txt
http://arxiv.org/abs/1308.0497,"A note on T\""uring's 1936","  T\""uring's argument that there can be no machine computing the diagonal on the enumeration of the computable sequences is not a demonstration. ","Computer Science - Computational Complexity ; 03D10, 68Qxx ; ","Cattabriga, Paola ; ","A note on T\""uring's 1936  T\""uring's argument that there can be no machine computing the diagonal on the enumeration of the computable sequences is not a demonstration. ",note uring uring argument machine compute diagonal enumeration computable sequence demonstration,11,4,1308.0497.txt
http://arxiv.org/abs/1308.0776,Dynamic Approximate All-Pairs Shortest Paths: Breaking the O(mn) Barrier   and Derandomization,"  We study dynamic $(1+\epsilon)$-approximation algorithms for the all-pairs shortest paths problem in unweighted undirected $n$-node $m$-edge graphs under edge deletions. The fastest algorithm for this problem is a randomized algorithm with a total update time of $\tilde O(mn/\epsilon)$ and constant query time by Roditty and Zwick [FOCS 2004]. The fastest deterministic algorithm is from a 1981 paper by Even and Shiloach [JACM 1981]; it has a total update time of $O(mn^2)$ and constant query time. We improve these results as follows: (1) We present an algorithm with a total update time of $\tilde O(n^{5/2}/\epsilon)$ and constant query time that has an additive error of $2$ in addition to the $1+\epsilon$ multiplicative error. This beats the previous $\tilde O(mn/\epsilon)$ time when $m=\Omega(n^{3/2})$. Note that the additive error is unavoidable since, even in the static case, an $O(n^{3-\delta})$-time (a so-called truly subcubic) combinatorial algorithm with $1+\epsilon$ multiplicative error cannot have an additive error less than $2-\epsilon$, unless we make a major breakthrough for Boolean matrix multiplication [Dor et al. FOCS 1996] and many other long-standing problems [Vassilevska Williams and Williams FOCS 2010]. The algorithm can also be turned into a $(2+\epsilon)$-approximation algorithm (without an additive error) with the same time guarantees, improving the recent $(3+\epsilon)$-approximation algorithm with $\tilde O(n^{5/2+O(\sqrt{\log{(1/\epsilon)}/\log n})})$ running time of Bernstein and Roditty [SODA 2011] in terms of both approximation and time guarantees. (2) We present a deterministic algorithm with a total update time of $\tilde O(mn/\epsilon)$ and a query time of $O(\log\log n)$. The algorithm has a multiplicative error of $1+\epsilon$ and gives the first improved deterministic algorithm since 1981. It also answers an open question raised by Bernstein [STOC 2013]. ",Computer Science - Data Structures and Algorithms ; F.2.0 ; G.2.2 ; ,"Henzinger, Monika ; Krinninger, Sebastian ; Nanongkai, Danupon ; ","Dynamic Approximate All-Pairs Shortest Paths: Breaking the O(mn) Barrier   and Derandomization  We study dynamic $(1+\epsilon)$-approximation algorithms for the all-pairs shortest paths problem in unweighted undirected $n$-node $m$-edge graphs under edge deletions. The fastest algorithm for this problem is a randomized algorithm with a total update time of $\tilde O(mn/\epsilon)$ and constant query time by Roditty and Zwick [FOCS 2004]. The fastest deterministic algorithm is from a 1981 paper by Even and Shiloach [JACM 1981]; it has a total update time of $O(mn^2)$ and constant query time. We improve these results as follows: (1) We present an algorithm with a total update time of $\tilde O(n^{5/2}/\epsilon)$ and constant query time that has an additive error of $2$ in addition to the $1+\epsilon$ multiplicative error. This beats the previous $\tilde O(mn/\epsilon)$ time when $m=\Omega(n^{3/2})$. Note that the additive error is unavoidable since, even in the static case, an $O(n^{3-\delta})$-time (a so-called truly subcubic) combinatorial algorithm with $1+\epsilon$ multiplicative error cannot have an additive error less than $2-\epsilon$, unless we make a major breakthrough for Boolean matrix multiplication [Dor et al. FOCS 1996] and many other long-standing problems [Vassilevska Williams and Williams FOCS 2010]. The algorithm can also be turned into a $(2+\epsilon)$-approximation algorithm (without an additive error) with the same time guarantees, improving the recent $(3+\epsilon)$-approximation algorithm with $\tilde O(n^{5/2+O(\sqrt{\log{(1/\epsilon)}/\log n})})$ running time of Bernstein and Roditty [SODA 2011] in terms of both approximation and time guarantees. (2) We present a deterministic algorithm with a total update time of $\tilde O(mn/\epsilon)$ and a query time of $O(\log\log n)$. The algorithm has a multiplicative error of $1+\epsilon$ and gives the first improved deterministic algorithm since 1981. It also answers an open question raised by Bernstein [STOC 2013]. ",dynamic approximate pair shortest paths break mn barrier derandomization study dynamic epsilon approximation algorithms pair shortest paths problem unweighted undirected node edge graph edge deletions fastest algorithm problem randomize algorithm total update time tilde mn epsilon constant query time roditty zwick focs fastest deterministic algorithm paper even shiloach jacm total update time mn constant query time improve result follow present algorithm total update time tilde epsilon constant query time additive error addition epsilon multiplicative error beat previous tilde mn epsilon time omega note additive error unavoidable since even static case delta time call truly subcubic combinatorial algorithm epsilon multiplicative error cannot additive error less epsilon unless make major breakthrough boolean matrix multiplication dor et al focs many long stand problems vassilevska williams williams focs algorithm also turn epsilon approximation algorithm without additive error time guarantee improve recent epsilon approximation algorithm tilde sqrt log epsilon log run time bernstein roditty soda term approximation time guarantee present deterministic algorithm total update time tilde mn epsilon query time log log algorithm multiplicative error epsilon give first improve deterministic algorithm since also answer open question raise bernstein stoc,184,1,1308.0776.txt
http://arxiv.org/abs/1308.0801,"Spectral Sequences, Exact Couples and Persistent Homology of filtrations","  In this paper we study the relationship between a very classical algebraic object associated to a filtration of spaces, namely a spectral sequence introduced by Leray in the 1940's, and a more recently invented object that has found many applications -- namely, its persistent homology groups. We show the existence of a long exact sequence of groups linking these two objects and using it derive formulas expressing the dimensions of each individual groups of one object in terms of the dimensions of the groups in the other object. The main tool used to mediate between these objects is the notion of exact couples first introduced by Massey in 1952. ",Mathematics - Algebraic Topology ; Computer Science - Computational Geometry ; 55T05 ; ,"Basu, Saugata ; Parida, Laxmi ; ","Spectral Sequences, Exact Couples and Persistent Homology of filtrations  In this paper we study the relationship between a very classical algebraic object associated to a filtration of spaces, namely a spectral sequence introduced by Leray in the 1940's, and a more recently invented object that has found many applications -- namely, its persistent homology groups. We show the existence of a long exact sequence of groups linking these two objects and using it derive formulas expressing the dimensions of each individual groups of one object in terms of the dimensions of the groups in the other object. The main tool used to mediate between these objects is the notion of exact couples first introduced by Massey in 1952. ",spectral sequence exact couple persistent homology filtrations paper study relationship classical algebraic object associate filtration space namely spectral sequence introduce leray recently invent object find many applications namely persistent homology group show existence long exact sequence group link two object use derive formulas express dimension individual group one object term dimension group object main tool use mediate object notion exact couple first introduce massey,64,4,1308.0801.txt
http://arxiv.org/abs/1308.1391,Low-Dimensional Reconciliation for Continuous-Variable Quantum Key   Distribution,"  We propose an efficient logical layer-based reconciliation method for continuous-variable quantum key distribution (CVQKD) to extract binary information from correlated Gaussian variables. We demonstrate that by operating on the raw-data level, the noise of the quantum channel can be corrected in the low-dimensional (scalar) space and the reconciliation can be extended to arbitrary dimensions. The CVQKD systems allow an unconditionally secret communication over standard telecommunication networks. To exploit the real potential of CVQKD a robust reconciliation technique is needed. It is currently unavailable, which makes it impossible to reach the real performance of the CVQKD protocols. The reconciliation is a post-processing step separated from the transmission of quantum states, which is aimed to derive the secret key from the raw data. The reconciliation process of correlated Gaussian variables is a complex problem that requires either tomography in the physical layer that is intractable in a practical scenario, or high-cost calculations in the multidimensional spherical space with strict dimensional limitations. To avoid these issues we define the low-dimensional reconciliation. We prove that the error probability of one-dimensional reconciliation is zero in any practical CVQKD scenario, and provides unconditional security. The results allow to significantly improve the currently available key rates and transmission distances of CVQKD. ",Quantum Physics ; Computer Science - Information Theory ; ,"Gyongyosi, Laszlo ; Imre, Sandor ; ","Low-Dimensional Reconciliation for Continuous-Variable Quantum Key   Distribution  We propose an efficient logical layer-based reconciliation method for continuous-variable quantum key distribution (CVQKD) to extract binary information from correlated Gaussian variables. We demonstrate that by operating on the raw-data level, the noise of the quantum channel can be corrected in the low-dimensional (scalar) space and the reconciliation can be extended to arbitrary dimensions. The CVQKD systems allow an unconditionally secret communication over standard telecommunication networks. To exploit the real potential of CVQKD a robust reconciliation technique is needed. It is currently unavailable, which makes it impossible to reach the real performance of the CVQKD protocols. The reconciliation is a post-processing step separated from the transmission of quantum states, which is aimed to derive the secret key from the raw data. The reconciliation process of correlated Gaussian variables is a complex problem that requires either tomography in the physical layer that is intractable in a practical scenario, or high-cost calculations in the multidimensional spherical space with strict dimensional limitations. To avoid these issues we define the low-dimensional reconciliation. We prove that the error probability of one-dimensional reconciliation is zero in any practical CVQKD scenario, and provides unconditional security. The results allow to significantly improve the currently available key rates and transmission distances of CVQKD. ",low dimensional reconciliation continuous variable quantum key distribution propose efficient logical layer base reconciliation method continuous variable quantum key distribution cvqkd extract binary information correlate gaussian variables demonstrate operate raw data level noise quantum channel correct low dimensional scalar space reconciliation extend arbitrary dimension cvqkd systems allow unconditionally secret communication standard telecommunication network exploit real potential cvqkd robust reconciliation technique need currently unavailable make impossible reach real performance cvqkd protocols reconciliation post process step separate transmission quantum state aim derive secret key raw data reconciliation process correlate gaussian variables complex problem require either tomography physical layer intractable practical scenario high cost calculations multidimensional spherical space strict dimensional limitations avoid issue define low dimensional reconciliation prove error probability one dimensional reconciliation zero practical cvqkd scenario provide unconditional security result allow significantly improve currently available key rat transmission distance cvqkd,138,9,1308.1391.txt
http://arxiv.org/abs/1308.1603,"A Note on Topology Preservation in Classification, and the Construction   of a Universal Neuron Grid","  It will be shown that according to theorems of K. Menger, every neuron grid if identified with a curve is able to preserve the adopted qualitative structure of a data space. Furthermore, if this identification is made, the neuron grid structure can always be mapped to a subset of a universal neuron grid which is constructable in three space dimensions. Conclusions will be drawn for established neuron grid types as well as neural fields. ",Computer Science - Neural and Evolutionary Computing ; Computer Science - Artificial Intelligence ; Nonlinear Sciences - Adaptation and Self-Organizing Systems ; Statistics - Machine Learning ; 92F99 ; ,"Volz, Dietmar ; ","A Note on Topology Preservation in Classification, and the Construction   of a Universal Neuron Grid  It will be shown that according to theorems of K. Menger, every neuron grid if identified with a curve is able to preserve the adopted qualitative structure of a data space. Furthermore, if this identification is made, the neuron grid structure can always be mapped to a subset of a universal neuron grid which is constructable in three space dimensions. Conclusions will be drawn for established neuron grid types as well as neural fields. ",note topology preservation classification construction universal neuron grid show accord theorems menger every neuron grid identify curve able preserve adopt qualitative structure data space furthermore identification make neuron grid structure always map subset universal neuron grid constructable three space dimension conclusions draw establish neuron grid type well neural field,49,8,1308.1603.txt
http://arxiv.org/abs/1308.3987,Cop and robber game and hyperbolicity,"  In this note, we prove that all cop-win graphs G in the game in which the robber and the cop move at different speeds s and s' with s'<s, are \delta-hyperbolic with \delta=O(s^2). We also show that the dependency between \delta and s is linear if s-s'=\Omega(s) and G obeys a slightly stronger condition. This solves an open question from the paper (J. Chalopin et al., Cop and robber games when the robber can hide and ride, SIAM J. Discr. Math. 25 (2011) 333-359). Since any \delta-hyperbolic graph is cop-win for s=2r and s'=r+2\delta for any r>0, this establishes a new - game-theoretical - characterization of Gromov hyperbolicity. We also show that for weakly modular graphs the dependency between \delta and s is linear for any s'<s. Using these results, we describe a simple constant-factor approximation of the hyperbolicity \delta of a graph on n vertices in O(n^2) time when the graph is given by its distance-matrix. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Chalopin, Jérémie ; Chepoi, Victor ; Papasoglu, Panos ; Pecatte, Timothée ; ","Cop and robber game and hyperbolicity  In this note, we prove that all cop-win graphs G in the game in which the robber and the cop move at different speeds s and s' with s'<s, are \delta-hyperbolic with \delta=O(s^2). We also show that the dependency between \delta and s is linear if s-s'=\Omega(s) and G obeys a slightly stronger condition. This solves an open question from the paper (J. Chalopin et al., Cop and robber games when the robber can hide and ride, SIAM J. Discr. Math. 25 (2011) 333-359). Since any \delta-hyperbolic graph is cop-win for s=2r and s'=r+2\delta for any r>0, this establishes a new - game-theoretical - characterization of Gromov hyperbolicity. We also show that for weakly modular graphs the dependency between \delta and s is linear for any s'<s. Using these results, we describe a simple constant-factor approximation of the hyperbolicity \delta of a graph on n vertices in O(n^2) time when the graph is given by its distance-matrix. ",cop robber game hyperbolicity note prove cop win graph game robber cop move different speed delta hyperbolic delta also show dependency delta linear omega obey slightly stronger condition solve open question paper chalopin et al cop robber game robber hide ride siam discr math since delta hyperbolic graph cop win delta establish new game theoretical characterization gromov hyperbolicity also show weakly modular graph dependency delta linear use result describe simple constant factor approximation hyperbolicity delta graph vertices time graph give distance matrix,82,3,1308.3987.txt
http://arxiv.org/abs/1308.4201,Full-Diversity Space-Time Block Codes for Integer-Forcing Linear   Receivers,"  In multiple-input multiple-output (MIMO) fading channels, the design criterion for full-diversity space-time block codes (STBCs) is primarily determined by the decoding method at the receiver. Although constructions of STBCs have predominantly matched the maximum-likelihood (ML) decoder, design criteria and constructions of full-diversity STBCs have also been reported for low-complexity linear receivers. A new receiver architecture called Integer-Forcing (IF) linear receiver has been proposed to MIMO channels by Zhan et al. which showed promising results for the high-rate V-BLAST encoding scheme. In this paper, we address the design of full-diversity STBCs for IF linear receivers. In particular, we are interested in characterizing the structure of STBCs that provide full-diversity with the IF receiver. Along that direction, we derive an upper bound on the probability of decoding error, and show that STBCs that satisfy the restricted non-vanishing singular value (RNVS) property provide full-diversity for the IF receiver. Furthermore, we prove that all known STBCs with the non-vanishing determinant property provide full-diversity with IF receivers, as they guarantee the RNVS property. By using the formulation of RNVS property, we also prove the existence of a full-diversity STBC outside the class of perfect STBCs, thereby adding significant insights compared to the existing works on STBCs with IF decoding. Finally, we present extensive simulation results to demonstrate that linear designs with RNVS property provide full-diversity for IF receiver. ",Computer Science - Information Theory ; ,"Harshan, J. ; Sakzad, Amin ; Viterbo, Emanuele ; ","Full-Diversity Space-Time Block Codes for Integer-Forcing Linear   Receivers  In multiple-input multiple-output (MIMO) fading channels, the design criterion for full-diversity space-time block codes (STBCs) is primarily determined by the decoding method at the receiver. Although constructions of STBCs have predominantly matched the maximum-likelihood (ML) decoder, design criteria and constructions of full-diversity STBCs have also been reported for low-complexity linear receivers. A new receiver architecture called Integer-Forcing (IF) linear receiver has been proposed to MIMO channels by Zhan et al. which showed promising results for the high-rate V-BLAST encoding scheme. In this paper, we address the design of full-diversity STBCs for IF linear receivers. In particular, we are interested in characterizing the structure of STBCs that provide full-diversity with the IF receiver. Along that direction, we derive an upper bound on the probability of decoding error, and show that STBCs that satisfy the restricted non-vanishing singular value (RNVS) property provide full-diversity for the IF receiver. Furthermore, we prove that all known STBCs with the non-vanishing determinant property provide full-diversity with IF receivers, as they guarantee the RNVS property. By using the formulation of RNVS property, we also prove the existence of a full-diversity STBC outside the class of perfect STBCs, thereby adding significant insights compared to the existing works on STBCs with IF decoding. Finally, we present extensive simulation results to demonstrate that linear designs with RNVS property provide full-diversity for IF receiver. ",full diversity space time block cod integer force linear receivers multiple input multiple output mimo fade channel design criterion full diversity space time block cod stbcs primarily determine decode method receiver although constructions stbcs predominantly match maximum likelihood ml decoder design criteria constructions full diversity stbcs also report low complexity linear receivers new receiver architecture call integer force linear receiver propose mimo channel zhan et al show promise result high rate blast encode scheme paper address design full diversity stbcs linear receivers particular interest characterize structure stbcs provide full diversity receiver along direction derive upper bind probability decode error show stbcs satisfy restrict non vanish singular value rnvs property provide full diversity receiver furthermore prove know stbcs non vanish determinant property provide full diversity receivers guarantee rnvs property use formulation rnvs property also prove existence full diversity stbc outside class perfect stbcs thereby add significant insights compare exist work stbcs decode finally present extensive simulation result demonstrate linear design rnvs property provide full diversity receiver,165,9,1308.4201.txt
http://arxiv.org/abs/1308.4273,Adaptive matching pursuit for off-grid compressed sensing,"  Compressive sensing (CS) can effectively recover a signal when it is sparse in some discrete atoms. However, in some applications, signals are sparse in a continuous parameter space, e.g., frequency space, rather than discrete atoms. Usually, we divide the continuous parameter into finite discrete grid points and build a dictionary from these grid points. However, the actual targets may not exactly lie on the grid points no matter how densely the parameter is grided, which introduces mismatch between the predefined dictionary and the actual one. In this article, a novel method, namely adaptive matching pursuit with constrained total least squares (AMP-CTLS), is proposed to find actual atoms even if they are not included in the initial dictionary. In AMP-CTLS, the grid and the dictionary are adaptively updated to better agree with measurements. The convergence of the algorithm is discussed, and numerical experiments demonstrate the advantages of AMP-CTLS. ",Electrical Engineering and Systems Science - Signal Processing ; Computer Science - Information Theory ; ,"Huang, Tianyao ; Liu, Yimin ; Meng, Huadong ; Wang, Xiqin ; ","Adaptive matching pursuit for off-grid compressed sensing  Compressive sensing (CS) can effectively recover a signal when it is sparse in some discrete atoms. However, in some applications, signals are sparse in a continuous parameter space, e.g., frequency space, rather than discrete atoms. Usually, we divide the continuous parameter into finite discrete grid points and build a dictionary from these grid points. However, the actual targets may not exactly lie on the grid points no matter how densely the parameter is grided, which introduces mismatch between the predefined dictionary and the actual one. In this article, a novel method, namely adaptive matching pursuit with constrained total least squares (AMP-CTLS), is proposed to find actual atoms even if they are not included in the initial dictionary. In AMP-CTLS, the grid and the dictionary are adaptively updated to better agree with measurements. The convergence of the algorithm is discussed, and numerical experiments demonstrate the advantages of AMP-CTLS. ",adaptive match pursuit grid compress sense compressive sense cs effectively recover signal sparse discrete atoms however applications signal sparse continuous parameter space frequency space rather discrete atoms usually divide continuous parameter finite discrete grid point build dictionary grid point however actual target may exactly lie grid point matter densely parameter grided introduce mismatch predefined dictionary actual one article novel method namely adaptive match pursuit constrain total least square amp ctls propose find actual atoms even include initial dictionary amp ctls grid dictionary adaptively update better agree measurements convergence algorithm discuss numerical experiment demonstrate advantage amp ctls,96,9,1308.4273.txt
http://arxiv.org/abs/1308.5146,Compressive Multiplexing of Correlated Signals,"  We present a general architecture for the acquisition of ensembles of correlated signals. The signals are multiplexed onto a single line by mixing each one against a different code and then adding them together, and the resulting signal is sampled at a high rate. We show that if the $M$ signals, each bandlimited to $W/2$ Hz, can be approximated by a superposition of $R < M$ underlying signals, then the ensemble can be recovered by sampling at a rate within a logarithmic factor of $RW$ (as compared to the Nyquist rate of $MW$). This sampling theorem shows that the correlation structure of the signal ensemble can be exploited in the acquisition process even though it is unknown a priori.   The reconstruction of the ensemble is recast as a low-rank matrix recovery problem from linear measurements. The architectures we are considering impose a certain type of structure on the linear operators. Although our results depend on the mixing forms being random, this imposed structure results in a very different type of random projection than those analyzed in the low-rank recovery literature to date. ",Computer Science - Information Theory ; Statistics - Applications ; ,"Ahmed, Ali ; Romberg, Justin ; ","Compressive Multiplexing of Correlated Signals  We present a general architecture for the acquisition of ensembles of correlated signals. The signals are multiplexed onto a single line by mixing each one against a different code and then adding them together, and the resulting signal is sampled at a high rate. We show that if the $M$ signals, each bandlimited to $W/2$ Hz, can be approximated by a superposition of $R < M$ underlying signals, then the ensemble can be recovered by sampling at a rate within a logarithmic factor of $RW$ (as compared to the Nyquist rate of $MW$). This sampling theorem shows that the correlation structure of the signal ensemble can be exploited in the acquisition process even though it is unknown a priori.   The reconstruction of the ensemble is recast as a low-rank matrix recovery problem from linear measurements. The architectures we are considering impose a certain type of structure on the linear operators. Although our results depend on the mixing forms being random, this imposed structure results in a very different type of random projection than those analyzed in the low-rank recovery literature to date. ",compressive multiplexing correlate signal present general architecture acquisition ensembles correlate signal signal multiplexed onto single line mix one different code add together result signal sample high rate show signal bandlimited hz approximate superposition underlie signal ensemble recover sample rate within logarithmic factor rw compare nyquist rate mw sample theorem show correlation structure signal ensemble exploit acquisition process even though unknown priori reconstruction ensemble recast low rank matrix recovery problem linear measurements architectures consider impose certain type structure linear operators although result depend mix form random impose structure result different type random projection analyze low rank recovery literature date,98,9,1308.5146.txt
http://arxiv.org/abs/1308.6702,Adversarial hypothesis testing and a quantum Stein's Lemma for   restricted measurements,"  Recall the classical hypothesis testing setting with two convex sets of probability distributions P and Q. One receives either n i.i.d. samples from a distribution p in P or from a distribution q in Q and wants to decide from which set the points were sampled. It is known that the optimal exponential rate at which errors decrease can be achieved by a simple maximum-likelihood ratio test which does not depend on p or q, but only on the sets P and Q.   We consider an adaptive generalization of this model where the choice of p in P and q in Q can change in each sample in some way that depends arbitrarily on the previous samples. In other words, in the k'th round, an adversary, having observed all the previous samples in rounds 1,...,k-1, chooses p_k in P and q_k in Q, with the goal of confusing the hypothesis test. We prove that even in this case, the optimal exponential error rate can be achieved by a simple maximum-likelihood test that depends only on P and Q.   We then show that the adversarial model has applications in hypothesis testing for quantum states using restricted measurements. For example, it can be used to study the problem of distinguishing entangled states from the set of all separable states using only measurements that can be implemented with local operations and classical communication (LOCC). The basic idea is that in our setup, the deleterious effects of entanglement can be simulated by an adaptive classical adversary.   We prove a quantum Stein's Lemma in this setting: In many circumstances, the optimal hypothesis testing rate is equal to an appropriate notion of quantum relative entropy between two states. In particular, our arguments yield an alternate proof of Li and Winter's recent strengthening of strong subadditivity for quantum relative entropy. ",Computer Science - Information Theory ; Mathematics - Probability ; Quantum Physics ; ,"Brandao, Fernando G. S. L. ; Harrow, Aram W. ; Lee, James R. ; Peres, Yuval ; ","Adversarial hypothesis testing and a quantum Stein's Lemma for   restricted measurements  Recall the classical hypothesis testing setting with two convex sets of probability distributions P and Q. One receives either n i.i.d. samples from a distribution p in P or from a distribution q in Q and wants to decide from which set the points were sampled. It is known that the optimal exponential rate at which errors decrease can be achieved by a simple maximum-likelihood ratio test which does not depend on p or q, but only on the sets P and Q.   We consider an adaptive generalization of this model where the choice of p in P and q in Q can change in each sample in some way that depends arbitrarily on the previous samples. In other words, in the k'th round, an adversary, having observed all the previous samples in rounds 1,...,k-1, chooses p_k in P and q_k in Q, with the goal of confusing the hypothesis test. We prove that even in this case, the optimal exponential error rate can be achieved by a simple maximum-likelihood test that depends only on P and Q.   We then show that the adversarial model has applications in hypothesis testing for quantum states using restricted measurements. For example, it can be used to study the problem of distinguishing entangled states from the set of all separable states using only measurements that can be implemented with local operations and classical communication (LOCC). The basic idea is that in our setup, the deleterious effects of entanglement can be simulated by an adaptive classical adversary.   We prove a quantum Stein's Lemma in this setting: In many circumstances, the optimal hypothesis testing rate is equal to an appropriate notion of quantum relative entropy between two states. In particular, our arguments yield an alternate proof of Li and Winter's recent strengthening of strong subadditivity for quantum relative entropy. ",adversarial hypothesis test quantum stein lemma restrict measurements recall classical hypothesis test set two convex set probability distributions one receive either sample distribution distribution want decide set point sample know optimal exponential rate errors decrease achieve simple maximum likelihood ratio test depend set consider adaptive generalization model choice change sample way depend arbitrarily previous sample word th round adversary observe previous sample round choose goal confuse hypothesis test prove even case optimal exponential error rate achieve simple maximum likelihood test depend show adversarial model applications hypothesis test quantum state use restrict measurements example use study problem distinguish entangle state set separable state use measurements implement local operations classical communication locc basic idea setup deleterious effect entanglement simulate adaptive classical adversary prove quantum stein lemma set many circumstances optimal hypothesis test rate equal appropriate notion quantum relative entropy two state particular arguments yield alternate proof li winter recent strengthen strong subadditivity quantum relative entropy,153,12,1308.6702.txt
http://arxiv.org/abs/1309.0671,BayesOpt: A Library for Bayesian optimization with Robotics Applications,"  The purpose of this paper is twofold. On one side, we present a general framework for Bayesian optimization and we compare it with some related fields in active learning and Bayesian numerical analysis. On the other hand, Bayesian optimization and related problems (bandits, sequential experimental design) are highly dependent on the surrogate model that is selected. However, there is no clear standard in the literature. Thus, we present a fast and flexible toolbox that allows to test and combine different models and criteria with little effort. It includes most of the state-of-the-art contributions, algorithms and models. Its speed also removes part of the stigma that Bayesian optimization methods are only good for ""expensive functions"". The software is free and it can be used in many operating systems and computer languages. ",Computer Science - Robotics ; Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; Computer Science - Mathematical Software ; ,"Martinez-Cantin, Ruben ; ","BayesOpt: A Library for Bayesian optimization with Robotics Applications  The purpose of this paper is twofold. On one side, we present a general framework for Bayesian optimization and we compare it with some related fields in active learning and Bayesian numerical analysis. On the other hand, Bayesian optimization and related problems (bandits, sequential experimental design) are highly dependent on the surrogate model that is selected. However, there is no clear standard in the literature. Thus, we present a fast and flexible toolbox that allows to test and combine different models and criteria with little effort. It includes most of the state-of-the-art contributions, algorithms and models. Its speed also removes part of the stigma that Bayesian optimization methods are only good for ""expensive functions"". The software is free and it can be used in many operating systems and computer languages. ",bayesopt library bayesian optimization robotics applications purpose paper twofold one side present general framework bayesian optimization compare relate field active learn bayesian numerical analysis hand bayesian optimization relate problems bandits sequential experimental design highly dependent surrogate model select however clear standard literature thus present fast flexible toolbox allow test combine different model criteria little effort include state art contributions algorithms model speed also remove part stigma bayesian optimization methods good expensive function software free use many operate systems computer languages,80,11,1309.0671.txt
http://arxiv.org/abs/1309.2348,An Overview of Nominal-Typing versus Structural-Typing in OOP,"  NOOP is a mathematical model of nominally-typed OOP that proves the identification of inheritance and subtyping in mainstream nominally-typed OO programming languages and the validity of this identification. This report gives an overview of the main notions in OOP relevant to constructing a mathematical model of OOP such as NOOP. The emphasis in this report is on defining nominality, nominal typing and nominal subtyping of mainstream nominally-typed OO languages, and on contrasting the three notions with their counterparts in structurally-typed OO languages, i.e., with structurality, structural typing and structural subtyping, respectively. An additional appendix demonstrates these notions and other related notions, and the differences between them, using some simple code examples. A detailed, more technical comparison between nominal typing and structural typing in OOP is presented in other publications. ",Computer Science - Programming Languages ; ,"AbdelGawad, Moez A. ; ","An Overview of Nominal-Typing versus Structural-Typing in OOP  NOOP is a mathematical model of nominally-typed OOP that proves the identification of inheritance and subtyping in mainstream nominally-typed OO programming languages and the validity of this identification. This report gives an overview of the main notions in OOP relevant to constructing a mathematical model of OOP such as NOOP. The emphasis in this report is on defining nominality, nominal typing and nominal subtyping of mainstream nominally-typed OO languages, and on contrasting the three notions with their counterparts in structurally-typed OO languages, i.e., with structurality, structural typing and structural subtyping, respectively. An additional appendix demonstrates these notions and other related notions, and the differences between them, using some simple code examples. A detailed, more technical comparison between nominal typing and structural typing in OOP is presented in other publications. ",overview nominal type versus structural type oop noop mathematical model nominally type oop prove identification inheritance subtyping mainstream nominally type oo program languages validity identification report give overview main notions oop relevant construct mathematical model oop noop emphasis report define nominality nominal type nominal subtyping mainstream nominally type oo languages contrast three notions counterparts structurally type oo languages structurality structural type structural subtyping respectively additional appendix demonstrate notions relate notions differences use simple code examples detail technical comparison nominal type structural type oop present publications,85,8,1309.2348.txt
http://arxiv.org/abs/1309.3014,Hypercontractivity of spherical averages in Hamming space,"  Consider the linear space of functions on the binary hypercube and the linear operator $S_\delta$ acting by averaging a function over a Hamming sphere of radius $\delta n$ around every point. It is shown that this operator has a dimension-independent bound on the norm $L_p \to L_2$ with $p = 1+(1-2\delta)^2$. This result evidently parallels a classical estimate of Bonami and Gross for $L_p \to L_q$ norms for the operator of convolution with a Bernoulli noise. The estimate for $S_\delta$ is harder to obtain since the latter is neither a part of a semigroup, nor a tensor power. The result is shown by a detailed study of the eigenvalues of $S_\delta$ and $L_p\to L_2$ norms of the Fourier multiplier operators $\Pi_a$ with symbol equal to a characteristic function of the Hamming sphere of radius $a$ (in the notation common in boolean analysis $\Pi_a f=f^{=a}$, where $f^{=a}$ is a degree-$a$ component of function $f$). A sample application of the result is given: Any set $A\subset \FF_2^n$ with the property that $A+A$ contains a large portion of some Hamming sphere (counted with multiplicity) must have cardinality a constant multiple of $2^n$. ",Mathematics - Probability ; Computer Science - Information Theory ; Mathematics - Combinatorics ; Mathematics - Functional Analysis ; ,"Polyanskiy, Yury ; ","Hypercontractivity of spherical averages in Hamming space  Consider the linear space of functions on the binary hypercube and the linear operator $S_\delta$ acting by averaging a function over a Hamming sphere of radius $\delta n$ around every point. It is shown that this operator has a dimension-independent bound on the norm $L_p \to L_2$ with $p = 1+(1-2\delta)^2$. This result evidently parallels a classical estimate of Bonami and Gross for $L_p \to L_q$ norms for the operator of convolution with a Bernoulli noise. The estimate for $S_\delta$ is harder to obtain since the latter is neither a part of a semigroup, nor a tensor power. The result is shown by a detailed study of the eigenvalues of $S_\delta$ and $L_p\to L_2$ norms of the Fourier multiplier operators $\Pi_a$ with symbol equal to a characteristic function of the Hamming sphere of radius $a$ (in the notation common in boolean analysis $\Pi_a f=f^{=a}$, where $f^{=a}$ is a degree-$a$ component of function $f$). A sample application of the result is given: Any set $A\subset \FF_2^n$ with the property that $A+A$ contains a large portion of some Hamming sphere (counted with multiplicity) must have cardinality a constant multiple of $2^n$. ",hypercontractivity spherical average ham space consider linear space function binary hypercube linear operator delta act average function ham sphere radius delta around every point show operator dimension independent bind norm delta result evidently parallel classical estimate bonami gross norms operator convolution bernoulli noise estimate delta harder obtain since latter neither part semigroup tensor power result show detail study eigenvalues delta norms fourier multiplier operators pi symbol equal characteristic function ham sphere radius notation common boolean analysis pi degree component function sample application result give set subset ff property contain large portion ham sphere count multiplicity must cardinality constant multiple,99,7,1309.3014.txt
http://arxiv.org/abs/1309.3699,Local Support Vector Machines:Formulation and Analysis,"  We provide a formulation for Local Support Vector Machines (LSVMs) that generalizes previous formulations, and brings out the explicit connections to local polynomial learning used in nonparametric estimation literature. We investigate the simplest type of LSVMs called Local Linear Support Vector Machines (LLSVMs). For the first time we establish conditions under which LLSVMs make Bayes consistent predictions at each test point $x_0$. We also establish rates at which the local risk of LLSVMs converges to the minimum value of expected local risk at each point $x_0$. Using stability arguments we establish generalization error bounds for LLSVMs. ",Statistics - Machine Learning ; Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; ,"Ganti, Ravi ; Gray, Alexander ; ","Local Support Vector Machines:Formulation and Analysis  We provide a formulation for Local Support Vector Machines (LSVMs) that generalizes previous formulations, and brings out the explicit connections to local polynomial learning used in nonparametric estimation literature. We investigate the simplest type of LSVMs called Local Linear Support Vector Machines (LLSVMs). For the first time we establish conditions under which LLSVMs make Bayes consistent predictions at each test point $x_0$. We also establish rates at which the local risk of LLSVMs converges to the minimum value of expected local risk at each point $x_0$. Using stability arguments we establish generalization error bounds for LLSVMs. ",local support vector machine formulation analysis provide formulation local support vector machine lsvms generalize previous formulations bring explicit connections local polynomial learn use nonparametric estimation literature investigate simplest type lsvms call local linear support vector machine llsvms first time establish condition llsvms make bay consistent predictions test point also establish rat local risk llsvms converge minimum value expect local risk point use stability arguments establish generalization error bound llsvms,69,12,1309.3699.txt
http://arxiv.org/abs/1309.3701,New and simple algorithms for stable flow problems,"  Stable flows generalize the well-known concept of stable matchings to markets in which transactions may involve several agents, forwarding flow from one to another. An instance of the problem consists of a capacitated directed network, in which vertices express their preferences over their incident edges. A network flow is stable if there is no group of vertices that all could benefit from rerouting the flow along a walk.   Fleiner established that a stable flow always exists by reducing it to the stable allocation problem. We present an augmenting-path algorithm for computing a stable flow, the first algorithm that achieves polynomial running time for this problem without using stable allocation as a black-box subroutine. We further consider the problem of finding a stable flow such that the flow value on every edge is within a given interval. For this problem, we present an elegant graph transformation and based on this, we devise a simple and fast algorithm, which also can be used to find a solution to the stable marriage problem with forced and forbidden edges.   Finally, we study the stable multicommodity flow model introduced by Kir\'{a}ly and Pap. The original model is highly involved and allows for commodity-dependent preference lists at the vertices and commodity-specific edge capacities. We present several graph-based reductions that show equivalence to a significantly simpler model. We further show that it is NP-complete to decide whether an integral solution exists. ",Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; ,"Cseh, Ágnes ; Matuschke, Jannik ; ","New and simple algorithms for stable flow problems  Stable flows generalize the well-known concept of stable matchings to markets in which transactions may involve several agents, forwarding flow from one to another. An instance of the problem consists of a capacitated directed network, in which vertices express their preferences over their incident edges. A network flow is stable if there is no group of vertices that all could benefit from rerouting the flow along a walk.   Fleiner established that a stable flow always exists by reducing it to the stable allocation problem. We present an augmenting-path algorithm for computing a stable flow, the first algorithm that achieves polynomial running time for this problem without using stable allocation as a black-box subroutine. We further consider the problem of finding a stable flow such that the flow value on every edge is within a given interval. For this problem, we present an elegant graph transformation and based on this, we devise a simple and fast algorithm, which also can be used to find a solution to the stable marriage problem with forced and forbidden edges.   Finally, we study the stable multicommodity flow model introduced by Kir\'{a}ly and Pap. The original model is highly involved and allows for commodity-dependent preference lists at the vertices and commodity-specific edge capacities. We present several graph-based reductions that show equivalence to a significantly simpler model. We further show that it is NP-complete to decide whether an integral solution exists. ",new simple algorithms stable flow problems stable flow generalize well know concept stable match market transactions may involve several agents forward flow one another instance problem consist capacitate direct network vertices express preferences incident edge network flow stable group vertices could benefit rerouting flow along walk fleiner establish stable flow always exist reduce stable allocation problem present augment path algorithm compute stable flow first algorithm achieve polynomial run time problem without use stable allocation black box subroutine consider problem find stable flow flow value every edge within give interval problem present elegant graph transformation base devise simple fast algorithm also use find solution stable marriage problem force forbid edge finally study stable multicommodity flow model introduce kir ly pap original model highly involve allow commodity dependent preference list vertices commodity specific edge capacities present several graph base reductions show equivalence significantly simpler model show np complete decide whether integral solution exist,151,4,1309.3701.txt
http://arxiv.org/abs/1309.3730,Automatically Extracting Instances of Code Change Patterns with AST   Analysis,"  A code change pattern represents a kind of recurrent modification in software. For instance, a known code change pattern consists of the change of the conditional expression of an if statement. Previous work has identified different change patterns. Complementary to the identification and definition of change patterns, the automatic extraction of pattern instances is essential to measure their empirical importance. For example, it enables one to count and compare the number of conditional expression changes in the history of different projects. In this paper we present a novel approach for search patterns instances from software history. Our technique is based on the analysis of Abstract Syntax Trees (AST) files within a given commit. We validate our approach by counting instances of 18 change patterns in 6 open-source Java projects. ",Computer Science - Software Engineering ; ,"Martinez, Matias ; Duchien, Laurence ; Monperrus, Martin ; ","Automatically Extracting Instances of Code Change Patterns with AST   Analysis  A code change pattern represents a kind of recurrent modification in software. For instance, a known code change pattern consists of the change of the conditional expression of an if statement. Previous work has identified different change patterns. Complementary to the identification and definition of change patterns, the automatic extraction of pattern instances is essential to measure their empirical importance. For example, it enables one to count and compare the number of conditional expression changes in the history of different projects. In this paper we present a novel approach for search patterns instances from software history. Our technique is based on the analysis of Abstract Syntax Trees (AST) files within a given commit. We validate our approach by counting instances of 18 change patterns in 6 open-source Java projects. ",automatically extract instance code change pattern ast analysis code change pattern represent kind recurrent modification software instance know code change pattern consist change conditional expression statement previous work identify different change pattern complementary identification definition change pattern automatic extraction pattern instance essential measure empirical importance example enable one count compare number conditional expression change history different project paper present novel approach search pattern instance software history technique base analysis abstract syntax tree ast file within give commit validate approach count instance change pattern open source java project,87,10,1309.3730.txt
http://arxiv.org/abs/1309.4958,Approximation of smallest linear tree grammar,"  A simple linear-time algorithm for constructing a linear context-free tree grammar of size O(rg + r g log (n/r g))for a given input tree T of size n is presented, where g is the size of a minimal linear context-free tree grammar for T, and r is the maximal rank of symbols in T (which is a constant in many applications). This is the first example of a grammar-based tree compression algorithm with a good, i.e. logarithmic in terms of the size of the input tree, approximation ratio. The analysis of the algorithm uses an extension of the recompression technique from strings to trees. ",Computer Science - Data Structures and Algorithms ; Computer Science - Formal Languages and Automata Theory ; F.4.2 ; F.2.2 ; E.4 ; ,"Jeż, Artur ; Lohrey, Markus ; ","Approximation of smallest linear tree grammar  A simple linear-time algorithm for constructing a linear context-free tree grammar of size O(rg + r g log (n/r g))for a given input tree T of size n is presented, where g is the size of a minimal linear context-free tree grammar for T, and r is the maximal rank of symbols in T (which is a constant in many applications). This is the first example of a grammar-based tree compression algorithm with a good, i.e. logarithmic in terms of the size of the input tree, approximation ratio. The analysis of the algorithm uses an extension of the recompression technique from strings to trees. ",approximation smallest linear tree grammar simple linear time algorithm construct linear context free tree grammar size rg log give input tree size present size minimal linear context free tree grammar maximal rank symbols constant many applications first example grammar base tree compression algorithm good logarithmic term size input tree approximation ratio analysis algorithm use extension recompression technique string tree,59,1,1309.4958.txt
http://arxiv.org/abs/1309.5310,Conditioning of Random Block Subdictionaries with Applications to   Block-Sparse Recovery and Regression,"  The linear model, in which a set of observations is assumed to be given by a linear combination of columns of a matrix, has long been the mainstay of the statistics and signal processing literature. One particular challenge for inference under linear models is understanding the conditions on the dictionary under which reliable inference is possible. This challenge has attracted renewed attention in recent years since many modern inference problems deal with the ""underdetermined"" setting, in which the number of observations is much smaller than the number of columns in the dictionary. This paper makes several contributions for this setting when the set of observations is given by a linear combination of a small number of groups of columns of the dictionary, termed the ""block-sparse"" case. First, it specifies conditions on the dictionary under which most block subdictionaries are well conditioned. This result is fundamentally different from prior work on block-sparse inference because (i) it provides conditions that can be explicitly computed in polynomial time, (ii) the given conditions translate into near-optimal scaling of the number of columns of the block subdictionaries as a function of the number of observations for a large class of dictionaries, and (iii) it suggests that the spectral norm and the quadratic-mean block coherence of the dictionary (rather than the worst-case coherences) fundamentally limit the scaling of dimensions of the well-conditioned block subdictionaries. Second, this paper investigates the problems of block-sparse recovery and block-sparse regression in underdetermined settings. Near-optimal block-sparse recovery and regression are possible for certain dictionaries as long as the dictionary satisfies easily computable conditions and the coefficients describing the linear combination of groups of columns can be modeled through a mild statistical prior. ",Mathematics - Statistics Theory ; Computer Science - Information Theory ; ,"Bajwa, Waheed U. ; Duarte, Marco F. ; Calderbank, Robert ; ","Conditioning of Random Block Subdictionaries with Applications to   Block-Sparse Recovery and Regression  The linear model, in which a set of observations is assumed to be given by a linear combination of columns of a matrix, has long been the mainstay of the statistics and signal processing literature. One particular challenge for inference under linear models is understanding the conditions on the dictionary under which reliable inference is possible. This challenge has attracted renewed attention in recent years since many modern inference problems deal with the ""underdetermined"" setting, in which the number of observations is much smaller than the number of columns in the dictionary. This paper makes several contributions for this setting when the set of observations is given by a linear combination of a small number of groups of columns of the dictionary, termed the ""block-sparse"" case. First, it specifies conditions on the dictionary under which most block subdictionaries are well conditioned. This result is fundamentally different from prior work on block-sparse inference because (i) it provides conditions that can be explicitly computed in polynomial time, (ii) the given conditions translate into near-optimal scaling of the number of columns of the block subdictionaries as a function of the number of observations for a large class of dictionaries, and (iii) it suggests that the spectral norm and the quadratic-mean block coherence of the dictionary (rather than the worst-case coherences) fundamentally limit the scaling of dimensions of the well-conditioned block subdictionaries. Second, this paper investigates the problems of block-sparse recovery and block-sparse regression in underdetermined settings. Near-optimal block-sparse recovery and regression are possible for certain dictionaries as long as the dictionary satisfies easily computable conditions and the coefficients describing the linear combination of groups of columns can be modeled through a mild statistical prior. ",condition random block subdictionaries applications block sparse recovery regression linear model set observations assume give linear combination columns matrix long mainstay statistics signal process literature one particular challenge inference linear model understand condition dictionary reliable inference possible challenge attract renew attention recent years since many modern inference problems deal underdetermined set number observations much smaller number columns dictionary paper make several contributions set set observations give linear combination small number group columns dictionary term block sparse case first specify condition dictionary block subdictionaries well condition result fundamentally different prior work block sparse inference provide condition explicitly compute polynomial time ii give condition translate near optimal scale number columns block subdictionaries function number observations large class dictionaries iii suggest spectral norm quadratic mean block coherence dictionary rather worst case coherences fundamentally limit scale dimension well condition block subdictionaries second paper investigate problems block sparse recovery block sparse regression underdetermined settings near optimal block sparse recovery regression possible certain dictionaries long dictionary satisfy easily computable condition coefficients describe linear combination group columns model mild statistical prior,174,9,1309.5310.txt
http://arxiv.org/abs/1309.5568,Integrating Communications and Merging Messaging via the eXtensible   Messaging and Presence Protocol,"  Common problems affecting modern email usage include spam, lack of sender verification, lack of built-in security and lack of message integrity. This paper looks at how we can utilise the extensible messaging and presence protocol also known as XMPP to, in time, replace email facilities. We present several methods for initiating a transition away from SMTP for email to rely upon the inherent benefits of XMPP with minimal disruption to existing networks and email infrastructure. We look at how a program might be used to open an existing POP3/IMAP account, scan for messages that can be sent to a XMPP network user, extract the message and then deliver it the XMPP user's client. We show that the system can be implemented and then deployed with a minimum of hassle and network disruption to demonstrate XMPP as a reliable and fast replacement for email as we know it today. ",Computer Science - Networking and Internet Architecture ; ,"Coleman, Martin A. ; ","Integrating Communications and Merging Messaging via the eXtensible   Messaging and Presence Protocol  Common problems affecting modern email usage include spam, lack of sender verification, lack of built-in security and lack of message integrity. This paper looks at how we can utilise the extensible messaging and presence protocol also known as XMPP to, in time, replace email facilities. We present several methods for initiating a transition away from SMTP for email to rely upon the inherent benefits of XMPP with minimal disruption to existing networks and email infrastructure. We look at how a program might be used to open an existing POP3/IMAP account, scan for messages that can be sent to a XMPP network user, extract the message and then deliver it the XMPP user's client. We show that the system can be implemented and then deployed with a minimum of hassle and network disruption to demonstrate XMPP as a reliable and fast replacement for email as we know it today. ",integrate communications merge message via extensible message presence protocol common problems affect modern email usage include spam lack sender verification lack build security lack message integrity paper look utilise extensible message presence protocol also know xmpp time replace email facilities present several methods initiate transition away smtp email rely upon inherent benefit xmpp minimal disruption exist network email infrastructure look program might use open exist pop imap account scan message send xmpp network user extract message deliver xmpp user client show system implement deploy minimum hassle network disruption demonstrate xmpp reliable fast replacement email know today,96,6,1309.5568.txt
http://arxiv.org/abs/1309.6610,Adversarial Multiple Access Channels with Individual Injection Rates,"  We study deterministic distributed broadcasting in synchronous multiple-access channels. Packets are injected into $n$ nodes by a window-type adversary that is constrained by a window $w$ and injection rates individually assigned to all nodes. We investigate what queue size and packet latency can be achieved with the maximum aggregate injection rate of one packet per round, depending on properties of channels and algorithms. We give a non-adaptive algorithm for channels with collision detection and an adaptive algorithm for channels without collision detection that achieve $O(\min(n+w,w\log n))$ packet latency. We show that packet latency has to be either $\Omega(w \max (1,\log_w n))$, when $w\le n$, or $\Omega(w+n)$, when $w>n$, as a matching lower bound to these algorithms. We develop a non-adaptive algorithm for channels without collision detection that achieves $O(n+w)$ queue size and $O(nw)$ packet latency. This is in contrast with the adversarial model of global injection rates, in which non-adaptive algorithms with bounded packet latency do not exist (Chlebus et al. Distributed Computing 22(2): 93 - 116, 2009). Our algorithm avoids collisions produced by simultaneous transmissions; we show that any algorithm with this property must have $\Omega(nw)$ packet latency. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Networking and Internet Architecture ; ","Anantharamu, Lakshmi ; Chlebus, Bogdan S. ; Rokicki, Mariusz A. ; ","Adversarial Multiple Access Channels with Individual Injection Rates  We study deterministic distributed broadcasting in synchronous multiple-access channels. Packets are injected into $n$ nodes by a window-type adversary that is constrained by a window $w$ and injection rates individually assigned to all nodes. We investigate what queue size and packet latency can be achieved with the maximum aggregate injection rate of one packet per round, depending on properties of channels and algorithms. We give a non-adaptive algorithm for channels with collision detection and an adaptive algorithm for channels without collision detection that achieve $O(\min(n+w,w\log n))$ packet latency. We show that packet latency has to be either $\Omega(w \max (1,\log_w n))$, when $w\le n$, or $\Omega(w+n)$, when $w>n$, as a matching lower bound to these algorithms. We develop a non-adaptive algorithm for channels without collision detection that achieves $O(n+w)$ queue size and $O(nw)$ packet latency. This is in contrast with the adversarial model of global injection rates, in which non-adaptive algorithms with bounded packet latency do not exist (Chlebus et al. Distributed Computing 22(2): 93 - 116, 2009). Our algorithm avoids collisions produced by simultaneous transmissions; we show that any algorithm with this property must have $\Omega(nw)$ packet latency. ",adversarial multiple access channel individual injection rat study deterministic distribute broadcast synchronous multiple access channel packets inject nod window type adversary constrain window injection rat individually assign nod investigate queue size packet latency achieve maximum aggregate injection rate one packet per round depend properties channel algorithms give non adaptive algorithm channel collision detection adaptive algorithm channel without collision detection achieve min log packet latency show packet latency either omega max log le omega match lower bind algorithms develop non adaptive algorithm channel without collision detection achieve queue size nw packet latency contrast adversarial model global injection rat non adaptive algorithms bound packet latency exist chlebus et al distribute compute algorithm avoid collisions produce simultaneous transmissions show algorithm property must omega nw packet latency,123,12,1309.6610.txt
http://arxiv.org/abs/1309.6838,Inverse Covariance Estimation for High-Dimensional Data in Linear Time   and Space: Spectral Methods for Riccati and Sparse Models,"  We propose maximum likelihood estimation for learning Gaussian graphical models with a Gaussian (ell_2^2) prior on the parameters. This is in contrast to the commonly used Laplace (ell_1) prior for encouraging sparseness. We show that our optimization problem leads to a Riccati matrix equation, which has a closed form solution. We propose an efficient algorithm that performs a singular value decomposition of the training data. Our algorithm is O(NT^2)-time and O(NT)-space for N variables and T samples. Our method is tailored to high-dimensional problems (N gg T), in which sparseness promoting methods become intractable. Furthermore, instead of obtaining a single solution for a specific regularization parameter, our algorithm finds the whole solution path. We show that the method has logarithmic sample complexity under the spiked covariance model. We also propose sparsification of the dense solution with provable performance guarantees. We provide techniques for using our learnt models, such as removing unimportant variables, computing likelihoods and conditional distributions. Finally, we show promising results in several gene expressions datasets. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Honorio, Jean ; Jaakkola, Tommi S. ; ","Inverse Covariance Estimation for High-Dimensional Data in Linear Time   and Space: Spectral Methods for Riccati and Sparse Models  We propose maximum likelihood estimation for learning Gaussian graphical models with a Gaussian (ell_2^2) prior on the parameters. This is in contrast to the commonly used Laplace (ell_1) prior for encouraging sparseness. We show that our optimization problem leads to a Riccati matrix equation, which has a closed form solution. We propose an efficient algorithm that performs a singular value decomposition of the training data. Our algorithm is O(NT^2)-time and O(NT)-space for N variables and T samples. Our method is tailored to high-dimensional problems (N gg T), in which sparseness promoting methods become intractable. Furthermore, instead of obtaining a single solution for a specific regularization parameter, our algorithm finds the whole solution path. We show that the method has logarithmic sample complexity under the spiked covariance model. We also propose sparsification of the dense solution with provable performance guarantees. We provide techniques for using our learnt models, such as removing unimportant variables, computing likelihoods and conditional distributions. Finally, we show promising results in several gene expressions datasets. ",inverse covariance estimation high dimensional data linear time space spectral methods riccati sparse model propose maximum likelihood estimation learn gaussian graphical model gaussian ell prior parameters contrast commonly use laplace ell prior encourage sparseness show optimization problem lead riccati matrix equation close form solution propose efficient algorithm perform singular value decomposition train data algorithm nt time nt space variables sample method tailor high dimensional problems gg sparseness promote methods become intractable furthermore instead obtain single solution specific regularization parameter algorithm find whole solution path show method logarithmic sample complexity spike covariance model also propose sparsification dense solution provable performance guarantee provide techniques use learn model remove unimportant variables compute likelihoods conditional distributions finally show promise result several gene expressions datasets,120,9,1309.6838.txt
http://arxiv.org/abs/1309.6927,Inclusion-exclusion enhanced by nerve stimulation,"  When evaluating the lengthy inclusion-exclusion expansion many of its terms may turn out to be zero, and hence should be discarded beforehand. Often this can be done. The main idea is that the index sets of nonzero terms constitute a set ideal (called the 'nerve'), which often can be encoded in a compact way (Upgrade B). As a further enhancement (Upgrade A), equal nonzero terms can sometimes be efficiently collected. ",Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Wild, Marcel ; ","Inclusion-exclusion enhanced by nerve stimulation  When evaluating the lengthy inclusion-exclusion expansion many of its terms may turn out to be zero, and hence should be discarded beforehand. Often this can be done. The main idea is that the index sets of nonzero terms constitute a set ideal (called the 'nerve'), which often can be encoded in a compact way (Upgrade B). As a further enhancement (Upgrade A), equal nonzero terms can sometimes be efficiently collected. ",inclusion exclusion enhance nerve stimulation evaluate lengthy inclusion exclusion expansion many term may turn zero hence discard beforehand often do main idea index set nonzero term constitute set ideal call nerve often encode compact way upgrade enhancement upgrade equal nonzero term sometimes efficiently collect,44,8,1309.6927.txt
http://arxiv.org/abs/1310.0441,Countering Wrapping Attack on XML Signature in SOAP Message for Cloud   Computing,"  It is known that the exchange of information between web applications is done by means of the SOAP protocol. Securing this protocol is obviously a vital issue for any computer network. However, when it comes to cloud computing systems, the sensitivity of this issue rises, as the clients of system, release their data to the cloud. XML signature is employed to secure SOAP messages. However, there are also some weak points that have been identified, named as XML signature wrapping attacks, which have been categorized into four major groups; Simple Ancestry Context Attack, Optional element context attacks, Sibling Value Context Attack, Sibling Order Context. In this paper, two existing methods, for referencing the signed part of SOAP Message, named as ID referencing and XPath method, are analyzed and examined. In addition, a new method is proposed and tested, to secure the SOAP message. In the new method, the XML any signature wrapping attack is prevented by employing the concept of XML digital signature on the SOAP message. The results of conducted experiments show that the proposed method is approximately three times faster than the XPath method and even a little faster than ID. ",Computer Science - Cryptography and Security ; ,"Kouchaksaraei, Hadi Razzaghi ; Chefranov, Alexander G. ; ","Countering Wrapping Attack on XML Signature in SOAP Message for Cloud   Computing  It is known that the exchange of information between web applications is done by means of the SOAP protocol. Securing this protocol is obviously a vital issue for any computer network. However, when it comes to cloud computing systems, the sensitivity of this issue rises, as the clients of system, release their data to the cloud. XML signature is employed to secure SOAP messages. However, there are also some weak points that have been identified, named as XML signature wrapping attacks, which have been categorized into four major groups; Simple Ancestry Context Attack, Optional element context attacks, Sibling Value Context Attack, Sibling Order Context. In this paper, two existing methods, for referencing the signed part of SOAP Message, named as ID referencing and XPath method, are analyzed and examined. In addition, a new method is proposed and tested, to secure the SOAP message. In the new method, the XML any signature wrapping attack is prevented by employing the concept of XML digital signature on the SOAP message. The results of conducted experiments show that the proposed method is approximately three times faster than the XPath method and even a little faster than ID. ",counter wrap attack xml signature soap message cloud compute know exchange information web applications do mean soap protocol secure protocol obviously vital issue computer network however come cloud compute systems sensitivity issue rise clients system release data cloud xml signature employ secure soap message however also weak point identify name xml signature wrap attack categorize four major group simple ancestry context attack optional element context attack sibling value context attack sibling order context paper two exist methods reference sign part soap message name id reference xpath method analyze examine addition new method propose test secure soap message new method xml signature wrap attack prevent employ concept xml digital signature soap message result conduct experiment show propose method approximately three time faster xpath method even little faster id,127,4,1310.0441.txt
http://arxiv.org/abs/1310.0833,Flips in combinatorial pointed pseudo-triangulations with face degree at   most four,"  In this paper we consider the flip operation for combinatorial pointed pseudo-triangulations where faces have size 3 or 4, so-called combinatorial 4-PPTs. We show that every combinatorial 4-PPT is stretchable to a geometric pseudo-triangulation, which in general is not the case if faces may have size larger than 4. Moreover, we prove that the flip graph of combinatorial 4-PPTs is connected and has diameter $O(n^2)$, even in the case of labeled vertices with fixed outer face. For this case we provide an $\Omega(n\log n)$ lower bound. ",Mathematics - Combinatorics ; Computer Science - Computational Geometry ; Computer Science - Discrete Mathematics ; ,"Aichholzer, Oswin ; Hackl, Thomas ; Orden, David ; Pilz, Alexander ; Saumell, Maria ; Vogtenhuber, Birgit ; ","Flips in combinatorial pointed pseudo-triangulations with face degree at   most four  In this paper we consider the flip operation for combinatorial pointed pseudo-triangulations where faces have size 3 or 4, so-called combinatorial 4-PPTs. We show that every combinatorial 4-PPT is stretchable to a geometric pseudo-triangulation, which in general is not the case if faces may have size larger than 4. Moreover, we prove that the flip graph of combinatorial 4-PPTs is connected and has diameter $O(n^2)$, even in the case of labeled vertices with fixed outer face. For this case we provide an $\Omega(n\log n)$ lower bound. ",flip combinatorial point pseudo triangulations face degree four paper consider flip operation combinatorial point pseudo triangulations face size call combinatorial ppts show every combinatorial ppt stretchable geometric pseudo triangulation general case face may size larger moreover prove flip graph combinatorial ppts connect diameter even case label vertices fix outer face case provide omega log lower bind,56,4,1310.0833.txt
http://arxiv.org/abs/1310.1250,Learning ambiguous functions by neural networks,"  It is not, in general, possible to have access to all variables that determine the behavior of a system. Having identified a number of variables whose values can be accessed, there may still be hidden variables which influence the dynamics of the system. The result is model ambiguity in the sense that, for the same (or very similar) input values, different objective outputs should have been obtained. In addition, the degree of ambiguity may vary widely across the whole range of input values. Thus, to evaluate the accuracy of a model it is of utmost importance to create a method to obtain the degree of reliability of each output result. In this paper we present such a scheme composed of two coupled artificial neural networks: the first one being responsible for outputting the predicted value, whereas the other evaluates the reliability of the output, which is learned from the error values of the first one. As an illustration, the scheme is applied to a model for tracking slopes in a straw chamber and to a credit scoring model. ","Computer Science - Neural and Evolutionary Computing ; Computer Science - Machine Learning ; Physics - Data Analysis, Statistics and Probability ; 68T37, 82C32 ; I.2.6 ; I.5.1 ; I.5.5 ; ","Ligeiro, Rui ; Mendes, R. Vilela ; ","Learning ambiguous functions by neural networks  It is not, in general, possible to have access to all variables that determine the behavior of a system. Having identified a number of variables whose values can be accessed, there may still be hidden variables which influence the dynamics of the system. The result is model ambiguity in the sense that, for the same (or very similar) input values, different objective outputs should have been obtained. In addition, the degree of ambiguity may vary widely across the whole range of input values. Thus, to evaluate the accuracy of a model it is of utmost importance to create a method to obtain the degree of reliability of each output result. In this paper we present such a scheme composed of two coupled artificial neural networks: the first one being responsible for outputting the predicted value, whereas the other evaluates the reliability of the output, which is learned from the error values of the first one. As an illustration, the scheme is applied to a model for tracking slopes in a straw chamber and to a credit scoring model. ",learn ambiguous function neural network general possible access variables determine behavior system identify number variables whose value access may still hide variables influence dynamics system result model ambiguity sense similar input value different objective output obtain addition degree ambiguity may vary widely across whole range input value thus evaluate accuracy model utmost importance create method obtain degree reliability output result paper present scheme compose two couple artificial neural network first one responsible output predict value whereas evaluate reliability output learn error value first one illustration scheme apply model track slop straw chamber credit score model,95,6,1310.1250.txt
http://arxiv.org/abs/1310.1861,Physical-Layer Cryptography Through Massive MIMO,"  We propose the new technique of physical-layer cryptography based on using a massive MIMO channel as a key between the sender and desired receiver, which need not be secret. The goal is for low-complexity encoding and decoding by the desired transmitter-receiver pair, whereas decoding by an eavesdropper is hard in terms of prohibitive complexity. The decoding complexity is analyzed by mapping the massive MIMO system to a lattice. We show that the eavesdropper's decoder for the MIMO system with M-PAM modulation is equivalent to solving standard lattice problems that are conjectured to be of exponential complexity for both classical and quantum computers. Hence, under the widely-held conjecture that standard lattice problems are hard to solve in the worst-case, the proposed encryption scheme has a more robust notion of security than that of the most common encryption methods used today such as RSA and Diffie-Hellman. Additionally, we show that this scheme could be used to securely communicate without a pre-shared secret and little computational overhead. Thus, by exploiting the physical layer properties of the radio channel, the massive MIMO system provides for low-complexity encryption commensurate with the most sophisticated forms of application-layer encryption that are currently known. ",Computer Science - Information Theory ; Computer Science - Cryptography and Security ; ,"Dean, Thomas ; Goldsmith, Andrea ; ","Physical-Layer Cryptography Through Massive MIMO  We propose the new technique of physical-layer cryptography based on using a massive MIMO channel as a key between the sender and desired receiver, which need not be secret. The goal is for low-complexity encoding and decoding by the desired transmitter-receiver pair, whereas decoding by an eavesdropper is hard in terms of prohibitive complexity. The decoding complexity is analyzed by mapping the massive MIMO system to a lattice. We show that the eavesdropper's decoder for the MIMO system with M-PAM modulation is equivalent to solving standard lattice problems that are conjectured to be of exponential complexity for both classical and quantum computers. Hence, under the widely-held conjecture that standard lattice problems are hard to solve in the worst-case, the proposed encryption scheme has a more robust notion of security than that of the most common encryption methods used today such as RSA and Diffie-Hellman. Additionally, we show that this scheme could be used to securely communicate without a pre-shared secret and little computational overhead. Thus, by exploiting the physical layer properties of the radio channel, the massive MIMO system provides for low-complexity encryption commensurate with the most sophisticated forms of application-layer encryption that are currently known. ",physical layer cryptography massive mimo propose new technique physical layer cryptography base use massive mimo channel key sender desire receiver need secret goal low complexity encode decode desire transmitter receiver pair whereas decode eavesdropper hard term prohibitive complexity decode complexity analyze map massive mimo system lattice show eavesdropper decoder mimo system pam modulation equivalent solve standard lattice problems conjecture exponential complexity classical quantum computers hence widely hold conjecture standard lattice problems hard solve worst case propose encryption scheme robust notion security common encryption methods use today rsa diffie hellman additionally show scheme could use securely communicate without pre share secret little computational overhead thus exploit physical layer properties radio channel massive mimo system provide low complexity encryption commensurate sophisticate form application layer encryption currently know,125,9,1310.1861.txt
http://arxiv.org/abs/1310.2728,The asymptotic $k$-SAT threshold,"  Since the early 2000s physicists have developed an ingenious but non-rigorous formalism called the cavity method to put forward precise conjectures on phase transitions in random problems [Mezard, Parisi, Zecchina: Science 2002]. The cavity method predicts that the satisfiability threshold in the random $k$-SAT problem is $2^k\ln2-\frac12(1+\ln 2)+\epsilon_k$, with $\lim_{k\rightarrow\infty}\epsilon_k=0$ [Mertens, Mezard, Zecchina: Random Structures and Algorithms 2006]. This paper contains a proof of that conjecture. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; Mathematics - Probability ; 05C80 ; ,"Coja-Oghlan, Amin ; Panagiotou, Konstantinos ; ","The asymptotic $k$-SAT threshold  Since the early 2000s physicists have developed an ingenious but non-rigorous formalism called the cavity method to put forward precise conjectures on phase transitions in random problems [Mezard, Parisi, Zecchina: Science 2002]. The cavity method predicts that the satisfiability threshold in the random $k$-SAT problem is $2^k\ln2-\frac12(1+\ln 2)+\epsilon_k$, with $\lim_{k\rightarrow\infty}\epsilon_k=0$ [Mertens, Mezard, Zecchina: Random Structures and Algorithms 2006]. This paper contains a proof of that conjecture. ",asymptotic sit threshold since early physicists develop ingenious non rigorous formalism call cavity method put forward precise conjecture phase transition random problems mezard parisi zecchina science cavity method predict satisfiability threshold random sit problem ln frac ln epsilon lim rightarrow infty epsilon mertens mezard zecchina random structure algorithms paper contain proof conjecture,52,8,1310.2728.txt
http://arxiv.org/abs/1310.3389,Spectra of random networks in the weak clustering regime,"  The asymptotic behaviour of dynamical processes in networks can be expressed as a function of spectral properties of the corresponding adjacency and Laplacian matrices. Although many theoretical results are known for the spectra of traditional configuration models, networks generated through these models fail to describe many topological features of real-world networks, in particular non-null values of the clustering coefficient. Here we study effects of cycles of order three (triangles) in network spectra. By using recent advances in random matrix theory, we determine the spectral distribution of the network adjacency matrix as a function of the average number of triangles attached to each node for networks without modular structure and degree-degree correlations. Implications to network dynamics are discussed. Our findings can shed light in the study of how particular kinds of subgraphs influence network dynamics. ",Physics - Physics and Society ; Condensed Matter - Statistical Mechanics ; Computer Science - Social and Information Networks ; ,"Peron, Thomas K. DM. ; Ji, Peng ; Kurths, Jürgen ; Rodrigues, Francisco A. ; ","Spectra of random networks in the weak clustering regime  The asymptotic behaviour of dynamical processes in networks can be expressed as a function of spectral properties of the corresponding adjacency and Laplacian matrices. Although many theoretical results are known for the spectra of traditional configuration models, networks generated through these models fail to describe many topological features of real-world networks, in particular non-null values of the clustering coefficient. Here we study effects of cycles of order three (triangles) in network spectra. By using recent advances in random matrix theory, we determine the spectral distribution of the network adjacency matrix as a function of the average number of triangles attached to each node for networks without modular structure and degree-degree correlations. Implications to network dynamics are discussed. Our findings can shed light in the study of how particular kinds of subgraphs influence network dynamics. ",spectra random network weak cluster regime asymptotic behaviour dynamical process network express function spectral properties correspond adjacency laplacian matrices although many theoretical result know spectra traditional configuration model network generate model fail describe many topological feature real world network particular non null value cluster coefficient study effect cycle order three triangles network spectra use recent advance random matrix theory determine spectral distribution network adjacency matrix function average number triangles attach node network without modular structure degree degree correlations implications network dynamics discuss find shed light study particular kinds subgraphs influence network dynamics,92,6,1310.3389.txt
http://arxiv.org/abs/1310.4345,Moser's Shadow Problem,"  Moser's shadow problem asks to estimate the shadow function $\mathfrak{s}_b(n)$, which is the largest number such that for each bounded convex polyhedron $P$ with $n$ vertices in $3$-space there is some direction ${\bf v}$ (depending on $P$) such that, when illuminated by parallel light rays from infinity in direction ${\bf v}$, the polyhedron casts a shadow having at least $\mathfrak{s}_b(n)$ vertices. A general version of the problem allows unbounded polyhedra as well, and has associated shadow function $\mathfrak{s}_u(n)$. This paper presents correct order of magnitude asymptotic bounds on these functions. The bounded case has answer $\mathfrak{s}_b(n) = \Theta \big( \log (n)/ (\log(\log (n))\big$. The unbounded shadow problem is shown to have the different asymptotic growth rate $\mathfrak{s}_u(n) = \Theta \big(1\big)$. Results on the bounded shadow problem follow from 1989 work of Chazelle, Edelsbrunner and Guibas on the (bounded) silhouette span number $\mathfrak{s}_b^{\ast}(n)$, defined analogously but with arbitrary light sources. We complete the picture by showing that the unbounded silhouette span number $\mathfrak{s}_u^{\ast}(n)$ grows as $\Theta \big( \log (n)/ (\log(\log (n))\big)$. ","Mathematics - Metric Geometry ; Computer Science - Computational Geometry ; Primary: 52B10, Secondary: 51N15, 65D18, 68U05, 90C05 ; ","Lagarias, Jeffrey C. ; Luo, Yusheng ; Padrol, Arnau ; ","Moser's Shadow Problem  Moser's shadow problem asks to estimate the shadow function $\mathfrak{s}_b(n)$, which is the largest number such that for each bounded convex polyhedron $P$ with $n$ vertices in $3$-space there is some direction ${\bf v}$ (depending on $P$) such that, when illuminated by parallel light rays from infinity in direction ${\bf v}$, the polyhedron casts a shadow having at least $\mathfrak{s}_b(n)$ vertices. A general version of the problem allows unbounded polyhedra as well, and has associated shadow function $\mathfrak{s}_u(n)$. This paper presents correct order of magnitude asymptotic bounds on these functions. The bounded case has answer $\mathfrak{s}_b(n) = \Theta \big( \log (n)/ (\log(\log (n))\big$. The unbounded shadow problem is shown to have the different asymptotic growth rate $\mathfrak{s}_u(n) = \Theta \big(1\big)$. Results on the bounded shadow problem follow from 1989 work of Chazelle, Edelsbrunner and Guibas on the (bounded) silhouette span number $\mathfrak{s}_b^{\ast}(n)$, defined analogously but with arbitrary light sources. We complete the picture by showing that the unbounded silhouette span number $\mathfrak{s}_u^{\ast}(n)$ grows as $\Theta \big( \log (n)/ (\log(\log (n))\big)$. ",moser shadow problem moser shadow problem ask estimate shadow function mathfrak largest number bound convex polyhedron vertices space direction bf depend illuminate parallel light ray infinity direction bf polyhedron cast shadow least mathfrak vertices general version problem allow unbounded polyhedra well associate shadow function mathfrak paper present correct order magnitude asymptotic bound function bound case answer mathfrak theta big log log log big unbounded shadow problem show different asymptotic growth rate mathfrak theta big big result bound shadow problem follow work chazelle edelsbrunner guibas bound silhouette span number mathfrak ast define analogously arbitrary light source complete picture show unbounded silhouette span number mathfrak ast grow theta big log log log big,111,1,1310.4345.txt
http://arxiv.org/abs/1310.4349,An Improved Majority-Logic Decoder Offering Massively Parallel Decoding   for Real-Time Control in Embedded Systems,"  We propose an easy-to-implement hard-decision majority-logic decoding algorithm for Reed-Muller codes RM(r,m) with m >= 3, m/2 >= r >= 1. The presented algorithm outperforms the best known majority-logic decoding algorithms and offers highly parallel decoding. The result is of special importance for safety- and time-critical applications in embedded systems. A simple combinational circuit can perform the proposed decoding. In particular, we show how our decoder for the three-error-correcting code RM(2,5) of dimension 16 and length 32 can be realized on hardware level. ","Computer Science - Information Theory ; Computer Science - Hardware Architecture ; Computer Science - Discrete Mathematics ; Computer Science - Emerging Technologies ; 94B35, 68P30 ; E.4 ; ","Bertram, Juliane ; Hauck, Peter ; Huber, Michael ; ","An Improved Majority-Logic Decoder Offering Massively Parallel Decoding   for Real-Time Control in Embedded Systems  We propose an easy-to-implement hard-decision majority-logic decoding algorithm for Reed-Muller codes RM(r,m) with m >= 3, m/2 >= r >= 1. The presented algorithm outperforms the best known majority-logic decoding algorithms and offers highly parallel decoding. The result is of special importance for safety- and time-critical applications in embedded systems. A simple combinational circuit can perform the proposed decoding. In particular, we show how our decoder for the three-error-correcting code RM(2,5) of dimension 16 and length 32 can be realized on hardware level. ",improve majority logic decoder offer massively parallel decode real time control embed systems propose easy implement hard decision majority logic decode algorithm reed muller cod rm present algorithm outperform best know majority logic decode algorithms offer highly parallel decode result special importance safety time critical applications embed systems simple combinational circuit perform propose decode particular show decoder three error correct code rm dimension length realize hardware level,67,5,1310.4349.txt
http://arxiv.org/abs/1310.5251,Sparsity-Promoting Sensor Selection for Non-linear Measurement Models,"  Sensor selection is an important design problem in large-scale sensor networks. Sensor selection can be interpreted as the problem of selecting the best subset of sensors that guarantees a certain estimation performance. We focus on observations that are related to a general non-linear model. The proposed framework is valid as long as the observations are independent, and its likelihood satisfies the regularity conditions. We use several functions of the Cram\'er-Rao bound (CRB) as a performance measure. We formulate the sensor selection problem as the design of a selection vector, which in its original form is a nonconvex l0-(quasi) norm optimization problem. We present relaxed sensor selection solvers that can be efficiently solved in polynomial time. We also propose a projected subgradient algorithm that is attractive for large-scale problems and also show how the algorithm can be easily distributed. The proposed framework is illustrated with a number of examples related to sensor placement design for localization. ",Computer Science - Information Theory ; Electrical Engineering and Systems Science - Signal Processing ; ,"Chepuri, Sundeep Prabhakar ; Leus, Geert ; ","Sparsity-Promoting Sensor Selection for Non-linear Measurement Models  Sensor selection is an important design problem in large-scale sensor networks. Sensor selection can be interpreted as the problem of selecting the best subset of sensors that guarantees a certain estimation performance. We focus on observations that are related to a general non-linear model. The proposed framework is valid as long as the observations are independent, and its likelihood satisfies the regularity conditions. We use several functions of the Cram\'er-Rao bound (CRB) as a performance measure. We formulate the sensor selection problem as the design of a selection vector, which in its original form is a nonconvex l0-(quasi) norm optimization problem. We present relaxed sensor selection solvers that can be efficiently solved in polynomial time. We also propose a projected subgradient algorithm that is attractive for large-scale problems and also show how the algorithm can be easily distributed. The proposed framework is illustrated with a number of examples related to sensor placement design for localization. ",sparsity promote sensor selection non linear measurement model sensor selection important design problem large scale sensor network sensor selection interpret problem select best subset sensors guarantee certain estimation performance focus observations relate general non linear model propose framework valid long observations independent likelihood satisfy regularity condition use several function cram er rao bind crb performance measure formulate sensor selection problem design selection vector original form nonconvex quasi norm optimization problem present relax sensor selection solvers efficiently solve polynomial time also propose project subgradient algorithm attractive large scale problems also show algorithm easily distribute propose framework illustrate number examples relate sensor placement design localization,103,2,1310.5251.txt
http://arxiv.org/abs/1310.6324,On Jacobian group arithmetic for typical divisors on curves,"  In a previous joint article with F. Abu Salem, we gave efficient algorithms for Jacobian group arithmetic of ""typical"" divisor classes on C_{3,4} curves, improving on similar results by other authors. At that time, we could only state that a generic divisor was typical, and hence unlikely to be encountered if one implemented these algorithms over a very large finite field. This article pins down an explicit characterization of these typical divisors, for an arbitrary smooth projective curve of genus g >= 1 having at least one rational point. We give general algorithms for Jacobian group arithmetic with these typical divisors, and prove not only that the algorithms are correct if various divisors are typical, but also that the success of our algorithms provides a guarantee that the resulting output is correct and that the resulting input and/or output divisors are also typical. These results apply in particular to our earlier algorithms for C_{3,4} curves. As a byproduct, we obtain a further speedup of approximately 15% on our previous algorithms for C_{3,4} curves. ","Mathematics - Number Theory ; Computer Science - Symbolic Computation ; Mathematics - Algebraic Geometry ; 14Q05 (primary), 11Y16, 14H40, 11G20 ; ","Khuri-Makdisi, Kamal ; ","On Jacobian group arithmetic for typical divisors on curves  In a previous joint article with F. Abu Salem, we gave efficient algorithms for Jacobian group arithmetic of ""typical"" divisor classes on C_{3,4} curves, improving on similar results by other authors. At that time, we could only state that a generic divisor was typical, and hence unlikely to be encountered if one implemented these algorithms over a very large finite field. This article pins down an explicit characterization of these typical divisors, for an arbitrary smooth projective curve of genus g >= 1 having at least one rational point. We give general algorithms for Jacobian group arithmetic with these typical divisors, and prove not only that the algorithms are correct if various divisors are typical, but also that the success of our algorithms provides a guarantee that the resulting output is correct and that the resulting input and/or output divisors are also typical. These results apply in particular to our earlier algorithms for C_{3,4} curves. As a byproduct, we obtain a further speedup of approximately 15% on our previous algorithms for C_{3,4} curves. ",jacobian group arithmetic typical divisors curve previous joint article abu salem give efficient algorithms jacobian group arithmetic typical divisor class curve improve similar result author time could state generic divisor typical hence unlikely encounter one implement algorithms large finite field article pin explicit characterization typical divisors arbitrary smooth projective curve genus least one rational point give general algorithms jacobian group arithmetic typical divisors prove algorithms correct various divisors typical also success algorithms provide guarantee result output correct result input output divisors also typical result apply particular earlier algorithms curve byproduct obtain speedup approximately previous algorithms curve,96,4,1310.6324.txt
http://arxiv.org/abs/1310.6398,Some Remarks on Lower Bounds for Queue Machines (Preliminary Report),"  We first give an improved lower bound for the deterministic online simulation of tapes or pushdown stores by queues. Then we inspect some proofs in a classical work on queue machines in the area of Formal Languages and outline why a main argument in the proofs is incomplete. Based on descriptional complexity, we show the intuition behind the argument to be correct. ",Computer Science - Computational Complexity ; Computer Science - Formal Languages and Automata Theory ; ,"Petersen, Holger ; ","Some Remarks on Lower Bounds for Queue Machines (Preliminary Report)  We first give an improved lower bound for the deterministic online simulation of tapes or pushdown stores by queues. Then we inspect some proofs in a classical work on queue machines in the area of Formal Languages and outline why a main argument in the proofs is incomplete. Based on descriptional complexity, we show the intuition behind the argument to be correct. ",remark lower bound queue machine preliminary report first give improve lower bind deterministic online simulation tap pushdown store queue inspect proof classical work queue machine area formal languages outline main argument proof incomplete base descriptional complexity show intuition behind argument correct,41,8,1310.6398.txt
http://arxiv.org/abs/1310.8097,Guaranteed Collision Detection With Toleranced Motions,"  We present a method for guaranteed collision detection with toleranced motions. The basic idea is to consider the motion as a curve in the 12-dimensional space of affine displacements, endowed with an object-oriented Euclidean metric, and cover it with balls. The associated orbits of points, lines, planes and polygons have particularly simple shapes that lend themselves well to exact and fast collision queries. We present formulas for elementary collision tests with these orbit shapes and we suggest an algorithm, based on motion subdivision and computation of bounding balls, that can give a no-collision guarantee. It allows a robust and efficient implementation and parallelization. At hand of several examples we explore the asymptotic behavior of the algorithm and compare different implementation strategies. ","Computer Science - Computational Geometry ; Computer Science - Robotics ; 65D18, 70B10 ; ","Schröcker, Hans-Peter ; Weber, Matthias J. ; ","Guaranteed Collision Detection With Toleranced Motions  We present a method for guaranteed collision detection with toleranced motions. The basic idea is to consider the motion as a curve in the 12-dimensional space of affine displacements, endowed with an object-oriented Euclidean metric, and cover it with balls. The associated orbits of points, lines, planes and polygons have particularly simple shapes that lend themselves well to exact and fast collision queries. We present formulas for elementary collision tests with these orbit shapes and we suggest an algorithm, based on motion subdivision and computation of bounding balls, that can give a no-collision guarantee. It allows a robust and efficient implementation and parallelization. At hand of several examples we explore the asymptotic behavior of the algorithm and compare different implementation strategies. ",guarantee collision detection toleranced motion present method guarantee collision detection toleranced motion basic idea consider motion curve dimensional space affine displacements endow object orient euclidean metric cover ball associate orbit point line plan polygons particularly simple shape lend well exact fast collision query present formulas elementary collision test orbit shape suggest algorithm base motion subdivision computation bound ball give collision guarantee allow robust efficient implementation parallelization hand several examples explore asymptotic behavior algorithm compare different implementation strategies,77,4,1310.8097.txt
http://arxiv.org/abs/1310.8121,Easy Accurate Reading and Writing of Floating-Point Numbers,"  Presented here are algorithms for converting between (decimal) scientific-notation and (binary) IEEE-754 double-precision floating-point numbers. By employing a rounding integer quotient operation these algorithms are much simpler than those previously published. The values are stable under repeated conversions between the formats. Unlike Java-1.8, the scientific representations generated use only the minimum number of mantissa digits needed to convert back to the original binary values. ",Computer Science - Numerical Analysis ; 65G04 ; G.1.0 ; ,"Jaffer, Aubrey ; ","Easy Accurate Reading and Writing of Floating-Point Numbers  Presented here are algorithms for converting between (decimal) scientific-notation and (binary) IEEE-754 double-precision floating-point numbers. By employing a rounding integer quotient operation these algorithms are much simpler than those previously published. The values are stable under repeated conversions between the formats. Unlike Java-1.8, the scientific representations generated use only the minimum number of mantissa digits needed to convert back to the original binary values. ",easy accurate read write float point number present algorithms convert decimal scientific notation binary ieee double precision float point number employ round integer quotient operation algorithms much simpler previously publish value stable repeat conversions format unlike java scientific representations generate use minimum number mantissa digits need convert back original binary value,51,4,1310.8121.txt
http://arxiv.org/abs/1311.0320,An Improved Solution for Restricted and Uncertain TRQ,"  CSPTRQ is an interesting problem and its has attracted much attention. The CSPTRQ is a variant of the traditional PTRQ. As objects moving in a constrained-space are common, clearly, it can also find many applications. At the first sight, our problem can be easily tackled by extending existing methods used to answer the PTRQ. Unfortunately, those classical techniques are not well suitable for our problem, due to a set of new challenges. We develop targeted solutions and demonstrate the efficiency and effectiveness of the proposed methods through extensive experiments. ",Computer Science - Databases ; H.3.3 ; G.3 ; G.3.1 ; ,"Wang, Jack ; ","An Improved Solution for Restricted and Uncertain TRQ  CSPTRQ is an interesting problem and its has attracted much attention. The CSPTRQ is a variant of the traditional PTRQ. As objects moving in a constrained-space are common, clearly, it can also find many applications. At the first sight, our problem can be easily tackled by extending existing methods used to answer the PTRQ. Unfortunately, those classical techniques are not well suitable for our problem, due to a set of new challenges. We develop targeted solutions and demonstrate the efficiency and effectiveness of the proposed methods through extensive experiments. ",improve solution restrict uncertain trq csptrq interest problem attract much attention csptrq variant traditional ptrq object move constrain space common clearly also find many applications first sight problem easily tackle extend exist methods use answer ptrq unfortunately classical techniques well suitable problem due set new challenge develop target solutions demonstrate efficiency effectiveness propose methods extensive experiment,56,2,1311.0320.txt
http://arxiv.org/abs/1311.0913,Bidding Games and Efficient Allocations,"  Richman games are zero-sum games, where in each turn players bid in order to determine who will play next [Lazarus et al.'99]. We extend the theory to impartial general-sum two player games called \emph{bidding games}, showing the existence of pure subgame-perfect equilibria (PSPE). In particular, we show that PSPEs form a semilattice, with a unique and natural \emph{Bottom Equilibrium}.   Our main result shows that if only two actions available to the players in each node, then the Bottom Equilibrium has additional properties: (a) utilities are monotone in budget; (b) every outcome is Pareto-efficient; and (c) any Pareto-efficient outcome is attained for some budget.   In the context of combinatorial bargaining, we show that a player with a fraction of X% of the total budget prefers her allocation to X% of the possible allocations. In addition, we provide a polynomial-time algorithm to compute the Bottom Equilibrium of a binary bidding game. ",Computer Science - Computer Science and Game Theory ; I.2.11 ; ,"Kalai, Gil ; Meir, Reshef ; Tennenholtz, Moshe ; ","Bidding Games and Efficient Allocations  Richman games are zero-sum games, where in each turn players bid in order to determine who will play next [Lazarus et al.'99]. We extend the theory to impartial general-sum two player games called \emph{bidding games}, showing the existence of pure subgame-perfect equilibria (PSPE). In particular, we show that PSPEs form a semilattice, with a unique and natural \emph{Bottom Equilibrium}.   Our main result shows that if only two actions available to the players in each node, then the Bottom Equilibrium has additional properties: (a) utilities are monotone in budget; (b) every outcome is Pareto-efficient; and (c) any Pareto-efficient outcome is attained for some budget.   In the context of combinatorial bargaining, we show that a player with a fraction of X% of the total budget prefers her allocation to X% of the possible allocations. In addition, we provide a polynomial-time algorithm to compute the Bottom Equilibrium of a binary bidding game. ",bid game efficient allocations richman game zero sum game turn players bid order determine play next lazarus et al extend theory impartial general sum two player game call emph bid game show existence pure subgame perfect equilibria pspe particular show pspes form semilattice unique natural emph bottom equilibrium main result show two action available players node bottom equilibrium additional properties utilities monotone budget every outcome pareto efficient pareto efficient outcome attain budget context combinatorial bargain show player fraction total budget prefer allocation possible allocations addition provide polynomial time algorithm compute bottom equilibrium binary bid game,95,8,1311.0913.txt
http://arxiv.org/abs/1311.1339,Zero-Error Capacity of a Class of Timing Channels,"  We analyze the problem of zero-error communication through timing channels that can be interpreted as discrete-time queues with bounded waiting times. The channel model includes the following assumptions: 1) Time is slotted, 2) at most $ N $ ""particles"" are sent in each time slot, 3) every particle is delayed in the channel for a number of slots chosen randomly from the set $ \{0, 1, \ldots, K\} $, and 4) the particles are identical. It is shown that the zero-error capacity of this channel is $ \log r $, where $ r $ is the unique positive real root of the polynomial $ x^{K+1} - x^{K} - N $. Capacity-achieving codes are explicitly constructed, and a linear-time decoding algorithm for these codes devised. In the particular case $ N = 1 $, $ K = 1 $, the capacity is equal to $ \log \phi $, where $ \phi = (1 + \sqrt{5}) / 2 $ is the golden ratio, and the constructed codes give another interpretation of the Fibonacci sequence. ","Computer Science - Information Theory ; Computer Science - Discrete Mathematics ; 94B25, 94A40, 94A24, 68R05, 65Q30 ; ","Kovačević, Mladen ; Popovski, Petar ; ","Zero-Error Capacity of a Class of Timing Channels  We analyze the problem of zero-error communication through timing channels that can be interpreted as discrete-time queues with bounded waiting times. The channel model includes the following assumptions: 1) Time is slotted, 2) at most $ N $ ""particles"" are sent in each time slot, 3) every particle is delayed in the channel for a number of slots chosen randomly from the set $ \{0, 1, \ldots, K\} $, and 4) the particles are identical. It is shown that the zero-error capacity of this channel is $ \log r $, where $ r $ is the unique positive real root of the polynomial $ x^{K+1} - x^{K} - N $. Capacity-achieving codes are explicitly constructed, and a linear-time decoding algorithm for these codes devised. In the particular case $ N = 1 $, $ K = 1 $, the capacity is equal to $ \log \phi $, where $ \phi = (1 + \sqrt{5}) / 2 $ is the golden ratio, and the constructed codes give another interpretation of the Fibonacci sequence. ",zero error capacity class time channel analyze problem zero error communication time channel interpret discrete time queue bound wait time channel model include follow assumptions time slot particles send time slot every particle delay channel number slot choose randomly set ldots particles identical show zero error capacity channel log unique positive real root polynomial capacity achieve cod explicitly construct linear time decode algorithm cod devise particular case capacity equal log phi phi sqrt golden ratio construct cod give another interpretation fibonacci sequence,82,5,1311.1339.txt
http://arxiv.org/abs/1311.2828,Private Matchings and Allocations,"  We consider a private variant of the classical allocation problem: given k goods and n agents with individual, private valuation functions over bundles of goods, how can we partition the goods amongst the agents to maximize social welfare? An important special case is when each agent desires at most one good, and specifies her (private) value for each good: in this case, the problem is exactly the maximum-weight matching problem in a bipartite graph.   Private matching and allocation problems have not been considered in the differential privacy literature, and for good reason: they are plainly impossible to solve under differential privacy. Informally, the allocation must match agents to their preferred goods in order to maximize social welfare, but this preference is exactly what agents wish to hide. Therefore, we consider the problem under the relaxed constraint of joint differential privacy: for any agent i, no coalition of agents excluding i should be able to learn about the valuation function of agent i. In this setting, the full allocation is no longer published---instead, each agent is told what good to get. We first show that with a small number of identical copies of each good, it is possible to efficiently and accurately solve the maximum weight matching problem while guaranteeing joint differential privacy. We then consider the more general allocation problem, when bidder valuations satisfy the gross substitutes condition. Finally, we prove that the allocation problem cannot be solved to non-trivial accuracy under joint differential privacy without requiring multiple copies of each type of good. ",Computer Science - Computer Science and Game Theory ; Computer Science - Cryptography and Security ; Computer Science - Data Structures and Algorithms ; ,"Hsu, Justin ; Huang, Zhiyi ; Roth, Aaron ; Roughgarden, Tim ; Wu, Zhiwei Steven ; ","Private Matchings and Allocations  We consider a private variant of the classical allocation problem: given k goods and n agents with individual, private valuation functions over bundles of goods, how can we partition the goods amongst the agents to maximize social welfare? An important special case is when each agent desires at most one good, and specifies her (private) value for each good: in this case, the problem is exactly the maximum-weight matching problem in a bipartite graph.   Private matching and allocation problems have not been considered in the differential privacy literature, and for good reason: they are plainly impossible to solve under differential privacy. Informally, the allocation must match agents to their preferred goods in order to maximize social welfare, but this preference is exactly what agents wish to hide. Therefore, we consider the problem under the relaxed constraint of joint differential privacy: for any agent i, no coalition of agents excluding i should be able to learn about the valuation function of agent i. In this setting, the full allocation is no longer published---instead, each agent is told what good to get. We first show that with a small number of identical copies of each good, it is possible to efficiently and accurately solve the maximum weight matching problem while guaranteeing joint differential privacy. We then consider the more general allocation problem, when bidder valuations satisfy the gross substitutes condition. Finally, we prove that the allocation problem cannot be solved to non-trivial accuracy under joint differential privacy without requiring multiple copies of each type of good. ",private match allocations consider private variant classical allocation problem give goods agents individual private valuation function bundle goods partition goods amongst agents maximize social welfare important special case agent desire one good specify private value good case problem exactly maximum weight match problem bipartite graph private match allocation problems consider differential privacy literature good reason plainly impossible solve differential privacy informally allocation must match agents prefer goods order maximize social welfare preference exactly agents wish hide therefore consider problem relax constraint joint differential privacy agent coalition agents exclude able learn valuation function agent set full allocation longer publish instead agent tell good get first show small number identical copy good possible efficiently accurately solve maximum weight match problem guarantee joint differential privacy consider general allocation problem bidder valuations satisfy gross substitute condition finally prove allocation problem cannot solve non trivial accuracy joint differential privacy without require multiple copy type good,150,0,1311.2828.txt
http://arxiv.org/abs/1311.2970,Coordinated Tethering for Multi-RAT Cellular Networks: An Algorithmic   Solution and Performance Analysis,"  The exploitation of already deployed wireless local area networks (WLAN)s (e.g., WiFi access points (AP)s) has attracted considerable attention, as an efficient and practical method to improve the performance of beyond 4G wireless networks. In this paper, we propose a novel communication paradigm to satisfy the performance demands of future wireless networks: a hybrid Cellular/WLAN network architecture with wireless offloading. In contrast to the commonly adopted practice of WiFi offloading, where the WLAN APs have a wired backhaul (e.g., Digital Subscriber Line), we propose a wireless offloading approach, where the WLAN APs will share their wireless cellular broadband connection with other users. These users will select their serving node, i.e., the macro-cell eNodeB or a WLAN AP, based on a certain selection criterion. Thus a challenging research field is originated, where interfering effects and wireless resources limitations play a dominant role. Important performance metrics of the proposed hybrid scheme, including the bit error probability, the ergodic capacity and the average signal-to-interference-plus noise ratio, are theoretically studied and closed form expressions are derived for the single-user case with multiple interferers, for both identical and non-identical fading conditions. Also, based on the general multi-cellular hybrid WLAN-Cellular concept, we first propose a intercell interference minimization approach. Then we present a novel scheme for achieving frequency reuse equal to one within a single macro-cell, under specific performance criteria and constraints, that guarantee the overall cell or the individual user QoS requirements. ",Computer Science - Networking and Internet Architecture ; ,"Bithas, Petros S. ; Lioumpas, Athanasios S. ; ","Coordinated Tethering for Multi-RAT Cellular Networks: An Algorithmic   Solution and Performance Analysis  The exploitation of already deployed wireless local area networks (WLAN)s (e.g., WiFi access points (AP)s) has attracted considerable attention, as an efficient and practical method to improve the performance of beyond 4G wireless networks. In this paper, we propose a novel communication paradigm to satisfy the performance demands of future wireless networks: a hybrid Cellular/WLAN network architecture with wireless offloading. In contrast to the commonly adopted practice of WiFi offloading, where the WLAN APs have a wired backhaul (e.g., Digital Subscriber Line), we propose a wireless offloading approach, where the WLAN APs will share their wireless cellular broadband connection with other users. These users will select their serving node, i.e., the macro-cell eNodeB or a WLAN AP, based on a certain selection criterion. Thus a challenging research field is originated, where interfering effects and wireless resources limitations play a dominant role. Important performance metrics of the proposed hybrid scheme, including the bit error probability, the ergodic capacity and the average signal-to-interference-plus noise ratio, are theoretically studied and closed form expressions are derived for the single-user case with multiple interferers, for both identical and non-identical fading conditions. Also, based on the general multi-cellular hybrid WLAN-Cellular concept, we first propose a intercell interference minimization approach. Then we present a novel scheme for achieving frequency reuse equal to one within a single macro-cell, under specific performance criteria and constraints, that guarantee the overall cell or the individual user QoS requirements. ",coordinate tether multi rat cellular network algorithmic solution performance analysis exploitation already deploy wireless local area network wlan wifi access point ap attract considerable attention efficient practical method improve performance beyond wireless network paper propose novel communication paradigm satisfy performance demand future wireless network hybrid cellular wlan network architecture wireless offload contrast commonly adopt practice wifi offload wlan aps wire backhaul digital subscriber line propose wireless offload approach wlan aps share wireless cellular broadband connection users users select serve node macro cell enodeb wlan ap base certain selection criterion thus challenge research field originate interfere effect wireless resources limitations play dominant role important performance metrics propose hybrid scheme include bite error probability ergodic capacity average signal interference plus noise ratio theoretically study close form expressions derive single user case multiple interferers identical non identical fade condition also base general multi cellular hybrid wlan cellular concept first propose intercell interference minimization approach present novel scheme achieve frequency reuse equal one within single macro cell specific performance criteria constraints guarantee overall cell individual user qos requirements,174,12,1311.2970.txt
http://arxiv.org/abs/1311.3158,Fingerprinting Codes and the Price of Approximate Differential Privacy,"  We show new lower bounds on the sample complexity of $(\varepsilon, \delta)$-differentially private algorithms that accurately answer large sets of counting queries. A counting query on a database $D \in (\{0,1\}^d)^n$ has the form ""What fraction of the individual records in the database satisfy the property $q$?"" We show that in order to answer an arbitrary set $\mathcal{Q}$ of $\gg nd$ counting queries on $D$ to within error $\pm \alpha$ it is necessary that $$ n \geq \tilde{\Omega}\Bigg(\frac{\sqrt{d} \log |\mathcal{Q}|}{\alpha^2 \varepsilon} \Bigg). $$ This bound is optimal up to poly-logarithmic factors, as demonstrated by the Private Multiplicative Weights algorithm (Hardt and Rothblum, FOCS'10). In particular, our lower bound is the first to show that the sample complexity required for accuracy and $(\varepsilon, \delta)$-differential privacy is asymptotically larger than what is required merely for accuracy, which is $O(\log |\mathcal{Q}| / \alpha^2)$. In addition, we show that our lower bound holds for the specific case of $k$-way marginal queries (where $|\mathcal{Q}| = 2^k \binom{d}{k}$) when $\alpha$ is not too small compared to $d$ (e.g. when $\alpha$ is any fixed constant).   Our results rely on the existence of short \emph{fingerprinting codes} (Boneh and Shaw, CRYPTO'95, Tardos, STOC'03), which we show are closely connected to the sample complexity of differentially private data release. We also give a new method for combining certain types of sample complexity lower bounds into stronger lower bounds. ",Computer Science - Cryptography and Security ; ,"Bun, Mark ; Ullman, Jonathan ; Vadhan, Salil ; ","Fingerprinting Codes and the Price of Approximate Differential Privacy  We show new lower bounds on the sample complexity of $(\varepsilon, \delta)$-differentially private algorithms that accurately answer large sets of counting queries. A counting query on a database $D \in (\{0,1\}^d)^n$ has the form ""What fraction of the individual records in the database satisfy the property $q$?"" We show that in order to answer an arbitrary set $\mathcal{Q}$ of $\gg nd$ counting queries on $D$ to within error $\pm \alpha$ it is necessary that $$ n \geq \tilde{\Omega}\Bigg(\frac{\sqrt{d} \log |\mathcal{Q}|}{\alpha^2 \varepsilon} \Bigg). $$ This bound is optimal up to poly-logarithmic factors, as demonstrated by the Private Multiplicative Weights algorithm (Hardt and Rothblum, FOCS'10). In particular, our lower bound is the first to show that the sample complexity required for accuracy and $(\varepsilon, \delta)$-differential privacy is asymptotically larger than what is required merely for accuracy, which is $O(\log |\mathcal{Q}| / \alpha^2)$. In addition, we show that our lower bound holds for the specific case of $k$-way marginal queries (where $|\mathcal{Q}| = 2^k \binom{d}{k}$) when $\alpha$ is not too small compared to $d$ (e.g. when $\alpha$ is any fixed constant).   Our results rely on the existence of short \emph{fingerprinting codes} (Boneh and Shaw, CRYPTO'95, Tardos, STOC'03), which we show are closely connected to the sample complexity of differentially private data release. We also give a new method for combining certain types of sample complexity lower bounds into stronger lower bounds. ",fingerprint cod price approximate differential privacy show new lower bound sample complexity varepsilon delta differentially private algorithms accurately answer large set count query count query database form fraction individual record database satisfy property show order answer arbitrary set mathcal gg nd count query within error pm alpha necessary geq tilde omega bigg frac sqrt log mathcal alpha varepsilon bigg bind optimal poly logarithmic factor demonstrate private multiplicative weight algorithm hardt rothblum focs particular lower bind first show sample complexity require accuracy varepsilon delta differential privacy asymptotically larger require merely accuracy log mathcal alpha addition show lower bind hold specific case way marginal query mathcal binom alpha small compare alpha fix constant result rely existence short emph fingerprint cod boneh shaw crypto tardos stoc show closely connect sample complexity differentially private data release also give new method combine certain type sample complexity lower bound stronger lower bound,146,1,1311.3158.txt
http://arxiv.org/abs/1311.3414,Mining Software Repair Models for Reasoning on the Search Space of   Automated Program Fixing,"  This paper is about understanding the nature of bug fixing by analyzing thousands of bug fix transactions of software repositories. It then places this learned knowledge in the context of automated program repair. We give extensive empirical results on the nature of human bug fixes at a large scale and a fine granularity with abstract syntax tree differencing. We set up mathematical reasoning on the search space of automated repair and the time to navigate through it. By applying our method on 14 repositories of Java software and 89,993 versioning transactions, we show that not all probabilistic repair models are equivalent. ",Computer Science - Software Engineering ; ,"Martinez, Matias ; Monperrus, Martin ; ","Mining Software Repair Models for Reasoning on the Search Space of   Automated Program Fixing  This paper is about understanding the nature of bug fixing by analyzing thousands of bug fix transactions of software repositories. It then places this learned knowledge in the context of automated program repair. We give extensive empirical results on the nature of human bug fixes at a large scale and a fine granularity with abstract syntax tree differencing. We set up mathematical reasoning on the search space of automated repair and the time to navigate through it. By applying our method on 14 repositories of Java software and 89,993 versioning transactions, we show that not all probabilistic repair models are equivalent. ",mine software repair model reason search space automate program fix paper understand nature bug fix analyze thousands bug fix transactions software repositories place learn knowledge context automate program repair give extensive empirical result nature human bug fix large scale fine granularity abstract syntax tree differencing set mathematical reason search space automate repair time navigate apply method repositories java software versioning transactions show probabilistic repair model equivalent,66,8,1311.3414.txt
http://arxiv.org/abs/1311.4257,A parallel directional Fast Multipole Method,"  This paper introduces a parallel directional fast multipole method (FMM) for solving N-body problems with highly oscillatory kernels, with a focus on the Helmholtz kernel in three dimensions. This class of oscillatory kernels requires a more restrictive low-rank criterion than that of the low-frequency regime, and thus effective parallelizations must adapt to the modified data dependencies. We propose a simple partition at a fixed level of the octree and show that, if the partitions are properly balanced between p processes, the overall runtime is essentially O(N log N/p+ p). By the structure of the low-rank criterion, we are able to avoid communication at the top of the octree. We demonstrate the effectiveness of our parallelization on several challenging models. ","Mathematics - Numerical Analysis ; Computer Science - Numerical Analysis ; 65Y05, 65Y20, 78A45 ; ","Benson, Austin R. ; Poulson, Jack ; Tran, Kenneth ; Engquist, Björn ; Ying, Lexing ; ","A parallel directional Fast Multipole Method  This paper introduces a parallel directional fast multipole method (FMM) for solving N-body problems with highly oscillatory kernels, with a focus on the Helmholtz kernel in three dimensions. This class of oscillatory kernels requires a more restrictive low-rank criterion than that of the low-frequency regime, and thus effective parallelizations must adapt to the modified data dependencies. We propose a simple partition at a fixed level of the octree and show that, if the partitions are properly balanced between p processes, the overall runtime is essentially O(N log N/p+ p). By the structure of the low-rank criterion, we are able to avoid communication at the top of the octree. We demonstrate the effectiveness of our parallelization on several challenging models. ",parallel directional fast multipole method paper introduce parallel directional fast multipole method fmm solve body problems highly oscillatory kernels focus helmholtz kernel three dimension class oscillatory kernels require restrictive low rank criterion low frequency regime thus effective parallelizations must adapt modify data dependencies propose simple partition fix level octree show partition properly balance process overall runtime essentially log structure low rank criterion able avoid communication top octree demonstrate effectiveness parallelization several challenge model,73,11,1311.4257.txt
http://arxiv.org/abs/1311.4766,Notions of Symmetry for Finite Strategic-Form Games,  In this paper we survey various notions of symmetry for finite strategic-form games; show that game bijections and game isomorphisms form groupoids; introduce matchings as a convenient characterisation of strategy triviality; and outline how to construct and partially order parameterised symmetric games with numerous examples that range all combinations of surveyed symmetry notions. ,Mathematics - Combinatorics ; Computer Science - Computer Science and Game Theory ; ,"Ham, Nicholas ; ",Notions of Symmetry for Finite Strategic-Form Games  In this paper we survey various notions of symmetry for finite strategic-form games; show that game bijections and game isomorphisms form groupoids; introduce matchings as a convenient characterisation of strategy triviality; and outline how to construct and partially order parameterised symmetric games with numerous examples that range all combinations of surveyed symmetry notions. ,notions symmetry finite strategic form game paper survey various notions symmetry finite strategic form game show game bijections game isomorphisms form groupoids introduce match convenient characterisation strategy triviality outline construct partially order parameterised symmetric game numerous examples range combinations survey symmetry notions,42,8,1311.4766.txt
http://arxiv.org/abs/1311.4821,On the Complexity of Random Satisfiability Problems with Planted   Solutions,"  The problem of identifying a planted assignment given a random $k$-SAT formula consistent with the assignment exhibits a large algorithmic gap: while the planted solution becomes unique and can be identified given a formula with $O(n\log n)$ clauses, there are distributions over clauses for which the best known efficient algorithms require $n^{k/2}$ clauses. We propose and study a unified model for planted $k$-SAT, which captures well-known special cases. An instance is described by a planted assignment $\sigma$ and a distribution on clauses with $k$ literals. We define its distribution complexity as the largest $r$ for which the distribution is not $r$-wise independent ($1 \le r \le k$ for any distribution with a planted assignment).   Our main result is an unconditional lower bound, tight up to logarithmic factors, for statistical (query) algorithms [Kearns 1998, Feldman et. al 2012], matching known upper bounds, which, as we show, can be implemented using a statistical algorithm. Since known approaches for problems over distributions have statistical analogues (spectral, MCMC, gradient-based, convex optimization etc.), this lower bound provides a rigorous explanation of the observed algorithmic gap. The proof introduces a new general technique for the analysis of statistical query algorithms. It also points to a geometric paring phenomenon in the space of all planted assignments.   We describe consequences of our lower bounds to Feige's refutation hypothesis [Feige 2002] and to lower bounds on general convex programs that solve planted $k$-SAT. Our bounds also extend to other planted $k$-CSP models, and, in particular, provide concrete evidence for the security of Goldreich's one-way function and the associated pseudorandom generator when used with a sufficiently hard predicate [Goldreich 2000]. ",Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; Mathematics - Probability ; ,"Feldman, Vitaly ; Perkins, Will ; Vempala, Santosh ; ","On the Complexity of Random Satisfiability Problems with Planted   Solutions  The problem of identifying a planted assignment given a random $k$-SAT formula consistent with the assignment exhibits a large algorithmic gap: while the planted solution becomes unique and can be identified given a formula with $O(n\log n)$ clauses, there are distributions over clauses for which the best known efficient algorithms require $n^{k/2}$ clauses. We propose and study a unified model for planted $k$-SAT, which captures well-known special cases. An instance is described by a planted assignment $\sigma$ and a distribution on clauses with $k$ literals. We define its distribution complexity as the largest $r$ for which the distribution is not $r$-wise independent ($1 \le r \le k$ for any distribution with a planted assignment).   Our main result is an unconditional lower bound, tight up to logarithmic factors, for statistical (query) algorithms [Kearns 1998, Feldman et. al 2012], matching known upper bounds, which, as we show, can be implemented using a statistical algorithm. Since known approaches for problems over distributions have statistical analogues (spectral, MCMC, gradient-based, convex optimization etc.), this lower bound provides a rigorous explanation of the observed algorithmic gap. The proof introduces a new general technique for the analysis of statistical query algorithms. It also points to a geometric paring phenomenon in the space of all planted assignments.   We describe consequences of our lower bounds to Feige's refutation hypothesis [Feige 2002] and to lower bounds on general convex programs that solve planted $k$-SAT. Our bounds also extend to other planted $k$-CSP models, and, in particular, provide concrete evidence for the security of Goldreich's one-way function and the associated pseudorandom generator when used with a sufficiently hard predicate [Goldreich 2000]. ",complexity random satisfiability problems plant solutions problem identify plant assignment give random sit formula consistent assignment exhibit large algorithmic gap plant solution become unique identify give formula log clauses distributions clauses best know efficient algorithms require clauses propose study unify model plant sit capture well know special case instance describe plant assignment sigma distribution clauses literals define distribution complexity largest distribution wise independent le le distribution plant assignment main result unconditional lower bind tight logarithmic factor statistical query algorithms kearns feldman et al match know upper bound show implement use statistical algorithm since know approach problems distributions statistical analogues spectral mcmc gradient base convex optimization etc lower bind provide rigorous explanation observe algorithmic gap proof introduce new general technique analysis statistical query algorithms also point geometric par phenomenon space plant assignments describe consequences lower bound feige refutation hypothesis feige lower bound general convex program solve plant sit bound also extend plant csp model particular provide concrete evidence security goldreich one way function associate pseudorandom generator use sufficiently hard predicate goldreich,170,8,1311.4821.txt
http://arxiv.org/abs/1311.6126,An energy function and its application to the periodic behavior of   k-reversible processes,"  We consider the graph dynamical systems known as k-reversible processes. In such processes, each vertex in the graph has one of two possible states at each discrete time step. Each vertex changes its state between the current time and the next if and only if it currently has at least k neighbors in a state different than its own. For such processes, we present a monotonic function similar to the decreasing energy functions used to study threshold networks. Using this new function, we show an alternative proof for the maximum period length in a k-reversible process and provide better upper bounds on the transient length in both the general case and the case of trees. ",Computer Science - Data Structures and Algorithms ; ,"Oliveira, Leonardo I. L. ; Barbosa, Valmir C. ; Protti, Fábio ; ","An energy function and its application to the periodic behavior of   k-reversible processes  We consider the graph dynamical systems known as k-reversible processes. In such processes, each vertex in the graph has one of two possible states at each discrete time step. Each vertex changes its state between the current time and the next if and only if it currently has at least k neighbors in a state different than its own. For such processes, we present a monotonic function similar to the decreasing energy functions used to study threshold networks. Using this new function, we show an alternative proof for the maximum period length in a k-reversible process and provide better upper bounds on the transient length in both the general case and the case of trees. ",energy function application periodic behavior reversible process consider graph dynamical systems know reversible process process vertex graph one two possible state discrete time step vertex change state current time next currently least neighbor state different process present monotonic function similar decrease energy function use study threshold network use new function show alternative proof maximum period length reversible process provide better upper bound transient length general case case tree,68,3,1311.6126.txt
http://arxiv.org/abs/1311.6876,Want a Good Answer? Ask a Good Question First!,"  Community Question Answering (CQA) websites have become valuable repositories which host a massive volume of human knowledge. To maximize the utility of such knowledge, it is essential to evaluate the quality of an existing question or answer, especially soon after it is posted on the CQA website.   In this paper, we study the problem of inferring the quality of questions and answers through a case study of a software CQA (Stack Overflow). Our key finding is that the quality of an answer is strongly positively correlated with that of its question. Armed with this observation, we propose a family of algorithms to jointly predict the quality of questions and answers, for both quantifying numerical quality scores and differentiating the high-quality questions/answers from those of low quality. We conduct extensive experimental evaluations to demonstrate the effectiveness and efficiency of our methods. ",Computer Science - Databases ; Computer Science - Artificial Intelligence ; Computer Science - Information Retrieval ; Computer Science - Software Engineering ; ,"Yao, Yuan ; Tong, Hanghang ; Xie, Tao ; Akoglu, Leman ; Xu, Feng ; Lu, Jian ; ","Want a Good Answer? Ask a Good Question First!  Community Question Answering (CQA) websites have become valuable repositories which host a massive volume of human knowledge. To maximize the utility of such knowledge, it is essential to evaluate the quality of an existing question or answer, especially soon after it is posted on the CQA website.   In this paper, we study the problem of inferring the quality of questions and answers through a case study of a software CQA (Stack Overflow). Our key finding is that the quality of an answer is strongly positively correlated with that of its question. Armed with this observation, we propose a family of algorithms to jointly predict the quality of questions and answers, for both quantifying numerical quality scores and differentiating the high-quality questions/answers from those of low quality. We conduct extensive experimental evaluations to demonstrate the effectiveness and efficiency of our methods. ",want good answer ask good question first community question answer cqa websites become valuable repositories host massive volume human knowledge maximize utility knowledge essential evaluate quality exist question answer especially soon post cqa website paper study problem infer quality question answer case study software cqa stack overflow key find quality answer strongly positively correlate question arm observation propose family algorithms jointly predict quality question answer quantify numerical quality score differentiate high quality question answer low quality conduct extensive experimental evaluations demonstrate effectiveness efficiency methods,84,10,1311.6876.txt
http://arxiv.org/abs/1312.0049,One-Class Classification: Taxonomy of Study and Review of Techniques,"  One-class classification (OCC) algorithms aim to build classification models when the negative class is either absent, poorly sampled or not well defined. This unique situation constrains the learning of efficient classifiers by defining class boundary just with the knowledge of positive class. The OCC problem has been considered and applied under many research themes, such as outlier/novelty detection and concept learning. In this paper we present a unified view of the general problem of OCC by presenting a taxonomy of study for OCC problems, which is based on the availability of training data, algorithms used and the application domains applied. We further delve into each of the categories of the proposed taxonomy and present a comprehensive literature review of the OCC algorithms, techniques and methodologies with a focus on their significance, limitations and applications. We conclude our paper by discussing some open research problems in the field of OCC and present our vision for future research. ",Computer Science - Machine Learning ; Computer Science - Artificial Intelligence ; ,"Khan, Shehroz S. ; Madden, Michael G. ; ","One-Class Classification: Taxonomy of Study and Review of Techniques  One-class classification (OCC) algorithms aim to build classification models when the negative class is either absent, poorly sampled or not well defined. This unique situation constrains the learning of efficient classifiers by defining class boundary just with the knowledge of positive class. The OCC problem has been considered and applied under many research themes, such as outlier/novelty detection and concept learning. In this paper we present a unified view of the general problem of OCC by presenting a taxonomy of study for OCC problems, which is based on the availability of training data, algorithms used and the application domains applied. We further delve into each of the categories of the proposed taxonomy and present a comprehensive literature review of the OCC algorithms, techniques and methodologies with a focus on their significance, limitations and applications. We conclude our paper by discussing some open research problems in the field of OCC and present our vision for future research. ",one class classification taxonomy study review techniques one class classification occ algorithms aim build classification model negative class either absent poorly sample well define unique situation constrain learn efficient classifiers define class boundary knowledge positive class occ problem consider apply many research theme outlier novelty detection concept learn paper present unify view general problem occ present taxonomy study occ problems base availability train data algorithms use application domains apply delve categories propose taxonomy present comprehensive literature review occ algorithms techniques methodologies focus significance limitations applications conclude paper discuss open research problems field occ present vision future research,97,10,1312.0049.txt
http://arxiv.org/abs/1312.0233,On Optimal Disc Covers and a New Characterization of the Steiner Center,"  Given N points in the plane $P_1 P_2...P_N$ and a location $\Omega$, the union of discs with diameters $[\Omega P_i], i = 1, 2,...N$ covers the convex hull of the points. The location $\Omega_s$ minimizing the area covered by the union of discs, is shown to be the Steiner center of the convex hull of the points. Similar results for $d$-dimensional Euclidean space are conjectured. ",Computer Science - Computational Geometry ; ,"Yankelevsky, Yael ; Bruckstein, Alfred M. ; ","On Optimal Disc Covers and a New Characterization of the Steiner Center  Given N points in the plane $P_1 P_2...P_N$ and a location $\Omega$, the union of discs with diameters $[\Omega P_i], i = 1, 2,...N$ covers the convex hull of the points. The location $\Omega_s$ minimizing the area covered by the union of discs, is shown to be the Steiner center of the convex hull of the points. Similar results for $d$-dimensional Euclidean space are conjectured. ",optimal disc cover new characterization steiner center give point plane location omega union discs diameters omega cover convex hull point location omega minimize area cover union discs show steiner center convex hull point similar result dimensional euclidean space conjecture,39,4,1312.0233.txt
http://arxiv.org/abs/1312.0461,Abmash: Mashing Up Legacy Web Applications by Automated Imitation of   Human Actions,"  Many business web-based applications do not offer applications programming interfaces (APIs) to enable other applications to access their data and functions in a programmatic manner. This makes their composition difficult (for instance to synchronize data between two applications). To address this challenge, this paper presents Abmash, an approach to facilitate the integration of such legacy web applications by automatically imitating human interactions with them. By automatically interacting with the graphical user interface (GUI) of web applications, the system supports all forms of integrations including bi-directional interactions and is able to interact with AJAX-based applications. Furthermore, the integration programs are easy to write since they deal with end-user, visual user-interface elements. The integration code is simple enough to be called a ""mashup"". ",Computer Science - Software Engineering ; ,"Ortac, Alper ; Monperrus, Martin ; Mezini, Mira ; ","Abmash: Mashing Up Legacy Web Applications by Automated Imitation of   Human Actions  Many business web-based applications do not offer applications programming interfaces (APIs) to enable other applications to access their data and functions in a programmatic manner. This makes their composition difficult (for instance to synchronize data between two applications). To address this challenge, this paper presents Abmash, an approach to facilitate the integration of such legacy web applications by automatically imitating human interactions with them. By automatically interacting with the graphical user interface (GUI) of web applications, the system supports all forms of integrations including bi-directional interactions and is able to interact with AJAX-based applications. Furthermore, the integration programs are easy to write since they deal with end-user, visual user-interface elements. The integration code is simple enough to be called a ""mashup"". ",abmash mash legacy web applications automate imitation human action many business web base applications offer applications program interfaces apis enable applications access data function programmatic manner make composition difficult instance synchronize data two applications address challenge paper present abmash approach facilitate integration legacy web applications automatically imitate human interactions automatically interact graphical user interface gui web applications system support form integrations include bi directional interactions able interact ajax base applications furthermore integration program easy write since deal end user visual user interface elements integration code simple enough call mashup,89,10,1312.0461.txt
http://arxiv.org/abs/1312.1001,Optimal detection of intersections between convex polyhedra,"  For a polyhedron $P$ in $\mathbb{R}^d$, denote by $|P|$ its combinatorial complexity, i.e., the number of faces of all dimensions of the polyhedra. In this paper, we revisit the classic problem of preprocessing polyhedra independently so that given two preprocessed polyhedra $P$ and $Q$ in $\mathbb{R}^d$, each translated and rotated, their intersection can be tested rapidly.   For $d=3$ we show how to perform such a test in $O(\log |P| + \log |Q|)$ time after linear preprocessing time and space. This running time is the best possible and improves upon the last best known query time of $O(\log|P| \log|Q|)$ by Dobkin and Kirkpatrick (1990).   We then generalize our method to any constant dimension $d$, achieving the same optimal $O(\log |P| + \log |Q|)$ query time using a representation of size $O(|P|^{\lfloor d/2\rfloor + \varepsilon})$ for any $\varepsilon>0$ arbitrarily small. This answers an even older question posed by Dobkin and Kirkpatrick 30 years ago.   In addition, we provide an alternative $O(\log |P| + \log |Q|)$ algorithm to test the intersection of two convex polygons $P$ and $Q$ in the plane. ",Computer Science - Computational Geometry ; ,"Barba, Luis ; Langerman, Stefan ; ","Optimal detection of intersections between convex polyhedra  For a polyhedron $P$ in $\mathbb{R}^d$, denote by $|P|$ its combinatorial complexity, i.e., the number of faces of all dimensions of the polyhedra. In this paper, we revisit the classic problem of preprocessing polyhedra independently so that given two preprocessed polyhedra $P$ and $Q$ in $\mathbb{R}^d$, each translated and rotated, their intersection can be tested rapidly.   For $d=3$ we show how to perform such a test in $O(\log |P| + \log |Q|)$ time after linear preprocessing time and space. This running time is the best possible and improves upon the last best known query time of $O(\log|P| \log|Q|)$ by Dobkin and Kirkpatrick (1990).   We then generalize our method to any constant dimension $d$, achieving the same optimal $O(\log |P| + \log |Q|)$ query time using a representation of size $O(|P|^{\lfloor d/2\rfloor + \varepsilon})$ for any $\varepsilon>0$ arbitrarily small. This answers an even older question posed by Dobkin and Kirkpatrick 30 years ago.   In addition, we provide an alternative $O(\log |P| + \log |Q|)$ algorithm to test the intersection of two convex polygons $P$ and $Q$ in the plane. ",optimal detection intersections convex polyhedra polyhedron mathbb denote combinatorial complexity number face dimension polyhedra paper revisit classic problem preprocessing polyhedra independently give two preprocessed polyhedra mathbb translate rotate intersection test rapidly show perform test log log time linear preprocessing time space run time best possible improve upon last best know query time log log dobkin kirkpatrick generalize method constant dimension achieve optimal log log query time use representation size lfloor rfloor varepsilon varepsilon arbitrarily small answer even older question pose dobkin kirkpatrick years ago addition provide alternative log log algorithm test intersection two convex polygons plane,96,1,1312.1001.txt
http://arxiv.org/abs/1312.1277,Bandits and Experts in Metric Spaces,"  In a multi-armed bandit problem, an online algorithm chooses from a set of strategies in a sequence of trials so as to maximize the total payoff of the chosen strategies. While the performance of bandit algorithms with a small finite strategy set is quite well understood, bandit problems with large strategy sets are still a topic of very active investigation, motivated by practical applications such as online auctions and web advertisement. The goal of such research is to identify broad and natural classes of strategy sets and payoff functions which enable the design of efficient solutions.   In this work we study a very general setting for the multi-armed bandit problem in which the strategies form a metric space, and the payoff function satisfies a Lipschitz condition with respect to the metric. We refer to this problem as the ""Lipschitz MAB problem"". We present a solution for the multi-armed bandit problem in this setting. That is, for every metric space we define an isometry invariant which bounds from below the performance of Lipschitz MAB algorithms for this metric space, and we present an algorithm which comes arbitrarily close to meeting this bound. Furthermore, our technique gives even better results for benign payoff functions. We also address the full-feedback (""best expert"") version of the problem, where after every round the payoffs from all arms are revealed. ",Computer Science - Data Structures and Algorithms ; Computer Science - Machine Learning ; ,"Kleinberg, Robert ; Slivkins, Aleksandrs ; Upfal, Eli ; ","Bandits and Experts in Metric Spaces  In a multi-armed bandit problem, an online algorithm chooses from a set of strategies in a sequence of trials so as to maximize the total payoff of the chosen strategies. While the performance of bandit algorithms with a small finite strategy set is quite well understood, bandit problems with large strategy sets are still a topic of very active investigation, motivated by practical applications such as online auctions and web advertisement. The goal of such research is to identify broad and natural classes of strategy sets and payoff functions which enable the design of efficient solutions.   In this work we study a very general setting for the multi-armed bandit problem in which the strategies form a metric space, and the payoff function satisfies a Lipschitz condition with respect to the metric. We refer to this problem as the ""Lipschitz MAB problem"". We present a solution for the multi-armed bandit problem in this setting. That is, for every metric space we define an isometry invariant which bounds from below the performance of Lipschitz MAB algorithms for this metric space, and we present an algorithm which comes arbitrarily close to meeting this bound. Furthermore, our technique gives even better results for benign payoff functions. We also address the full-feedback (""best expert"") version of the problem, where after every round the payoffs from all arms are revealed. ",bandits experts metric space multi arm bandit problem online algorithm choose set strategies sequence trials maximize total payoff choose strategies performance bandit algorithms small finite strategy set quite well understand bandit problems large strategy set still topic active investigation motivate practical applications online auction web advertisement goal research identify broad natural class strategy set payoff function enable design efficient solutions work study general set multi arm bandit problem strategies form metric space payoff function satisfy lipschitz condition respect metric refer problem lipschitz mab problem present solution multi arm bandit problem set every metric space define isometry invariant bound performance lipschitz mab algorithms metric space present algorithm come arbitrarily close meet bind furthermore technique give even better result benign payoff function also address full feedback best expert version problem every round payoffs arm reveal,133,2,1312.1277.txt
http://arxiv.org/abs/1312.1529,Instruction sequences expressing multiplication algorithms,"  For each function on bit strings, its restriction to bit strings of any given length can be computed by a finite instruction sequence that contains only instructions to set and get the content of Boolean registers, forward jump instructions, and a termination instruction. We describe instruction sequences of this kind that compute the function on bit strings that models multiplication on natural numbers less than $2^N$ with respect to their binary representation by bit strings of length $N$, for a fixed but arbitrary $N > 0$, according to the long multiplication algorithm and the Karatsuba multiplication algorithm. We find among other things that the instruction sequence expressing the former algorithm is longer than the one expressing the latter algorithm only if the length of the bit strings involved is greater than $2^8$. We also go into the use of an instruction sequence with backward jump instructions for expressing the long multiplication algorithm. This leads to an instruction sequence that it is shorter than the other two if the length of the bit strings involved is greater than $2$. ",Computer Science - Programming Languages ; F.1.1 ; F.2.1 ; ,"Bergstra, J. A. ; Middelburg, C. A. ; ","Instruction sequences expressing multiplication algorithms  For each function on bit strings, its restriction to bit strings of any given length can be computed by a finite instruction sequence that contains only instructions to set and get the content of Boolean registers, forward jump instructions, and a termination instruction. We describe instruction sequences of this kind that compute the function on bit strings that models multiplication on natural numbers less than $2^N$ with respect to their binary representation by bit strings of length $N$, for a fixed but arbitrary $N > 0$, according to the long multiplication algorithm and the Karatsuba multiplication algorithm. We find among other things that the instruction sequence expressing the former algorithm is longer than the one expressing the latter algorithm only if the length of the bit strings involved is greater than $2^8$. We also go into the use of an instruction sequence with backward jump instructions for expressing the long multiplication algorithm. This leads to an instruction sequence that it is shorter than the other two if the length of the bit strings involved is greater than $2$. ",instruction sequence express multiplication algorithms function bite string restriction bite string give length compute finite instruction sequence contain instructions set get content boolean register forward jump instructions termination instruction describe instruction sequence kind compute function bite string model multiplication natural number less respect binary representation bite string length fix arbitrary accord long multiplication algorithm karatsuba multiplication algorithm find among things instruction sequence express former algorithm longer one express latter algorithm length bite string involve greater also go use instruction sequence backward jump instructions express long multiplication algorithm lead instruction sequence shorter two length bite string involve greater,97,4,1312.1529.txt
http://arxiv.org/abs/1312.1559,Outerstring graphs are $\chi$-bounded,"  An outerstring graph is an intersection graph of curves that lie in a common half-plane and have one endpoint on the boundary of that half-plane. We prove that the class of outerstring graphs is $\chi$-bounded, which means that their chromatic number is bounded by a function of their clique number. This generalizes a series of previous results on $\chi$-boundedness of outerstring graphs with various additional restrictions on the shape of curves or the number of times the pairs of curves can cross. The assumption that each curve has an endpoint on the boundary of the half-plane is justified by the known fact that triangle-free intersection graphs of straight-line segments can have arbitrarily large chromatic number. ","Mathematics - Combinatorics ; Computer Science - Computational Geometry ; Computer Science - Discrete Mathematics ; 05C62, 05C15 ; ","Rok, Alexandre ; Walczak, Bartosz ; ","Outerstring graphs are $\chi$-bounded  An outerstring graph is an intersection graph of curves that lie in a common half-plane and have one endpoint on the boundary of that half-plane. We prove that the class of outerstring graphs is $\chi$-bounded, which means that their chromatic number is bounded by a function of their clique number. This generalizes a series of previous results on $\chi$-boundedness of outerstring graphs with various additional restrictions on the shape of curves or the number of times the pairs of curves can cross. The assumption that each curve has an endpoint on the boundary of the half-plane is justified by the known fact that triangle-free intersection graphs of straight-line segments can have arbitrarily large chromatic number. ",outerstring graph chi bound outerstring graph intersection graph curve lie common half plane one endpoint boundary half plane prove class outerstring graph chi bound mean chromatic number bound function clique number generalize series previous result chi boundedness outerstring graph various additional restrictions shape curve number time pair curve cross assumption curve endpoint boundary half plane justify know fact triangle free intersection graph straight line segment arbitrarily large chromatic number,69,3,1312.1559.txt
http://arxiv.org/abs/1312.1664,Simplicial Homology for Future Cellular Networks,"  Simplicial homology is a tool that provides a mathematical way to compute the connectivity and the coverage of a cellular network without any node location information. In this article, we use simplicial homology in order to not only compute the topology of a cellular network, but also to discover the clusters of nodes still with no location information. We propose three algorithms for the management of future cellular networks. The first one is a frequency auto-planning algorithm for the self-configuration of future cellular networks. It aims at minimizing the number of planned frequencies while maximizing the usage of each one. Then, our energy conservation algorithm falls into the self-optimization feature of future cellular networks. It optimizes the energy consumption of the cellular network during off-peak hours while taking into account both coverage and user traffic. Finally, we present and discuss the performance of a disaster recovery algorithm using determinantal point processes to patch coverage holes. ",Computer Science - Networking and Internet Architecture ; ,"Vergne, Anaïs ; Decreusefond, Laurent ; Martins, Philippe ; ","Simplicial Homology for Future Cellular Networks  Simplicial homology is a tool that provides a mathematical way to compute the connectivity and the coverage of a cellular network without any node location information. In this article, we use simplicial homology in order to not only compute the topology of a cellular network, but also to discover the clusters of nodes still with no location information. We propose three algorithms for the management of future cellular networks. The first one is a frequency auto-planning algorithm for the self-configuration of future cellular networks. It aims at minimizing the number of planned frequencies while maximizing the usage of each one. Then, our energy conservation algorithm falls into the self-optimization feature of future cellular networks. It optimizes the energy consumption of the cellular network during off-peak hours while taking into account both coverage and user traffic. Finally, we present and discuss the performance of a disaster recovery algorithm using determinantal point processes to patch coverage holes. ",simplicial homology future cellular network simplicial homology tool provide mathematical way compute connectivity coverage cellular network without node location information article use simplicial homology order compute topology cellular network also discover cluster nod still location information propose three algorithms management future cellular network first one frequency auto plan algorithm self configuration future cellular network aim minimize number plan frequencies maximize usage one energy conservation algorithm fall self optimization feature future cellular network optimize energy consumption cellular network peak hours take account coverage user traffic finally present discuss performance disaster recovery algorithm use determinantal point process patch coverage hole,98,6,1312.1664.txt
http://arxiv.org/abs/1312.2048,The False Premises and Promises of Bitcoin,"  Designed to compete with fiat currencies, bitcoin proposes it is a crypto-currency alternative. Bitcoin makes a number of false claims, including: solving the double-spending problem is a good thing; bitcoin can be a reserve currency for banking; hoarding equals saving, and that we should believe bitcoin can expand by deflation to become a global transactional currency supply. Bitcoin's developers combine technical implementation proficiency with ignorance of currency and banking fundamentals. This has resulted in a failed attempt to change finance. A set of recommendations to change finance are provided in the Afterword: Investment/venture banking for the masses; Venture banking to bring back what investment banks once were; Open-outcry exchange for all CDS contracts; Attempting to develop CDS type contracts on investments in startup and existing enterprises; and Improving the connection between startup tech/ideas, business organization and investment. ","Computer Science - Computational Engineering, Finance, and Science ; Quantitative Finance - General Finance ; J.4.1 ; ","Hanley, Brian P. ; ","The False Premises and Promises of Bitcoin  Designed to compete with fiat currencies, bitcoin proposes it is a crypto-currency alternative. Bitcoin makes a number of false claims, including: solving the double-spending problem is a good thing; bitcoin can be a reserve currency for banking; hoarding equals saving, and that we should believe bitcoin can expand by deflation to become a global transactional currency supply. Bitcoin's developers combine technical implementation proficiency with ignorance of currency and banking fundamentals. This has resulted in a failed attempt to change finance. A set of recommendations to change finance are provided in the Afterword: Investment/venture banking for the masses; Venture banking to bring back what investment banks once were; Open-outcry exchange for all CDS contracts; Attempting to develop CDS type contracts on investments in startup and existing enterprises; and Improving the connection between startup tech/ideas, business organization and investment. ",false premise promise bitcoin design compete fiat currencies bitcoin propose crypto currency alternative bitcoin make number false claim include solve double spend problem good thing bitcoin reserve currency bank hoard equal save believe bitcoin expand deflation become global transactional currency supply bitcoin developers combine technical implementation proficiency ignorance currency bank fundamentals result fail attempt change finance set recommendations change finance provide afterword investment venture bank mass venture bank bring back investment bank open outcry exchange cds contract attempt develop cds type contract investments startup exist enterprises improve connection startup tech ideas business organization investment,94,8,1312.2048.txt
http://arxiv.org/abs/1312.2226,On two Algorithmic Problems about Synchronizing Automata,"  Under the assumption $\mathcal{P} \neq \mathcal{NP}$, we prove that two natural problems from the theory of synchronizing automata cannot be solved in polynomial time. The first problem is to decide whether a given reachable partial automaton is synchronizing. The second one is, given an $n$-state binary complete synchronizing automaton, to compute its reset threshold within performance ratio less than $d \ln{(n)}$ for a specific constant $d>0$. ",Computer Science - Formal Languages and Automata Theory ; Computer Science - Computational Complexity ; F.2.0 ; F.4.3 ; ,"Berlinkov, Mikhail V. ; ","On two Algorithmic Problems about Synchronizing Automata  Under the assumption $\mathcal{P} \neq \mathcal{NP}$, we prove that two natural problems from the theory of synchronizing automata cannot be solved in polynomial time. The first problem is to decide whether a given reachable partial automaton is synchronizing. The second one is, given an $n$-state binary complete synchronizing automaton, to compute its reset threshold within performance ratio less than $d \ln{(n)}$ for a specific constant $d>0$. ",two algorithmic problems synchronize automata assumption mathcal neq mathcal np prove two natural problems theory synchronize automata cannot solve polynomial time first problem decide whether give reachable partial automaton synchronize second one give state binary complete synchronize automaton compute reset threshold within performance ratio less ln specific constant,48,14,1312.2226.txt
http://arxiv.org/abs/1312.2674,Silent error detection in numerical time-stepping schemes,"  Errors due to hardware or low level software problems, if detected, can be fixed by various schemes, such as recomputation from a checkpoint. Silent errors are errors in application state that have escaped low-level error detection. At extreme scale, where machines can perform astronomically many operations per second, silent errors threaten the validity of computed results.   We propose a new paradigm for detecting silent errors at the application level. Our central idea is to frequently compare computed values to those provided by a cheap checking computation, and to build error detectors based on the difference between the two output sequences. Numerical analysis provides us with usable checking computations for the solution of initial-value problems in ODEs and PDEs, arguably the most common problems in computational science. Here, we provide, optimize, and test methods based on Runge-Kutta and linear multistep methods for ODEs, and on implicit and explicit finite difference schemes for PDEs. We take the heat equation and Navier-Stokes equations as examples. In tests with artificially injected errors, this approach effectively detects almost all meaningful errors, without significant slowdown. ",Computer Science - Numerical Analysis ; Computer Science - Mathematical Software ; Mathematics - Numerical Analysis ; ,"Benson, Austin R. ; Schmit, Sven ; Schreiber, Robert ; ","Silent error detection in numerical time-stepping schemes  Errors due to hardware or low level software problems, if detected, can be fixed by various schemes, such as recomputation from a checkpoint. Silent errors are errors in application state that have escaped low-level error detection. At extreme scale, where machines can perform astronomically many operations per second, silent errors threaten the validity of computed results.   We propose a new paradigm for detecting silent errors at the application level. Our central idea is to frequently compare computed values to those provided by a cheap checking computation, and to build error detectors based on the difference between the two output sequences. Numerical analysis provides us with usable checking computations for the solution of initial-value problems in ODEs and PDEs, arguably the most common problems in computational science. Here, we provide, optimize, and test methods based on Runge-Kutta and linear multistep methods for ODEs, and on implicit and explicit finite difference schemes for PDEs. We take the heat equation and Navier-Stokes equations as examples. In tests with artificially injected errors, this approach effectively detects almost all meaningful errors, without significant slowdown. ",silent error detection numerical time step scheme errors due hardware low level software problems detect fix various scheme recomputation checkpoint silent errors errors application state escape low level error detection extreme scale machine perform astronomically many operations per second silent errors threaten validity compute result propose new paradigm detect silent errors application level central idea frequently compare compute value provide cheap check computation build error detectors base difference two output sequence numerical analysis provide us usable check computations solution initial value problems odes pdes arguably common problems computational science provide optimize test methods base runge kutta linear multistep methods odes implicit explicit finite difference scheme pdes take heat equation navier stoke equations examples test artificially inject errors approach effectively detect almost meaningful errors without significant slowdown,126,12,1312.2674.txt
http://arxiv.org/abs/1312.3614,Multiple Access Multicarrier Continuous-Variable Quantum Key   Distribution,"  One of the most important practical realizations of the fundamentals of quantum mechanics is continuous-variable quantum key distribution (CVQKD). Here we propose the adaptive multicarrier quadrature division-multiuser quadrature allocation (AMQD-MQA) multiple access technique for continuous-variable quantum key distribution. The MQA scheme is based on the AMQD modulation, which granulates the inputs of the users into Gaussian subcarrier continuous-variables (CVs). In an AMQD-MQA multiple access scenario, the simultaneous reliable transmission of the users is handled by the dynamic allocation of the Gaussian subcarrier CVs. We propose two different settings of AMQD-MQA for multiple input-multiple output communication. We introduce a rate-selection strategy that tunes the modulation variances and allocates adaptively the quadratures of the users over the sub-channels. We also prove the rate formulas if only partial channel side information is available for the users of the sub-channel conditions. We show a technique for the compensation of a nonideal Gaussian input modulation, which allows the users to overwhelm the modulation imperfections to reach optimal capacity-achieving communication over the Gaussian sub-channels. We investigate the diversity amplification of the sub-channel transmittance coefficients and reveal that a strong diversity can be exploited by opportunistic Gaussian modulation. ",Quantum Physics ; Computer Science - Information Theory ; ,"Gyongyosi, Laszlo ; Imre, Sandor ; ","Multiple Access Multicarrier Continuous-Variable Quantum Key   Distribution  One of the most important practical realizations of the fundamentals of quantum mechanics is continuous-variable quantum key distribution (CVQKD). Here we propose the adaptive multicarrier quadrature division-multiuser quadrature allocation (AMQD-MQA) multiple access technique for continuous-variable quantum key distribution. The MQA scheme is based on the AMQD modulation, which granulates the inputs of the users into Gaussian subcarrier continuous-variables (CVs). In an AMQD-MQA multiple access scenario, the simultaneous reliable transmission of the users is handled by the dynamic allocation of the Gaussian subcarrier CVs. We propose two different settings of AMQD-MQA for multiple input-multiple output communication. We introduce a rate-selection strategy that tunes the modulation variances and allocates adaptively the quadratures of the users over the sub-channels. We also prove the rate formulas if only partial channel side information is available for the users of the sub-channel conditions. We show a technique for the compensation of a nonideal Gaussian input modulation, which allows the users to overwhelm the modulation imperfections to reach optimal capacity-achieving communication over the Gaussian sub-channels. We investigate the diversity amplification of the sub-channel transmittance coefficients and reveal that a strong diversity can be exploited by opportunistic Gaussian modulation. ",multiple access multicarrier continuous variable quantum key distribution one important practical realizations fundamentals quantum mechanics continuous variable quantum key distribution cvqkd propose adaptive multicarrier quadrature division multiuser quadrature allocation amqd mqa multiple access technique continuous variable quantum key distribution mqa scheme base amqd modulation granulate input users gaussian subcarrier continuous variables cvs amqd mqa multiple access scenario simultaneous reliable transmission users handle dynamic allocation gaussian subcarrier cvs propose two different settings amqd mqa multiple input multiple output communication introduce rate selection strategy tune modulation variances allocate adaptively quadratures users sub channel also prove rate formulas partial channel side information available users sub channel condition show technique compensation nonideal gaussian input modulation allow users overwhelm modulation imperfections reach optimal capacity achieve communication gaussian sub channel investigate diversity amplification sub channel transmittance coefficients reveal strong diversity exploit opportunistic gaussian modulation,138,12,1312.3614.txt
http://arxiv.org/abs/1312.3748,On Eavesdropper-Tolerance Capability of Two-Hop Wireless Networks,"  Two-hop wireless network serves as the basic net-work model for the study of general wireless networks, while cooperative jamming is a promising scheme to achieve the physi-cal layer security. This paper establishes a theoretical framework for the study of eavesdropper-tolerance capability (i.e., the exact maximum number of eavesdroppers that can be tolerated) in a two-hop wireless network, where the cooperative jamming is adopted to ensure security defined by secrecy outage probability (SOP) and opportunistic relaying is adopted to guarantee relia-bility defined by transmission outage probability (TOP). For the concerned network, closed form modeling for both SOP and TOP is first conducted based on the Central Limit Theorem. With the help of SOP and TOP models and also the Stochastic Ordering Theory, the model for eavesdropper-tolerance capability analysis is then developed. Finally, extensive simulation and numerical results are provided to illustrate the efficiency of our theoretical framework as well as the eavesdropper-tolerance capability of the concerned network from adopting cooperative jamming and opportunistic relaying. ",Computer Science - Information Theory ; ,"Zhang, Yuanyu ; Shen, Yulong ; Wang, Hua ; Jiang, Xiaohong ; ","On Eavesdropper-Tolerance Capability of Two-Hop Wireless Networks  Two-hop wireless network serves as the basic net-work model for the study of general wireless networks, while cooperative jamming is a promising scheme to achieve the physi-cal layer security. This paper establishes a theoretical framework for the study of eavesdropper-tolerance capability (i.e., the exact maximum number of eavesdroppers that can be tolerated) in a two-hop wireless network, where the cooperative jamming is adopted to ensure security defined by secrecy outage probability (SOP) and opportunistic relaying is adopted to guarantee relia-bility defined by transmission outage probability (TOP). For the concerned network, closed form modeling for both SOP and TOP is first conducted based on the Central Limit Theorem. With the help of SOP and TOP models and also the Stochastic Ordering Theory, the model for eavesdropper-tolerance capability analysis is then developed. Finally, extensive simulation and numerical results are provided to illustrate the efficiency of our theoretical framework as well as the eavesdropper-tolerance capability of the concerned network from adopting cooperative jamming and opportunistic relaying. ",eavesdropper tolerance capability two hop wireless network two hop wireless network serve basic net work model study general wireless network cooperative jam promise scheme achieve physi cal layer security paper establish theoretical framework study eavesdropper tolerance capability exact maximum number eavesdroppers tolerate two hop wireless network cooperative jam adopt ensure security define secrecy outage probability sop opportunistic relay adopt guarantee relia bility define transmission outage probability top concern network close form model sop top first conduct base central limit theorem help sop top model also stochastic order theory model eavesdropper tolerance capability analysis develop finally extensive simulation numerical result provide illustrate efficiency theoretical framework well eavesdropper tolerance capability concern network adopt cooperative jam opportunistic relay,115,6,1312.3748.txt
http://arxiv.org/abs/1312.3876,The Symmetric Convex Ordering: A Novel Partial Order for B-DMCs Ordering   the Information Sets of Polar Codes,"  In this paper, we propose a novel partial order for binary discrete memoryless channels that we call the symmetric convex ordering. We show that Ar{\i}kan's polar transform preserves 'symmetric convex orders'. Furthermore, we show that while for symmetric channels this ordering turns out to be equivalent to the stochastic degradation ordering already known to order the information sets of polar codes, a strictly weaker partial order is obtained when at least one of the channels is asymmetric. In between, we also discuss two tools which can be useful for verifying this ordering: a criterion known as the cut criterion and channel symmetrization. Finally, we discuss potential applications of the results to polar coding over non-stationary channels. ",Computer Science - Information Theory ; ,"Alsan, Mine ; ","The Symmetric Convex Ordering: A Novel Partial Order for B-DMCs Ordering   the Information Sets of Polar Codes  In this paper, we propose a novel partial order for binary discrete memoryless channels that we call the symmetric convex ordering. We show that Ar{\i}kan's polar transform preserves 'symmetric convex orders'. Furthermore, we show that while for symmetric channels this ordering turns out to be equivalent to the stochastic degradation ordering already known to order the information sets of polar codes, a strictly weaker partial order is obtained when at least one of the channels is asymmetric. In between, we also discuss two tools which can be useful for verifying this ordering: a criterion known as the cut criterion and channel symmetrization. Finally, we discuss potential applications of the results to polar coding over non-stationary channels. ",symmetric convex order novel partial order dmcs order information set polar cod paper propose novel partial order binary discrete memoryless channel call symmetric convex order show ar kan polar transform preserve symmetric convex order furthermore show symmetric channel order turn equivalent stochastic degradation order already know order information set polar cod strictly weaker partial order obtain least one channel asymmetric also discuss two tool useful verify order criterion know cut criterion channel symmetrization finally discuss potential applications result polar cod non stationary channel,83,5,1312.3876.txt
http://arxiv.org/abs/1312.4510,On the genericity of Whitehead minimality,"  We show that a finitely generated subgroup of a free group, chosen uniformly at random, is strictly Whitehead minimal with overwhelming probability. Whitehead minimality is one of the key elements of the solution of the orbit problem in free groups. The proofs strongly rely on combinatorial tools, notably those of analytic combinatorics. The result we prove actually depends implicitly on the choice of a distribution on finitely generated subgroups, and we establish it for the two distributions which appear in the literature on random subgroups. ",Mathematics - Group Theory ; Computer Science - Computational Complexity ; Mathematics - Combinatorics ; ,"Bassino, Frédérique ; Nicaud, Cyril ; Weil, Pascal ; ","On the genericity of Whitehead minimality  We show that a finitely generated subgroup of a free group, chosen uniformly at random, is strictly Whitehead minimal with overwhelming probability. Whitehead minimality is one of the key elements of the solution of the orbit problem in free groups. The proofs strongly rely on combinatorial tools, notably those of analytic combinatorics. The result we prove actually depends implicitly on the choice of a distribution on finitely generated subgroups, and we establish it for the two distributions which appear in the literature on random subgroups. ",genericity whitehead minimality show finitely generate subgroup free group choose uniformly random strictly whitehead minimal overwhelm probability whitehead minimality one key elements solution orbit problem free group proof strongly rely combinatorial tool notably analytic combinatorics result prove actually depend implicitly choice distribution finitely generate subgroups establish two distributions appear literature random subgroups,52,7,1312.4510.txt
http://arxiv.org/abs/1312.6809,The Micro Dynamics of Collective Violence,"  Collective violence in direct confrontations between two opposing groups happens in short bursts wherein small subgroups briefly attack small numbers of opponents, while the others form a non-fighting audience. The mechanism is fighters' synchronization of intentionalities during preliminary interactions, by which they feel one and overcome their fear. To explain these bursts, subgroups' small sizes and leaders' role, a social influence model and a synchronization model are compared. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Bruggeman, Jeroen ; ","The Micro Dynamics of Collective Violence  Collective violence in direct confrontations between two opposing groups happens in short bursts wherein small subgroups briefly attack small numbers of opponents, while the others form a non-fighting audience. The mechanism is fighters' synchronization of intentionalities during preliminary interactions, by which they feel one and overcome their fear. To explain these bursts, subgroups' small sizes and leaders' role, a social influence model and a synchronization model are compared. ",micro dynamics collective violence collective violence direct confrontations two oppose group happen short burst wherein small subgroups briefly attack small number opponents others form non fight audience mechanism fighters synchronization intentionalities preliminary interactions feel one overcome fear explain burst subgroups small size leaders role social influence model synchronization model compare,50,4,1312.6809.txt
http://arxiv.org/abs/1401.1061,Learning optimization models in the presence of unknown relations,"  In a sequential auction with multiple bidding agents, it is highly challenging to determine the ordering of the items to sell in order to maximize the revenue due to the fact that the autonomy and private information of the agents heavily influence the outcome of the auction.   The main contribution of this paper is two-fold. First, we demonstrate how to apply machine learning techniques to solve the optimal ordering problem in sequential auctions. We learn regression models from historical auctions, which are subsequently used to predict the expected value of orderings for new auctions. Given the learned models, we propose two types of optimization methods: a black-box best-first search approach, and a novel white-box approach that maps learned models to integer linear programs (ILP) which can then be solved by any ILP-solver. Although the studied auction design problem is hard, our proposed optimization methods obtain good orderings with high revenues.   Our second main contribution is the insight that the internal structure of regression models can be efficiently evaluated inside an ILP solver for optimization purposes. To this end, we provide efficient encodings of regression trees and linear regression models as ILP constraints. This new way of using learned models for optimization is promising. As the experimental results show, it significantly outperforms the black-box best-first search in nearly all settings. ",Computer Science - Artificial Intelligence ; Computer Science - Computer Science and Game Theory ; F.5.3 ; K.3 ; K.4 ; ,"Verwer, Sicco ; Zhang, Yingqian ; Ye, Qing Chuan ; ","Learning optimization models in the presence of unknown relations  In a sequential auction with multiple bidding agents, it is highly challenging to determine the ordering of the items to sell in order to maximize the revenue due to the fact that the autonomy and private information of the agents heavily influence the outcome of the auction.   The main contribution of this paper is two-fold. First, we demonstrate how to apply machine learning techniques to solve the optimal ordering problem in sequential auctions. We learn regression models from historical auctions, which are subsequently used to predict the expected value of orderings for new auctions. Given the learned models, we propose two types of optimization methods: a black-box best-first search approach, and a novel white-box approach that maps learned models to integer linear programs (ILP) which can then be solved by any ILP-solver. Although the studied auction design problem is hard, our proposed optimization methods obtain good orderings with high revenues.   Our second main contribution is the insight that the internal structure of regression models can be efficiently evaluated inside an ILP solver for optimization purposes. To this end, we provide efficient encodings of regression trees and linear regression models as ILP constraints. This new way of using learned models for optimization is promising. As the experimental results show, it significantly outperforms the black-box best-first search in nearly all settings. ",learn optimization model presence unknown relations sequential auction multiple bid agents highly challenge determine order items sell order maximize revenue due fact autonomy private information agents heavily influence outcome auction main contribution paper two fold first demonstrate apply machine learn techniques solve optimal order problem sequential auction learn regression model historical auction subsequently use predict expect value order new auction give learn model propose two type optimization methods black box best first search approach novel white box approach map learn model integer linear program ilp solve ilp solver although study auction design problem hard propose optimization methods obtain good order high revenues second main contribution insight internal structure regression model efficiently evaluate inside ilp solver optimization purpose end provide efficient encode regression tree linear regression model ilp constraints new way use learn model optimization promise experimental result show significantly outperform black box best first search nearly settings,147,11,1401.1061.txt
http://arxiv.org/abs/1401.1140,Efficient random sampling of binary and unary-binary trees via holonomic   equations,  We present a new uniform random sampler for binary trees with $n$ internal nodes consuming $2n + \Theta(\log(n)^2)$ random bits on average. This makes it quasi-optimal and out-performs the classical Remy algorithm. We also present a sampler for unary-binary trees with $n$ nodes taking $\Theta(n)$ random bits on average. Both are the first linear-time algorithms to be optimal up to a constant. ,Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; ,"Bacher, Axel ; Bodini, Olivier ; Jacquot, Alice ; ",Efficient random sampling of binary and unary-binary trees via holonomic   equations  We present a new uniform random sampler for binary trees with $n$ internal nodes consuming $2n + \Theta(\log(n)^2)$ random bits on average. This makes it quasi-optimal and out-performs the classical Remy algorithm. We also present a sampler for unary-binary trees with $n$ nodes taking $\Theta(n)$ random bits on average. Both are the first linear-time algorithms to be optimal up to a constant. ,efficient random sample binary unary binary tree via holonomic equations present new uniform random sampler binary tree internal nod consume theta log random bits average make quasi optimal perform classical remy algorithm also present sampler unary binary tree nod take theta random bits average first linear time algorithms optimal constant,50,11,1401.1140.txt
http://arxiv.org/abs/1401.1333,Time series forecasting using neural networks,  Recent studies have shown the classification and prediction power of the Neural Networks. It has been demonstrated that a NN can approximate any continuous function. Neural networks have been successfully used for forecasting of financial data series. The classical methods used for time series prediction like Box-Jenkins or ARIMA assumes that there is a linear relationship between inputs and outputs. Neural Networks have the advantage that can approximate nonlinear functions. In this paper we compared the performances of different feed forward and recurrent neural networks and training algorithms for predicting the exchange rate EUR/RON and USD/RON. We used data series with daily exchange rates starting from 2005 until 2013. ,Computer Science - Neural and Evolutionary Computing ; ,"Oancea, Bogdan ; Ciucu, ŞTefan Cristian ; ",Time series forecasting using neural networks  Recent studies have shown the classification and prediction power of the Neural Networks. It has been demonstrated that a NN can approximate any continuous function. Neural networks have been successfully used for forecasting of financial data series. The classical methods used for time series prediction like Box-Jenkins or ARIMA assumes that there is a linear relationship between inputs and outputs. Neural Networks have the advantage that can approximate nonlinear functions. In this paper we compared the performances of different feed forward and recurrent neural networks and training algorithms for predicting the exchange rate EUR/RON and USD/RON. We used data series with daily exchange rates starting from 2005 until 2013. ,time series forecast use neural network recent study show classification prediction power neural network demonstrate nn approximate continuous function neural network successfully use forecast financial data series classical methods use time series prediction like box jenkins arima assume linear relationship input output neural network advantage approximate nonlinear function paper compare performances different fee forward recurrent neural network train algorithms predict exchange rate eur ron usd ron use data series daily exchange rat start,73,6,1401.1333.txt
http://arxiv.org/abs/1401.1671,Distributed Energy Efficient Channel Allocation,"  Design of energy efficient protocols for modern wireless systems has become an important area of research. In this paper, we propose a distributed optimization algorithm for the channel assignment problem for multiple interfering transceiver pairs that cannot communicate with each other. We first modify the auction algorithm for maximal energy efficiency and show that the problem can be solved without explicit message passing using the carrier sense multiple access (CSMA) protocols. We then develop a novel scheme by converting the channel assignment problem into perfect matchings on bipartite graphs. The proposed scheme improves the energy efficiency and does not require any explicit message passing or a shared memory between the users. We derive bounds on the convergence rate and show that the proposed algorithm converges faster than the distributed auction algorithm and achieves near-optimal performance under Rayleigh fading channels. We also present an asymptotic performance analysis of the fast matching algorithm for energy efficient resource allocation and prove the optimality for large enough number of users and number of channels. Finally, we provide numerical assessments that confirm the energy efficiency gains compared to the state of the art. ",Computer Science - Networking and Internet Architecture ; Computer Science - Information Theory ; ,"Naparstek, Oshri ; Zafaruddin, S. M. ; Leshem, Amir ; Jorswieck, Eduard ; ","Distributed Energy Efficient Channel Allocation  Design of energy efficient protocols for modern wireless systems has become an important area of research. In this paper, we propose a distributed optimization algorithm for the channel assignment problem for multiple interfering transceiver pairs that cannot communicate with each other. We first modify the auction algorithm for maximal energy efficiency and show that the problem can be solved without explicit message passing using the carrier sense multiple access (CSMA) protocols. We then develop a novel scheme by converting the channel assignment problem into perfect matchings on bipartite graphs. The proposed scheme improves the energy efficiency and does not require any explicit message passing or a shared memory between the users. We derive bounds on the convergence rate and show that the proposed algorithm converges faster than the distributed auction algorithm and achieves near-optimal performance under Rayleigh fading channels. We also present an asymptotic performance analysis of the fast matching algorithm for energy efficient resource allocation and prove the optimality for large enough number of users and number of channels. Finally, we provide numerical assessments that confirm the energy efficiency gains compared to the state of the art. ",distribute energy efficient channel allocation design energy efficient protocols modern wireless systems become important area research paper propose distribute optimization algorithm channel assignment problem multiple interfere transceiver pair cannot communicate first modify auction algorithm maximal energy efficiency show problem solve without explicit message pass use carrier sense multiple access csma protocols develop novel scheme convert channel assignment problem perfect match bipartite graph propose scheme improve energy efficiency require explicit message pass share memory users derive bound convergence rate show propose algorithm converge faster distribute auction algorithm achieve near optimal performance rayleigh fade channel also present asymptotic performance analysis fast match algorithm energy efficient resource allocation prove optimality large enough number users number channel finally provide numerical assessments confirm energy efficiency gain compare state art,124,12,1401.1671.txt
http://arxiv.org/abs/1401.1861,Empirical Patterns in Google Scholar Citation Counts,"  Scholarly impact may be metricized using an author's total number of citations as a stand-in for real worth, but this measure varies in applicability between disciplines. The detail of the number of citations per publication is nowadays mapped in much more detail on the Web, exposing certain empirical patterns. This paper explores those patterns, using the citation data from Google Scholar for a number of authors. ","Computer Science - Digital Libraries ; 62P99, 01A90 ; I.5.1 ; I.7.5 ; ","Breuer, Peter T. ; Bowen, Jonathan P. ; ","Empirical Patterns in Google Scholar Citation Counts  Scholarly impact may be metricized using an author's total number of citations as a stand-in for real worth, but this measure varies in applicability between disciplines. The detail of the number of citations per publication is nowadays mapped in much more detail on the Web, exposing certain empirical patterns. This paper explores those patterns, using the citation data from Google Scholar for a number of authors. ",empirical pattern google scholar citation count scholarly impact may metricize use author total number citations stand real worth measure vary applicability discipline detail number citations per publication nowadays map much detail web expose certain empirical pattern paper explore pattern use citation data google scholar number author,46,10,1401.1861.txt
http://arxiv.org/abs/1401.2411,"Clustering, Coding, and the Concept of Similarity","  This paper develops a theory of clustering and coding which combines a geometric model with a probabilistic model in a principled way. The geometric model is a Riemannian manifold with a Riemannian metric, ${g}_{ij}({\bf x})$, which we interpret as a measure of dissimilarity. The probabilistic model consists of a stochastic process with an invariant probability measure which matches the density of the sample input data. The link between the two models is a potential function, $U({\bf x})$, and its gradient, $\nabla U({\bf x})$. We use the gradient to define the dissimilarity metric, which guarantees that our measure of dissimilarity will depend on the probability measure. Finally, we use the dissimilarity metric to define a coordinate system on the embedded Riemannian manifold, which gives us a low-dimensional encoding of our original data. ",Computer Science - Machine Learning ; ,"McCarty, L. Thorne ; ","Clustering, Coding, and the Concept of Similarity  This paper develops a theory of clustering and coding which combines a geometric model with a probabilistic model in a principled way. The geometric model is a Riemannian manifold with a Riemannian metric, ${g}_{ij}({\bf x})$, which we interpret as a measure of dissimilarity. The probabilistic model consists of a stochastic process with an invariant probability measure which matches the density of the sample input data. The link between the two models is a potential function, $U({\bf x})$, and its gradient, $\nabla U({\bf x})$. We use the gradient to define the dissimilarity metric, which guarantees that our measure of dissimilarity will depend on the probability measure. Finally, we use the dissimilarity metric to define a coordinate system on the embedded Riemannian manifold, which gives us a low-dimensional encoding of our original data. ",cluster cod concept similarity paper develop theory cluster cod combine geometric model probabilistic model principled way geometric model riemannian manifold riemannian metric ij bf interpret measure dissimilarity probabilistic model consist stochastic process invariant probability measure match density sample input data link two model potential function bf gradient nabla bf use gradient define dissimilarity metric guarantee measure dissimilarity depend probability measure finally use dissimilarity metric define coordinate system embed riemannian manifold give us low dimensional encode original data,77,11,1401.2411.txt
http://arxiv.org/abs/1401.3580,Bits Through Bufferless Queues,"  This paper investigates the capacity of a channel in which information is conveyed by the timing of consecutive packets passing through a queue with independent and identically distributed service times. Such timing channels are commonly studied under the assumption of a work-conserving queue. In contrast, this paper studies the case of a bufferless queue that drops arriving packets while a packet is in service. Under this bufferless model, the paper provides upper bounds on the capacity of timing channels and establishes achievable rates for the case of bufferless M/M/1 and M/G/1 queues. In particular, it is shown that a bufferless M/M/1 queue at worst suffers less than 10% reduction in capacity when compared to an M/M/1 work-conserving queue. ",Computer Science - Information Theory ; ,"Tavan, Mehrnaz ; Yates, Roy D. ; Bajwa, Waheed U. ; ","Bits Through Bufferless Queues  This paper investigates the capacity of a channel in which information is conveyed by the timing of consecutive packets passing through a queue with independent and identically distributed service times. Such timing channels are commonly studied under the assumption of a work-conserving queue. In contrast, this paper studies the case of a bufferless queue that drops arriving packets while a packet is in service. Under this bufferless model, the paper provides upper bounds on the capacity of timing channels and establishes achievable rates for the case of bufferless M/M/1 and M/G/1 queues. In particular, it is shown that a bufferless M/M/1 queue at worst suffers less than 10% reduction in capacity when compared to an M/M/1 work-conserving queue. ",bits bufferless queue paper investigate capacity channel information convey time consecutive packets pass queue independent identically distribute service time time channel commonly study assumption work conserve queue contrast paper study case bufferless queue drop arrive packets packet service bufferless model paper provide upper bound capacity time channel establish achievable rat case bufferless queue particular show bufferless queue worst suffer less reduction capacity compare work conserve queue,66,11,1401.3580.txt
http://arxiv.org/abs/1401.3667,Group Testing with Prior Statistics,"  We consider a new group testing model wherein each item is a binary random variable defined by an a priori probability of being defective. We assume that each probability is small and that items are independent, but not necessarily identically distributed. The goal of group testing algorithms is to identify with high probability the subset of defectives via non-linear (disjunctive) binary measurements. Our main contributions are two classes of algorithms: (1) adaptive algorithms with tests based either on a maximum entropy principle, or on a Shannon-Fano/Huffman code; (2) non-adaptive algorithms. Under loose assumptions and with high probability, our algorithms only need a number of measurements that is close to the information-theoretic lower bound, up to an explicitly-calculated universal constant factor. We provide simulations to support our results. ",Computer Science - Information Theory ; ,"Li, Tongxin ; Chan, Chun Lam ; Huang, Wenhao ; Kaced, Tarik ; Jaggi, Sidharth ; ","Group Testing with Prior Statistics  We consider a new group testing model wherein each item is a binary random variable defined by an a priori probability of being defective. We assume that each probability is small and that items are independent, but not necessarily identically distributed. The goal of group testing algorithms is to identify with high probability the subset of defectives via non-linear (disjunctive) binary measurements. Our main contributions are two classes of algorithms: (1) adaptive algorithms with tests based either on a maximum entropy principle, or on a Shannon-Fano/Huffman code; (2) non-adaptive algorithms. Under loose assumptions and with high probability, our algorithms only need a number of measurements that is close to the information-theoretic lower bound, up to an explicitly-calculated universal constant factor. We provide simulations to support our results. ",group test prior statistics consider new group test model wherein item binary random variable define priori probability defective assume probability small items independent necessarily identically distribute goal group test algorithms identify high probability subset defectives via non linear disjunctive binary measurements main contributions two class algorithms adaptive algorithms test base either maximum entropy principle shannon fano huffman code non adaptive algorithms loose assumptions high probability algorithms need number measurements close information theoretic lower bind explicitly calculate universal constant factor provide simulations support result,83,12,1401.3667.txt
http://arxiv.org/abs/1401.5277,Towards a Uniform Theory of Effectful State Machines,"  We use recent developments on coalgebraic and monad-based semantics to obtain a generic notion of a T-automaton, where T is a monad. This enables a uniform study of various notions of machines: e.g. finite state machines, multi-stack machines, Turing machines, valence automata, and weighted automata. We use the generalized powerset construction to define a generic language semantics for T-automata, and we show by numerous examples that it correctly instantiates for some known classes of machines/languages, including regular, context-free, recursively-enumerable and various subclasses of context free languages (e.g. deterministic and real-time ones). Moreover, our approach provides new generic techniques for studying expressivity power of various machine-based models. ",Computer Science - Logic in Computer Science ; Computer Science - Formal Languages and Automata Theory ; ,"Goncharov, Sergey ; Milius, Stefan ; Silva, Alexandra ; ","Towards a Uniform Theory of Effectful State Machines  We use recent developments on coalgebraic and monad-based semantics to obtain a generic notion of a T-automaton, where T is a monad. This enables a uniform study of various notions of machines: e.g. finite state machines, multi-stack machines, Turing machines, valence automata, and weighted automata. We use the generalized powerset construction to define a generic language semantics for T-automata, and we show by numerous examples that it correctly instantiates for some known classes of machines/languages, including regular, context-free, recursively-enumerable and various subclasses of context free languages (e.g. deterministic and real-time ones). Moreover, our approach provides new generic techniques for studying expressivity power of various machine-based models. ",towards uniform theory effectful state machine use recent developments coalgebraic monad base semantics obtain generic notion automaton monad enable uniform study various notions machine finite state machine multi stack machine turing machine valence automata weight automata use generalize powerset construction define generic language semantics automata show numerous examples correctly instantiate know class machine languages include regular context free recursively enumerable various subclasses context free languages deterministic real time ones moreover approach provide new generic techniques study expressivity power various machine base model,82,14,1401.5277.txt
http://arxiv.org/abs/1401.5791,Advanced Signal Processing Techniqes to Study Normal and Epileptic EEG,"  EEG monitoring has an important milestone provide valuable information of those candidates who suffer from epilepsy.In this paper human normal and epileptic Electroencephalogram signals are analyzed with popular and efficient signal processing techniques like Fourier and Wavelet transform. The delta, theta, alpha, beta and gamma sub bands of EEG are obtained and studied for detection of seizure and epilepsy. The extracted feature is then applied to ANN for classification of the EEG signals. ","Computer Science - Computational Engineering, Finance, and Science ; ","Dash, Debadatta ; ","Advanced Signal Processing Techniqes to Study Normal and Epileptic EEG  EEG monitoring has an important milestone provide valuable information of those candidates who suffer from epilepsy.In this paper human normal and epileptic Electroencephalogram signals are analyzed with popular and efficient signal processing techniques like Fourier and Wavelet transform. The delta, theta, alpha, beta and gamma sub bands of EEG are obtained and studied for detection of seizure and epilepsy. The extracted feature is then applied to ANN for classification of the EEG signals. ",advance signal process techniqes study normal epileptic eeg eeg monitor important milestone provide valuable information candidates suffer epilepsy paper human normal epileptic electroencephalogram signal analyze popular efficient signal process techniques like fourier wavelet transform delta theta alpha beta gamma sub band eeg obtain study detection seizure epilepsy extract feature apply ann classification eeg signal,54,9,1401.5791.txt
http://arxiv.org/abs/1401.6312,Predicate Logic as a Modelling Language: The IDP System,"  With the technology of the time, Kowalski's seminal 1974 paper {\em Predicate Logic as a Programming Language} was a breakthrough for the use of logic in computer science. It introduced two fundamental ideas: on the declarative side, the use of the Horn clause logic fragment of classical logic, which was soon extended with negation as failure, on the procedural side the procedural interpretation which made it possible to write algorithms in the formalism.   Since then, strong progress was made both on the declarative understanding of the logic programming formalism and in automated reasoning technologies, particularly in SAT solving, Constraint Programming and Answer Set Programming. This has paved the way for the development of an extension of logic programming that embodies a more pure view of logic as a modelling language and its role for problem solving.   In this paper, we present the \idp language and system. The language is essentially classical logic extended with one of logic programmings most important contributions to knowledge representation: the representation of complex definitions as rule sets under well-founded semantics. The system is a knowledge base system: a system in which complex declarative information is stored in a knowledge base which can be used to solve different computational problems by applying multiple forms of inference. In this view, theories are declarative modellings, bags of information, descriptions of possible states of affairs. They are neither procedures nor descriptions of computational problems. As such, the \idp language and system preserve the fundamental idea of a declarative reading of logic programs, while they break with the fundamental idea of the procedural interpretation of logic programs. ",Computer Science - Logic in Computer Science ; ,"De Cat, Broes ; Bogaerts, Bart ; Bruynooghe, Maurice ; Janssens, Gerda ; Denecker, Marc ; ","Predicate Logic as a Modelling Language: The IDP System  With the technology of the time, Kowalski's seminal 1974 paper {\em Predicate Logic as a Programming Language} was a breakthrough for the use of logic in computer science. It introduced two fundamental ideas: on the declarative side, the use of the Horn clause logic fragment of classical logic, which was soon extended with negation as failure, on the procedural side the procedural interpretation which made it possible to write algorithms in the formalism.   Since then, strong progress was made both on the declarative understanding of the logic programming formalism and in automated reasoning technologies, particularly in SAT solving, Constraint Programming and Answer Set Programming. This has paved the way for the development of an extension of logic programming that embodies a more pure view of logic as a modelling language and its role for problem solving.   In this paper, we present the \idp language and system. The language is essentially classical logic extended with one of logic programmings most important contributions to knowledge representation: the representation of complex definitions as rule sets under well-founded semantics. The system is a knowledge base system: a system in which complex declarative information is stored in a knowledge base which can be used to solve different computational problems by applying multiple forms of inference. In this view, theories are declarative modellings, bags of information, descriptions of possible states of affairs. They are neither procedures nor descriptions of computational problems. As such, the \idp language and system preserve the fundamental idea of a declarative reading of logic programs, while they break with the fundamental idea of the procedural interpretation of logic programs. ",predicate logic model language idp system technology time kowalski seminal paper em predicate logic program language breakthrough use logic computer science introduce two fundamental ideas declarative side use horn clause logic fragment classical logic soon extend negation failure procedural side procedural interpretation make possible write algorithms formalism since strong progress make declarative understand logic program formalism automate reason technologies particularly sit solve constraint program answer set program pave way development extension logic program embody pure view logic model language role problem solve paper present idp language system language essentially classical logic extend one logic programme important contributions knowledge representation representation complex definitions rule set well found semantics system knowledge base system system complex declarative information store knowledge base use solve different computational problems apply multiple form inference view theories declarative modellings bag information descriptions possible state affairs neither procedures descriptions computational problems idp language system preserve fundamental idea declarative read logic program break fundamental idea procedural interpretation logic program,159,8,1401.6312.txt
http://arxiv.org/abs/1401.6681,On giant components and treewidth in the layers model,"  Given an undirected $n$-vertex graph $G(V,E)$ and an integer $k$, let $T_k(G)$ denote the random vertex induced subgraph of $G$ generated by ordering $V$ according to a random permutation $\pi$ and including in $T_k(G)$ those vertices with at most $k-1$ of their neighbors preceding them in this order. The distribution of subgraphs sampled in this manner is called the \emph{layers model with parameter} $k$. The layers model has found applications in studying $\ell$-degenerate subgraphs, the design of algorithms for the maximum independent set problem, and in bootstrap percolation.   In the current work we expand the study of structural properties of the layers model.   We prove that there are $3$-regular graphs $G$ for which with high probability $T_3(G)$ has a connected component of size $\Omega(n)$. Moreover, this connected component has treewidth $\Omega(n)$. This lower bound on the treewidth extends to many other random graph models. In contrast, $T_2(G)$ is known to be a forest (hence of treewidth~1), and we establish that if $G$ is of bounded degree then with high probability the largest connected component in $T_2(G)$ is of size $O(\log n)$. We also consider the infinite two-dimensional grid, for which we prove that the first four layers contain a unique infinite connected component with probability $1$. ",Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Feige, Uriel ; Hermon, Jonathan ; Reichman, Daniel ; ","On giant components and treewidth in the layers model  Given an undirected $n$-vertex graph $G(V,E)$ and an integer $k$, let $T_k(G)$ denote the random vertex induced subgraph of $G$ generated by ordering $V$ according to a random permutation $\pi$ and including in $T_k(G)$ those vertices with at most $k-1$ of their neighbors preceding them in this order. The distribution of subgraphs sampled in this manner is called the \emph{layers model with parameter} $k$. The layers model has found applications in studying $\ell$-degenerate subgraphs, the design of algorithms for the maximum independent set problem, and in bootstrap percolation.   In the current work we expand the study of structural properties of the layers model.   We prove that there are $3$-regular graphs $G$ for which with high probability $T_3(G)$ has a connected component of size $\Omega(n)$. Moreover, this connected component has treewidth $\Omega(n)$. This lower bound on the treewidth extends to many other random graph models. In contrast, $T_2(G)$ is known to be a forest (hence of treewidth~1), and we establish that if $G$ is of bounded degree then with high probability the largest connected component in $T_2(G)$ is of size $O(\log n)$. We also consider the infinite two-dimensional grid, for which we prove that the first four layers contain a unique infinite connected component with probability $1$. ",giant components treewidth layer model give undirected vertex graph integer let denote random vertex induce subgraph generate order accord random permutation pi include vertices neighbor precede order distribution subgraphs sample manner call emph layer model parameter layer model find applications study ell degenerate subgraphs design algorithms maximum independent set problem bootstrap percolation current work expand study structural properties layer model prove regular graph high probability connect component size omega moreover connect component treewidth omega lower bind treewidth extend many random graph model contrast know forest hence treewidth establish bound degree high probability largest connect component size log also consider infinite two dimensional grid prove first four layer contain unique infinite connect component probability,113,3,1401.6681.txt
http://arxiv.org/abs/1401.7860,Motion planning and control of a planar polygonal linkage,"  For a polygonal linkage, we produce a fast navigation algorithm on its configuration space. The basic idea is to approximate the configuration space by the vertex-edge graph of its cell decomposition discovered by the first author. The algorithm has three aspects: (1) the number of navigation steps does not exceed 15 (independent of the linkage), (2) each step is a disguised flex of a quadrilateral from one triangular configuration to another, which is a well understood type of flex, and (3) each step can be performed explicitly by adding some extra bars and obtaining a mechanism with one degree of freedom. ",Mathematics - Metric Geometry ; Computer Science - Robotics ; 57Q99 52C99 57Q55 ; ,"Panina, Gaiane ; Siersma, Dirk ; ","Motion planning and control of a planar polygonal linkage  For a polygonal linkage, we produce a fast navigation algorithm on its configuration space. The basic idea is to approximate the configuration space by the vertex-edge graph of its cell decomposition discovered by the first author. The algorithm has three aspects: (1) the number of navigation steps does not exceed 15 (independent of the linkage), (2) each step is a disguised flex of a quadrilateral from one triangular configuration to another, which is a well understood type of flex, and (3) each step can be performed explicitly by adding some extra bars and obtaining a mechanism with one degree of freedom. ",motion plan control planar polygonal linkage polygonal linkage produce fast navigation algorithm configuration space basic idea approximate configuration space vertex edge graph cell decomposition discover first author algorithm three aspects number navigation step exceed independent linkage step disguise flex quadrilateral one triangular configuration another well understand type flex step perform explicitly add extra bar obtain mechanism one degree freedom,59,4,1401.7860.txt
http://arxiv.org/abs/1401.8219,On the Properties of the Priority Deriving Procedure in the Pairwise   Comparisons Method,"  The pairwise comparisons method is a convenient tool used when the relative order of preferences among different concepts (alternatives) needs to be determined. There are several popular implementations of this method, including the Eigenvector Method, the Least Squares Method, the Chi Squares Method and others. Each of the above methods comes with one or more inconsistency indices that help to decide whether the consistency of input guarantees obtaining a reliable output, thus taking the optimal decision. This article explores the relationship between inconsistency of input and discrepancy of output. A global ranking discrepancy describes to what extent the obtained results correspond to the single expert's assessments. On the basis of the inconsistency and discrepancy indices, two properties of the weight deriving procedure are formulated. These properties are proven for Eigenvector Method and Koczkodaj's Inconsistency Index. Several estimates using Koczkodaj's Inconsistency Index for a principal eigenvalue, Saaty's inconsistency index and the Condition of Order Preservation are also provided. ",Computer Science - Discrete Mathematics ; Computer Science - Computer Science and Game Theory ; ,"Kułakowski, Konrad ; ","On the Properties of the Priority Deriving Procedure in the Pairwise   Comparisons Method  The pairwise comparisons method is a convenient tool used when the relative order of preferences among different concepts (alternatives) needs to be determined. There are several popular implementations of this method, including the Eigenvector Method, the Least Squares Method, the Chi Squares Method and others. Each of the above methods comes with one or more inconsistency indices that help to decide whether the consistency of input guarantees obtaining a reliable output, thus taking the optimal decision. This article explores the relationship between inconsistency of input and discrepancy of output. A global ranking discrepancy describes to what extent the obtained results correspond to the single expert's assessments. On the basis of the inconsistency and discrepancy indices, two properties of the weight deriving procedure are formulated. These properties are proven for Eigenvector Method and Koczkodaj's Inconsistency Index. Several estimates using Koczkodaj's Inconsistency Index for a principal eigenvalue, Saaty's inconsistency index and the Condition of Order Preservation are also provided. ",properties priority derive procedure pairwise comparisons method pairwise comparisons method convenient tool use relative order preferences among different concepts alternatives need determine several popular implementations method include eigenvector method least square method chi square method others methods come one inconsistency indices help decide whether consistency input guarantee obtain reliable output thus take optimal decision article explore relationship inconsistency input discrepancy output global rank discrepancy describe extent obtain result correspond single expert assessments basis inconsistency discrepancy indices two properties weight derive procedure formulate properties prove eigenvector method koczkodaj inconsistency index several estimate use koczkodaj inconsistency index principal eigenvalue saaty inconsistency index condition order preservation also provide,105,8,1401.8219.txt
http://arxiv.org/abs/1402.0485,Local algorithms for independent sets are half-optimal,"  We show that the largest density of factor of i.i.d. independent sets on the d-regular tree is asymptotically at most (log d)/d as d tends to infinity. This matches the lower bound given by previous constructions. It follows that the largest independent sets given by local algorithms on random d-regular graphs have the same asymptotic density. In contrast, the density of the largest independent sets on these graphs is asymptotically 2(log d)/d. We also prove analogous results for Poisson-Galton-Watson trees, which yield bounds for local algorithms on sparse Erdos-Renyi graphs. ","Mathematics - Probability ; Computer Science - Distributed, Parallel, and Cluster Computing ; Mathematics - Combinatorics ; ","Rahman, Mustazee ; Virag, Balint ; ","Local algorithms for independent sets are half-optimal  We show that the largest density of factor of i.i.d. independent sets on the d-regular tree is asymptotically at most (log d)/d as d tends to infinity. This matches the lower bound given by previous constructions. It follows that the largest independent sets given by local algorithms on random d-regular graphs have the same asymptotic density. In contrast, the density of the largest independent sets on these graphs is asymptotically 2(log d)/d. We also prove analogous results for Poisson-Galton-Watson trees, which yield bounds for local algorithms on sparse Erdos-Renyi graphs. ",local algorithms independent set half optimal show largest density factor independent set regular tree asymptotically log tend infinity match lower bind give previous constructions follow largest independent set give local algorithms random regular graph asymptotic density contrast density largest independent set graph asymptotically log also prove analogous result poisson galton watson tree yield bound local algorithms sparse erdos renyi graph,60,3,1402.0485.txt
http://arxiv.org/abs/1402.1526,Dual Query: Practical Private Query Release for High Dimensional Data,"  We present a practical, differentially private algorithm for answering a large number of queries on high dimensional datasets. Like all algorithms for this task, ours necessarily has worst-case complexity exponential in the dimension of the data. However, our algorithm packages the computationally hard step into a concisely defined integer program, which can be solved non-privately using standard solvers. We prove accuracy and privacy theorems for our algorithm, and then demonstrate experimentally that our algorithm performs well in practice. For example, our algorithm can efficiently and accurately answer millions of queries on the Netflix dataset, which has over 17,000 attributes; this is an improvement on the state of the art by multiple orders of magnitude. ",Computer Science - Data Structures and Algorithms ; Computer Science - Cryptography and Security ; Computer Science - Databases ; Computer Science - Machine Learning ; ,"Gaboardi, Marco ; Arias, Emilio Jesús Gallego ; Hsu, Justin ; Roth, Aaron ; Wu, Zhiwei Steven ; ","Dual Query: Practical Private Query Release for High Dimensional Data  We present a practical, differentially private algorithm for answering a large number of queries on high dimensional datasets. Like all algorithms for this task, ours necessarily has worst-case complexity exponential in the dimension of the data. However, our algorithm packages the computationally hard step into a concisely defined integer program, which can be solved non-privately using standard solvers. We prove accuracy and privacy theorems for our algorithm, and then demonstrate experimentally that our algorithm performs well in practice. For example, our algorithm can efficiently and accurately answer millions of queries on the Netflix dataset, which has over 17,000 attributes; this is an improvement on the state of the art by multiple orders of magnitude. ",dual query practical private query release high dimensional data present practical differentially private algorithm answer large number query high dimensional datasets like algorithms task necessarily worst case complexity exponential dimension data however algorithm package computationally hard step concisely define integer program solve non privately use standard solvers prove accuracy privacy theorems algorithm demonstrate experimentally algorithm perform well practice example algorithm efficiently accurately answer millions query netflix dataset attribute improvement state art multiple order magnitude,74,1,1402.1526.txt
http://arxiv.org/abs/1402.1607,Generalized Signal Alignment For MIMO Two-Way X Relay Channels,"  We study the degrees of freedom (DoF) of MIMO two-way X relay channels. Previous work studied the case $N < 2M$, where $N$ and $M$ denote the number of antennas at the relay and each source, respectively, and showed that the maximum DoF of $2N$ is achievable when $N \leq \lfloor\frac{8M}{5}\rfloor$ by applying signal alignment (SA) for network coding and interference cancelation. This work considers the case $N>2M$ where the performance is limited by the number of antennas at each source node and conventional SA is not feasible. We propose a \textit{generalized signal alignment} (GSA) based transmission scheme. The key is to let the signals to be exchanged between every source node align in a transformed subspace, rather than the direct subspace, at the relay so as to form network-coded signals. This is realized by jointly designing the precoding matrices at all source nodes and the processing matrix at the relay. Moreover, the aligned subspaces are orthogonal to each other. By applying the GSA, we show that the DoF upper bound $4M$ is achievable when $M \leq \lfloor\frac{2N}{5}\rfloor$ ($M$ is even) or $M \leq \lfloor\frac{2N-1}{5}\rfloor$ ($M$ is odd). Numerical results also demonstrate that our proposed transmission scheme is feasible and effective. ",Computer Science - Information Theory ; ,"Liu, Kangqi ; Tao, Meixia ; Xiang, Zhengzheng ; Long, Xin ; ","Generalized Signal Alignment For MIMO Two-Way X Relay Channels  We study the degrees of freedom (DoF) of MIMO two-way X relay channels. Previous work studied the case $N < 2M$, where $N$ and $M$ denote the number of antennas at the relay and each source, respectively, and showed that the maximum DoF of $2N$ is achievable when $N \leq \lfloor\frac{8M}{5}\rfloor$ by applying signal alignment (SA) for network coding and interference cancelation. This work considers the case $N>2M$ where the performance is limited by the number of antennas at each source node and conventional SA is not feasible. We propose a \textit{generalized signal alignment} (GSA) based transmission scheme. The key is to let the signals to be exchanged between every source node align in a transformed subspace, rather than the direct subspace, at the relay so as to form network-coded signals. This is realized by jointly designing the precoding matrices at all source nodes and the processing matrix at the relay. Moreover, the aligned subspaces are orthogonal to each other. By applying the GSA, we show that the DoF upper bound $4M$ is achievable when $M \leq \lfloor\frac{2N}{5}\rfloor$ ($M$ is even) or $M \leq \lfloor\frac{2N-1}{5}\rfloor$ ($M$ is odd). Numerical results also demonstrate that our proposed transmission scheme is feasible and effective. ",generalize signal alignment mimo two way relay channel study degrees freedom dof mimo two way relay channel previous work study case denote number antennas relay source respectively show maximum dof achievable leq lfloor frac rfloor apply signal alignment sa network cod interference cancelation work consider case performance limit number antennas source node conventional sa feasible propose textit generalize signal alignment gsa base transmission scheme key let signal exchange every source node align transform subspace rather direct subspace relay form network cod signal realize jointly design precoding matrices source nod process matrix relay moreover align subspaces orthogonal apply gsa show dof upper bind achievable leq lfloor frac rfloor even leq lfloor frac rfloor odd numerical result also demonstrate propose transmission scheme feasible effective,122,9,1402.1607.txt
http://arxiv.org/abs/1402.1794,In silico Proteome Cleavage Reveals Iterative Digestion Strategy for   High Sequence Coverage,"  In the post-genome era, biologists have sought to measure the complete complement of proteins, termed proteomics. Currently, the most effective method to measure the proteome is with shotgun, or bottom-up, proteomics, in which the proteome is digested into peptides that are identified followed by protein inference. Despite continuous improvements to all steps of the shotgun proteomics workflow, observed proteome coverage is often low; some proteins are identified by a single peptide sequence. Complete proteome sequence coverage would allow comprehensive characterization of RNA splicing variants and all post translational modifications, which would drastically improve the accuracy of biological models. There are many reasons for the sequence coverage deficit, but ultimately peptide length determines sequence observability. Peptides that are too short are lost because they match many protein sequences and their true origin is ambiguous. The maximum observable peptide length is determined by several analytical challenges. This paper explores computationally how peptide lengths produced from several common proteome digestion methods limit observable proteome coverage. Iterative proteome cleavage strategies are also explored. These simulations reveal that maximized proteome coverage can be achieved by use of an iterative digestion protocol involving multiple proteases and chemical cleavages that theoretically allow 91.1% proteome coverage. ","Quantitative Biology - Genomics ; Computer Science - Computational Engineering, Finance, and Science ; ","Meyer, Jesse G. ; ","In silico Proteome Cleavage Reveals Iterative Digestion Strategy for   High Sequence Coverage  In the post-genome era, biologists have sought to measure the complete complement of proteins, termed proteomics. Currently, the most effective method to measure the proteome is with shotgun, or bottom-up, proteomics, in which the proteome is digested into peptides that are identified followed by protein inference. Despite continuous improvements to all steps of the shotgun proteomics workflow, observed proteome coverage is often low; some proteins are identified by a single peptide sequence. Complete proteome sequence coverage would allow comprehensive characterization of RNA splicing variants and all post translational modifications, which would drastically improve the accuracy of biological models. There are many reasons for the sequence coverage deficit, but ultimately peptide length determines sequence observability. Peptides that are too short are lost because they match many protein sequences and their true origin is ambiguous. The maximum observable peptide length is determined by several analytical challenges. This paper explores computationally how peptide lengths produced from several common proteome digestion methods limit observable proteome coverage. Iterative proteome cleavage strategies are also explored. These simulations reveal that maximized proteome coverage can be achieved by use of an iterative digestion protocol involving multiple proteases and chemical cleavages that theoretically allow 91.1% proteome coverage. ",silico proteome cleavage reveal iterative digestion strategy high sequence coverage post genome era biologists seek measure complete complement proteins term proteomics currently effective method measure proteome shotgun bottom proteomics proteome digest peptides identify follow protein inference despite continuous improvements step shotgun proteomics workflow observe proteome coverage often low proteins identify single peptide sequence complete proteome sequence coverage would allow comprehensive characterization rna splice variants post translational modifications would drastically improve accuracy biological model many reason sequence coverage deficit ultimately peptide length determine sequence observability peptides short lose match many protein sequence true origin ambiguous maximum observable peptide length determine several analytical challenge paper explore computationally peptide lengths produce several common proteome digestion methods limit observable proteome coverage iterative proteome cleavage strategies also explore simulations reveal maximize proteome coverage achieve use iterative digestion protocol involve multiple proteases chemical cleavages theoretically allow proteome coverage,142,4,1402.1794.txt
http://arxiv.org/abs/1402.2016,Leveraging Long-Term Predictions and Online-Learning in Agent-based   Multiple Person Tracking,"  We present a multiple-person tracking algorithm, based on combining particle filters and RVO, an agent-based crowd model that infers collision-free velocities so as to predict pedestrian's motion. In addition to position and velocity, our tracking algorithm can estimate the internal goals (desired destination or desired velocity) of the tracked pedestrian in an online manner, thus removing the need to specify this information beforehand. Furthermore, we leverage the longer-term predictions of RVO by deriving a higher-order particle filter, which aggregates multiple predictions from different prior time steps. This yields a tracker that can recover from short-term occlusions and spurious noise in the appearance model. Experimental results show that our tracking algorithm is suitable for predicting pedestrians' behaviors online without needing scene priors or hand-annotated goal information, and improves tracking in real-world crowded scenes under low frame rates. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Liu, Wenxi ; Chan, Antoni B. ; Lau, Rynson W. H. ; Manocha, Dinesh ; ","Leveraging Long-Term Predictions and Online-Learning in Agent-based   Multiple Person Tracking  We present a multiple-person tracking algorithm, based on combining particle filters and RVO, an agent-based crowd model that infers collision-free velocities so as to predict pedestrian's motion. In addition to position and velocity, our tracking algorithm can estimate the internal goals (desired destination or desired velocity) of the tracked pedestrian in an online manner, thus removing the need to specify this information beforehand. Furthermore, we leverage the longer-term predictions of RVO by deriving a higher-order particle filter, which aggregates multiple predictions from different prior time steps. This yields a tracker that can recover from short-term occlusions and spurious noise in the appearance model. Experimental results show that our tracking algorithm is suitable for predicting pedestrians' behaviors online without needing scene priors or hand-annotated goal information, and improves tracking in real-world crowded scenes under low frame rates. ",leverage long term predictions online learn agent base multiple person track present multiple person track algorithm base combine particle filter rvo agent base crowd model infer collision free velocities predict pedestrian motion addition position velocity track algorithm estimate internal goals desire destination desire velocity track pedestrian online manner thus remove need specify information beforehand furthermore leverage longer term predictions rvo derive higher order particle filter aggregate multiple predictions different prior time step yield tracker recover short term occlusions spurious noise appearance model experimental result show track algorithm suitable predict pedestrians behaviors online without need scene priors hand annotate goal information improve track real world crowd scenes low frame rat,109,2,1402.2016.txt
http://arxiv.org/abs/1402.2760,Rendezvous in Networks in Spite of Delay Faults,"  Two mobile agents, starting from different nodes of an unknown network, have to meet at the same node. Agents move in synchronous rounds using a deterministic algorithm. Each agent has a different label, which it can use in the execution of the algorithm, but it does not know the label of the other agent. Agents do not know any bound on the size of the network. In each round an agent decides if it remains idle or if it wants to move to one of the adjacent nodes. Agents are subject to delay faults: if an agent incurs a fault in a given round, it remains in the current node, regardless of its decision. If it planned to move and the fault happened, the agent is aware of it. We consider three scenarios of fault distribution: random (independently in each round and for each agent with constant probability 0 < p < 1), unbounded adver- sarial (the adversary can delay an agent for an arbitrary finite number of consecutive rounds) and bounded adversarial (the adversary can delay an agent for at most c consecutive rounds, where c is unknown to the agents). The quality measure of a rendezvous algorithm is its cost, which is the total number of edge traversals. For random faults, we show an algorithm with cost polynomial in the size n of the network and polylogarithmic in the larger label L, which achieves rendezvous with very high probability in arbitrary networks. By contrast, for unbounded adversarial faults we show that rendezvous is not feasible, even in the class of rings. Under this scenario we give a rendezvous algorithm with cost O(nl), where l is the smaller label, working in arbitrary trees, and we show that \Omega(l) is the lower bound on rendezvous cost, even for the two-node tree. For bounded adversarial faults, we give a rendezvous algorithm working for arbitrary networks, with cost polynomial in n, and logarithmic in the bound c and in the larger label L. ",Computer Science - Data Structures and Algorithms ; ,"Chalopin, Jérémie ; Dieudonné, Yoann ; Labourel, Arnaud ; Pelc, Andrzej ; ","Rendezvous in Networks in Spite of Delay Faults  Two mobile agents, starting from different nodes of an unknown network, have to meet at the same node. Agents move in synchronous rounds using a deterministic algorithm. Each agent has a different label, which it can use in the execution of the algorithm, but it does not know the label of the other agent. Agents do not know any bound on the size of the network. In each round an agent decides if it remains idle or if it wants to move to one of the adjacent nodes. Agents are subject to delay faults: if an agent incurs a fault in a given round, it remains in the current node, regardless of its decision. If it planned to move and the fault happened, the agent is aware of it. We consider three scenarios of fault distribution: random (independently in each round and for each agent with constant probability 0 < p < 1), unbounded adver- sarial (the adversary can delay an agent for an arbitrary finite number of consecutive rounds) and bounded adversarial (the adversary can delay an agent for at most c consecutive rounds, where c is unknown to the agents). The quality measure of a rendezvous algorithm is its cost, which is the total number of edge traversals. For random faults, we show an algorithm with cost polynomial in the size n of the network and polylogarithmic in the larger label L, which achieves rendezvous with very high probability in arbitrary networks. By contrast, for unbounded adversarial faults we show that rendezvous is not feasible, even in the class of rings. Under this scenario we give a rendezvous algorithm with cost O(nl), where l is the smaller label, working in arbitrary trees, and we show that \Omega(l) is the lower bound on rendezvous cost, even for the two-node tree. For bounded adversarial faults, we give a rendezvous algorithm working for arbitrary networks, with cost polynomial in n, and logarithmic in the bound c and in the larger label L. ",rendezvous network spite delay fault two mobile agents start different nod unknown network meet node agents move synchronous round use deterministic algorithm agent different label use execution algorithm know label agent agents know bind size network round agent decide remain idle want move one adjacent nod agents subject delay fault agent incur fault give round remain current node regardless decision plan move fault happen agent aware consider three scenarios fault distribution random independently round agent constant probability unbounded adver sarial adversary delay agent arbitrary finite number consecutive round bound adversarial adversary delay agent consecutive round unknown agents quality measure rendezvous algorithm cost total number edge traversals random fault show algorithm cost polynomial size network polylogarithmic larger label achieve rendezvous high probability arbitrary network contrast unbounded adversarial fault show rendezvous feasible even class ring scenario give rendezvous algorithm cost nl smaller label work arbitrary tree show omega lower bind rendezvous cost even two node tree bound adversarial fault give rendezvous algorithm work arbitrary network cost polynomial logarithmic bind larger label,169,6,1402.2760.txt
http://arxiv.org/abs/1402.3175,Information-Geometric Equivalence of Transportation Polytopes,"  This paper deals with transportation polytopes in the probability simplex (that is, sets of categorical bivariate probability distributions with prescribed marginals). Information projections between such polytopes are studied, and a sufficient condition is described under which these mappings are homeomorphisms. ","Computer Science - Information Theory ; Mathematics - Combinatorics ; 94A17, 52B11, 52B12, 62B10, 62H17, 54C99 ; ","Kovačević, Mladen ; Stanojević, Ivan ; Šenk, Vojin ; ","Information-Geometric Equivalence of Transportation Polytopes  This paper deals with transportation polytopes in the probability simplex (that is, sets of categorical bivariate probability distributions with prescribed marginals). Information projections between such polytopes are studied, and a sufficient condition is described under which these mappings are homeomorphisms. ",information geometric equivalence transportation polytopes paper deal transportation polytopes probability simplex set categorical bivariate probability distributions prescribe marginals information projections polytopes study sufficient condition describe mappings homeomorphisms,27,8,1402.3175.txt
http://arxiv.org/abs/1402.3210,On the Convergence of Approximate Message Passing with Arbitrary   Matrices,"  Approximate message passing (AMP) methods and their variants have attracted considerable recent attention for the problem of estimating a random vector $\mathbf{x}$ observed through a linear transform $\mathbf{A}$. In the case of large i.i.d. zero-mean Gaussian $\mathbf{A}$, the methods exhibit fast convergence with precise analytic characterizations on the algorithm behavior. However, the convergence of AMP under general transforms $\mathbf{A}$ is not fully understood. In this paper, we provide sufficient conditions for the convergence of a damped version of the generalized AMP (GAMP) algorithm in the case of quadratic cost functions (i.e., Gaussian likelihood and prior). It is shown that, with sufficient damping, the algorithm is guaranteed to converge, although the amount of damping grows with peak-to-average ratio of the squared singular values of the transforms $\mathbf{A}$. This result explains the good performance of AMP on i.i.d. Gaussian transforms $\mathbf{A}$, but also their difficulties with ill-conditioned or non-zero-mean transforms $\mathbf{A}$. A related sufficient condition is then derived for the local stability of the damped GAMP method under general cost functions, assuming certain strict convexity conditions. ",Computer Science - Information Theory ; ,"Rangan, Sundeep ; Schniter, Philip ; Fletcher, Alyson K. ; Sarkar, Subrata ; ","On the Convergence of Approximate Message Passing with Arbitrary   Matrices  Approximate message passing (AMP) methods and their variants have attracted considerable recent attention for the problem of estimating a random vector $\mathbf{x}$ observed through a linear transform $\mathbf{A}$. In the case of large i.i.d. zero-mean Gaussian $\mathbf{A}$, the methods exhibit fast convergence with precise analytic characterizations on the algorithm behavior. However, the convergence of AMP under general transforms $\mathbf{A}$ is not fully understood. In this paper, we provide sufficient conditions for the convergence of a damped version of the generalized AMP (GAMP) algorithm in the case of quadratic cost functions (i.e., Gaussian likelihood and prior). It is shown that, with sufficient damping, the algorithm is guaranteed to converge, although the amount of damping grows with peak-to-average ratio of the squared singular values of the transforms $\mathbf{A}$. This result explains the good performance of AMP on i.i.d. Gaussian transforms $\mathbf{A}$, but also their difficulties with ill-conditioned or non-zero-mean transforms $\mathbf{A}$. A related sufficient condition is then derived for the local stability of the damped GAMP method under general cost functions, assuming certain strict convexity conditions. ",convergence approximate message pass arbitrary matrices approximate message pass amp methods variants attract considerable recent attention problem estimate random vector mathbf observe linear transform mathbf case large zero mean gaussian mathbf methods exhibit fast convergence precise analytic characterizations algorithm behavior however convergence amp general transform mathbf fully understand paper provide sufficient condition convergence damp version generalize amp gamp algorithm case quadratic cost function gaussian likelihood prior show sufficient damp algorithm guarantee converge although amount damp grow peak average ratio square singular value transform mathbf result explain good performance amp gaussian transform mathbf also difficulties ill condition non zero mean transform mathbf relate sufficient condition derive local stability damp gamp method general cost function assume certain strict convexity condition,118,7,1402.3210.txt
http://arxiv.org/abs/1402.3329,Differential Privacy: An Economic Method for Choosing Epsilon,"  Differential privacy is becoming a gold standard for privacy research; it offers a guaranteed bound on loss of privacy due to release of query results, even under worst-case assumptions. The theory of differential privacy is an active research area, and there are now differentially private algorithms for a wide range of interesting problems.   However, the question of when differential privacy works in practice has received relatively little attention. In particular, there is still no rigorous method for choosing the key parameter $\epsilon$, which controls the crucial tradeoff between the strength of the privacy guarantee and the accuracy of the published results.   In this paper, we examine the role that these parameters play in concrete applications, identifying the key questions that must be addressed when choosing specific values. This choice requires balancing the interests of two different parties: the data analyst and the prospective participant, who must decide whether to allow their data to be included in the analysis. We propose a simple model that expresses this balance as formulas over a handful of parameters, and we use our model to choose $\epsilon$ on a series of simple statistical studies. We also explore a surprising insight: in some circumstances, a differentially private study can be more accurate than a non-private study for the same cost, under our model. Finally, we discuss the simplifying assumptions in our model and outline a research agenda for possible refinements. ",Computer Science - Databases ; ,"Hsu, Justin ; Gaboardi, Marco ; Haeberlen, Andreas ; Khanna, Sanjeev ; Narayan, Arjun ; Pierce, Benjamin C. ; Roth, Aaron ; ","Differential Privacy: An Economic Method for Choosing Epsilon  Differential privacy is becoming a gold standard for privacy research; it offers a guaranteed bound on loss of privacy due to release of query results, even under worst-case assumptions. The theory of differential privacy is an active research area, and there are now differentially private algorithms for a wide range of interesting problems.   However, the question of when differential privacy works in practice has received relatively little attention. In particular, there is still no rigorous method for choosing the key parameter $\epsilon$, which controls the crucial tradeoff between the strength of the privacy guarantee and the accuracy of the published results.   In this paper, we examine the role that these parameters play in concrete applications, identifying the key questions that must be addressed when choosing specific values. This choice requires balancing the interests of two different parties: the data analyst and the prospective participant, who must decide whether to allow their data to be included in the analysis. We propose a simple model that expresses this balance as formulas over a handful of parameters, and we use our model to choose $\epsilon$ on a series of simple statistical studies. We also explore a surprising insight: in some circumstances, a differentially private study can be more accurate than a non-private study for the same cost, under our model. Finally, we discuss the simplifying assumptions in our model and outline a research agenda for possible refinements. ",differential privacy economic method choose epsilon differential privacy become gold standard privacy research offer guarantee bind loss privacy due release query result even worst case assumptions theory differential privacy active research area differentially private algorithms wide range interest problems however question differential privacy work practice receive relatively little attention particular still rigorous method choose key parameter epsilon control crucial tradeoff strength privacy guarantee accuracy publish result paper examine role parameters play concrete applications identify key question must address choose specific value choice require balance interest two different party data analyst prospective participant must decide whether allow data include analysis propose simple model express balance formulas handful parameters use model choose epsilon series simple statistical study also explore surprise insight circumstances differentially private study accurate non private study cost model finally discuss simplify assumptions model outline research agenda possible refinements,139,10,1402.3329.txt
http://arxiv.org/abs/1402.3427,Indian Buffet Process Deep Generative Models for Semi-Supervised   Classification,"  Deep generative models (DGMs) have brought about a major breakthrough, as well as renewed interest, in generative latent variable models. However, DGMs do not allow for performing data-driven inference of the number of latent features needed to represent the observed data. Traditional linear formulations address this issue by resorting to tools from the field of nonparametric statistics. Indeed, linear latent variable models imposed an Indian Buffet Process (IBP) prior have been extensively studied by the machine learning community; inference for such models can been performed either via exact sampling or via approximate variational techniques. Based on this inspiration, in this paper we examine whether similar ideas from the field of Bayesian nonparametrics can be utilized in the context of modern DGMs in order to address the latent variable dimensionality inference problem. To this end, we propose a novel DGM formulation, based on the imposition of an IBP prior. We devise an efficient Black-Box Variational inference algorithm for our model, and exhibit its efficacy in a number of semi-supervised classification experiments. In all cases, we use popular benchmark datasets, and compare to state-of-the-art DGMs. ",Computer Science - Machine Learning ; ,"Chatzis, Sotirios P. ; ","Indian Buffet Process Deep Generative Models for Semi-Supervised   Classification  Deep generative models (DGMs) have brought about a major breakthrough, as well as renewed interest, in generative latent variable models. However, DGMs do not allow for performing data-driven inference of the number of latent features needed to represent the observed data. Traditional linear formulations address this issue by resorting to tools from the field of nonparametric statistics. Indeed, linear latent variable models imposed an Indian Buffet Process (IBP) prior have been extensively studied by the machine learning community; inference for such models can been performed either via exact sampling or via approximate variational techniques. Based on this inspiration, in this paper we examine whether similar ideas from the field of Bayesian nonparametrics can be utilized in the context of modern DGMs in order to address the latent variable dimensionality inference problem. To this end, we propose a novel DGM formulation, based on the imposition of an IBP prior. We devise an efficient Black-Box Variational inference algorithm for our model, and exhibit its efficacy in a number of semi-supervised classification experiments. In all cases, we use popular benchmark datasets, and compare to state-of-the-art DGMs. ",indian buffet process deep generative model semi supervise classification deep generative model dgms bring major breakthrough well renew interest generative latent variable model however dgms allow perform data drive inference number latent feature need represent observe data traditional linear formulations address issue resort tool field nonparametric statistics indeed linear latent variable model impose indian buffet process ibp prior extensively study machine learn community inference model perform either via exact sample via approximate variational techniques base inspiration paper examine whether similar ideas field bayesian nonparametrics utilize context modern dgms order address latent variable dimensionality inference problem end propose novel dgm formulation base imposition ibp prior devise efficient black box variational inference algorithm model exhibit efficacy number semi supervise classification experiment case use popular benchmark datasets compare state art dgms,128,11,1402.3427.txt
http://arxiv.org/abs/1402.3631,Privately Solving Linear Programs,"  In this paper, we initiate the systematic study of solving linear programs under differential privacy. The first step is simply to define the problem: to this end, we introduce several natural classes of private linear programs that capture different ways sensitive data can be incorporated into a linear program. For each class of linear programs we give an efficient, differentially private solver based on the multiplicative weights framework, or we give an impossibility result. ",Computer Science - Data Structures and Algorithms ; Computer Science - Cryptography and Security ; Computer Science - Machine Learning ; ,"Hsu, Justin ; Roth, Aaron ; Roughgarden, Tim ; Ullman, Jonathan ; ","Privately Solving Linear Programs  In this paper, we initiate the systematic study of solving linear programs under differential privacy. The first step is simply to define the problem: to this end, we introduce several natural classes of private linear programs that capture different ways sensitive data can be incorporated into a linear program. For each class of linear programs we give an efficient, differentially private solver based on the multiplicative weights framework, or we give an impossibility result. ",privately solve linear program paper initiate systematic study solve linear program differential privacy first step simply define problem end introduce several natural class private linear program capture different ways sensitive data incorporate linear program class linear program give efficient differentially private solver base multiplicative weight framework give impossibility result,49,8,1402.3631.txt
http://arxiv.org/abs/1402.4178,A reclaimer scheduling problem arising in coal stockyard management,"  We study a number of variants of an abstract scheduling problem inspired by the scheduling of reclaimers in the stockyard of a coal export terminal. We analyze the complexity of each of the variants, providing complexity proofs for some and polynomial algorithms for others. For one, especially interesting variant, we also develop a constant factor approximation algorithm. ",Computer Science - Data Structures and Algorithms ; ,"Angelelli, Enrico ; Kalinowski, Thomas ; Kapoor, Reena ; Savelsbergh, Martin W. P. ; ","A reclaimer scheduling problem arising in coal stockyard management  We study a number of variants of an abstract scheduling problem inspired by the scheduling of reclaimers in the stockyard of a coal export terminal. We analyze the complexity of each of the variants, providing complexity proofs for some and polynomial algorithms for others. For one, especially interesting variant, we also develop a constant factor approximation algorithm. ",reclaimer schedule problem arise coal stockyard management study number variants abstract schedule problem inspire schedule reclaimers stockyard coal export terminal analyze complexity variants provide complexity proof polynomial algorithms others one especially interest variant also develop constant factor approximation algorithm,39,1,1402.4178.txt
http://arxiv.org/abs/1402.4327,Unification and Logarithmic Space,"  We present an algebraic characterization of the complexity classes Logspace and NLogspace, using an algebra with a composition law based on unification. This new bridge between unification and complexity classes is inspired from proof theory and more specifically linear logic and Geometry of Interaction.   We show how unification can be used to build a model of computation by means of specific subalgebras associated to finite permutations groups. We then prove that whether an observation (the algebraic counterpart of a program) accepts a word can be decided within logarithmic space. We also show that the construction can naturally represent pointer machines, an intuitive way of understanding logarithmic space computing. ",Computer Science - Logic in Computer Science ; ,"Aubert, Clément ; Bagnol, Marc ; ","Unification and Logarithmic Space  We present an algebraic characterization of the complexity classes Logspace and NLogspace, using an algebra with a composition law based on unification. This new bridge between unification and complexity classes is inspired from proof theory and more specifically linear logic and Geometry of Interaction.   We show how unification can be used to build a model of computation by means of specific subalgebras associated to finite permutations groups. We then prove that whether an observation (the algebraic counterpart of a program) accepts a word can be decided within logarithmic space. We also show that the construction can naturally represent pointer machines, an intuitive way of understanding logarithmic space computing. ",unification logarithmic space present algebraic characterization complexity class logspace nlogspace use algebra composition law base unification new bridge unification complexity class inspire proof theory specifically linear logic geometry interaction show unification use build model computation mean specific subalgebras associate finite permutations group prove whether observation algebraic counterpart program accept word decide within logarithmic space also show construction naturally represent pointer machine intuitive way understand logarithmic space compute,67,8,1402.4327.txt
http://arxiv.org/abs/1402.4338,Proof Complexity and the Kneser-Lov\'asz Theorem,"  We investigate the proof complexity of a class of propositional formulas expressing a combinatorial principle known as the Kneser-Lov\'{a}sz Theorem. This is a family of propositional tautologies, indexed by an nonnegative integer parameter $k$ that generalizes the Pigeonhole Principle (obtained for $k=1$).   We show, for all fixed $k$, $2^{\Omega(n)}$ lower bounds on resolution complexity and exponential lower bounds for bounded depth Frege proofs. These results hold even for the more restricted class of formulas encoding Schrijver's strenghtening of the Kneser-Lov\'{a}sz Theorem. On the other hand for the cases $k=2,3$ (for which combinatorial proofs of the Kneser-Lov\'{a}sz Theorem are known) we give polynomial size Frege ($k=2$), respectively extended Frege ($k=3$) proofs. The paper concludes with a brief announcement of the results (presented in subsequent work) on the proof complexity of the general case of the Kneser-Lov\'{a}sz theorem. ",Computer Science - Computational Complexity ; Computer Science - Logic in Computer Science ; ,"Istrate, Gabriel ; Crăciun, Adrian ; ","Proof Complexity and the Kneser-Lov\'asz Theorem  We investigate the proof complexity of a class of propositional formulas expressing a combinatorial principle known as the Kneser-Lov\'{a}sz Theorem. This is a family of propositional tautologies, indexed by an nonnegative integer parameter $k$ that generalizes the Pigeonhole Principle (obtained for $k=1$).   We show, for all fixed $k$, $2^{\Omega(n)}$ lower bounds on resolution complexity and exponential lower bounds for bounded depth Frege proofs. These results hold even for the more restricted class of formulas encoding Schrijver's strenghtening of the Kneser-Lov\'{a}sz Theorem. On the other hand for the cases $k=2,3$ (for which combinatorial proofs of the Kneser-Lov\'{a}sz Theorem are known) we give polynomial size Frege ($k=2$), respectively extended Frege ($k=3$) proofs. The paper concludes with a brief announcement of the results (presented in subsequent work) on the proof complexity of the general case of the Kneser-Lov\'{a}sz theorem. ",proof complexity kneser lov asz theorem investigate proof complexity class propositional formulas express combinatorial principle know kneser lov sz theorem family propositional tautologies index nonnegative integer parameter generalize pigeonhole principle obtain show fix omega lower bound resolution complexity exponential lower bound bound depth frege proof result hold even restrict class formulas encode schrijver strenghtening kneser lov sz theorem hand case combinatorial proof kneser lov sz theorem know give polynomial size frege respectively extend frege proof paper conclude brief announcement result present subsequent work proof complexity general case kneser lov sz theorem,91,8,1402.4338.txt
http://arxiv.org/abs/1402.5208,Densely Entangled Financial Systems,"  In [1] Zawadoski introduces a banking network model in which the asset and counter-party risks are treated separately and the banks hedge their assets risks by appropriate OTC contracts. In his model, each bank has only two counter-party neighbors, a bank fails due to the counter-party risk only if at least one of its two neighbors default, and such a counter-party risk is a low probability event. Informally, the author shows that the banks will hedge their asset risks by appropriate OTC contracts, and, though it may be socially optimal to insure against counter-party risk, in equilibrium banks will {\em not} choose to insure this low probability event.   In this paper, we consider the above model for more general network topologies, namely when each node has exactly 2r counter-party neighbors for some integer r>0. We extend the analysis of [1] to show that as the number of counter-party neighbors increase the probability of counter-party risk also increases, and in particular the socially optimal solution becomes privately sustainable when each bank hedges its risk to at least n/2 banks, where n is the number of banks in the network, i.e., when 2r is at least n/2, banks not only hedge their asset risk but also hedge its counter-party risk. ","Quantitative Finance - Risk Management ; Computer Science - Computational Engineering, Finance, and Science ; 91G99, 91B30 ; J.1 ; J.4 ; ","DasGupta, Bhaskar ; Kaligounder, Lakshmi ; ","Densely Entangled Financial Systems  In [1] Zawadoski introduces a banking network model in which the asset and counter-party risks are treated separately and the banks hedge their assets risks by appropriate OTC contracts. In his model, each bank has only two counter-party neighbors, a bank fails due to the counter-party risk only if at least one of its two neighbors default, and such a counter-party risk is a low probability event. Informally, the author shows that the banks will hedge their asset risks by appropriate OTC contracts, and, though it may be socially optimal to insure against counter-party risk, in equilibrium banks will {\em not} choose to insure this low probability event.   In this paper, we consider the above model for more general network topologies, namely when each node has exactly 2r counter-party neighbors for some integer r>0. We extend the analysis of [1] to show that as the number of counter-party neighbors increase the probability of counter-party risk also increases, and in particular the socially optimal solution becomes privately sustainable when each bank hedges its risk to at least n/2 banks, where n is the number of banks in the network, i.e., when 2r is at least n/2, banks not only hedge their asset risk but also hedge its counter-party risk. ",densely entangle financial systems zawadoski introduce bank network model asset counter party risk treat separately bank hedge assets risk appropriate otc contract model bank two counter party neighbor bank fail due counter party risk least one two neighbor default counter party risk low probability event informally author show bank hedge asset risk appropriate otc contract though may socially optimal insure counter party risk equilibrium bank em choose insure low probability event paper consider model general network topologies namely node exactly counter party neighbor integer extend analysis show number counter party neighbor increase probability counter party risk also increase particular socially optimal solution become privately sustainable bank hedge risk least bank number bank network least bank hedge asset risk also hedge counter party risk,123,8,1402.5208.txt
http://arxiv.org/abs/1402.5481,From Predictive to Prescriptive Analytics,"  In this paper, we combine ideas from machine learning (ML) and operations research and management science (OR/MS) in developing a framework, along with specific methods, for using data to prescribe optimal decisions in OR/MS problems. In a departure from other work on data-driven optimization and reflecting our practical experience with the data available in applications of OR/MS, we consider data consisting, not only of observations of quantities with direct effect on costs/revenues, such as demand or returns, but predominantly of observations of associated auxiliary quantities. The main problem of interest is a conditional stochastic optimization problem, given imperfect observations, where the joint probability distributions that specify the problem are unknown. We demonstrate that our proposed solution methods, which are inspired by ML methods such as local regression, CART, and random forests, are generally applicable to a wide range of decision problems. We prove that they are tractable and asymptotically optimal even when data is not iid and may be censored. We extend this to the case where decision variables may directly affect uncertainty in unknown ways, such as pricing's effect on demand. As an analogue to R^2, we develop a metric P termed the coefficient of prescriptiveness to measure the prescriptive content of data and the efficacy of a policy from an operations perspective. To demonstrate the power of our approach in a real-world setting we study an inventory management problem faced by the distribution arm of an international media conglomerate, which ships an average of 1bil units per year. We leverage internal data and public online data harvested from IMDb, Rotten Tomatoes, and Google to prescribe operational decisions that outperform baseline measures. Specifically, the data we collect, leveraged by our methods, accounts for an 88\% improvement as measured by our P. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; Mathematics - Optimization and Control ; ,"Bertsimas, Dimitris ; Kallus, Nathan ; ","From Predictive to Prescriptive Analytics  In this paper, we combine ideas from machine learning (ML) and operations research and management science (OR/MS) in developing a framework, along with specific methods, for using data to prescribe optimal decisions in OR/MS problems. In a departure from other work on data-driven optimization and reflecting our practical experience with the data available in applications of OR/MS, we consider data consisting, not only of observations of quantities with direct effect on costs/revenues, such as demand or returns, but predominantly of observations of associated auxiliary quantities. The main problem of interest is a conditional stochastic optimization problem, given imperfect observations, where the joint probability distributions that specify the problem are unknown. We demonstrate that our proposed solution methods, which are inspired by ML methods such as local regression, CART, and random forests, are generally applicable to a wide range of decision problems. We prove that they are tractable and asymptotically optimal even when data is not iid and may be censored. We extend this to the case where decision variables may directly affect uncertainty in unknown ways, such as pricing's effect on demand. As an analogue to R^2, we develop a metric P termed the coefficient of prescriptiveness to measure the prescriptive content of data and the efficacy of a policy from an operations perspective. To demonstrate the power of our approach in a real-world setting we study an inventory management problem faced by the distribution arm of an international media conglomerate, which ships an average of 1bil units per year. We leverage internal data and public online data harvested from IMDb, Rotten Tomatoes, and Google to prescribe operational decisions that outperform baseline measures. Specifically, the data we collect, leveraged by our methods, accounts for an 88\% improvement as measured by our P. ",predictive prescriptive analytics paper combine ideas machine learn ml operations research management science ms develop framework along specific methods use data prescribe optimal decisions ms problems departure work data drive optimization reflect practical experience data available applications ms consider data consist observations quantities direct effect cost revenues demand return predominantly observations associate auxiliary quantities main problem interest conditional stochastic optimization problem give imperfect observations joint probability distributions specify problem unknown demonstrate propose solution methods inspire ml methods local regression cart random forest generally applicable wide range decision problems prove tractable asymptotically optimal even data iid may censor extend case decision variables may directly affect uncertainty unknown ways price effect demand analogue develop metric term coefficient prescriptiveness measure prescriptive content data efficacy policy operations perspective demonstrate power approach real world set study inventory management problem face distribution arm international media conglomerate ship average bil units per year leverage internal data public online data harvest imdb rotten tomatoes google prescribe operational decisions outperform baseline measure specifically data collect leverage methods account improvement measure,171,10,1402.5481.txt
http://arxiv.org/abs/1402.6208,The Anatomy of a Modular System for Media Content Analysis,"  Intelligent systems for the annotation of media content are increasingly being used for the automation of parts of social science research. In this domain the problem of integrating various Artificial Intelligence (AI) algorithms into a single intelligent system arises spontaneously. As part of our ongoing effort in automating media content analysis for the social sciences, we have built a modular system by combining multiple AI modules into a flexible framework in which they can cooperate in complex tasks. Our system combines data gathering, machine translation, topic classification, extraction and annotation of entities and social networks, as well as many other tasks that have been perfected over the past years of AI research. Over the last few years, it has allowed us to realise a series of scientific studies over a vast range of applications including comparative studies between news outlets and media content in different countries, modelling of user preferences, and monitoring public mood. The framework is flexible and allows the design and implementation of modular agents, where simple modules cooperate in the annotation of a large dataset without central coordination. ","Computer Science - Multiagent Systems ; Computer Science - Artificial Intelligence ; Computer Science - Distributed, Parallel, and Cluster Computing ; ","Flaounas, Ilias ; Lansdall-Welfare, Thomas ; Antonakaki, Panagiota ; Cristianini, Nello ; ","The Anatomy of a Modular System for Media Content Analysis  Intelligent systems for the annotation of media content are increasingly being used for the automation of parts of social science research. In this domain the problem of integrating various Artificial Intelligence (AI) algorithms into a single intelligent system arises spontaneously. As part of our ongoing effort in automating media content analysis for the social sciences, we have built a modular system by combining multiple AI modules into a flexible framework in which they can cooperate in complex tasks. Our system combines data gathering, machine translation, topic classification, extraction and annotation of entities and social networks, as well as many other tasks that have been perfected over the past years of AI research. Over the last few years, it has allowed us to realise a series of scientific studies over a vast range of applications including comparative studies between news outlets and media content in different countries, modelling of user preferences, and monitoring public mood. The framework is flexible and allows the design and implementation of modular agents, where simple modules cooperate in the annotation of a large dataset without central coordination. ",anatomy modular system media content analysis intelligent systems annotation media content increasingly use automation part social science research domain problem integrate various artificial intelligence ai algorithms single intelligent system arise spontaneously part ongoing effort automate media content analysis social sciences build modular system combine multiple ai modules flexible framework cooperate complex task system combine data gather machine translation topic classification extraction annotation entities social network well many task perfect past years ai research last years allow us realise series scientific study vast range applications include comparative study news outlets media content different countries model user preferences monitor public mood framework flexible allow design implementation modular agents simple modules cooperate annotation large dataset without central coordination,115,10,1402.6208.txt
http://arxiv.org/abs/1402.6787,Learning multifractal structure in large networks,"  Generating random graphs to model networks has a rich history. In this paper, we analyze and improve upon the multifractal network generator (MFNG) introduced by Palla et al. We provide a new result on the probability of subgraphs existing in graphs generated with MFNG. From this result it follows that we can quickly compute moments of an important set of graph properties, such as the expected number of edges, stars, and cliques. Specifically, we show how to compute these moments in time complexity independent of the size of the graph and the number of recursive levels in the generative model. We leverage this theory to a new method of moments algorithm for fitting large networks to MFNG. Empirically, this new approach effectively simulates properties of several social and information networks. In terms of matching subgraph counts, our method outperforms similar algorithms used with the Stochastic Kronecker Graph model. Furthermore, we present a fast approximation algorithm to generate graph instances following the multi- fractal structure. The approximation scheme is an improvement over previous methods, which ran in time complexity quadratic in the number of vertices. Combined, our method of moments and fast sampling scheme provide the first scalable framework for effectively modeling large networks with MFNG. ",Computer Science - Social and Information Networks ; H.4.0 ; E.1 ; ,"Benson, Austin R. ; Riquelme, Carlos ; Schmit, Sven ; ","Learning multifractal structure in large networks  Generating random graphs to model networks has a rich history. In this paper, we analyze and improve upon the multifractal network generator (MFNG) introduced by Palla et al. We provide a new result on the probability of subgraphs existing in graphs generated with MFNG. From this result it follows that we can quickly compute moments of an important set of graph properties, such as the expected number of edges, stars, and cliques. Specifically, we show how to compute these moments in time complexity independent of the size of the graph and the number of recursive levels in the generative model. We leverage this theory to a new method of moments algorithm for fitting large networks to MFNG. Empirically, this new approach effectively simulates properties of several social and information networks. In terms of matching subgraph counts, our method outperforms similar algorithms used with the Stochastic Kronecker Graph model. Furthermore, we present a fast approximation algorithm to generate graph instances following the multi- fractal structure. The approximation scheme is an improvement over previous methods, which ran in time complexity quadratic in the number of vertices. Combined, our method of moments and fast sampling scheme provide the first scalable framework for effectively modeling large networks with MFNG. ",learn multifractal structure large network generate random graph model network rich history paper analyze improve upon multifractal network generator mfng introduce palla et al provide new result probability subgraphs exist graph generate mfng result follow quickly compute moments important set graph properties expect number edge star cliques specifically show compute moments time complexity independent size graph number recursive level generative model leverage theory new method moments algorithm fit large network mfng empirically new approach effectively simulate properties several social information network term match subgraph count method outperform similar algorithms use stochastic kronecker graph model furthermore present fast approximation algorithm generate graph instance follow multi fractal structure approximation scheme improvement previous methods run time complexity quadratic number vertices combine method moments fast sample scheme provide first scalable framework effectively model large network mfng,132,6,1402.6787.txt
http://arxiv.org/abs/1402.6964,Scalable methods for nonnegative matrix factorizations of near-separable   tall-and-skinny matrices,"  Numerous algorithms are used for nonnegative matrix factorization under the assumption that the matrix is nearly separable. In this paper, we show how to make these algorithms efficient for data matrices that have many more rows than columns, so-called ""tall-and-skinny matrices"". One key component to these improved methods is an orthogonal matrix transformation that preserves the separability of the NMF problem. Our final methods need a single pass over the data matrix and are suitable for streaming, multi-core, and MapReduce architectures. We demonstrate the efficacy of these algorithms on terabyte-sized synthetic matrices and real-world matrices from scientific computing and bioinformatics. ","Computer Science - Machine Learning ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Numerical Analysis ; Statistics - Machine Learning ; G.1.3 ; G.1.6 ; ","Benson, Austin R. ; Lee, Jason D. ; Rajwa, Bartek ; Gleich, David F. ; ","Scalable methods for nonnegative matrix factorizations of near-separable   tall-and-skinny matrices  Numerous algorithms are used for nonnegative matrix factorization under the assumption that the matrix is nearly separable. In this paper, we show how to make these algorithms efficient for data matrices that have many more rows than columns, so-called ""tall-and-skinny matrices"". One key component to these improved methods is an orthogonal matrix transformation that preserves the separability of the NMF problem. Our final methods need a single pass over the data matrix and are suitable for streaming, multi-core, and MapReduce architectures. We demonstrate the efficacy of these algorithms on terabyte-sized synthetic matrices and real-world matrices from scientific computing and bioinformatics. ",scalable methods nonnegative matrix factorizations near separable tall skinny matrices numerous algorithms use nonnegative matrix factorization assumption matrix nearly separable paper show make algorithms efficient data matrices many row columns call tall skinny matrices one key component improve methods orthogonal matrix transformation preserve separability nmf problem final methods need single pass data matrix suitable stream multi core mapreduce architectures demonstrate efficacy algorithms terabyte size synthetic matrices real world matrices scientific compute bioinformatics,72,7,1402.6964.txt
http://arxiv.org/abs/1402.7242,Percolation with small clusters on random graphs,"  Consider the problem of determining the maximal induced subgraph in a random $d$-regular graph such that its components remain bounded as the size of the graph becomes arbitrarily large. We show, for asymptotically large $d$, that any such induced subgraph has size density at most $2(\log d)/d$ with high probability. A matching lower bound is known for independent sets. We also prove the analogous result for sparse Erd\H{o}s-R\'{e}nyi graphs. ",Mathematics - Probability ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Rahman, Mustazee ; ","Percolation with small clusters on random graphs  Consider the problem of determining the maximal induced subgraph in a random $d$-regular graph such that its components remain bounded as the size of the graph becomes arbitrarily large. We show, for asymptotically large $d$, that any such induced subgraph has size density at most $2(\log d)/d$ with high probability. A matching lower bound is known for independent sets. We also prove the analogous result for sparse Erd\H{o}s-R\'{e}nyi graphs. ",percolation small cluster random graph consider problem determine maximal induce subgraph random regular graph components remain bound size graph become arbitrarily large show asymptotically large induce subgraph size density log high probability match lower bind know independent set also prove analogous result sparse erd nyi graph,46,3,1402.7242.txt
http://arxiv.org/abs/1403.0505,A search for quantum coin-flipping protocols using optimization   techniques,"  Coin-flipping is a cryptographic task in which two physically separated, mistrustful parties wish to generate a fair coin-flip by communicating with each other. Chailloux and Kerenidis (2009) designed quantum protocols that guarantee coin-flips with near optimal bias. The probability of any outcome in these protocols is provably at most $1/\sqrt{2} + \delta$ for any given $\delta > 0$. However, no explicit description of these protocols is known, and the number of rounds in the protocols tends to infinity as $\delta$ goes to 0. In fact, the smallest bias achieved by known explicit protocols is $1/4$ (Ambainis, 2001).   We take a computational optimization approach, based mostly on convex optimization, to the search for simple and explicit quantum strong coin-flipping protocols. We present a search algorithm to identify protocols with low bias within a natural class, protocols based on bit-commitment (Nayak and Shor, 2003) restricting to commitment states used by Mochon (2005). An analysis of the resulting protocols via semidefinite programs (SDPs) unveils a simple structure. For example, we show that the SDPs reduce to second-order cone programs. We devise novel cheating strategies in the protocol by restricting the semidefinite programs and use the strategies to prune the search.   The techniques we develop enable a computational search for protocols given by a mesh over the parameter space. The protocols have up to six rounds of communication, with messages of varying dimension and include the best known explicit protocol (with bias 1/4). We conduct two kinds of search: one for protocols with bias below 0.2499, and one for protocols in the neighbourhood of protocols with bias 1/4. Neither of these searches yields better bias. Based on the mathematical ideas behind the search algorithm, we prove a lower bound on the bias of a class of four-round protocols. ",Mathematics - Optimization and Control ; Computer Science - Cryptography and Security ; Quantum Physics ; ,"Nayak, Ashwin ; Sikora, Jamie ; Tunçel, Levent ; ","A search for quantum coin-flipping protocols using optimization   techniques  Coin-flipping is a cryptographic task in which two physically separated, mistrustful parties wish to generate a fair coin-flip by communicating with each other. Chailloux and Kerenidis (2009) designed quantum protocols that guarantee coin-flips with near optimal bias. The probability of any outcome in these protocols is provably at most $1/\sqrt{2} + \delta$ for any given $\delta > 0$. However, no explicit description of these protocols is known, and the number of rounds in the protocols tends to infinity as $\delta$ goes to 0. In fact, the smallest bias achieved by known explicit protocols is $1/4$ (Ambainis, 2001).   We take a computational optimization approach, based mostly on convex optimization, to the search for simple and explicit quantum strong coin-flipping protocols. We present a search algorithm to identify protocols with low bias within a natural class, protocols based on bit-commitment (Nayak and Shor, 2003) restricting to commitment states used by Mochon (2005). An analysis of the resulting protocols via semidefinite programs (SDPs) unveils a simple structure. For example, we show that the SDPs reduce to second-order cone programs. We devise novel cheating strategies in the protocol by restricting the semidefinite programs and use the strategies to prune the search.   The techniques we develop enable a computational search for protocols given by a mesh over the parameter space. The protocols have up to six rounds of communication, with messages of varying dimension and include the best known explicit protocol (with bias 1/4). We conduct two kinds of search: one for protocols with bias below 0.2499, and one for protocols in the neighbourhood of protocols with bias 1/4. Neither of these searches yields better bias. Based on the mathematical ideas behind the search algorithm, we prove a lower bound on the bias of a class of four-round protocols. ",search quantum coin flip protocols use optimization techniques coin flip cryptographic task two physically separate mistrustful party wish generate fair coin flip communicate chailloux kerenidis design quantum protocols guarantee coin flip near optimal bias probability outcome protocols provably sqrt delta give delta however explicit description protocols know number round protocols tend infinity delta go fact smallest bias achieve know explicit protocols ambainis take computational optimization approach base mostly convex optimization search simple explicit quantum strong coin flip protocols present search algorithm identify protocols low bias within natural class protocols base bite commitment nayak shor restrict commitment state use mochon analysis result protocols via semidefinite program sdps unveil simple structure example show sdps reduce second order cone program devise novel cheat strategies protocol restrict semidefinite program use strategies prune search techniques develop enable computational search protocols give mesh parameter space protocols six round communication message vary dimension include best know explicit protocol bias conduct two kinds search one protocols bias one protocols neighbourhood protocols bias neither search yield better bias base mathematical ideas behind search algorithm prove lower bind bias class four round protocols,183,4,1403.0505.txt
http://arxiv.org/abs/1403.0734,Clique counting in MapReduce: theory and experiments,"  We tackle the problem of counting the number of $k$-cliques in large-scale graphs, for any constant $k \ge 3$. Clique counting is essential in a variety of applications, among which social network analysis. Due to its computationally intensive nature, we settle for parallel solutions in the MapReduce framework, which has become in the last few years a {\em de facto} standard for batch processing of massive data sets. We give both theoretical and experimental contributions.   On the theory side, we design the first exact scalable algorithm for counting (and listing) $k$-cliques. Our algorithm uses $O(m^{3/2})$ total space and $O(m^{k/2})$ work, where $m$ is the number of graph edges. This matches the best-known bounds for triangle listing when $k=3$ and is work-optimal in the worst case for any $k$, while keeping the communication cost independent of $k$. We also design a sampling-based estimator that can dramatically reduce the running time and space requirements of the exact approach, while providing very accurate solutions with high probability.   We then assess the effectiveness of different clique counting approaches through an extensive experimental analysis over the Amazon EC2 platform, considering both our algorithms and their state-of-the-art competitors. The experimental results clearly highlight the algorithm of choice in different scenarios and prove our exact approach to be the most effective when the number of $k$-cliques is large, gracefully scaling to non-trivial values of $k$ even on clusters of small/medium size. Our approximation algorithm achieves extremely accurate estimates and large speedups, especially on the toughest instances for the exact algorithms. As a side effect, our study also sheds light on the number of $k$-cliques of several real-world graphs, mainly social networks, and on its growth rate as a function of $k$. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Data Structures and Algorithms ; ","Finocchi, Irene ; Finocchi, Marco ; Fusco, Emanuele G. ; ","Clique counting in MapReduce: theory and experiments  We tackle the problem of counting the number of $k$-cliques in large-scale graphs, for any constant $k \ge 3$. Clique counting is essential in a variety of applications, among which social network analysis. Due to its computationally intensive nature, we settle for parallel solutions in the MapReduce framework, which has become in the last few years a {\em de facto} standard for batch processing of massive data sets. We give both theoretical and experimental contributions.   On the theory side, we design the first exact scalable algorithm for counting (and listing) $k$-cliques. Our algorithm uses $O(m^{3/2})$ total space and $O(m^{k/2})$ work, where $m$ is the number of graph edges. This matches the best-known bounds for triangle listing when $k=3$ and is work-optimal in the worst case for any $k$, while keeping the communication cost independent of $k$. We also design a sampling-based estimator that can dramatically reduce the running time and space requirements of the exact approach, while providing very accurate solutions with high probability.   We then assess the effectiveness of different clique counting approaches through an extensive experimental analysis over the Amazon EC2 platform, considering both our algorithms and their state-of-the-art competitors. The experimental results clearly highlight the algorithm of choice in different scenarios and prove our exact approach to be the most effective when the number of $k$-cliques is large, gracefully scaling to non-trivial values of $k$ even on clusters of small/medium size. Our approximation algorithm achieves extremely accurate estimates and large speedups, especially on the toughest instances for the exact algorithms. As a side effect, our study also sheds light on the number of $k$-cliques of several real-world graphs, mainly social networks, and on its growth rate as a function of $k$. ",clique count mapreduce theory experiment tackle problem count number cliques large scale graph constant ge clique count essential variety applications among social network analysis due computationally intensive nature settle parallel solutions mapreduce framework become last years em de facto standard batch process massive data set give theoretical experimental contributions theory side design first exact scalable algorithm count list cliques algorithm use total space work number graph edge match best know bound triangle list work optimal worst case keep communication cost independent also design sample base estimator dramatically reduce run time space requirements exact approach provide accurate solutions high probability assess effectiveness different clique count approach extensive experimental analysis amazon ec platform consider algorithms state art competitors experimental result clearly highlight algorithm choice different scenarios prove exact approach effective number cliques large gracefully scale non trivial value even cluster small medium size approximation algorithm achieve extremely accurate estimate large speedups especially toughest instance exact algorithms side effect study also shed light number cliques several real world graph mainly social network growth rate function,172,3,1403.0734.txt
http://arxiv.org/abs/1403.1080,New Ideas for Brain Modelling,"  This paper describes some biologically-inspired processes that could be used to build the sort of networks that we associate with the human brain. New to this paper, a 'refined' neuron will be proposed. This is a group of neurons that by joining together can produce a more analogue system, but with the same level of control and reliability that a binary neuron would have. With this new structure, it will be possible to think of an essentially binary system in terms of a more variable set of values. The paper also shows how recent research associated with the new model, can be combined with established theories, to produce a more complete picture. The propositions are largely in line with conventional thinking, but possibly with one or two more radical suggestions. An earlier cognitive model can be filled in with more specific details, based on the new research results, where the components appear to fit together almost seamlessly. The intention of the research has been to describe plausible 'mechanical' processes that can produce the appropriate brain structures and mechanisms, but that could be used without the magical 'intelligence' part that is still not fully understood. There are also some important updates from an earlier version of this paper. ",Computer Science - Artificial Intelligence ; Quantitative Biology - Neurons and Cognition ; ,"Greer, Kieran ; ","New Ideas for Brain Modelling  This paper describes some biologically-inspired processes that could be used to build the sort of networks that we associate with the human brain. New to this paper, a 'refined' neuron will be proposed. This is a group of neurons that by joining together can produce a more analogue system, but with the same level of control and reliability that a binary neuron would have. With this new structure, it will be possible to think of an essentially binary system in terms of a more variable set of values. The paper also shows how recent research associated with the new model, can be combined with established theories, to produce a more complete picture. The propositions are largely in line with conventional thinking, but possibly with one or two more radical suggestions. An earlier cognitive model can be filled in with more specific details, based on the new research results, where the components appear to fit together almost seamlessly. The intention of the research has been to describe plausible 'mechanical' processes that can produce the appropriate brain structures and mechanisms, but that could be used without the magical 'intelligence' part that is still not fully understood. There are also some important updates from an earlier version of this paper. ",new ideas brain model paper describe biologically inspire process could use build sort network associate human brain new paper refine neuron propose group neurons join together produce analogue system level control reliability binary neuron would new structure possible think essentially binary system term variable set value paper also show recent research associate new model combine establish theories produce complete picture proposition largely line conventional think possibly one two radical suggestions earlier cognitive model fill specific detail base new research result components appear fit together almost seamlessly intention research describe plausible mechanical process produce appropriate brain structure mechanisms could use without magical intelligence part still fully understand also important update earlier version paper,112,11,1403.1080.txt
http://arxiv.org/abs/1403.1142,Automated analysis of security protocols with global state,"  Security APIs, key servers and protocols that need to keep the status of transactions, require to maintain a global, non-monotonic state, e.g., in the form of a database or register. However, most existing automated verification tools do not support the analysis of such stateful security protocols - sometimes because of fundamental reasons, such as the encoding of the protocol as Horn clauses, which are inherently monotonic. A notable exception is the recent tamarin prover which allows specifying protocols as multiset rewrite (msr) rules, a formalism expressive enough to encode state. As multiset rewriting is a ""low-level"" specification language with no direct support for concurrent message passing, encoding protocols correctly is a difficult and error-prone process. We propose a process calculus which is a variant of the applied pi calculus with constructs for manipulation of a global state by processes running in parallel. We show that this language can be translated to msr rules whilst preserving all security properties expressible in a dedicated first-order logic for security properties. The translation has been implemented in a prototype tool which uses the tamarin prover as a backend. We apply the tool to several case studies among which a simplified fragment of PKCS\#11, the Yubikey security token, and an optimistic contract signing protocol. ",Computer Science - Cryptography and Security ; ,"Kremer, Steve ; Künnemann, Robert ; ","Automated analysis of security protocols with global state  Security APIs, key servers and protocols that need to keep the status of transactions, require to maintain a global, non-monotonic state, e.g., in the form of a database or register. However, most existing automated verification tools do not support the analysis of such stateful security protocols - sometimes because of fundamental reasons, such as the encoding of the protocol as Horn clauses, which are inherently monotonic. A notable exception is the recent tamarin prover which allows specifying protocols as multiset rewrite (msr) rules, a formalism expressive enough to encode state. As multiset rewriting is a ""low-level"" specification language with no direct support for concurrent message passing, encoding protocols correctly is a difficult and error-prone process. We propose a process calculus which is a variant of the applied pi calculus with constructs for manipulation of a global state by processes running in parallel. We show that this language can be translated to msr rules whilst preserving all security properties expressible in a dedicated first-order logic for security properties. The translation has been implemented in a prototype tool which uses the tamarin prover as a backend. We apply the tool to several case studies among which a simplified fragment of PKCS\#11, the Yubikey security token, and an optimistic contract signing protocol. ",automate analysis security protocols global state security apis key servers protocols need keep status transactions require maintain global non monotonic state form database register however exist automate verification tool support analysis stateful security protocols sometimes fundamental reason encode protocol horn clauses inherently monotonic notable exception recent tamarin prover allow specify protocols multiset rewrite msr rule formalism expressive enough encode state multiset rewrite low level specification language direct support concurrent message pass encode protocols correctly difficult error prone process propose process calculus variant apply pi calculus construct manipulation global state process run parallel show language translate msr rule whilst preserve security properties expressible dedicate first order logic security properties translation implement prototype tool use tamarin prover backend apply tool several case study among simplify fragment pkcs yubikey security token optimistic contract sign protocol,132,8,1403.1142.txt
http://arxiv.org/abs/1403.1639,Optimal Patching in Clustered Malware Epidemics,"  Studies on the propagation of malware in mobile networks have revealed that the spread of malware can be highly inhomogeneous. Platform diversity, contact list utilization by the malware, clustering in the network structure, etc. can also lead to differing spreading rates. In this paper, a general formal framework is proposed for leveraging such heterogeneity to derive optimal patching policies that attain the minimum aggregate cost due to the spread of malware and the surcharge of patching. Using Pontryagin's Maximum Principle for a stratified epidemic model, it is analytically proven that in the mean-field deterministic regime, optimal patch disseminations are simple single-threshold policies. Through numerical simulations, the behavior of optimal patching policies is investigated in sample topologies and their advantages are demonstrated. ",Computer Science - Cryptography and Security ; Computer Science - Networking and Internet Architecture ; Computer Science - Social and Information Networks ; Computer Science - Systems and Control ; Mathematics - Optimization and Control ; ,"Eshghi, Soheil ; Khouzani, MHR. ; Sarkar, Saswati ; Venkatesh, Santosh S. ; ","Optimal Patching in Clustered Malware Epidemics  Studies on the propagation of malware in mobile networks have revealed that the spread of malware can be highly inhomogeneous. Platform diversity, contact list utilization by the malware, clustering in the network structure, etc. can also lead to differing spreading rates. In this paper, a general formal framework is proposed for leveraging such heterogeneity to derive optimal patching policies that attain the minimum aggregate cost due to the spread of malware and the surcharge of patching. Using Pontryagin's Maximum Principle for a stratified epidemic model, it is analytically proven that in the mean-field deterministic regime, optimal patch disseminations are simple single-threshold policies. Through numerical simulations, the behavior of optimal patching policies is investigated in sample topologies and their advantages are demonstrated. ",optimal patch cluster malware epidemics study propagation malware mobile network reveal spread malware highly inhomogeneous platform diversity contact list utilization malware cluster network structure etc also lead differ spread rat paper general formal framework propose leverage heterogeneity derive optimal patch policies attain minimum aggregate cost due spread malware surcharge patch use pontryagin maximum principle stratify epidemic model analytically prove mean field deterministic regime optimal patch disseminations simple single threshold policies numerical simulations behavior optimal patch policies investigate sample topologies advantage demonstrate,81,0,1403.1639.txt
http://arxiv.org/abs/1403.1642,Optimal Energy-Aware Epidemic Routing in DTNs,"  In this work, we investigate the use of epidemic routing in energy constrained Delay Tolerant Networks (DTNs). In epidemic routing, messages are relayed by intermediate nodes at contact opportunities, i.e., when pairs of nodes come within the transmission range of each other. Each node needs to decide whether to forward its message upon contact with a new node based on its own residual energy level and the age of that message. We mathematically characterize the fundamental trade-off between energy conservation and a measure of Quality of Service as a dynamic energy-dependent optimal control problem. We prove that in the mean-field regime, the optimal dynamic forwarding decisions follow simple threshold-based structures in which the forwarding threshold for each node depends on its current remaining energy. We then characterize the nature of this dependence. Our simulations reveal that the optimal dynamic policy significantly outperforms heuristics. ","Computer Science - Systems and Control ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Networking and Internet Architecture ; Mathematics - Optimization and Control ; ","Eshghi, Soheil ; Khouzani, MHR. ; Sarkar, Saswati ; Shroff, Ness B. ; Venkatesh, Santosh S. ; ","Optimal Energy-Aware Epidemic Routing in DTNs  In this work, we investigate the use of epidemic routing in energy constrained Delay Tolerant Networks (DTNs). In epidemic routing, messages are relayed by intermediate nodes at contact opportunities, i.e., when pairs of nodes come within the transmission range of each other. Each node needs to decide whether to forward its message upon contact with a new node based on its own residual energy level and the age of that message. We mathematically characterize the fundamental trade-off between energy conservation and a measure of Quality of Service as a dynamic energy-dependent optimal control problem. We prove that in the mean-field regime, the optimal dynamic forwarding decisions follow simple threshold-based structures in which the forwarding threshold for each node depends on its current remaining energy. We then characterize the nature of this dependence. Our simulations reveal that the optimal dynamic policy significantly outperforms heuristics. ",optimal energy aware epidemic rout dtns work investigate use epidemic rout energy constrain delay tolerant network dtns epidemic rout message relay intermediate nod contact opportunities pair nod come within transmission range node need decide whether forward message upon contact new node base residual energy level age message mathematically characterize fundamental trade energy conservation measure quality service dynamic energy dependent optimal control problem prove mean field regime optimal dynamic forward decisions follow simple threshold base structure forward threshold node depend current remain energy characterize nature dependence simulations reveal optimal dynamic policy significantly outperform heuristics,93,0,1403.1642.txt
http://arxiv.org/abs/1403.2975,Optimal ancilla-free Clifford+T approximation of z-rotations,"  We consider the problem of approximating arbitrary single-qubit z-rotations by ancilla-free Clifford+T circuits, up to given epsilon. We present a fast new probabilistic algorithm for solving this problem optimally, i.e., for finding the shortest possible circuit whatsoever for the given problem instance. The algorithm requires a factoring oracle (such as a quantum computer). Even in the absence of a factoring oracle, the algorithm is still near-optimal under a mild number-theoretic hypothesis. In this case, the algorithm finds a solution of T-count m + O(log(log(1/epsilon))), where m is the T-count of the second-to-optimal solution. In the typical case, this yields circuit approximations of T-count 3log_2(1/epsilon) + O(log(log(1/epsilon))). Our algorithm is efficient in practice, and provably efficient under the above-mentioned number-theoretic hypothesis, in the sense that its expected runtime is O(polylog(1/epsilon)). ",Quantum Physics ; Computer Science - Emerging Technologies ; ,"Ross, Neil J. ; Selinger, Peter ; ","Optimal ancilla-free Clifford+T approximation of z-rotations  We consider the problem of approximating arbitrary single-qubit z-rotations by ancilla-free Clifford+T circuits, up to given epsilon. We present a fast new probabilistic algorithm for solving this problem optimally, i.e., for finding the shortest possible circuit whatsoever for the given problem instance. The algorithm requires a factoring oracle (such as a quantum computer). Even in the absence of a factoring oracle, the algorithm is still near-optimal under a mild number-theoretic hypothesis. In this case, the algorithm finds a solution of T-count m + O(log(log(1/epsilon))), where m is the T-count of the second-to-optimal solution. In the typical case, this yields circuit approximations of T-count 3log_2(1/epsilon) + O(log(log(1/epsilon))). Our algorithm is efficient in practice, and provably efficient under the above-mentioned number-theoretic hypothesis, in the sense that its expected runtime is O(polylog(1/epsilon)). ",optimal ancilla free clifford approximation rotations consider problem approximate arbitrary single qubit rotations ancilla free clifford circuit give epsilon present fast new probabilistic algorithm solve problem optimally find shortest possible circuit whatsoever give problem instance algorithm require factor oracle quantum computer even absence factor oracle algorithm still near optimal mild number theoretic hypothesis case algorithm find solution count log log epsilon count second optimal solution typical case yield circuit approximations count log epsilon log log epsilon algorithm efficient practice provably efficient mention number theoretic hypothesis sense expect runtime polylog epsilon,90,1,1403.2975.txt
http://arxiv.org/abs/1403.3772,Study of Behaviours via Visitable Paths,"  Around 2000, J.-Y. Girard developed a logical theory, called Ludics. This theory was a step in his program of Geometry of Interaction, the aim of which being to account for the dynamics of logical proofs. In Ludics, objects called designs keep only what is relevant for the cut elimination process, hence the dynamics of a proof: a design is an abstraction of a formal proof. The notion of behaviour is the counterpart in Ludics of the notion of type or the logical notion of formula. Formally a behaviour is a closed set of designs. Our aim is to explore the constructions of behaviours and to analyse their properties. In this paper a design is viewed as a set of coherent paths. We recall or give variants of properties concerning visitable paths, where a visitable path is a path in a design or a set of designs that may be traversed by interaction with a design of the orthogonal of the set. We are then able to answer the following question: which properties should satisfy a set of paths for being exactly the set of visitable paths of a behaviour? Such a set and its dual should be prefix-closed, daimon-closed and satisfy two saturation properties. This allows us to have a means for defining the whole set of visitable paths of a given set of designs without closing it explicitly, that is without computing the orthogonal of this set of designs. We finally apply all these results for making explicit the structure of a behaviour generated by constants and multiplicative/additive connectives. We end by proposing an oriented tensor for which we give basic properties. ",Computer Science - Logic in Computer Science ; F.4.1 ; ,"Fouqueré, Christophe ; Quatrini, Myriam ; ","Study of Behaviours via Visitable Paths  Around 2000, J.-Y. Girard developed a logical theory, called Ludics. This theory was a step in his program of Geometry of Interaction, the aim of which being to account for the dynamics of logical proofs. In Ludics, objects called designs keep only what is relevant for the cut elimination process, hence the dynamics of a proof: a design is an abstraction of a formal proof. The notion of behaviour is the counterpart in Ludics of the notion of type or the logical notion of formula. Formally a behaviour is a closed set of designs. Our aim is to explore the constructions of behaviours and to analyse their properties. In this paper a design is viewed as a set of coherent paths. We recall or give variants of properties concerning visitable paths, where a visitable path is a path in a design or a set of designs that may be traversed by interaction with a design of the orthogonal of the set. We are then able to answer the following question: which properties should satisfy a set of paths for being exactly the set of visitable paths of a behaviour? Such a set and its dual should be prefix-closed, daimon-closed and satisfy two saturation properties. This allows us to have a means for defining the whole set of visitable paths of a given set of designs without closing it explicitly, that is without computing the orthogonal of this set of designs. We finally apply all these results for making explicit the structure of a behaviour generated by constants and multiplicative/additive connectives. We end by proposing an oriented tensor for which we give basic properties. ",study behaviours via visitable paths around girard develop logical theory call ludics theory step program geometry interaction aim account dynamics logical proof ludics object call design keep relevant cut elimination process hence dynamics proof design abstraction formal proof notion behaviour counterpart ludics notion type logical notion formula formally behaviour close set design aim explore constructions behaviours analyse properties paper design view set coherent paths recall give variants properties concern visitable paths visitable path path design set design may traverse interaction design orthogonal set able answer follow question properties satisfy set paths exactly set visitable paths behaviour set dual prefix close daimon close satisfy two saturation properties allow us mean define whole set visitable paths give set design without close explicitly without compute orthogonal set design finally apply result make explicit structure behaviour generate constants multiplicative additive connectives end propose orient tensor give basic properties,144,8,1403.3772.txt
http://arxiv.org/abs/1403.4143,P is not equal to NP by Modus Tollens,"  An artificially designed Turing Machine algorithm $\mathbf{M}_{}^{o}$ generates the instances of the satisfiability problem, and check their satisfiability. Under the assumption $\mathcal{P}=\mathcal{NP}$, we show that $\mathbf{M}_{}^{o}$ has a certain property, which, without the assumption, $\mathbf{M}_{}^{o}$ does not have. This leads to $\mathcal{P}\neq\mathcal{NP}$ $ $ by modus tollens. ",Computer Science - Computational Complexity ; ,"Kim, Joonmo ; ","P is not equal to NP by Modus Tollens  An artificially designed Turing Machine algorithm $\mathbf{M}_{}^{o}$ generates the instances of the satisfiability problem, and check their satisfiability. Under the assumption $\mathcal{P}=\mathcal{NP}$, we show that $\mathbf{M}_{}^{o}$ has a certain property, which, without the assumption, $\mathbf{M}_{}^{o}$ does not have. This leads to $\mathcal{P}\neq\mathcal{NP}$ $ $ by modus tollens. ",equal np modus tollens artificially design turing machine algorithm mathbf generate instance satisfiability problem check satisfiability assumption mathcal mathcal np show mathbf certain property without assumption mathbf lead mathcal neq mathcal np modus tollens,34,7,1403.4143.txt
http://arxiv.org/abs/1403.4539,Occam Bound on Lowest Complexity of Elements,"  The combined universal probability M(D) of strings x in sets D is close to max_{x \in D} M({x}): their ~ logs differ by at most D's information j = I(D:H) about the halting sequence H. Thus if all x have complexity K(x) > k, D carries > i bits of information on each x where i+j ~ k. Note, there are no ways (whether natural or artificial) to generate D with significant I(D:H). ",Computer Science - Computational Complexity ; ,"Levin, Leonid A. ; ","Occam Bound on Lowest Complexity of Elements  The combined universal probability M(D) of strings x in sets D is close to max_{x \in D} M({x}): their ~ logs differ by at most D's information j = I(D:H) about the halting sequence H. Thus if all x have complexity K(x) > k, D carries > i bits of information on each x where i+j ~ k. Note, there are no ways (whether natural or artificial) to generate D with significant I(D:H). ",occam bind lowest complexity elements combine universal probability string set close max log differ information halt sequence thus complexity carry bits information note ways whether natural artificial generate significant,29,4,1403.4539.txt
http://arxiv.org/abs/1403.4622,Complete simultaneous conjugacy invariants in Artin's braid groups,"  We solve the simultaneous conjugacy problem in Artin's braid groups and, more generally, in Garside groups, by means of a complete, effectively computable, finite invariant. This invariant generalizes the one-dimensional notion of super summit set to arbitrary dimensions. One key ingredient in our solution is the introduction of a provable high-dimensional version of the Birman--Ko--Lee cycling theorem. The complexity of this solution is a small degree polynomial in the cardinalities of our generalized super summit sets and the input parameters. Computer experiments suggest that the cardinality of this invariant, for a list of order $N$ independent elements of Artin's braid group $B_N$, is generically close to~1. ","Mathematics - Group Theory ; Computer Science - Computational Complexity ; Computer Science - Cryptography and Security ; 20F36, 20F65, 20C40 ; ","Kalka, Arkadius ; Tsaban, Boaz ; Vinokur, Gary ; ","Complete simultaneous conjugacy invariants in Artin's braid groups  We solve the simultaneous conjugacy problem in Artin's braid groups and, more generally, in Garside groups, by means of a complete, effectively computable, finite invariant. This invariant generalizes the one-dimensional notion of super summit set to arbitrary dimensions. One key ingredient in our solution is the introduction of a provable high-dimensional version of the Birman--Ko--Lee cycling theorem. The complexity of this solution is a small degree polynomial in the cardinalities of our generalized super summit sets and the input parameters. Computer experiments suggest that the cardinality of this invariant, for a list of order $N$ independent elements of Artin's braid group $B_N$, is generically close to~1. ",complete simultaneous conjugacy invariants artin braid group solve simultaneous conjugacy problem artin braid group generally garside group mean complete effectively computable finite invariant invariant generalize one dimensional notion super summit set arbitrary dimension one key ingredient solution introduction provable high dimensional version birman ko lee cycle theorem complexity solution small degree polynomial cardinalities generalize super summit set input parameters computer experiment suggest cardinality invariant list order independent elements artin braid group generically close,73,4,1403.4622.txt
http://arxiv.org/abs/1403.4861,Improved Approximation Algorithms for Box Contact Representations,"  We study the following geometric representation problem: Given a graph whose vertices correspond to axis-aligned rectangles with fixed dimensions, arrange the rectangles without overlaps in the plane such that two rectangles touch if the graph contains an edge between them. This problem is called \textsc{Contact Representation of Word Networks} (\textsc{Crown}) since it formalizes the geometric problem behind drawing word clouds in which semantically related words are close to each other. \textsc{Crown} is known to be NP-hard, and there are approximation algorithms for certain graph classes for the optimization version, \textsc{Max-Crown}, in which realizing each desired adjacency yields a certain profit. We present the first $O(1)$-approximation algorithm for the general case, when the input is a complete weighted graph, and for the bipartite case. Since the subgraph of realized adjacencies is necessarily planar, we also consider several planar graph classes (namely stars, trees, outerplanar, and planar graphs), improving upon the known results. For some graph classes, we also describe improvements in the unweighted case, where each adjacency yields the same profit. Finally, we show that the problem is APX-hard on bipartite graphs of bounded maximum degree. ",Computer Science - Data Structures and Algorithms ; ,"Bekos, Michael A. ; van Dijk, Thomas C. ; Fink, Martin ; Kindermann, Philipp ; Kobourov, Stephen ; Pupyrev, Sergey ; Spoerhase, Joachim ; Wolff, Alexander ; ","Improved Approximation Algorithms for Box Contact Representations  We study the following geometric representation problem: Given a graph whose vertices correspond to axis-aligned rectangles with fixed dimensions, arrange the rectangles without overlaps in the plane such that two rectangles touch if the graph contains an edge between them. This problem is called \textsc{Contact Representation of Word Networks} (\textsc{Crown}) since it formalizes the geometric problem behind drawing word clouds in which semantically related words are close to each other. \textsc{Crown} is known to be NP-hard, and there are approximation algorithms for certain graph classes for the optimization version, \textsc{Max-Crown}, in which realizing each desired adjacency yields a certain profit. We present the first $O(1)$-approximation algorithm for the general case, when the input is a complete weighted graph, and for the bipartite case. Since the subgraph of realized adjacencies is necessarily planar, we also consider several planar graph classes (namely stars, trees, outerplanar, and planar graphs), improving upon the known results. For some graph classes, we also describe improvements in the unweighted case, where each adjacency yields the same profit. Finally, we show that the problem is APX-hard on bipartite graphs of bounded maximum degree. ",improve approximation algorithms box contact representations study follow geometric representation problem give graph whose vertices correspond axis align rectangles fix dimension arrange rectangles without overlap plane two rectangles touch graph contain edge problem call textsc contact representation word network textsc crown since formalize geometric problem behind draw word cloud semantically relate word close textsc crown know np hard approximation algorithms certain graph class optimization version textsc max crown realize desire adjacency yield certain profit present first approximation algorithm general case input complete weight graph bipartite case since subgraph realize adjacencies necessarily planar also consider several planar graph class namely star tree outerplanar planar graph improve upon know result graph class also describe improvements unweighted case adjacency yield profit finally show problem apx hard bipartite graph bound maximum degree,128,3,1403.4861.txt
http://arxiv.org/abs/1403.5361,Parameter Estimation of Social Forces in Crowd Dynamics Models via a   Probabilistic Method,"  Focusing on a specific crowd dynamics situation, including real life experiments and measurements, our paper targets a twofold aim: (1) we present a Bayesian probabilistic method to estimate the value and the uncertainty (in the form of a probability density function) of parameters in crowd dynamic models from the experimental data; and (2) we introduce a fitness measure for the models to classify a couple of model structures (forces) according to their fitness to the experimental data, preparing the stage for a more general model-selection and validation strategy inspired by probabilistic data analysis. Finally, we review the essential aspects of our experimental setup and measurement technique. ","Physics - Data Analysis, Statistics and Probability ; Computer Science - Social and Information Networks ; Mathematics - Probability ; Mathematics - Statistics Theory ; Physics - Physics and Society ; ","Corbetta, Alessandro ; Muntean, Adrian ; Toschi, Federico ; Vafayi, Kiamars ; ","Parameter Estimation of Social Forces in Crowd Dynamics Models via a   Probabilistic Method  Focusing on a specific crowd dynamics situation, including real life experiments and measurements, our paper targets a twofold aim: (1) we present a Bayesian probabilistic method to estimate the value and the uncertainty (in the form of a probability density function) of parameters in crowd dynamic models from the experimental data; and (2) we introduce a fitness measure for the models to classify a couple of model structures (forces) according to their fitness to the experimental data, preparing the stage for a more general model-selection and validation strategy inspired by probabilistic data analysis. Finally, we review the essential aspects of our experimental setup and measurement technique. ",parameter estimation social force crowd dynamics model via probabilistic method focus specific crowd dynamics situation include real life experiment measurements paper target twofold aim present bayesian probabilistic method estimate value uncertainty form probability density function parameters crowd dynamic model experimental data introduce fitness measure model classify couple model structure force accord fitness experimental data prepare stage general model selection validation strategy inspire probabilistic data analysis finally review essential aspects experimental setup measurement technique,73,11,1403.5361.txt
http://arxiv.org/abs/1403.5543,Disaster Recovery in Wireless Networks: A Homology-Based Algorithm,"  In this paper, we present an algorithm for the recovery of wireless networks after a disaster. Considering a damaged wireless network, presenting coverage holes or/and many disconnected components, we propose a disaster recovery algorithm which repairs the network. It provides the list of locations where to put new nodes in order to patch the coverage holes and mend the disconnected components. In order to do this we first consider the simplicial complex representation of the network, then the algorithm adds supplementary vertices in excessive number, and afterwards runs a reduction algorithm in order to reach an optimal result. One of the novelty of this work resides in the proposed method for the addition of vertices. We use a determinantal point process: the Ginibre point process which has inherent repulsion between vertices, and has never been simulated before for wireless networks representation. We compare both the determinantal point process addition method with other vertices addition methods, and the whole disaster recovery algorithm to the greedy algorithm for the set cover problem. ",Mathematics - Probability ; Computer Science - Networking and Internet Architecture ; ,"Vergne, Anaïs ; Flint, Ian ; Decreusefond, Laurent ; Martins, Philippe ; ","Disaster Recovery in Wireless Networks: A Homology-Based Algorithm  In this paper, we present an algorithm for the recovery of wireless networks after a disaster. Considering a damaged wireless network, presenting coverage holes or/and many disconnected components, we propose a disaster recovery algorithm which repairs the network. It provides the list of locations where to put new nodes in order to patch the coverage holes and mend the disconnected components. In order to do this we first consider the simplicial complex representation of the network, then the algorithm adds supplementary vertices in excessive number, and afterwards runs a reduction algorithm in order to reach an optimal result. One of the novelty of this work resides in the proposed method for the addition of vertices. We use a determinantal point process: the Ginibre point process which has inherent repulsion between vertices, and has never been simulated before for wireless networks representation. We compare both the determinantal point process addition method with other vertices addition methods, and the whole disaster recovery algorithm to the greedy algorithm for the set cover problem. ",disaster recovery wireless network homology base algorithm paper present algorithm recovery wireless network disaster consider damage wireless network present coverage hole many disconnect components propose disaster recovery algorithm repair network provide list locations put new nod order patch coverage hole mend disconnect components order first consider simplicial complex representation network algorithm add supplementary vertices excessive number afterwards run reduction algorithm order reach optimal result one novelty work reside propose method addition vertices use determinantal point process ginibre point process inherent repulsion vertices never simulate wireless network representation compare determinantal point process addition method vertices addition methods whole disaster recovery algorithm greedy algorithm set cover problem,105,6,1403.5543.txt
http://arxiv.org/abs/1403.5715,Mining Attribute-Based Access Control Policies from Logs,"  Attribute-based access control (ABAC) provides a high level of flexibility that promotes security and information sharing. ABAC policy mining algorithms have potential to significantly reduce the cost of migration to ABAC, by partially automating the development of an ABAC policy from information about the existing access-control policy and attribute data. This paper presents an algorithm for mining ABAC policies from operation logs and attribute data. To the best of our knowledge, it is the first algorithm for this problem. ",Computer Science - Cryptography and Security ; Computer Science - Databases ; ,"Xu, Zhongyuan ; Stoller, Scott D. ; ","Mining Attribute-Based Access Control Policies from Logs  Attribute-based access control (ABAC) provides a high level of flexibility that promotes security and information sharing. ABAC policy mining algorithms have potential to significantly reduce the cost of migration to ABAC, by partially automating the development of an ABAC policy from information about the existing access-control policy and attribute data. This paper presents an algorithm for mining ABAC policies from operation logs and attribute data. To the best of our knowledge, it is the first algorithm for this problem. ",mine attribute base access control policies log attribute base access control abac provide high level flexibility promote security information share abac policy mine algorithms potential significantly reduce cost migration abac partially automate development abac policy information exist access control policy attribute data paper present algorithm mine abac policies operation log attribute data best knowledge first algorithm problem,57,0,1403.5715.txt
http://arxiv.org/abs/1403.6322,Do the Fix Ingredients Already Exist? An Empirical Inquiry into the   Redundancy Assumptions of Program Repair Approaches,"  Much initial research on automatic program repair has focused on experimental results to probe their potential to find patches and reduce development effort. Relatively less effort has been put into understanding the hows and whys of such approaches. For example, a critical assumption of the GenProg technique is that certain bugs can be fixed by copying and re-arranging existing code. In other words, GenProg assumes that the fix ingredients already exist elsewhere in the code. In this paper, we formalize these assumptions around the concept of ''temporal redundancy''. A temporally redundant commit is only composed of what has already existed in previous commits. Our experiments show that a large proportion of commits that add existing code are temporally redundant. This validates the fundamental redundancy assumption of GenProg. ",Computer Science - Software Engineering ; ,"Martinez, Matias ; Weimer, Westley ; Monperrus, Martin ; ","Do the Fix Ingredients Already Exist? An Empirical Inquiry into the   Redundancy Assumptions of Program Repair Approaches  Much initial research on automatic program repair has focused on experimental results to probe their potential to find patches and reduce development effort. Relatively less effort has been put into understanding the hows and whys of such approaches. For example, a critical assumption of the GenProg technique is that certain bugs can be fixed by copying and re-arranging existing code. In other words, GenProg assumes that the fix ingredients already exist elsewhere in the code. In this paper, we formalize these assumptions around the concept of ''temporal redundancy''. A temporally redundant commit is only composed of what has already existed in previous commits. Our experiments show that a large proportion of commits that add existing code are temporally redundant. This validates the fundamental redundancy assumption of GenProg. ",fix ingredients already exist empirical inquiry redundancy assumptions program repair approach much initial research automatic program repair focus experimental result probe potential find patch reduce development effort relatively less effort put understand hows whys approach example critical assumption genprog technique certain bug fix copy arrange exist code word genprog assume fix ingredients already exist elsewhere code paper formalize assumptions around concept temporal redundancy temporally redundant commit compose already exist previous commit experiment show large proportion commit add exist code temporally redundant validate fundamental redundancy assumption genprog,86,14,1403.6322.txt
http://arxiv.org/abs/1404.1008,Spectral concentration and greedy k-clustering,"  A popular graph clustering method is to consider the embedding of an input graph into R^k induced by the first k eigenvectors of its Laplacian, and to partition the graph via geometric manipulations on the resulting metric space. Despite the practical success of this methodology, there is limited understanding of several heuristics that follow this framework. We provide theoretical justification for one such natural and computationally efficient variant.   Our result can be summarized as follows. A partition of a graph is called strong if each cluster has small external conductance, and large internal conductance. We present a simple greedy spectral clustering algorithm which returns a partition that is provably close to a suitably strong partition, provided that such a partition exists. A recent result shows that strong partitions exist for graphs with a sufficiently large spectral gap between the k-th and (k+1)-st eigenvalues. Taking this together with our main theorem gives a spectral algorithm which finds a partition close to a strong one for graphs with large enough spectral gap. We also show how this simple greedy algorithm can be implemented in near-linear time for any fixed k and error guarantee. Finally, we evaluate our algorithm on some real-world and synthetic inputs. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Geometry ; ,"Dey, Tamal K. ; Peng, Pan ; Rossi, Alfred ; Sidiropoulos, Anastasios ; ","Spectral concentration and greedy k-clustering  A popular graph clustering method is to consider the embedding of an input graph into R^k induced by the first k eigenvectors of its Laplacian, and to partition the graph via geometric manipulations on the resulting metric space. Despite the practical success of this methodology, there is limited understanding of several heuristics that follow this framework. We provide theoretical justification for one such natural and computationally efficient variant.   Our result can be summarized as follows. A partition of a graph is called strong if each cluster has small external conductance, and large internal conductance. We present a simple greedy spectral clustering algorithm which returns a partition that is provably close to a suitably strong partition, provided that such a partition exists. A recent result shows that strong partitions exist for graphs with a sufficiently large spectral gap between the k-th and (k+1)-st eigenvalues. Taking this together with our main theorem gives a spectral algorithm which finds a partition close to a strong one for graphs with large enough spectral gap. We also show how this simple greedy algorithm can be implemented in near-linear time for any fixed k and error guarantee. Finally, we evaluate our algorithm on some real-world and synthetic inputs. ",spectral concentration greedy cluster popular graph cluster method consider embed input graph induce first eigenvectors laplacian partition graph via geometric manipulations result metric space despite practical success methodology limit understand several heuristics follow framework provide theoretical justification one natural computationally efficient variant result summarize follow partition graph call strong cluster small external conductance large internal conductance present simple greedy spectral cluster algorithm return partition provably close suitably strong partition provide partition exist recent result show strong partition exist graph sufficiently large spectral gap th st eigenvalues take together main theorem give spectral algorithm find partition close strong one graph large enough spectral gap also show simple greedy algorithm implement near linear time fix error guarantee finally evaluate algorithm real world synthetic input,122,3,1404.1008.txt
http://arxiv.org/abs/1404.1864,Sublinear algorithms for local graph centrality estimation,"  We study the complexity of local graph centrality estimation, with the goal of approximating the centrality score of a given target node while exploring only a sublinear number of nodes/arcs of the graph and performing a sublinear number of elementary operations. We develop a technique, that we apply to the PageRank and Heat Kernel centralities, for building a low-variance score estimator through a local exploration of the graph. We obtain an algorithm that, given any node in any graph of $m$ arcs, with probability $(1-\delta)$ computes a multiplicative $(1\pm\epsilon)$-approximation of its score by examining only $\tilde{O}(\min(m^{2/3} \Delta^{1/3} d^{-2/3},\, m^{4/5} d^{-3/5}))$ nodes/arcs, where $\Delta$ and $d$ are respectively the maximum and average outdegree of the graph (omitting for readability $\operatorname{poly}(\epsilon^{-1})$ and $\operatorname{polylog}(\delta^{-1})$ factors). A similar bound holds for computational complexity. We also prove a lower bound of $\Omega(\min(m^{1/2} \Delta^{1/2} d^{-1/2}, \, m^{2/3} d^{-1/3}))$ for both query complexity and computational complexity. Moreover, our technique yields a $\tilde{O}(n^{2/3})$ query complexity algorithm for the graph access model of [Brautbar et al., 2010], widely used in social network mining; we show this algorithm is optimal up to a sublogarithmic factor. These are the first algorithms yielding worst-case sublinear bounds for general directed graphs and any choice of the target node. ",Computer Science - Data Structures and Algorithms ; Computer Science - Information Retrieval ; Computer Science - Social and Information Networks ; ,"Bressan, Marco ; Peserico, Enoch ; Pretto, Luca ; ","Sublinear algorithms for local graph centrality estimation  We study the complexity of local graph centrality estimation, with the goal of approximating the centrality score of a given target node while exploring only a sublinear number of nodes/arcs of the graph and performing a sublinear number of elementary operations. We develop a technique, that we apply to the PageRank and Heat Kernel centralities, for building a low-variance score estimator through a local exploration of the graph. We obtain an algorithm that, given any node in any graph of $m$ arcs, with probability $(1-\delta)$ computes a multiplicative $(1\pm\epsilon)$-approximation of its score by examining only $\tilde{O}(\min(m^{2/3} \Delta^{1/3} d^{-2/3},\, m^{4/5} d^{-3/5}))$ nodes/arcs, where $\Delta$ and $d$ are respectively the maximum and average outdegree of the graph (omitting for readability $\operatorname{poly}(\epsilon^{-1})$ and $\operatorname{polylog}(\delta^{-1})$ factors). A similar bound holds for computational complexity. We also prove a lower bound of $\Omega(\min(m^{1/2} \Delta^{1/2} d^{-1/2}, \, m^{2/3} d^{-1/3}))$ for both query complexity and computational complexity. Moreover, our technique yields a $\tilde{O}(n^{2/3})$ query complexity algorithm for the graph access model of [Brautbar et al., 2010], widely used in social network mining; we show this algorithm is optimal up to a sublogarithmic factor. These are the first algorithms yielding worst-case sublinear bounds for general directed graphs and any choice of the target node. ",sublinear algorithms local graph centrality estimation study complexity local graph centrality estimation goal approximate centrality score give target node explore sublinear number nod arc graph perform sublinear number elementary operations develop technique apply pagerank heat kernel centralities build low variance score estimator local exploration graph obtain algorithm give node graph arc probability delta compute multiplicative pm epsilon approximation score examine tilde min delta nod arc delta respectively maximum average outdegree graph omit readability operatorname poly epsilon operatorname polylog delta factor similar bind hold computational complexity also prove lower bind omega min delta query complexity computational complexity moreover technique yield tilde query complexity algorithm graph access model brautbar et al widely use social network mine show algorithm optimal sublogarithmic factor first algorithms yield worst case sublinear bound general direct graph choice target node,132,3,1404.1864.txt
http://arxiv.org/abs/1404.2329,Duality and Optimality of Auctions for Uniform Distributions,"  We develop a general duality-theory framework for revenue maximization in additive Bayesian auctions. The framework extends linear programming duality and complementarity to constraints with partial derivatives. The dual system reveals the geometric nature of the problem and highlights its connection with the theory of bipartite graph matchings. We demonstrate the power of the framework by applying it to a multiple-good monopoly setting where the buyer has uniformly distributed valuations for the items, the canonical long-standing open problem in the area. We propose a deterministic selling mechanism called Straight-Jacket Auction (SJA), which we prove to be exactly optimal for up to 6 items, and conjecture its optimality for any number of goods. The duality framework is used not only for proving optimality, but perhaps more importantly for deriving the optimal mechanism itself; as a result, SJA is defined by natural geometric constraints. ",Computer Science - Computer Science and Game Theory ; ,"Giannakopoulos, Yiannis ; Koutsoupias, Elias ; ","Duality and Optimality of Auctions for Uniform Distributions  We develop a general duality-theory framework for revenue maximization in additive Bayesian auctions. The framework extends linear programming duality and complementarity to constraints with partial derivatives. The dual system reveals the geometric nature of the problem and highlights its connection with the theory of bipartite graph matchings. We demonstrate the power of the framework by applying it to a multiple-good monopoly setting where the buyer has uniformly distributed valuations for the items, the canonical long-standing open problem in the area. We propose a deterministic selling mechanism called Straight-Jacket Auction (SJA), which we prove to be exactly optimal for up to 6 items, and conjecture its optimality for any number of goods. The duality framework is used not only for proving optimality, but perhaps more importantly for deriving the optimal mechanism itself; as a result, SJA is defined by natural geometric constraints. ",duality optimality auction uniform distributions develop general duality theory framework revenue maximization additive bayesian auction framework extend linear program duality complementarity constraints partial derivatives dual system reveal geometric nature problem highlight connection theory bipartite graph match demonstrate power framework apply multiple good monopoly set buyer uniformly distribute valuations items canonical long stand open problem area propose deterministic sell mechanism call straight jacket auction sja prove exactly optimal items conjecture optimality number goods duality framework use prove optimality perhaps importantly derive optimal mechanism result sja define natural geometric constraints,88,0,1404.2329.txt
http://arxiv.org/abs/1404.2458,r-Extreme Signalling for Congestion Control,"  In many ""smart city"" applications, congestion arises in part due to the nature of signals received by individuals from a central authority. In the model of Marecek et al. [arXiv:1406.7639, Int. J. Control 88(10), 2015], each agent uses one out of multiple resources at each time instant. The per-use cost of a resource depends on the number of concurrent users. A central authority has up-to-date knowledge of the congestion across all resources and uses randomisation to provide a scalar or an interval for each resource at each time. In this paper, the interval to broadcast per resource is obtained by taking the minima and maxima of costs observed within a time window of length r, rather than by randomisation. We show that the resulting distribution of agents across resources also converges in distribution, under plausible assumptions about the evolution of the population over time. ",Mathematics - Optimization and Control ; Computer Science - Artificial Intelligence ; Computer Science - Multiagent Systems ; ,"Marecek, Jakub ; Shorten, Robert ; Yu, Jia Yuan ; ","r-Extreme Signalling for Congestion Control  In many ""smart city"" applications, congestion arises in part due to the nature of signals received by individuals from a central authority. In the model of Marecek et al. [arXiv:1406.7639, Int. J. Control 88(10), 2015], each agent uses one out of multiple resources at each time instant. The per-use cost of a resource depends on the number of concurrent users. A central authority has up-to-date knowledge of the congestion across all resources and uses randomisation to provide a scalar or an interval for each resource at each time. In this paper, the interval to broadcast per resource is obtained by taking the minima and maxima of costs observed within a time window of length r, rather than by randomisation. We show that the resulting distribution of agents across resources also converges in distribution, under plausible assumptions about the evolution of the population over time. ",extreme signal congestion control many smart city applications congestion arise part due nature signal receive individuals central authority model marecek et al arxiv int control agent use one multiple resources time instant per use cost resource depend number concurrent users central authority date knowledge congestion across resources use randomisation provide scalar interval resource time paper interval broadcast per resource obtain take minima maxima cost observe within time window length rather randomisation show result distribution agents across resources also converge distribution plausible assumptions evolution population time,85,9,1404.2458.txt
http://arxiv.org/abs/1404.2743,Infinite dimensional finitely forcible graphon,"  Graphons are analytic objects associated with convergent sequences of dense graphs. Finitely forcible graphons, i.e., those determined by finitely many subgraph densities, are of particular interest because of their relation to various problems in extremal combinatorics and theoretical computer science. Lovasz and Szegedy conjectured that the topological space of typical vertices of a finitely forcible graphon always has finite dimension, which would have implications on the minimum number of parts in its weak eps-regular partition. We disprove the conjecture by constructing a finitely forcible graphon with the space of typical vertices that has infinite dimension. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Glebov, Roman ; Klimosova, Tereza ; Kral, Daniel ; ","Infinite dimensional finitely forcible graphon  Graphons are analytic objects associated with convergent sequences of dense graphs. Finitely forcible graphons, i.e., those determined by finitely many subgraph densities, are of particular interest because of their relation to various problems in extremal combinatorics and theoretical computer science. Lovasz and Szegedy conjectured that the topological space of typical vertices of a finitely forcible graphon always has finite dimension, which would have implications on the minimum number of parts in its weak eps-regular partition. We disprove the conjecture by constructing a finitely forcible graphon with the space of typical vertices that has infinite dimension. ",infinite dimensional finitely forcible graphon graphons analytic object associate convergent sequence dense graph finitely forcible graphons determine finitely many subgraph densities particular interest relation various problems extremal combinatorics theoretical computer science lovasz szegedy conjecture topological space typical vertices finitely forcible graphon always finite dimension would implications minimum number part weak eps regular partition disprove conjecture construct finitely forcible graphon space typical vertices infinite dimension,64,4,1404.2743.txt
http://arxiv.org/abs/1404.3056,Principles of Antifragile Software,"  The goal of this paper is to study and define the concept of ""antifragile software"". For this, I start from Taleb's statement that antifragile systems love errors, and discuss whether traditional software dependability fits into this class. The answer is somewhat negative, although adaptive fault tolerance is antifragile: the system learns something when an error happens, and always imrpoves. Automatic runtime bug fixing is changing the code in response to errors, fault injection in production means injecting errors in business critical software. I claim that both correspond to antifragility. Finally, I hypothesize that antifragile development processes are better at producing antifragile software systems. ",Computer Science - Software Engineering ; ,"Monperrus, Martin ; ","Principles of Antifragile Software  The goal of this paper is to study and define the concept of ""antifragile software"". For this, I start from Taleb's statement that antifragile systems love errors, and discuss whether traditional software dependability fits into this class. The answer is somewhat negative, although adaptive fault tolerance is antifragile: the system learns something when an error happens, and always imrpoves. Automatic runtime bug fixing is changing the code in response to errors, fault injection in production means injecting errors in business critical software. I claim that both correspond to antifragility. Finally, I hypothesize that antifragile development processes are better at producing antifragile software systems. ",principles antifragile software goal paper study define concept antifragile software start taleb statement antifragile systems love errors discuss whether traditional software dependability fit class answer somewhat negative although adaptive fault tolerance antifragile system learn something error happen always imrpoves automatic runtime bug fix change code response errors fault injection production mean inject errors business critical software claim correspond antifragility finally hypothesize antifragile development process better produce antifragile software systems,69,8,1404.3056.txt
http://arxiv.org/abs/1404.3186,Automatic Repair of Buggy If Conditions and Missing Preconditions with   SMT,"  We present Nopol, an approach for automatically repairing buggy if conditions and missing preconditions. As input, it takes a program and a test suite which contains passing test cases modeling the expected behavior of the program and at least one failing test case embodying the bug to be repaired. It consists of collecting data from multiple instrumented test suite executions, transforming this data into a Satisfiability Modulo Theory (SMT) problem, and translating the SMT result -- if there exists one -- into a source code patch. Nopol repairs object oriented code and allows the patches to contain nullness checks as well as specific method calls. ",Computer Science - Software Engineering ; ,"Demarco, Favio ; Xuan, Jifeng ; Berre, Daniel Le ; Monperrus, Martin ; ","Automatic Repair of Buggy If Conditions and Missing Preconditions with   SMT  We present Nopol, an approach for automatically repairing buggy if conditions and missing preconditions. As input, it takes a program and a test suite which contains passing test cases modeling the expected behavior of the program and at least one failing test case embodying the bug to be repaired. It consists of collecting data from multiple instrumented test suite executions, transforming this data into a Satisfiability Modulo Theory (SMT) problem, and translating the SMT result -- if there exists one -- into a source code patch. Nopol repairs object oriented code and allows the patches to contain nullness checks as well as specific method calls. ",automatic repair buggy condition miss precondition smt present nopol approach automatically repair buggy condition miss precondition input take program test suite contain pass test case model expect behavior program least one fail test case embody bug repair consist collect data multiple instrument test suite executions transform data satisfiability modulo theory smt problem translate smt result exist one source code patch nopol repair object orient code allow patch contain nullness check well specific method call,74,0,1404.3186.txt
http://arxiv.org/abs/1404.3311,Generating Synchronizing Automata with Large Reset Lengths,"  We study synchronizing automata with the shortest reset words of relatively large length. First, we refine the Frankl-Pin result on the length of the shortest words of rank $m$, and the B\'eal, Berlinkov, Perrin, and Steinberg results on the length of the shortest reset words in one-cluster automata. The obtained results are useful in computation aimed in extending the class of small automata for which the \v{C}ern\'y conjecture is verified and discovering new automata with special properties regarding synchronization. ",Computer Science - Formal Languages and Automata Theory ; ,"Kisielewicz, Andrzej ; Szykuła, Marek ; ","Generating Synchronizing Automata with Large Reset Lengths  We study synchronizing automata with the shortest reset words of relatively large length. First, we refine the Frankl-Pin result on the length of the shortest words of rank $m$, and the B\'eal, Berlinkov, Perrin, and Steinberg results on the length of the shortest reset words in one-cluster automata. The obtained results are useful in computation aimed in extending the class of small automata for which the \v{C}ern\'y conjecture is verified and discovering new automata with special properties regarding synchronization. ",generate synchronize automata large reset lengths study synchronize automata shortest reset word relatively large length first refine frankl pin result length shortest word rank eal berlinkov perrin steinberg result length shortest reset word one cluster automata obtain result useful computation aim extend class small automata ern conjecture verify discover new automata special properties regard synchronization,55,14,1404.3311.txt
http://arxiv.org/abs/1404.3368,Near-optimal sample compression for nearest neighbors,"  We present the first sample compression algorithm for nearest neighbors with non-trivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented. ",Computer Science - Machine Learning ; Computer Science - Computational Complexity ; ,"Gottlieb, Lee-Ad ; Kontorovich, Aryeh ; Nisnevitch, Pinhas ; ","Near-optimal sample compression for nearest neighbors  We present the first sample compression algorithm for nearest neighbors with non-trivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented. ",near optimal sample compression nearest neighbor present first sample compression algorithm nearest neighbor non trivial performance guarantee complement guarantee demonstrate almost match hardness lower bound show bind nearly optimal result yield new insight margin base nearest neighbor classification metric space allow us significantly sharpen simplify exist bound encourage empirical result also present,52,11,1404.3368.txt
http://arxiv.org/abs/1404.3442,Optimal versus Nash Equilibrium Computation for Networked Resource   Allocation,"  Motivated by emerging resource allocation and data placement problems such as web caches and peer-to-peer systems, we consider and study a class of resource allocation problems over a network of agents (nodes). In this model, nodes can store only a limited number of resources while accessing the remaining ones through their closest neighbors. We consider this problem under both optimization and game-theoretic frameworks. In the case of optimal resource allocation we will first show that when there are only k=2 resources, the optimal allocation can be found efficiently in O(n^2\log n) steps, where n denotes the total number of nodes. However, for k>2 this problem becomes NP-hard with no polynomial time approximation algorithm with a performance guarantee better than 1+1/102k^2, even under metric access costs. We then provide a 3-approximation algorithm for the optimal resource allocation which runs only in linear time O(n). Subsequently, we look at this problem under a selfish setting formulated as a noncooperative game and provide a 3-approximation algorithm for obtaining its pure Nash equilibria under metric access costs. We then establish an equivalence between the set of pure Nash equilibria and flip-optimal solutions of the Max-k-Cut problem over a specific weighted complete graph. Using this reduction, we show that finding the lexicographically smallest Nash equilibrium for k> 2 is NP-hard, and provide an algorithm to find it in O(n^3 2^n) steps. While the reduction to weighted Max-k-Cut suggests that finding a pure Nash equilibrium using best response dynamics might be PLS-hard, it allows us to use tools from quadratic programming to devise more systematic algorithms towards obtaining Nash equilibrium points. ",Computer Science - Computer Science and Game Theory ; Computer Science - Discrete Mathematics ; Computer Science - Systems and Control ; Mathematics - Combinatorics ; ,"Etesami, S. Rasoul ; ","Optimal versus Nash Equilibrium Computation for Networked Resource   Allocation  Motivated by emerging resource allocation and data placement problems such as web caches and peer-to-peer systems, we consider and study a class of resource allocation problems over a network of agents (nodes). In this model, nodes can store only a limited number of resources while accessing the remaining ones through their closest neighbors. We consider this problem under both optimization and game-theoretic frameworks. In the case of optimal resource allocation we will first show that when there are only k=2 resources, the optimal allocation can be found efficiently in O(n^2\log n) steps, where n denotes the total number of nodes. However, for k>2 this problem becomes NP-hard with no polynomial time approximation algorithm with a performance guarantee better than 1+1/102k^2, even under metric access costs. We then provide a 3-approximation algorithm for the optimal resource allocation which runs only in linear time O(n). Subsequently, we look at this problem under a selfish setting formulated as a noncooperative game and provide a 3-approximation algorithm for obtaining its pure Nash equilibria under metric access costs. We then establish an equivalence between the set of pure Nash equilibria and flip-optimal solutions of the Max-k-Cut problem over a specific weighted complete graph. Using this reduction, we show that finding the lexicographically smallest Nash equilibrium for k> 2 is NP-hard, and provide an algorithm to find it in O(n^3 2^n) steps. While the reduction to weighted Max-k-Cut suggests that finding a pure Nash equilibrium using best response dynamics might be PLS-hard, it allows us to use tools from quadratic programming to devise more systematic algorithms towards obtaining Nash equilibrium points. ",optimal versus nash equilibrium computation network resource allocation motivate emerge resource allocation data placement problems web cache peer peer systems consider study class resource allocation problems network agents nod model nod store limit number resources access remain ones closest neighbor consider problem optimization game theoretic frameworks case optimal resource allocation first show resources optimal allocation find efficiently log step denote total number nod however problem become np hard polynomial time approximation algorithm performance guarantee better even metric access cost provide approximation algorithm optimal resource allocation run linear time subsequently look problem selfish set formulate noncooperative game provide approximation algorithm obtain pure nash equilibria metric access cost establish equivalence set pure nash equilibria flip optimal solutions max cut problem specific weight complete graph use reduction show find lexicographically smallest nash equilibrium np hard provide algorithm find step reduction weight max cut suggest find pure nash equilibrium use best response dynamics might pls hard allow us use tool quadratic program devise systematic algorithms towards obtain nash equilibrium point,166,1,1404.3442.txt
http://arxiv.org/abs/1404.3626,Optimal Power Flow as a Polynomial Optimization Problem,  Formulating the alternating current optimal power flow (ACOPF) as a polynomial optimization problem makes it possible to solve large instances in practice and to guarantee asymptotic convergence in theory. ,Mathematics - Optimization and Control ; Computer Science - Systems and Control ; ,"Ghaddar, Bissan ; Marecek, Jakub ; Mevissen, Martin ; ",Optimal Power Flow as a Polynomial Optimization Problem  Formulating the alternating current optimal power flow (ACOPF) as a polynomial optimization problem makes it possible to solve large instances in practice and to guarantee asymptotic convergence in theory. ,optimal power flow polynomial optimization problem formulate alternate current optimal power flow acopf polynomial optimization problem make possible solve large instance practice guarantee asymptotic convergence theory,26,8,1404.3626.txt
http://arxiv.org/abs/1404.5029,Using Covert Topological Information for Defense Against Malicious   Attacks on DC State Estimation,"  Accurate state estimation is of paramount importance to maintain the power system operating in a secure and efficient state. The recently identified coordinated data injection attacks to meter measurements can bypass the current security system and introduce errors to the state estimates. The conventional wisdom to mitigate such attacks is by securing meter measurements to evade malicious injections. In this paper, we provide a novel alternative to defend against false-data injection attacks using covert power network topological information. By keeping the exact reactance of a set of transmission lines from attackers, no false data injection attack can be launched to compromise any set of state variables. We first investigate from the attackers' perspective the necessary condition to perform injection attack. Based on the arguments, we characterize the optimal protection problem, which protects the state variables with minimum cost, as a well-studied Steiner tree problem in a graph. Besides, we also propose a mixed defending strategy that jointly considers the use of covert topological information and secure meter measurements when either method alone is costly or unable to achieve the protection objective. A mixed integer linear programming (MILP) formulation is introduced to obtain the optimal mixed defending strategy. To tackle the NP-hardness of the problem, a tree pruning-based heuristic is further presented to produce an approximate solution in polynomial time. The advantageous performance of the proposed defending mechanisms is verified in IEEE standard power system testcases. ",Computer Science - Cryptography and Security ; ,"Bi, Suzhi ; Zhang, Ying Jun ; ","Using Covert Topological Information for Defense Against Malicious   Attacks on DC State Estimation  Accurate state estimation is of paramount importance to maintain the power system operating in a secure and efficient state. The recently identified coordinated data injection attacks to meter measurements can bypass the current security system and introduce errors to the state estimates. The conventional wisdom to mitigate such attacks is by securing meter measurements to evade malicious injections. In this paper, we provide a novel alternative to defend against false-data injection attacks using covert power network topological information. By keeping the exact reactance of a set of transmission lines from attackers, no false data injection attack can be launched to compromise any set of state variables. We first investigate from the attackers' perspective the necessary condition to perform injection attack. Based on the arguments, we characterize the optimal protection problem, which protects the state variables with minimum cost, as a well-studied Steiner tree problem in a graph. Besides, we also propose a mixed defending strategy that jointly considers the use of covert topological information and secure meter measurements when either method alone is costly or unable to achieve the protection objective. A mixed integer linear programming (MILP) formulation is introduced to obtain the optimal mixed defending strategy. To tackle the NP-hardness of the problem, a tree pruning-based heuristic is further presented to produce an approximate solution in polynomial time. The advantageous performance of the proposed defending mechanisms is verified in IEEE standard power system testcases. ",use covert topological information defense malicious attack dc state estimation accurate state estimation paramount importance maintain power system operate secure efficient state recently identify coordinate data injection attack meter measurements bypass current security system introduce errors state estimate conventional wisdom mitigate attack secure meter measurements evade malicious injections paper provide novel alternative defend false data injection attack use covert power network topological information keep exact reactance set transmission line attackers false data injection attack launch compromise set state variables first investigate attackers perspective necessary condition perform injection attack base arguments characterize optimal protection problem protect state variables minimum cost well study steiner tree problem graph besides also propose mix defend strategy jointly consider use covert topological information secure meter measurements either method alone costly unable achieve protection objective mix integer linear program milp formulation introduce obtain optimal mix defend strategy tackle np hardness problem tree prune base heuristic present produce approximate solution polynomial time advantageous performance propose defend mechanisms verify ieee standard power system testcases,165,11,1404.5029.txt
http://arxiv.org/abs/1404.6898,Quantum Attacks on Classical Proof Systems - The Hardness of Quantum   Rewinding,"  Quantum zero-knowledge proofs and quantum proofs of knowledge are inherently difficult to analyze because their security analysis uses rewinding. Certain cases of quantum rewinding are handled by the results by Watrous (SIAM J Comput, 2009) and Unruh (Eurocrypt 2012), yet in general the problem remains elusive. We show that this is not only due to a lack of proof techniques: relative to an oracle, we show that classically secure proofs and proofs of knowledge are insecure in the quantum setting.   More specifically, sigma-protocols, the Fiat-Shamir construction, and Fischlin's proof system are quantum insecure under assumptions that are sufficient for classical security. Additionally, we show that for similar reasons, computationally binding commitments provide almost no security guarantees in a quantum setting.   To show these results, we develop the ""pick-one trick"", a general technique that allows an adversary to find one value satisfying a given predicate, but not two. ",Quantum Physics ; Computer Science - Cryptography and Security ; ,"Ambainis, Andris ; Rosmanis, Ansis ; Unruh, Dominique ; ","Quantum Attacks on Classical Proof Systems - The Hardness of Quantum   Rewinding  Quantum zero-knowledge proofs and quantum proofs of knowledge are inherently difficult to analyze because their security analysis uses rewinding. Certain cases of quantum rewinding are handled by the results by Watrous (SIAM J Comput, 2009) and Unruh (Eurocrypt 2012), yet in general the problem remains elusive. We show that this is not only due to a lack of proof techniques: relative to an oracle, we show that classically secure proofs and proofs of knowledge are insecure in the quantum setting.   More specifically, sigma-protocols, the Fiat-Shamir construction, and Fischlin's proof system are quantum insecure under assumptions that are sufficient for classical security. Additionally, we show that for similar reasons, computationally binding commitments provide almost no security guarantees in a quantum setting.   To show these results, we develop the ""pick-one trick"", a general technique that allows an adversary to find one value satisfying a given predicate, but not two. ",quantum attack classical proof systems hardness quantum rewinding quantum zero knowledge proof quantum proof knowledge inherently difficult analyze security analysis use rewinding certain case quantum rewinding handle result watrous siam comput unruh eurocrypt yet general problem remain elusive show due lack proof techniques relative oracle show classically secure proof proof knowledge insecure quantum set specifically sigma protocols fiat shamir construction fischlin proof system quantum insecure assumptions sufficient classical security additionally show similar reason computationally bind commitments provide almost security guarantee quantum set show result develop pick one trick general technique allow adversary find one value satisfy give predicate two,99,8,1404.6898.txt
http://arxiv.org/abs/1404.7325,Tight Bounds for Restricted Grid Scheduling,"  The following online bin packing problem is considered: Items with integer sizes are given and variable sized bins arrive online. A bin must be used if there is still an item remaining which fits in it when the bin arrives. The goal is to minimize the total size of all the bins used. Previously, a lower bound of 5/4 on the competitive ratio of this problem was achieved using jobs of size S and 2S-1. For these item sizes and maximum bin size 4S-3, we obtain asymptotically matching upper and lower bounds, which vary depending on the ratio of the number of small jobs to the number of large jobs. ",Computer Science - Data Structures and Algorithms ; ,"Boyar, Joan ; Ellen, Faith ; ","Tight Bounds for Restricted Grid Scheduling  The following online bin packing problem is considered: Items with integer sizes are given and variable sized bins arrive online. A bin must be used if there is still an item remaining which fits in it when the bin arrives. The goal is to minimize the total size of all the bins used. Previously, a lower bound of 5/4 on the competitive ratio of this problem was achieved using jobs of size S and 2S-1. For these item sizes and maximum bin size 4S-3, we obtain asymptotically matching upper and lower bounds, which vary depending on the ratio of the number of small jobs to the number of large jobs. ",tight bound restrict grid schedule follow online bin pack problem consider items integer size give variable size bin arrive online bin must use still item remain fit bin arrive goal minimize total size bin use previously lower bind competitive ratio problem achieve use job size item size maximum bin size obtain asymptotically match upper lower bound vary depend ratio number small job number large job,65,8,1404.7325.txt
http://arxiv.org/abs/1405.0149,Coding Theoretic Construction of Quantum Ramp Secret Sharing,  We show a construction of a quantum ramp secret sharing scheme from a nested pair of linear codes. Necessary and sufficient conditions for qualified sets and forbidden sets are given in terms of combinatorial properties of nested linear codes. An algebraic geometric construction for quantum secret sharing is also given. ,"Computer Science - Information Theory ; Mathematics - Algebraic Geometry ; Mathematics - Combinatorics ; Quantum Physics ; 81P94 (Primary) 94A62, 94B27 (Secondary) ; E.3 ; ","Matsumoto, Ryutaroh ; ",Coding Theoretic Construction of Quantum Ramp Secret Sharing  We show a construction of a quantum ramp secret sharing scheme from a nested pair of linear codes. Necessary and sufficient conditions for qualified sets and forbidden sets are given in terms of combinatorial properties of nested linear codes. An algebraic geometric construction for quantum secret sharing is also given. ,cod theoretic construction quantum ramp secret share show construction quantum ramp secret share scheme nest pair linear cod necessary sufficient condition qualify set forbid set give term combinatorial properties nest linear cod algebraic geometric construction quantum secret share also give,40,5,1405.0149.txt
http://arxiv.org/abs/1405.0637,Crux: Locality-Preserving Distributed Services,"  Distributed systems achieve scalability by distributing load across many machines, but wide-area deployments can introduce worst-case response latencies proportional to the network's diameter. Crux is a general framework to build locality-preserving distributed systems, by transforming an existing scalable distributed algorithm A into a new locality-preserving algorithm ALP, which guarantees for any two clients u and v interacting via ALP that their interactions exhibit worst-case response latencies proportional to the network latency between u and v. Crux builds on compact-routing theory, but generalizes these techniques beyond routing applications. Crux provides weak and strong consistency flavors, and shows latency improvements for localized interactions in both cases, specifically up to several orders of magnitude for weakly-consistent Crux (from roughly 900ms to 1ms). We deployed on PlanetLab locality-preserving versions of a Memcached distributed cache, a Bamboo distributed hash table, and a Redis publish/subscribe. Our results indicate that Crux is effective and applicable to a variety of existing distributed algorithms. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Basescu, Cristina ; Nowlan, Michael F. ; Nikitin, Kirill ; Faleiro, Jose M. ; Ford, Bryan ; ","Crux: Locality-Preserving Distributed Services  Distributed systems achieve scalability by distributing load across many machines, but wide-area deployments can introduce worst-case response latencies proportional to the network's diameter. Crux is a general framework to build locality-preserving distributed systems, by transforming an existing scalable distributed algorithm A into a new locality-preserving algorithm ALP, which guarantees for any two clients u and v interacting via ALP that their interactions exhibit worst-case response latencies proportional to the network latency between u and v. Crux builds on compact-routing theory, but generalizes these techniques beyond routing applications. Crux provides weak and strong consistency flavors, and shows latency improvements for localized interactions in both cases, specifically up to several orders of magnitude for weakly-consistent Crux (from roughly 900ms to 1ms). We deployed on PlanetLab locality-preserving versions of a Memcached distributed cache, a Bamboo distributed hash table, and a Redis publish/subscribe. Our results indicate that Crux is effective and applicable to a variety of existing distributed algorithms. ",crux locality preserve distribute service distribute systems achieve scalability distribute load across many machine wide area deployments introduce worst case response latencies proportional network diameter crux general framework build locality preserve distribute systems transform exist scalable distribute algorithm new locality preserve algorithm alp guarantee two clients interact via alp interactions exhibit worst case response latencies proportional network latency crux build compact rout theory generalize techniques beyond rout applications crux provide weak strong consistency flavor show latency improvements localize interactions case specifically several order magnitude weakly consistent crux roughly ms ms deploy planetlab locality preserve versions memcached distribute cache bamboo distribute hash table redis publish subscribe result indicate crux effective applicable variety exist distribute algorithms,114,2,1405.0637.txt
http://arxiv.org/abs/1405.0713,Further result on acyclic chromatic index of planar graphs,"  An acyclic edge coloring of a graph $G$ is a proper edge coloring such that every cycle is colored with at least three colors. The acyclic chromatic index $\chiup_{a}'(G)$ of a graph $G$ is the least number of colors in an acyclic edge coloring of $G$. It was conjectured that $\chiup'_{a}(G)\leq \Delta(G) + 2$ for any simple graph $G$ with maximum degree $\Delta(G)$. In this paper, we prove that every planar graph $G$ admits an acyclic edge coloring with $\Delta(G) + 6$ colors. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C15 ; ,"Wang, Tao ; Zhang, Yaqiong ; ","Further result on acyclic chromatic index of planar graphs  An acyclic edge coloring of a graph $G$ is a proper edge coloring such that every cycle is colored with at least three colors. The acyclic chromatic index $\chiup_{a}'(G)$ of a graph $G$ is the least number of colors in an acyclic edge coloring of $G$. It was conjectured that $\chiup'_{a}(G)\leq \Delta(G) + 2$ for any simple graph $G$ with maximum degree $\Delta(G)$. In this paper, we prove that every planar graph $G$ admits an acyclic edge coloring with $\Delta(G) + 6$ colors. ",result acyclic chromatic index planar graph acyclic edge color graph proper edge color every cycle color least three color acyclic chromatic index chiup graph least number color acyclic edge color conjecture chiup leq delta simple graph maximum degree delta paper prove every planar graph admit acyclic edge color delta color,50,13,1405.0713.txt
http://arxiv.org/abs/1405.0718,Generalized Signal Alignment: On the Achievable DoF for Multi-User MIMO   Two-Way Relay Channels,"  This paper studies the achievable degrees of freedom for multi-user MIMO two-way relay channels, where there are $K$ source nodes, each equipped with $M$ antennas, one relay node, equipped with $N$ antennas, and each source node exchanges independent messages with an arbitrary set of other source nodes via the relay. By allowing an arbitrary information exchange pattern, the considered channel model is a unified one. It includes several existing channel models as special cases: $K$-user MIMO Y channel, multi-pair MIMO two-way relay channel, generalized MIMO two-way X relay channel, and $L$-cluster MIMO multiway relay channel. Previous studies mainly considered the achievability of the DoF cut-set bound $2N$ at the antenna configuration $N < 2M$ by applying signal alignment. This work aims to investigate the achievability of the DoF cut-set bound $KM$ for the case $N\geq 2M$. To this end, we first derive tighter DoF upper bounds for three special cases of the considered channel model. Then, we propose a new transmission framework, generalized signal alignment, to approach these bounds. The notion of GSA is to form network-coded symbols by aligning every pair of signals to be exchanged in a compressed subspace at the relay. A necessary and sufficient condition to construct the relay compression matrix is given. We show that using GSA, the new DoF upper bound is achievable when i) $\frac{N}{M} \in \big(0, 2+\frac{4}{K(K-1)}\big] \cup \big[K-2, +\infty\big)$ for the $K$-user MIMO Y channel; ii) $\frac{N}{M} \in \big(0, 2+\frac{4}{K}\big] \cup \big[K-2, +\infty\big)$ for the multi-pair MIMO two-way relay channel; iii) $\frac{N}{M} \in \big(0, 2+\frac{8}{K^2}\big] \cup \big[K-2, +\infty\big)$ for the generalized MIMO two-way X relay channel. We also provide the antenna configuration regions for the general multi-user MIMO two-way relay channel to achieve the total DoF $KM$. ",Computer Science - Information Theory ; ,"Liu, Kangqi ; Tao, Meixia ; ","Generalized Signal Alignment: On the Achievable DoF for Multi-User MIMO   Two-Way Relay Channels  This paper studies the achievable degrees of freedom for multi-user MIMO two-way relay channels, where there are $K$ source nodes, each equipped with $M$ antennas, one relay node, equipped with $N$ antennas, and each source node exchanges independent messages with an arbitrary set of other source nodes via the relay. By allowing an arbitrary information exchange pattern, the considered channel model is a unified one. It includes several existing channel models as special cases: $K$-user MIMO Y channel, multi-pair MIMO two-way relay channel, generalized MIMO two-way X relay channel, and $L$-cluster MIMO multiway relay channel. Previous studies mainly considered the achievability of the DoF cut-set bound $2N$ at the antenna configuration $N < 2M$ by applying signal alignment. This work aims to investigate the achievability of the DoF cut-set bound $KM$ for the case $N\geq 2M$. To this end, we first derive tighter DoF upper bounds for three special cases of the considered channel model. Then, we propose a new transmission framework, generalized signal alignment, to approach these bounds. The notion of GSA is to form network-coded symbols by aligning every pair of signals to be exchanged in a compressed subspace at the relay. A necessary and sufficient condition to construct the relay compression matrix is given. We show that using GSA, the new DoF upper bound is achievable when i) $\frac{N}{M} \in \big(0, 2+\frac{4}{K(K-1)}\big] \cup \big[K-2, +\infty\big)$ for the $K$-user MIMO Y channel; ii) $\frac{N}{M} \in \big(0, 2+\frac{4}{K}\big] \cup \big[K-2, +\infty\big)$ for the multi-pair MIMO two-way relay channel; iii) $\frac{N}{M} \in \big(0, 2+\frac{8}{K^2}\big] \cup \big[K-2, +\infty\big)$ for the generalized MIMO two-way X relay channel. We also provide the antenna configuration regions for the general multi-user MIMO two-way relay channel to achieve the total DoF $KM$. ",generalize signal alignment achievable dof multi user mimo two way relay channel paper study achievable degrees freedom multi user mimo two way relay channel source nod equip antennas one relay node equip antennas source node exchange independent message arbitrary set source nod via relay allow arbitrary information exchange pattern consider channel model unify one include several exist channel model special case user mimo channel multi pair mimo two way relay channel generalize mimo two way relay channel cluster mimo multiway relay channel previous study mainly consider achievability dof cut set bind antenna configuration apply signal alignment work aim investigate achievability dof cut set bind km case geq end first derive tighter dof upper bound three special case consider channel model propose new transmission framework generalize signal alignment approach bound notion gsa form network cod symbols align every pair signal exchange compress subspace relay necessary sufficient condition construct relay compression matrix give show use gsa new dof upper bind achievable frac big frac big cup big infty big user mimo channel ii frac big frac big cup big infty big multi pair mimo two way relay channel iii frac big frac big cup big infty big generalize mimo two way relay channel also provide antenna configuration regions general multi user mimo two way relay channel achieve total dof km,218,9,1405.0718.txt
http://arxiv.org/abs/1405.0854,Unguarded Recursion on Coinductive Resumptions,"  We study a model of side-effecting processes obtained by starting from a monad modelling base effects and adjoining free operations using a cofree coalgebra construction; one thus arrives at what one may think of as types of non-wellfounded side-effecting trees, generalizing the infinite resumption monad. Correspondingly, the arising monad transformer has been termed the coinductive generalized resumption transformer. Monads of this kind have received some attention in the recent literature; in particular, it has been shown that they admit guarded iteration. Here, we show that they also admit unguarded iteration, i.e. form complete Elgot monads, provided that the underlying base effect supports unguarded iteration. Moreover, we provide a universal characterization of the coinductive resumption monad transformer in terms of coproducts of complete Elgot monads. ",Computer Science - Logic in Computer Science ; F.3.2 ; F.3.3 ; D.3.3 ; ,"Goncharov, Sergey ; Schröder, Lutz ; Rauch, Christoph ; Jakob, Julian ; ","Unguarded Recursion on Coinductive Resumptions  We study a model of side-effecting processes obtained by starting from a monad modelling base effects and adjoining free operations using a cofree coalgebra construction; one thus arrives at what one may think of as types of non-wellfounded side-effecting trees, generalizing the infinite resumption monad. Correspondingly, the arising monad transformer has been termed the coinductive generalized resumption transformer. Monads of this kind have received some attention in the recent literature; in particular, it has been shown that they admit guarded iteration. Here, we show that they also admit unguarded iteration, i.e. form complete Elgot monads, provided that the underlying base effect supports unguarded iteration. Moreover, we provide a universal characterization of the coinductive resumption monad transformer in terms of coproducts of complete Elgot monads. ",unguarded recursion coinductive resumptions study model side effect process obtain start monad model base effect adjoin free operations use cofree coalgebra construction one thus arrive one may think type non wellfounded side effect tree generalize infinite resumption monad correspondingly arise monad transformer term coinductive generalize resumption transformer monads kind receive attention recent literature particular show admit guard iteration show also admit unguarded iteration form complete elgot monads provide underlie base effect support unguarded iteration moreover provide universal characterization coinductive resumption monad transformer term coproducts complete elgot monads,87,8,1405.0854.txt
http://arxiv.org/abs/1405.0982,Some undecidability results for asynchronous transducers and the   Brin-Thompson group 2V,"  Using a result of Kari and Ollinger, we prove that the torsion problem for elements of the Brin-Thompson group 2V is undecidable. As a result, we show that there does not exist an algorithm to determine whether an element of the rational group R of Grigorchuk, Nekrashevich, and Sushchanskii has finite order. A modification of the construction gives other undecidability results about the dynamics of the action of elements of 2V on Cantor Space. Arzhantseva, Lafont, and Minasyanin prove in 2012 that there exists a finitely presented group with solvable word problem and unsolvable torsion problem. To our knowledge, 2V furnishes the first concrete example of such a group, and gives an example of a direct undecidability result in the extended family of R. Thompson type groups. ","Mathematics - Group Theory ; Computer Science - Formal Languages and Automata Theory ; Mathematics - Dynamical Systems ; Mathematics - Logic ; 20F10 (Primary) 68Q45, 37B99, 03B25 (Secondary) ; ","Belk, James ; Bleak, Collin ; ","Some undecidability results for asynchronous transducers and the   Brin-Thompson group 2V  Using a result of Kari and Ollinger, we prove that the torsion problem for elements of the Brin-Thompson group 2V is undecidable. As a result, we show that there does not exist an algorithm to determine whether an element of the rational group R of Grigorchuk, Nekrashevich, and Sushchanskii has finite order. A modification of the construction gives other undecidability results about the dynamics of the action of elements of 2V on Cantor Space. Arzhantseva, Lafont, and Minasyanin prove in 2012 that there exists a finitely presented group with solvable word problem and unsolvable torsion problem. To our knowledge, 2V furnishes the first concrete example of such a group, and gives an example of a direct undecidability result in the extended family of R. Thompson type groups. ",undecidability result asynchronous transducers brin thompson group use result kari ollinger prove torsion problem elements brin thompson group undecidable result show exist algorithm determine whether element rational group grigorchuk nekrashevich sushchanskii finite order modification construction give undecidability result dynamics action elements cantor space arzhantseva lafont minasyanin prove exist finitely present group solvable word problem unsolvable torsion problem knowledge furnish first concrete example group give example direct undecidability result extend family thompson type group,73,14,1405.0982.txt
http://arxiv.org/abs/1405.1481,Graphical potential games,"  We study the class of potential games that are also graphical games with respect to a given graph $G$ of connections between the players. We show that, up to strategic equivalence, this class of games can be identified with the set of Markov random fields on $G$.   From this characterization, and from the Hammersley-Clifford theorem, it follows that the potentials of such games can be decomposed to local potentials. We use this decomposition to strongly bound the number of strategy changes of a single player along a better response path. This result extends to generalized graphical potential games, which are played on infinite graphs. ",Mathematics - Probability ; Computer Science - Computer Science and Game Theory ; Economics - Theoretical Economics ; ,"Babichenko, Yakov ; Tamuz, Omer ; ","Graphical potential games  We study the class of potential games that are also graphical games with respect to a given graph $G$ of connections between the players. We show that, up to strategic equivalence, this class of games can be identified with the set of Markov random fields on $G$.   From this characterization, and from the Hammersley-Clifford theorem, it follows that the potentials of such games can be decomposed to local potentials. We use this decomposition to strongly bound the number of strategy changes of a single player along a better response path. This result extends to generalized graphical potential games, which are played on infinite graphs. ",graphical potential game study class potential game also graphical game respect give graph connections players show strategic equivalence class game identify set markov random field characterization hammersley clifford theorem follow potentials game decompose local potentials use decomposition strongly bind number strategy change single player along better response path result extend generalize graphical potential game play infinite graph,57,8,1405.1481.txt
http://arxiv.org/abs/1405.1906,Leader-following Consensus of Multi-agent Systems over Finite Fields,"  The leader-following consensus problem of multi-agent systems over finite fields ${\mathbb F}_p$ is considered in this paper. Dynamics of each agent is governed by a linear equation over ${\mathbb F}_p$, where a distributed control protocol is utilized by the followers.Sufficient and/or necessary conditions on system matrices and graph weights in ${\mathbb F}_p$ are provided for the followers to track the leader. ",Mathematics - Optimization and Control ; Computer Science - Systems and Control ; ,"Xu, Xiangru ; Hong, Yiguang ; ","Leader-following Consensus of Multi-agent Systems over Finite Fields  The leader-following consensus problem of multi-agent systems over finite fields ${\mathbb F}_p$ is considered in this paper. Dynamics of each agent is governed by a linear equation over ${\mathbb F}_p$, where a distributed control protocol is utilized by the followers.Sufficient and/or necessary conditions on system matrices and graph weights in ${\mathbb F}_p$ are provided for the followers to track the leader. ",leader follow consensus multi agent systems finite field leader follow consensus problem multi agent systems finite field mathbb consider paper dynamics agent govern linear equation mathbb distribute control protocol utilize followers sufficient necessary condition system matrices graph weight mathbb provide followers track leader,43,2,1405.1906.txt
http://arxiv.org/abs/1405.2690,Policy Gradients for CVaR-Constrained MDPs,"  We study a risk-constrained version of the stochastic shortest path (SSP) problem, where the risk measure considered is Conditional Value-at-Risk (CVaR). We propose two algorithms that obtain a locally risk-optimal policy by employing four tools: stochastic approximation, mini batches, policy gradients and importance sampling. Both the algorithms incorporate a CVaR estimation procedure, along the lines of Bardou et al. [2009], which in turn is based on Rockafellar-Uryasev's representation for CVaR and utilize the likelihood ratio principle for estimating the gradient of the sum of one cost function (objective of the SSP) and the gradient of the CVaR of the sum of another cost function (in the constraint of SSP). The algorithms differ in the manner in which they approximate the CVaR estimates/necessary gradients - the first algorithm uses stochastic approximation, while the second employ mini-batches in the spirit of Monte Carlo methods. We establish asymptotic convergence of both the algorithms. Further, since estimating CVaR is related to rare-event simulation, we incorporate an importance sampling based variance reduction scheme into our proposed algorithms. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; Mathematics - Optimization and Control ; ,"A., Prashanth L. ; ","Policy Gradients for CVaR-Constrained MDPs  We study a risk-constrained version of the stochastic shortest path (SSP) problem, where the risk measure considered is Conditional Value-at-Risk (CVaR). We propose two algorithms that obtain a locally risk-optimal policy by employing four tools: stochastic approximation, mini batches, policy gradients and importance sampling. Both the algorithms incorporate a CVaR estimation procedure, along the lines of Bardou et al. [2009], which in turn is based on Rockafellar-Uryasev's representation for CVaR and utilize the likelihood ratio principle for estimating the gradient of the sum of one cost function (objective of the SSP) and the gradient of the CVaR of the sum of another cost function (in the constraint of SSP). The algorithms differ in the manner in which they approximate the CVaR estimates/necessary gradients - the first algorithm uses stochastic approximation, while the second employ mini-batches in the spirit of Monte Carlo methods. We establish asymptotic convergence of both the algorithms. Further, since estimating CVaR is related to rare-event simulation, we incorporate an importance sampling based variance reduction scheme into our proposed algorithms. ",policy gradients cvar constrain mdps study risk constrain version stochastic shortest path ssp problem risk measure consider conditional value risk cvar propose two algorithms obtain locally risk optimal policy employ four tool stochastic approximation mini batch policy gradients importance sample algorithms incorporate cvar estimation procedure along line bardou et al turn base rockafellar uryasev representation cvar utilize likelihood ratio principle estimate gradient sum one cost function objective ssp gradient cvar sum another cost function constraint ssp algorithms differ manner approximate cvar estimate necessary gradients first algorithm use stochastic approximation second employ mini batch spirit monte carlo methods establish asymptotic convergence algorithms since estimate cvar relate rare event simulation incorporate importance sample base variance reduction scheme propose algorithms,117,12,1405.2690.txt
http://arxiv.org/abs/1405.4472,AND-compression of NP-complete problems: Streamlined proof and minor   observations,"  Drucker (2012) proved the following result: Unless the unlikely complexity-theoretic collapse coNP is in NP/poly occurs, there is no AND-compression for SAT. The result has implications for the compressibility and kernelizability of a whole range of NP-complete parameterized problems. We present a streamlined proof of Drucker's theorem.   An AND-compression is a deterministic polynomial-time algorithm that maps a set of SAT-instances $x_1,\dots,x_t$ to a single SAT-instance $y$ of size poly(max $|x_i|$) such that $y$ is satisfiable if and only if all $x_i$ are satisfiable. The ""AND"" in the name stems from the fact that the predicate ""$y$ is satisfiable"" can be written as the AND of all predicates ""$x_i$ is satisfiable"". Drucker's result complements the result by Bodlaender et al. (2009) and Fortnow and Santhanam (2010), who proved the analogous statement for OR-compressions, and Drucker's proof not only subsumes that result but also extends it to randomized compression algorithms that are allowed to have a certain probability of failure.   Drucker (2012) presented two proofs: The first uses information theory and the minimax theorem from game theory, and the second is an elementary, iterative proof that is not as general. In our proof, we realize the iterative structure as a generalization of the arguments of Ko (1983) for P-selective sets, which use the fact that tournaments have dominating sets of logarithmic size. We generalize this fact to hypergraph tournaments. Our proof achieves the full generality of Drucker's theorem, avoids the minimax theorem, and restricts the use of information theory to a single, intuitive lemma about the average noise sensitivity of compressive maps. To prove this lemma, we use the same information-theoretic inequalities as Drucker. ",Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; ,"Dell, Holger ; ","AND-compression of NP-complete problems: Streamlined proof and minor   observations  Drucker (2012) proved the following result: Unless the unlikely complexity-theoretic collapse coNP is in NP/poly occurs, there is no AND-compression for SAT. The result has implications for the compressibility and kernelizability of a whole range of NP-complete parameterized problems. We present a streamlined proof of Drucker's theorem.   An AND-compression is a deterministic polynomial-time algorithm that maps a set of SAT-instances $x_1,\dots,x_t$ to a single SAT-instance $y$ of size poly(max $|x_i|$) such that $y$ is satisfiable if and only if all $x_i$ are satisfiable. The ""AND"" in the name stems from the fact that the predicate ""$y$ is satisfiable"" can be written as the AND of all predicates ""$x_i$ is satisfiable"". Drucker's result complements the result by Bodlaender et al. (2009) and Fortnow and Santhanam (2010), who proved the analogous statement for OR-compressions, and Drucker's proof not only subsumes that result but also extends it to randomized compression algorithms that are allowed to have a certain probability of failure.   Drucker (2012) presented two proofs: The first uses information theory and the minimax theorem from game theory, and the second is an elementary, iterative proof that is not as general. In our proof, we realize the iterative structure as a generalization of the arguments of Ko (1983) for P-selective sets, which use the fact that tournaments have dominating sets of logarithmic size. We generalize this fact to hypergraph tournaments. Our proof achieves the full generality of Drucker's theorem, avoids the minimax theorem, and restricts the use of information theory to a single, intuitive lemma about the average noise sensitivity of compressive maps. To prove this lemma, we use the same information-theoretic inequalities as Drucker. ",compression np complete problems streamline proof minor observations drucker prove follow result unless unlikely complexity theoretic collapse conp np poly occur compression sit result implications compressibility kernelizability whole range np complete parameterized problems present streamline proof drucker theorem compression deterministic polynomial time algorithm map set sit instance dot single sit instance size poly max satisfiable satisfiable name stem fact predicate satisfiable write predicate satisfiable drucker result complement result bodlaender et al fortnow santhanam prove analogous statement compressions drucker proof subsume result also extend randomize compression algorithms allow certain probability failure drucker present two proof first use information theory minimax theorem game theory second elementary iterative proof general proof realize iterative structure generalization arguments ko selective set use fact tournaments dominate set logarithmic size generalize fact hypergraph tournaments proof achieve full generality drucker theorem avoid minimax theorem restrict use information theory single intuitive lemma average noise sensitivity compressive map prove lemma use information theoretic inequalities drucker,155,8,1405.4472.txt
http://arxiv.org/abs/1405.4713,Signal-noise search RMT estimator with adaptive decision criterion for   estimating the number of signals based on random matrix theory,"  Estimating the number of signals embedded in noise is a fundamental problem in signal processing. As a classic estimator based on random matrix theory (RMT), the RMT estimator estimates the number of signals via sequentially testing the likelihood of an eigenvalue as arising from a signal or noise for a given over-detection probability. However, it tends to down-estimate the number of signals as weak signal eigenvalues may be immersed in the bias term among eigenvalues. In order to solve this problem, in this paper we focus on developing novel RMT-based estimators by incorporating this bias term into RMT estimator. Firstly, we derive a novel decision statistics for signal detection by incorporating the bias term into the RMT estimator, and propose a signal-test RMT estimator for signal number estimation for a given miss-detection probability. Secondly, we analyze the effect of the bias term on the detection performance of the signal-test RMT estimator and the RMT estimator. It shows that the signal-test RMT estimator has lower down-estimation probability than the RMT estimator when weak signal eigenvalues are immersed in the bias term, but has higher over-estimation probability than the RMT estimator when all signals are strong enough to be detected by the RMT estimator. Thirdly, we derive analytical formulas for the increased over-estimation probability of the signal-test RMT estimator and the increased down-estimation probability of the RMT estimator incurred by this bias term, and then propose a signal-noise-test RMT estimator which can adaptively select its decision criterion between the RMT estimator and the signal-test RMT estimator to make benefits of these two estimators while avoiding their individual drawbacks. Finally, simulation results are presented to show that the proposed signal-noise-test RMT estimator significantly outperforms the existing estimators in all cases. ",Computer Science - Information Theory ; Statistics - Methodology ; ,"Yi, Huiyue ; ","Signal-noise search RMT estimator with adaptive decision criterion for   estimating the number of signals based on random matrix theory  Estimating the number of signals embedded in noise is a fundamental problem in signal processing. As a classic estimator based on random matrix theory (RMT), the RMT estimator estimates the number of signals via sequentially testing the likelihood of an eigenvalue as arising from a signal or noise for a given over-detection probability. However, it tends to down-estimate the number of signals as weak signal eigenvalues may be immersed in the bias term among eigenvalues. In order to solve this problem, in this paper we focus on developing novel RMT-based estimators by incorporating this bias term into RMT estimator. Firstly, we derive a novel decision statistics for signal detection by incorporating the bias term into the RMT estimator, and propose a signal-test RMT estimator for signal number estimation for a given miss-detection probability. Secondly, we analyze the effect of the bias term on the detection performance of the signal-test RMT estimator and the RMT estimator. It shows that the signal-test RMT estimator has lower down-estimation probability than the RMT estimator when weak signal eigenvalues are immersed in the bias term, but has higher over-estimation probability than the RMT estimator when all signals are strong enough to be detected by the RMT estimator. Thirdly, we derive analytical formulas for the increased over-estimation probability of the signal-test RMT estimator and the increased down-estimation probability of the RMT estimator incurred by this bias term, and then propose a signal-noise-test RMT estimator which can adaptively select its decision criterion between the RMT estimator and the signal-test RMT estimator to make benefits of these two estimators while avoiding their individual drawbacks. Finally, simulation results are presented to show that the proposed signal-noise-test RMT estimator significantly outperforms the existing estimators in all cases. ",signal noise search rmt estimator adaptive decision criterion estimate number signal base random matrix theory estimate number signal embed noise fundamental problem signal process classic estimator base random matrix theory rmt rmt estimator estimate number signal via sequentially test likelihood eigenvalue arise signal noise give detection probability however tend estimate number signal weak signal eigenvalues may immerse bias term among eigenvalues order solve problem paper focus develop novel rmt base estimators incorporate bias term rmt estimator firstly derive novel decision statistics signal detection incorporate bias term rmt estimator propose signal test rmt estimator signal number estimation give miss detection probability secondly analyze effect bias term detection performance signal test rmt estimator rmt estimator show signal test rmt estimator lower estimation probability rmt estimator weak signal eigenvalues immerse bias term higher estimation probability rmt estimator signal strong enough detect rmt estimator thirdly derive analytical formulas increase estimation probability signal test rmt estimator increase estimation probability rmt estimator incur bias term propose signal noise test rmt estimator adaptively select decision criterion rmt estimator signal test rmt estimator make benefit two estimators avoid individual drawbacks finally simulation result present show propose signal noise test rmt estimator significantly outperform exist estimators case,198,9,1405.4713.txt
http://arxiv.org/abs/1405.6397,Efficient Evaluation of the Probability Density Function of a Wrapped   Normal Distribution,"  The wrapped normal distribution arises when a the density of a one-dimensional normal distribution is wrapped around the circle infinitely many times. At first look, evaluation of its probability density function appears tedious as an infinite series is involved. In this paper, we investigate the evaluation of two truncated series representations. As one representation performs well for small uncertainties whereas the other performs well for large uncertainties, we show that in all cases a small number of summands is sufficient to achieve high accuracy. ",Statistics - Computation ; Computer Science - Systems and Control ; Mathematics - Numerical Analysis ; ,"Kurz, Gerhard ; Gilitschenski, Igor ; Hanebeck, Uwe D. ; ","Efficient Evaluation of the Probability Density Function of a Wrapped   Normal Distribution  The wrapped normal distribution arises when a the density of a one-dimensional normal distribution is wrapped around the circle infinitely many times. At first look, evaluation of its probability density function appears tedious as an infinite series is involved. In this paper, we investigate the evaluation of two truncated series representations. As one representation performs well for small uncertainties whereas the other performs well for large uncertainties, we show that in all cases a small number of summands is sufficient to achieve high accuracy. ",efficient evaluation probability density function wrap normal distribution wrap normal distribution arise density one dimensional normal distribution wrap around circle infinitely many time first look evaluation probability density function appear tedious infinite series involve paper investigate evaluation two truncate series representations one representation perform well small uncertainties whereas perform well large uncertainties show case small number summands sufficient achieve high accuracy,61,11,1405.6397.txt
http://arxiv.org/abs/1405.7264,"A Datalog-based Computational Model for Coordination-free, Data-Parallel   Systems","  Cloud computing refers to maximizing efficiency by sharing computational and storage resources, while data-parallel systems exploit the resources available in the cloud to perform parallel transformations over large amounts of data. In the same line, considerable emphasis has been recently given to two apparently disjoint research topics: data-parallel, and eventually consistent, distributed systems. Declarative networking has been recently proposed to ease the task of programming in the cloud, by allowing the programmer to express only the desired result and leave the implementation details to the responsibility of the run-time system. In this context, we propose a study on a logic-programming-based computational model for eventually consistent, data-parallel systems, the keystone of which is provided by the recent finding that the class of programs that can be computed in an eventually consistent, coordination-free way is that of monotonic programs. This principle is called CALM and has been proven by Ameloot et al. for distributed, asynchronous settings. We advocate that CALM should be employed as a basic theoretical tool also for data-parallel systems, wherein computation usually proceeds synchronously in rounds and where communication is assumed to be reliable. It is general opinion that coordination-freedom can be seen as a major discriminant factor. In this work we make the case that the current form of CALM does not hold in general for data-parallel systems, and show how, using novel techniques, the satisfiability of the CALM principle can still be obtained although just for the subclass of programs called connected monotonic queries. We complete the study with considerations on the relationships between our model and the one employed by Ameloot et al., showing that our techniques subsume the latter when the synchronization constraints imposed on the system are loosened. ",Computer Science - Databases ; ,"Interlandi, Matteo ; Tanca, Letizia ; ","A Datalog-based Computational Model for Coordination-free, Data-Parallel   Systems  Cloud computing refers to maximizing efficiency by sharing computational and storage resources, while data-parallel systems exploit the resources available in the cloud to perform parallel transformations over large amounts of data. In the same line, considerable emphasis has been recently given to two apparently disjoint research topics: data-parallel, and eventually consistent, distributed systems. Declarative networking has been recently proposed to ease the task of programming in the cloud, by allowing the programmer to express only the desired result and leave the implementation details to the responsibility of the run-time system. In this context, we propose a study on a logic-programming-based computational model for eventually consistent, data-parallel systems, the keystone of which is provided by the recent finding that the class of programs that can be computed in an eventually consistent, coordination-free way is that of monotonic programs. This principle is called CALM and has been proven by Ameloot et al. for distributed, asynchronous settings. We advocate that CALM should be employed as a basic theoretical tool also for data-parallel systems, wherein computation usually proceeds synchronously in rounds and where communication is assumed to be reliable. It is general opinion that coordination-freedom can be seen as a major discriminant factor. In this work we make the case that the current form of CALM does not hold in general for data-parallel systems, and show how, using novel techniques, the satisfiability of the CALM principle can still be obtained although just for the subclass of programs called connected monotonic queries. We complete the study with considerations on the relationships between our model and the one employed by Ameloot et al., showing that our techniques subsume the latter when the synchronization constraints imposed on the system are loosened. ",datalog base computational model coordination free data parallel systems cloud compute refer maximize efficiency share computational storage resources data parallel systems exploit resources available cloud perform parallel transformations large amount data line considerable emphasis recently give two apparently disjoint research topics data parallel eventually consistent distribute systems declarative network recently propose ease task program cloud allow programmer express desire result leave implementation detail responsibility run time system context propose study logic program base computational model eventually consistent data parallel systems keystone provide recent find class program compute eventually consistent coordination free way monotonic program principle call calm prove ameloot et al distribute asynchronous settings advocate calm employ basic theoretical tool also data parallel systems wherein computation usually proceed synchronously round communication assume reliable general opinion coordination freedom see major discriminant factor work make case current form calm hold general data parallel systems show use novel techniques satisfiability calm principle still obtain although subclass program call connect monotonic query complete study considerations relationships model one employ ameloot et al show techniques subsume latter synchronization constraints impose system loosen,177,10,1405.7264.txt
http://arxiv.org/abs/1405.7709,A Stable Marriage Requires Communication,"  The Gale-Shapley algorithm for the Stable Marriage Problem is known to take $\Theta(n^2)$ steps to find a stable marriage in the worst case, but only $\Theta(n \log n)$ steps in the average case (with $n$ women and $n$ men). In 1976, Knuth asked whether the worst-case running time can be improved in a model of computation that does not require sequential access to the whole input. A partial negative answer was given by Ng and Hirschberg, who showed that $\Theta(n^2)$ queries are required in a model that allows certain natural random-access queries to the participants' preferences. A significantly more general - albeit slightly weaker - lower bound follows from Segal's general analysis of communication complexity, namely that $\Omega(n^2)$ Boolean queries are required in order to find a stable marriage, regardless of the set of allowed Boolean queries.   Using a reduction to the communication complexity of the disjointness problem, we give a far simpler, yet significantly more powerful argument showing that $\Omega(n^2)$ Boolean queries of any type are indeed required for finding a stable - or even an approximately stable - marriage. Notably, unlike Segal's lower bound, our lower bound generalizes also to (A) randomized algorithms, (B) allowing arbitrary separate preprocessing of the women's preferences profile and of the men's preferences profile, (C) several variants of the basic problem, such as whether a given pair is married in every/some stable marriage, and (D) determining whether a proposed marriage is stable or far from stable. In order to analyze ""approximately stable"" marriages, we introduce the notion of ""distance to stability"" and provide an efficient algorithm for its computation. ",Computer Science - Computer Science and Game Theory ; Computer Science - Computational Complexity ; ,"Gonczarowski, Yannai A. ; Nisan, Noam ; Ostrovsky, Rafail ; Rosenbaum, Will ; ","A Stable Marriage Requires Communication  The Gale-Shapley algorithm for the Stable Marriage Problem is known to take $\Theta(n^2)$ steps to find a stable marriage in the worst case, but only $\Theta(n \log n)$ steps in the average case (with $n$ women and $n$ men). In 1976, Knuth asked whether the worst-case running time can be improved in a model of computation that does not require sequential access to the whole input. A partial negative answer was given by Ng and Hirschberg, who showed that $\Theta(n^2)$ queries are required in a model that allows certain natural random-access queries to the participants' preferences. A significantly more general - albeit slightly weaker - lower bound follows from Segal's general analysis of communication complexity, namely that $\Omega(n^2)$ Boolean queries are required in order to find a stable marriage, regardless of the set of allowed Boolean queries.   Using a reduction to the communication complexity of the disjointness problem, we give a far simpler, yet significantly more powerful argument showing that $\Omega(n^2)$ Boolean queries of any type are indeed required for finding a stable - or even an approximately stable - marriage. Notably, unlike Segal's lower bound, our lower bound generalizes also to (A) randomized algorithms, (B) allowing arbitrary separate preprocessing of the women's preferences profile and of the men's preferences profile, (C) several variants of the basic problem, such as whether a given pair is married in every/some stable marriage, and (D) determining whether a proposed marriage is stable or far from stable. In order to analyze ""approximately stable"" marriages, we introduce the notion of ""distance to stability"" and provide an efficient algorithm for its computation. ",stable marriage require communication gale shapley algorithm stable marriage problem know take theta step find stable marriage worst case theta log step average case women men knuth ask whether worst case run time improve model computation require sequential access whole input partial negative answer give ng hirschberg show theta query require model allow certain natural random access query participants preferences significantly general albeit slightly weaker lower bind follow segal general analysis communication complexity namely omega boolean query require order find stable marriage regardless set allow boolean query use reduction communication complexity disjointness problem give far simpler yet significantly powerful argument show omega boolean query type indeed require find stable even approximately stable marriage notably unlike segal lower bind lower bind generalize also randomize algorithms allow arbitrary separate preprocessing women preferences profile men preferences profile several variants basic problem whether give pair marry every stable marriage determine whether propose marriage stable far stable order analyze approximately stable marriages introduce notion distance stability provide efficient algorithm computation,165,1,1405.7709.txt
http://arxiv.org/abs/1406.0263,"The ""Runs"" Theorem","  We give a new characterization of maximal repetitions (or runs) in strings based on Lyndon words. The characterization leads to a proof of what was known as the ""runs"" conjecture (Kolpakov \& Kucherov (FOCS '99)), which states that the maximum number of runs $\rho(n)$ in a string of length $n$ is less than $n$. The proof is remarkably simple, considering the numerous endeavors to tackle this problem in the last 15 years, and significantly improves our understanding of how runs can occur in strings. In addition, we obtain an upper bound of $3n$ for the maximum sum of exponents $\sigma(n)$ of runs in a string of length $n$, improving on the best known bound of $4.1n$ by Crochemore et al. (JDA 2012), as well as other improved bounds on related problems. The characterization also gives rise to a new, conceptually simple linear-time algorithm for computing all the runs in a string. A notable characteristic of our algorithm is that, unlike all existing linear-time algorithms, it does not utilize the Lempel-Ziv factorization of the string. We also establish a relationship between runs and nodes of the Lyndon tree, which gives a simple optimal solution to the 2-Period Query problem that was recently solved by Kociumaka et al. (SODA 2015). ",Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; ,"Bannai, Hideo ; I, Tomohiro ; Inenaga, Shunsuke ; Nakashima, Yuto ; Takeda, Masayuki ; Tsuruta, Kazuya ; ","The ""Runs"" Theorem  We give a new characterization of maximal repetitions (or runs) in strings based on Lyndon words. The characterization leads to a proof of what was known as the ""runs"" conjecture (Kolpakov \& Kucherov (FOCS '99)), which states that the maximum number of runs $\rho(n)$ in a string of length $n$ is less than $n$. The proof is remarkably simple, considering the numerous endeavors to tackle this problem in the last 15 years, and significantly improves our understanding of how runs can occur in strings. In addition, we obtain an upper bound of $3n$ for the maximum sum of exponents $\sigma(n)$ of runs in a string of length $n$, improving on the best known bound of $4.1n$ by Crochemore et al. (JDA 2012), as well as other improved bounds on related problems. The characterization also gives rise to a new, conceptually simple linear-time algorithm for computing all the runs in a string. A notable characteristic of our algorithm is that, unlike all existing linear-time algorithms, it does not utilize the Lempel-Ziv factorization of the string. We also establish a relationship between runs and nodes of the Lyndon tree, which gives a simple optimal solution to the 2-Period Query problem that was recently solved by Kociumaka et al. (SODA 2015). ",run theorem give new characterization maximal repetitions run string base lyndon word characterization lead proof know run conjecture kolpakov kucherov focs state maximum number run rho string length less proof remarkably simple consider numerous endeavor tackle problem last years significantly improve understand run occur string addition obtain upper bind maximum sum exponents sigma run string length improve best know bind crochemore et al jda well improve bound relate problems characterization also give rise new conceptually simple linear time algorithm compute run string notable characteristic algorithm unlike exist linear time algorithms utilize lempel ziv factorization string also establish relationship run nod lyndon tree give simple optimal solution period query problem recently solve kociumaka et al soda,115,4,1406.0263.txt
http://arxiv.org/abs/1406.0342,A faster method for computing Gama-Nguyen-Regev's extreme pruning   coefficients,"  This paper considers Gama-Nguyen-Regev's strategy [GNR10] for optimizing pruning coefficients for lattice vector enumeration. We give a table of optimized coefficients and proposes a faster method for computing near-optimized coefficients for any parameters by interpolation.   From the first version published in 2014, we inserted new Section 3.3 to introduce our recent technique to compute approximations of enumeration cost and success probability; both are completed in O(n^2) floating point operations where n is the lattice dimension.   For readers who are interested in this topic, we keep the descriptions of our heuristic optimization method in Section 4 although they are outdated now. ",Computer Science - Cryptography and Security ; ,"Aono, Yoshinori ; ","A faster method for computing Gama-Nguyen-Regev's extreme pruning   coefficients  This paper considers Gama-Nguyen-Regev's strategy [GNR10] for optimizing pruning coefficients for lattice vector enumeration. We give a table of optimized coefficients and proposes a faster method for computing near-optimized coefficients for any parameters by interpolation.   From the first version published in 2014, we inserted new Section 3.3 to introduce our recent technique to compute approximations of enumeration cost and success probability; both are completed in O(n^2) floating point operations where n is the lattice dimension.   For readers who are interested in this topic, we keep the descriptions of our heuristic optimization method in Section 4 although they are outdated now. ",faster method compute gama nguyen regev extreme prune coefficients paper consider gama nguyen regev strategy gnr optimize prune coefficients lattice vector enumeration give table optimize coefficients propose faster method compute near optimize coefficients parameters interpolation first version publish insert new section introduce recent technique compute approximations enumeration cost success probability complete float point operations lattice dimension readers interest topic keep descriptions heuristic optimization method section although outdated,67,4,1406.0342.txt
http://arxiv.org/abs/1406.0641,Extensions of Configuration Structures,"  The present paper defines ST-structures (and an extension of these, called STC-structures). The main purpose is to provide concrete relationships between highly expressive concurrency models coming from two different schools of thought: the higher dimensional automata, a \textit{state-based} approach of Pratt and van Glabbeek; and the configuration structures and (in)pure event structures, an \textit{event-based} approach of van Glabbeek and Plotkin. In this respect we make comparative studies of the expressive power of ST-structures relative to the above models. Moreover, standard notions from other concurrency models are defined for ST(C)-structures, like steps and paths, bisimilarities, and action refinement, and related results are given. These investigations of ST(C)-structures are intended to provide a better understanding of the \textit{state-event duality} described by Pratt, and also of the (a)cyclic structures of higher dimensional automata. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Logic in Computer Science ; ","Prisacariu, Cristian ; ","Extensions of Configuration Structures  The present paper defines ST-structures (and an extension of these, called STC-structures). The main purpose is to provide concrete relationships between highly expressive concurrency models coming from two different schools of thought: the higher dimensional automata, a \textit{state-based} approach of Pratt and van Glabbeek; and the configuration structures and (in)pure event structures, an \textit{event-based} approach of van Glabbeek and Plotkin. In this respect we make comparative studies of the expressive power of ST-structures relative to the above models. Moreover, standard notions from other concurrency models are defined for ST(C)-structures, like steps and paths, bisimilarities, and action refinement, and related results are given. These investigations of ST(C)-structures are intended to provide a better understanding of the \textit{state-event duality} described by Pratt, and also of the (a)cyclic structures of higher dimensional automata. ",extensions configuration structure present paper define st structure extension call stc structure main purpose provide concrete relationships highly expressive concurrency model come two different school think higher dimensional automata textit state base approach pratt van glabbeek configuration structure pure event structure textit event base approach van glabbeek plotkin respect make comparative study expressive power st structure relative model moreover standard notions concurrency model define st structure like step paths bisimilarities action refinement relate result give investigations st structure intend provide better understand textit state event duality describe pratt also cyclic structure higher dimensional automata,94,14,1406.0641.txt
http://arxiv.org/abs/1406.1758,Scaling limits and influence of the seed graph in preferential   attachment trees,"  We are interested in the asymptotics of random trees built by linear preferential attachment, also known in the literature as Barab\'asi-Albert trees or plane-oriented recursive trees. We first prove a conjecture of Bubeck, Mossel \& R\'acz concerning the influence of the seed graph on the asymptotic behavior of such trees. Separately we study the geometric structure of nodes of large degrees in a plane version of Barab\'asi-Albert trees via their associated looptrees. As the number of nodes grows, we show that these looptrees, appropriately rescaled, converge in the Gromov-Hausdorff sense towards a random compact metric space which we call the Brownian looptree. The latter is constructed as a quotient space of Aldous' Brownian Continuum Random Tree and is shown to have almost sure Hausdorff dimension $2$. ",Mathematics - Probability ; Computer Science - Discrete Mathematics ; Mathematics - Statistics Theory ; ,"Curien, Nicolas ; Duquesne, Thomas ; Kortchemski, Igor ; Manolescu, Ioan ; ","Scaling limits and influence of the seed graph in preferential   attachment trees  We are interested in the asymptotics of random trees built by linear preferential attachment, also known in the literature as Barab\'asi-Albert trees or plane-oriented recursive trees. We first prove a conjecture of Bubeck, Mossel \& R\'acz concerning the influence of the seed graph on the asymptotic behavior of such trees. Separately we study the geometric structure of nodes of large degrees in a plane version of Barab\'asi-Albert trees via their associated looptrees. As the number of nodes grows, we show that these looptrees, appropriately rescaled, converge in the Gromov-Hausdorff sense towards a random compact metric space which we call the Brownian looptree. The latter is constructed as a quotient space of Aldous' Brownian Continuum Random Tree and is shown to have almost sure Hausdorff dimension $2$. ",scale limit influence seed graph preferential attachment tree interest asymptotics random tree build linear preferential attachment also know literature barab asi albert tree plane orient recursive tree first prove conjecture bubeck mossel acz concern influence seed graph asymptotic behavior tree separately study geometric structure nod large degrees plane version barab asi albert tree via associate looptrees number nod grow show looptrees appropriately rescale converge gromov hausdorff sense towards random compact metric space call brownian looptree latter construct quotient space aldous brownian continuum random tree show almost sure hausdorff dimension,89,3,1406.1758.txt
http://arxiv.org/abs/1406.1949,Distinct Distances: Open Problems and Current Bounds,  We survey the variants of Erd\H{o}s' distinct distances problem and the current best bounds for each of those. ,Mathematics - Combinatorics ; Computer Science - Computational Geometry ; ,"Sheffer, Adam ; ",Distinct Distances: Open Problems and Current Bounds  We survey the variants of Erd\H{o}s' distinct distances problem and the current best bounds for each of those. ,distinct distance open problems current bound survey variants erd distinct distance problem current best bound,15,8,1406.1949.txt
http://arxiv.org/abs/1406.2534,Load Hiding of Household's Power Demand,"  With the development and introduction of smart metering, the energy information for costumers will change from infrequent manual meter readings to fine-grained energy consumption data. On the one hand these fine-grained measurements will lead to an improvement in costumers' energy habits, but on the other hand the fined-grained data produces information about a household and also households' inhabitants, which are the basis for many future privacy issues. To ensure household privacy and smart meter information owned by the household inhabitants, load hiding techniques were introduced to obfuscate the load demand visible at the household energy meter. In this work, a state-of-the-art battery-based load hiding (BLH) technique, which uses a controllable battery to disguise the power consumption and a novel load hiding technique called load-based load hiding (LLH) are presented. An LLH system uses an controllable household appliance to obfuscate the household's power demand. We evaluate and compare both load hiding techniques on real household data and show that both techniques can strengthen household privacy but only LLH can increase appliance level privacy. ",Computer Science - Other Computer Science ; ,"Egarter, Dominik ; Prokop, Christoph ; Elmenreich, Wilfried ; ","Load Hiding of Household's Power Demand  With the development and introduction of smart metering, the energy information for costumers will change from infrequent manual meter readings to fine-grained energy consumption data. On the one hand these fine-grained measurements will lead to an improvement in costumers' energy habits, but on the other hand the fined-grained data produces information about a household and also households' inhabitants, which are the basis for many future privacy issues. To ensure household privacy and smart meter information owned by the household inhabitants, load hiding techniques were introduced to obfuscate the load demand visible at the household energy meter. In this work, a state-of-the-art battery-based load hiding (BLH) technique, which uses a controllable battery to disguise the power consumption and a novel load hiding technique called load-based load hiding (LLH) are presented. An LLH system uses an controllable household appliance to obfuscate the household's power demand. We evaluate and compare both load hiding techniques on real household data and show that both techniques can strengthen household privacy but only LLH can increase appliance level privacy. ",load hide household power demand development introduction smart meter energy information costumers change infrequent manual meter read fine grain energy consumption data one hand fine grain measurements lead improvement costumers energy habit hand fin grain data produce information household also households inhabitants basis many future privacy issue ensure household privacy smart meter information own household inhabitants load hide techniques introduce obfuscate load demand visible household energy meter work state art battery base load hide blh technique use controllable battery disguise power consumption novel load hide technique call load base load hide llh present llh system use controllable household appliance obfuscate household power demand evaluate compare load hide techniques real household data show techniques strengthen household privacy llh increase appliance level privacy,121,10,1406.2534.txt
http://arxiv.org/abs/1406.2587,Structural Sparsity of Complex Networks: Bounded Expansion in Random   Models and Real-World Graphs,"  This research establishes that many real-world networks exhibit bounded expansion, a strong notion of structural sparsity, and demonstrates that it can be leveraged to design efficient algorithms for network analysis. We analyze several common network models regarding their structural sparsity. We show that, with high probability, (1) graphs sampled with a prescribed s parse degree sequence; (2) perturbed bounded-degree graphs; (3) stochastic block models with small probabilities; result in graphs of bounded expansion.   In contrast, we show that the Kleinberg and the Barabasi-Albert model have unbounded expansion. We support our findings with empirical measurements on a corpus of real-world networks. ",Computer Science - Social and Information Networks ; Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Physics - Physics and Society ; ,"Demaine, Erik D. ; Reidl, Felix ; Rossmanith, Peter ; Villaamil, Fernando Sanchez ; Sikdar, Somnath ; Sullivan, Blair D. ; ","Structural Sparsity of Complex Networks: Bounded Expansion in Random   Models and Real-World Graphs  This research establishes that many real-world networks exhibit bounded expansion, a strong notion of structural sparsity, and demonstrates that it can be leveraged to design efficient algorithms for network analysis. We analyze several common network models regarding their structural sparsity. We show that, with high probability, (1) graphs sampled with a prescribed s parse degree sequence; (2) perturbed bounded-degree graphs; (3) stochastic block models with small probabilities; result in graphs of bounded expansion.   In contrast, we show that the Kleinberg and the Barabasi-Albert model have unbounded expansion. We support our findings with empirical measurements on a corpus of real-world networks. ",structural sparsity complex network bound expansion random model real world graph research establish many real world network exhibit bound expansion strong notion structural sparsity demonstrate leverage design efficient algorithms network analysis analyze several common network model regard structural sparsity show high probability graph sample prescribe parse degree sequence perturb bound degree graph stochastic block model small probabilities result graph bound expansion contrast show kleinberg barabasi albert model unbounded expansion support find empirical measurements corpus real world network,77,6,1406.2587.txt
http://arxiv.org/abs/1406.3065,Lower Bounds for Tropical Circuits and Dynamic Programs,"  Tropical circuits are circuits with Min and Plus, or Max and Plus operations as gates. Their importance stems from their intimate relation to dynamic programming algorithms. The power of tropical circuits lies somewhere between that of monotone boolean circuits and monotone arithmetic circuits. In this paper we present some lower bounds arguments for tropical circuits, and hence, for dynamic programs. ",Computer Science - Computational Complexity ; ,"Jukna, Stasys ; ","Lower Bounds for Tropical Circuits and Dynamic Programs  Tropical circuits are circuits with Min and Plus, or Max and Plus operations as gates. Their importance stems from their intimate relation to dynamic programming algorithms. The power of tropical circuits lies somewhere between that of monotone boolean circuits and monotone arithmetic circuits. In this paper we present some lower bounds arguments for tropical circuits, and hence, for dynamic programs. ",lower bound tropical circuit dynamic program tropical circuit circuit min plus max plus operations gate importance stem intimate relation dynamic program algorithms power tropical circuit lie somewhere monotone boolean circuit monotone arithmetic circuit paper present lower bound arguments tropical circuit hence dynamic program,43,8,1406.3065.txt
http://arxiv.org/abs/1406.4060,Consistency of Quine's New Foundations using nominal techniques,  We build a model in nominal sets for TST+; typed set theory with typical ambiguity. It is known that this is equivalent to the consistency of Quine's New Foundations.   Nominal techniques are used to constrain the size of powersets and thus model typical ambiguity. ,"Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03E35 (Primary), 03B70 (Secondary) ; F.4.1 ; ","Gabbay, Murdoch J. ; ",Consistency of Quine's New Foundations using nominal techniques  We build a model in nominal sets for TST+; typed set theory with typical ambiguity. It is known that this is equivalent to the consistency of Quine's New Foundations.   Nominal techniques are used to constrain the size of powersets and thus model typical ambiguity. ,consistency quine new foundations use nominal techniques build model nominal set tst type set theory typical ambiguity know equivalent consistency quine new foundations nominal techniques use constrain size powersets thus model typical ambiguity,33,8,1406.4060.txt
http://arxiv.org/abs/1406.4426,The number system hidden inside the Boolean satisfiability problem,"  This paper gives a novel approach to analyze SAT problem more deeply. First, I define new elements of Boolean formula such as dominant variable, decision chain, and chain coupler. Through the analysis of the SAT problem using the elements, I prove that we can construct a k-SAT (k>2) instance where the coefficients of cutting planes take exponentially large values in the input size. This exponential property is caused by the number system formed from the calculation of coefficients. In addition, I show that 2-SAT does not form the number system and Horn-SAT partially forms the number system according to the feasible value of the dominant variable. Whether or not the coefficients of cutting planes in cutting plane proof are polynomially bounded was open problem. Many researchers believed that cutting plane proofs with large coefficients are highly non-intuitive20. However, we can construct a k-SAT (k>2) instance in which cutting planes take exponentially large coefficients by the number system. In addition, this exponential property is so strong that it gives definite answers for several questions: why Horn-SAT has the intermediate property between 2-SAT and 3-SAT; why random-SAT is so easy; and why k-SAT (k>2) cannot be solved with the linear programming technique. As we know, 2-SAT is NL-complete, Horn-SAT is P-complete, and k-SAT (k>2) is NP-complete. In terms of computational complexity, this paper gives a clear mathematical property by which SAT problems in three different classes are distinguished. ",Computer Science - Computational Complexity ; Mathematics - Logic ; ,"Cho, Keum-Bae ; ","The number system hidden inside the Boolean satisfiability problem  This paper gives a novel approach to analyze SAT problem more deeply. First, I define new elements of Boolean formula such as dominant variable, decision chain, and chain coupler. Through the analysis of the SAT problem using the elements, I prove that we can construct a k-SAT (k>2) instance where the coefficients of cutting planes take exponentially large values in the input size. This exponential property is caused by the number system formed from the calculation of coefficients. In addition, I show that 2-SAT does not form the number system and Horn-SAT partially forms the number system according to the feasible value of the dominant variable. Whether or not the coefficients of cutting planes in cutting plane proof are polynomially bounded was open problem. Many researchers believed that cutting plane proofs with large coefficients are highly non-intuitive20. However, we can construct a k-SAT (k>2) instance in which cutting planes take exponentially large coefficients by the number system. In addition, this exponential property is so strong that it gives definite answers for several questions: why Horn-SAT has the intermediate property between 2-SAT and 3-SAT; why random-SAT is so easy; and why k-SAT (k>2) cannot be solved with the linear programming technique. As we know, 2-SAT is NL-complete, Horn-SAT is P-complete, and k-SAT (k>2) is NP-complete. In terms of computational complexity, this paper gives a clear mathematical property by which SAT problems in three different classes are distinguished. ",number system hide inside boolean satisfiability problem paper give novel approach analyze sit problem deeply first define new elements boolean formula dominant variable decision chain chain coupler analysis sit problem use elements prove construct sit instance coefficients cut plan take exponentially large value input size exponential property cause number system form calculation coefficients addition show sit form number system horn sit partially form number system accord feasible value dominant variable whether coefficients cut plan cut plane proof polynomially bound open problem many researchers believe cut plane proof large coefficients highly non intuitive however construct sit instance cut plan take exponentially large coefficients number system addition exponential property strong give definite answer several question horn sit intermediate property sit sit random sit easy sit cannot solve linear program technique know sit nl complete horn sit complete sit np complete term computational complexity paper give clear mathematical property sit problems three different class distinguish,152,8,1406.4426.txt
http://arxiv.org/abs/1406.5688,"Information, Meaning, and Intellectual Organization in Networks of   Inter-Human Communication","  The Shannon-Weaver model of linear information transmission is extended with two loops potentially generating redundancies: (i) meaning is provided locally to the information from the perspective of hindsight, and (ii) meanings can be codified differently and then refer to other horizons of meaning. Thus, three layers are distinguished: variations in the communications, historical organization at each moment of time, and evolutionary self-organization of the codes of communication over time. Furthermore, the codes of communication can functionally be different and then the system is both horizontally and vertically differentiated. All these subdynamics operate in parallel and necessarily generate uncertainty. However, meaningful information can be considered as the specific selection of a signal from the noise; the codes of communication are social constructs that can generate redundancy by giving different meanings to the same information. Reflexively, one can translate among codes in more elaborate discourses. The second (instantiating) layer can be operationalized in terms of semantic maps using the vector space model; the third in terms of mutual redundancy among the latent dimensions of the vector space. Using Blaise Cronin's {\oe}uvre, the different operations of the three layers are demonstrated empirically. ",Computer Science - Digital Libraries ; Computer Science - Computers and Society ; ,"Leydesdorff, Loet ; ","Information, Meaning, and Intellectual Organization in Networks of   Inter-Human Communication  The Shannon-Weaver model of linear information transmission is extended with two loops potentially generating redundancies: (i) meaning is provided locally to the information from the perspective of hindsight, and (ii) meanings can be codified differently and then refer to other horizons of meaning. Thus, three layers are distinguished: variations in the communications, historical organization at each moment of time, and evolutionary self-organization of the codes of communication over time. Furthermore, the codes of communication can functionally be different and then the system is both horizontally and vertically differentiated. All these subdynamics operate in parallel and necessarily generate uncertainty. However, meaningful information can be considered as the specific selection of a signal from the noise; the codes of communication are social constructs that can generate redundancy by giving different meanings to the same information. Reflexively, one can translate among codes in more elaborate discourses. The second (instantiating) layer can be operationalized in terms of semantic maps using the vector space model; the third in terms of mutual redundancy among the latent dimensions of the vector space. Using Blaise Cronin's {\oe}uvre, the different operations of the three layers are demonstrated empirically. ",information mean intellectual organization network inter human communication shannon weaver model linear information transmission extend two loop potentially generate redundancies mean provide locally information perspective hindsight ii mean codify differently refer horizons mean thus three layer distinguish variations communications historical organization moment time evolutionary self organization cod communication time furthermore cod communication functionally different system horizontally vertically differentiate subdynamics operate parallel necessarily generate uncertainty however meaningful information consider specific selection signal noise cod communication social construct generate redundancy give different mean information reflexively one translate among cod elaborate discourse second instantiate layer operationalized term semantic map use vector space model third term mutual redundancy among latent dimension vector space use blaise cronin oe uvre different operations three layer demonstrate empirically,120,5,1406.5688.txt
http://arxiv.org/abs/1406.5943,The Moser-Tardos Framework with Partial Resampling,"  The resampling algorithm of Moser \& Tardos is a powerful approach to develop constructive versions of the Lov\'{a}sz Local Lemma (LLL). We generalize this to partial resampling: when a bad event holds, we resample an appropriately-random subset of the variables that define this event, rather than the entire set as in Moser & Tardos. This is particularly useful when the bad events are determined by sums of random variables. This leads to several improved algorithmic applications in scheduling, graph transversals, packet routing etc. For instance, we settle a conjecture of Szab\'{o} & Tardos (2006) on graph transversals asymptotically, and obtain improved approximation ratios for a packet routing problem of Leighton, Maggs, & Rao (1994). ",Mathematics - Combinatorics ; Computer Science - Data Structures and Algorithms ; ,"Harris, David G. ; Srinivasan, Aravind ; ","The Moser-Tardos Framework with Partial Resampling  The resampling algorithm of Moser \& Tardos is a powerful approach to develop constructive versions of the Lov\'{a}sz Local Lemma (LLL). We generalize this to partial resampling: when a bad event holds, we resample an appropriately-random subset of the variables that define this event, rather than the entire set as in Moser & Tardos. This is particularly useful when the bad events are determined by sums of random variables. This leads to several improved algorithmic applications in scheduling, graph transversals, packet routing etc. For instance, we settle a conjecture of Szab\'{o} & Tardos (2006) on graph transversals asymptotically, and obtain improved approximation ratios for a packet routing problem of Leighton, Maggs, & Rao (1994). ",moser tardos framework partial resampling resampling algorithm moser tardos powerful approach develop constructive versions lov sz local lemma lll generalize partial resampling bad event hold resample appropriately random subset variables define event rather entire set moser tardos particularly useful bad events determine sum random variables lead several improve algorithmic applications schedule graph transversals packet rout etc instance settle conjecture szab tardos graph transversals asymptotically obtain improve approximation ratios packet rout problem leighton maggs rao,74,3,1406.5943.txt
http://arxiv.org/abs/1406.6145,"Fast, Robust and Non-convex Subspace Recovery","  This work presents a fast and non-convex algorithm for robust subspace recovery. The data sets considered include inliers drawn around a low-dimensional subspace of a higher dimensional ambient space, and a possibly large portion of outliers that do not lie nearby this subspace. The proposed algorithm, which we refer to as Fast Median Subspace (FMS), is designed to robustly determine the underlying subspace of such data sets, while having lower computational complexity than existing methods. We prove convergence of the FMS iterates to a stationary point. Further, under a special model of data, FMS converges to a point which is near to the global minimum with overwhelming probability. Under this model, we show that the iteration complexity is globally bounded and locally $r$-linear. The latter theorem holds for any fixed fraction of outliers (less than 1) and any fixed positive distance between the limit point and the global minimum. Numerical experiments on synthetic and real data demonstrate its competitive speed and accuracy. ",Computer Science - Machine Learning ; Computer Science - Computer Vision and Pattern Recognition ; Statistics - Applications ; Statistics - Machine Learning ; ,"Lerman, Gilad ; Maunu, Tyler ; ","Fast, Robust and Non-convex Subspace Recovery  This work presents a fast and non-convex algorithm for robust subspace recovery. The data sets considered include inliers drawn around a low-dimensional subspace of a higher dimensional ambient space, and a possibly large portion of outliers that do not lie nearby this subspace. The proposed algorithm, which we refer to as Fast Median Subspace (FMS), is designed to robustly determine the underlying subspace of such data sets, while having lower computational complexity than existing methods. We prove convergence of the FMS iterates to a stationary point. Further, under a special model of data, FMS converges to a point which is near to the global minimum with overwhelming probability. Under this model, we show that the iteration complexity is globally bounded and locally $r$-linear. The latter theorem holds for any fixed fraction of outliers (less than 1) and any fixed positive distance between the limit point and the global minimum. Numerical experiments on synthetic and real data demonstrate its competitive speed and accuracy. ",fast robust non convex subspace recovery work present fast non convex algorithm robust subspace recovery data set consider include inliers draw around low dimensional subspace higher dimensional ambient space possibly large portion outliers lie nearby subspace propose algorithm refer fast median subspace fms design robustly determine underlie subspace data set lower computational complexity exist methods prove convergence fms iterate stationary point special model data fms converge point near global minimum overwhelm probability model show iteration complexity globally bound locally linear latter theorem hold fix fraction outliers less fix positive distance limit point global minimum numerical experiment synthetic real data demonstrate competitive speed accuracy,103,7,1406.6145.txt
http://arxiv.org/abs/1406.6924,Strongly stable ideals and Hilbert polynomials,  The \texttt{StronglyStableIdeals} package for \textit{Macaulay2} provides a method to compute all saturated strongly stable ideals in a given polynomial ring with a fixed Hilbert polynomial. A description of the main method and auxiliary tools is given. ,"Computer Science - Symbolic Computation ; Computer Science - Mathematical Software ; Mathematics - Commutative Algebra ; Mathematics - Algebraic Geometry ; Mathematics - Combinatorics ; 13P10, 13P99 ; ","Alberelli, Davide ; Lella, Paolo ; ",Strongly stable ideals and Hilbert polynomials  The \texttt{StronglyStableIdeals} package for \textit{Macaulay2} provides a method to compute all saturated strongly stable ideals in a given polynomial ring with a fixed Hilbert polynomial. A description of the main method and auxiliary tools is given. ,strongly stable ideals hilbert polynomials texttt stronglystableideals package textit macaulay provide method compute saturate strongly stable ideals give polynomial ring fix hilbert polynomial description main method auxiliary tool give,29,4,1406.6924.txt
http://arxiv.org/abs/1406.7373,How to Achieve the Capacity of Asymmetric Channels,"  We survey coding techniques that enable reliable transmission at rates that approach the capacity of an arbitrary discrete memoryless channel. In particular, we take the point of view of modern coding theory and discuss how recent advances in coding for symmetric channels help provide more efficient solutions for the asymmetric case. We consider, in more detail, three basic coding paradigms.   The first one is Gallager's scheme that consists of concatenating a linear code with a non-linear mapping so that the input distribution can be appropriately shaped. We explicitly show that both polar codes and spatially coupled codes can be employed in this scenario. Furthermore, we derive a scaling law between the gap to capacity, the cardinality of the input and output alphabets, and the required size of the mapper.   The second one is an integrated scheme in which the code is used both for source coding, in order to create codewords distributed according to the capacity-achieving input distribution, and for channel coding, in order to provide error protection. Such a technique has been recently introduced by Honda and Yamamoto in the context of polar codes, and we show how to apply it also to the design of sparse graph codes.   The third paradigm is based on an idea of B\""ocherer and Mathar, and separates the two tasks of source coding and channel coding by a chaining construction that binds together several codewords. We present conditions for the source code and the channel code, and we describe how to combine any source code with any channel code that fulfill those conditions, in order to provide capacity-achieving schemes for asymmetric channels. In particular, we show that polar codes, spatially coupled codes, and homophonic codes are suitable as basic building blocks of the proposed coding strategy. ",Computer Science - Information Theory ; ,"Mondelli, Marco ; Hassani, S. Hamed ; Urbanke, Rüdiger ; ","How to Achieve the Capacity of Asymmetric Channels  We survey coding techniques that enable reliable transmission at rates that approach the capacity of an arbitrary discrete memoryless channel. In particular, we take the point of view of modern coding theory and discuss how recent advances in coding for symmetric channels help provide more efficient solutions for the asymmetric case. We consider, in more detail, three basic coding paradigms.   The first one is Gallager's scheme that consists of concatenating a linear code with a non-linear mapping so that the input distribution can be appropriately shaped. We explicitly show that both polar codes and spatially coupled codes can be employed in this scenario. Furthermore, we derive a scaling law between the gap to capacity, the cardinality of the input and output alphabets, and the required size of the mapper.   The second one is an integrated scheme in which the code is used both for source coding, in order to create codewords distributed according to the capacity-achieving input distribution, and for channel coding, in order to provide error protection. Such a technique has been recently introduced by Honda and Yamamoto in the context of polar codes, and we show how to apply it also to the design of sparse graph codes.   The third paradigm is based on an idea of B\""ocherer and Mathar, and separates the two tasks of source coding and channel coding by a chaining construction that binds together several codewords. We present conditions for the source code and the channel code, and we describe how to combine any source code with any channel code that fulfill those conditions, in order to provide capacity-achieving schemes for asymmetric channels. In particular, we show that polar codes, spatially coupled codes, and homophonic codes are suitable as basic building blocks of the proposed coding strategy. ",achieve capacity asymmetric channel survey cod techniques enable reliable transmission rat approach capacity arbitrary discrete memoryless channel particular take point view modern cod theory discuss recent advance cod symmetric channel help provide efficient solutions asymmetric case consider detail three basic cod paradigms first one gallager scheme consist concatenate linear code non linear map input distribution appropriately shape explicitly show polar cod spatially couple cod employ scenario furthermore derive scale law gap capacity cardinality input output alphabets require size mapper second one integrate scheme code use source cod order create codewords distribute accord capacity achieve input distribution channel cod order provide error protection technique recently introduce honda yamamoto context polar cod show apply also design sparse graph cod third paradigm base idea ocherer mathar separate two task source cod channel cod chain construction bind together several codewords present condition source code channel code describe combine source code channel code fulfill condition order provide capacity achieve scheme asymmetric channel particular show polar cod spatially couple cod homophonic cod suitable basic build block propose cod strategy,173,5,1406.7373.txt
http://arxiv.org/abs/1407.0208,A Bayes consistent 1-NN classifier,"  We show that a simple modification of the 1-nearest neighbor classifier yields a strongly Bayes consistent learner. Prior to this work, the only strongly Bayes consistent proximity-based method was the k-nearest neighbor classifier, for k growing appropriately with sample size. We will argue that a margin-regularized 1-NN enjoys considerable statistical and algorithmic advantages over the k-NN classifier. These include user-friendly finite-sample error bounds, as well as time- and memory-efficient learning and test-point evaluation algorithms with a principled speed-accuracy tradeoff. Encouraging empirical results are reported. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Kontorovich, Aryeh ; Weiss, Roi ; ","A Bayes consistent 1-NN classifier  We show that a simple modification of the 1-nearest neighbor classifier yields a strongly Bayes consistent learner. Prior to this work, the only strongly Bayes consistent proximity-based method was the k-nearest neighbor classifier, for k growing appropriately with sample size. We will argue that a margin-regularized 1-NN enjoys considerable statistical and algorithmic advantages over the k-NN classifier. These include user-friendly finite-sample error bounds, as well as time- and memory-efficient learning and test-point evaluation algorithms with a principled speed-accuracy tradeoff. Encouraging empirical results are reported. ",bay consistent nn classifier show simple modification nearest neighbor classifier yield strongly bay consistent learner prior work strongly bay consistent proximity base method nearest neighbor classifier grow appropriately sample size argue margin regularize nn enjoy considerable statistical algorithmic advantage nn classifier include user friendly finite sample error bound well time memory efficient learn test point evaluation algorithms principled speed accuracy tradeoff encourage empirical result report,65,12,1407.0208.txt
http://arxiv.org/abs/1407.0756,Geometrical Localization Algorithm for 3-D Wireless Sensor Networks,"  In this paper, we propose an efficient range free localization scheme for large scale three dimensional wireless sensor networks. Our system environment consists of two type of sensors, randomly deployed static sensors and global positioning system equipped moving sensors. These moving anchors travels across the network field and broadcast their current locations on specified intervals. As soon as the sensors which are deployed in random fashion receives three beacon messages (known locations broadcasted by anchors), they computes their locations automatically by using our proposed algorithm. One of our significant contributions is, we use only three different beacon messages to localize one sensor, while in the best of our knowledge, all previously proposed methods use at least four different known locations. The ability of our method to localize by using only three known locations not only saves computation, time, energy, but also reduces the number of anchors needed to be deployed and more importantly reduces the communication overheads. Experimental results demonstrate that our proposed scheme improves the overall efficiency of localization process significantly.   Important Note: Final version of this paper is accepted and published by Journal of Wireless Personal Communication, Springer : June, 2014 The final version of publication is available at link.springer.com Link: http://link.springer.com/article/10.1007\%2Fs11277-014-1852-6 ",Computer Science - Networking and Internet Architecture ; ,"Kumar, Rajesh ; Kumar, Sushil ; Shukla, Diksha ; Raw, Ram Shringar ; ","Geometrical Localization Algorithm for 3-D Wireless Sensor Networks  In this paper, we propose an efficient range free localization scheme for large scale three dimensional wireless sensor networks. Our system environment consists of two type of sensors, randomly deployed static sensors and global positioning system equipped moving sensors. These moving anchors travels across the network field and broadcast their current locations on specified intervals. As soon as the sensors which are deployed in random fashion receives three beacon messages (known locations broadcasted by anchors), they computes their locations automatically by using our proposed algorithm. One of our significant contributions is, we use only three different beacon messages to localize one sensor, while in the best of our knowledge, all previously proposed methods use at least four different known locations. The ability of our method to localize by using only three known locations not only saves computation, time, energy, but also reduces the number of anchors needed to be deployed and more importantly reduces the communication overheads. Experimental results demonstrate that our proposed scheme improves the overall efficiency of localization process significantly.   Important Note: Final version of this paper is accepted and published by Journal of Wireless Personal Communication, Springer : June, 2014 The final version of publication is available at link.springer.com Link: http://link.springer.com/article/10.1007\%2Fs11277-014-1852-6 ",geometrical localization algorithm wireless sensor network paper propose efficient range free localization scheme large scale three dimensional wireless sensor network system environment consist two type sensors randomly deploy static sensors global position system equip move sensors move anchor travel across network field broadcast current locations specify intervals soon sensors deploy random fashion receive three beacon message know locations broadcast anchor compute locations automatically use propose algorithm one significant contributions use three different beacon message localize one sensor best knowledge previously propose methods use least four different know locations ability method localize use three know locations save computation time energy also reduce number anchor need deploy importantly reduce communication overheads experimental result demonstrate propose scheme improve overall efficiency localization process significantly important note final version paper accept publish journal wireless personal communication springer june final version publication available link springer com link http link springer com article fs,147,12,1407.0756.txt
http://arxiv.org/abs/1407.1103,Synchronization of finite-state pulse-coupled oscillators,"  We propose a novel generalized cellular automaton(GCA) model for discrete-time pulse-coupled oscillators and study the emergence of synchrony. Given a finite simple graph and an integer $n\ge 3$, each vertex is an identical oscillator of period $n$ with the following weak coupling along the edges: each oscillator inhibits its phase update if it has at least one neighboring oscillator at a particular ""blinking"" state and if its state is ahead of this blinking state. We obtain conditions on initial configurations and on network topologies for which states of all vertices eventually synchronize. We show that our GCA model synchronizes arbitrary initial configurations on paths, trees, and with random perturbation, any connected graph. In particular, our main result is the following local-global principle for tree networks: for $n\in \{3,4,5,6\}$, any $n$-periodic network on a tree synchronizes arbitrary initial configuration if and only if the maximum degree of the tree is less than the period $n$. ",Computer Science - Systems and Control ; Mathematics - Combinatorics ; Mathematics - Dynamical Systems ; Mathematics - Optimization and Control ; Nonlinear Sciences - Cellular Automata and Lattice Gases ; ,"Lyu, Hanbaek ; ","Synchronization of finite-state pulse-coupled oscillators  We propose a novel generalized cellular automaton(GCA) model for discrete-time pulse-coupled oscillators and study the emergence of synchrony. Given a finite simple graph and an integer $n\ge 3$, each vertex is an identical oscillator of period $n$ with the following weak coupling along the edges: each oscillator inhibits its phase update if it has at least one neighboring oscillator at a particular ""blinking"" state and if its state is ahead of this blinking state. We obtain conditions on initial configurations and on network topologies for which states of all vertices eventually synchronize. We show that our GCA model synchronizes arbitrary initial configurations on paths, trees, and with random perturbation, any connected graph. In particular, our main result is the following local-global principle for tree networks: for $n\in \{3,4,5,6\}$, any $n$-periodic network on a tree synchronizes arbitrary initial configuration if and only if the maximum degree of the tree is less than the period $n$. ",synchronization finite state pulse couple oscillators propose novel generalize cellular automaton gca model discrete time pulse couple oscillators study emergence synchrony give finite simple graph integer ge vertex identical oscillator period follow weak couple along edge oscillator inhibit phase update least one neighbor oscillator particular blink state state ahead blink state obtain condition initial configurations network topologies state vertices eventually synchronize show gca model synchronize arbitrary initial configurations paths tree random perturbation connect graph particular main result follow local global principle tree network periodic network tree synchronize arbitrary initial configuration maximum degree tree less period,95,14,1407.1103.txt
http://arxiv.org/abs/1407.2109,Planar Graphs: Random Walks and Bipartiteness Testing,"  We initiate the study of property testing in arbitrary planar graphs. We prove that bipartiteness can be tested in constant time, improving on the previous bound of $\tilde{O}(\sqrt{n})$ for graphs on $n$ vertices. The constant-time testability was only known for planar graphs with bounded degree.   Our algorithm is based on random walks. Since planar graphs have good separators, i.e., bad expansion, our analysis diverges from standard techniques that involve the fast convergence of random walks on expanders. We reduce the problem to the task of detecting an odd-parity cycle in a multigraph induced by constant-length cycles. We iteratively reduce the length of cycles while preserving the detection probability, until the multigraph collapses to a collection of easily discoverable self-loops.   Our approach extends to arbitrary minor-free graphs. We also believe that our techniques will find applications to testing other properties in arbitrary minor-free graphs. ",Computer Science - Data Structures and Algorithms ; ,"Czumaj, Artur ; Monemizadeh, Morteza ; Onak, Krzysztof ; Sohler, Christian ; ","Planar Graphs: Random Walks and Bipartiteness Testing  We initiate the study of property testing in arbitrary planar graphs. We prove that bipartiteness can be tested in constant time, improving on the previous bound of $\tilde{O}(\sqrt{n})$ for graphs on $n$ vertices. The constant-time testability was only known for planar graphs with bounded degree.   Our algorithm is based on random walks. Since planar graphs have good separators, i.e., bad expansion, our analysis diverges from standard techniques that involve the fast convergence of random walks on expanders. We reduce the problem to the task of detecting an odd-parity cycle in a multigraph induced by constant-length cycles. We iteratively reduce the length of cycles while preserving the detection probability, until the multigraph collapses to a collection of easily discoverable self-loops.   Our approach extends to arbitrary minor-free graphs. We also believe that our techniques will find applications to testing other properties in arbitrary minor-free graphs. ",planar graph random walk bipartiteness test initiate study property test arbitrary planar graph prove bipartiteness test constant time improve previous bind tilde sqrt graph vertices constant time testability know planar graph bound degree algorithm base random walk since planar graph good separators bad expansion analysis diverge standard techniques involve fast convergence random walk expanders reduce problem task detect odd parity cycle multigraph induce constant length cycle iteratively reduce length cycle preserve detection probability multigraph collapse collection easily discoverable self loop approach extend arbitrary minor free graph also believe techniques find applications test properties arbitrary minor free graph,97,3,1407.2109.txt
http://arxiv.org/abs/1407.2506,Discovery of Important Crossroads in Road Network using Massive Taxi   Trajectories,"  A major problem in road network analysis is discovery of important crossroads, which can provide useful information for transport planning. However, none of existing approaches addresses the problem of identifying network-wide important crossroads in real road network. In this paper, we propose a novel data-driven based approach named CRRank to rank important crossroads. Our key innovation is that we model the trip network reflecting real travel demands with a tripartite graph, instead of solely analysis on the topology of road network. To compute the importance scores of crossroads accurately, we propose a HITS-like ranking algorithm, in which a procedure of score propagation on our tripartite graph is performed. We conduct experiments on CRRank using a real-world dataset of taxi trajectories. Experiments verify the utility of CRRank. ",Computer Science - Artificial Intelligence ; Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Xu, Ming ; Wu, Jianping ; Du, Yiman ; Wang, Haohan ; Qi, Geqi ; Hu, Kezhen ; Xiao, Yunpeng ; ","Discovery of Important Crossroads in Road Network using Massive Taxi   Trajectories  A major problem in road network analysis is discovery of important crossroads, which can provide useful information for transport planning. However, none of existing approaches addresses the problem of identifying network-wide important crossroads in real road network. In this paper, we propose a novel data-driven based approach named CRRank to rank important crossroads. Our key innovation is that we model the trip network reflecting real travel demands with a tripartite graph, instead of solely analysis on the topology of road network. To compute the importance scores of crossroads accurately, we propose a HITS-like ranking algorithm, in which a procedure of score propagation on our tripartite graph is performed. We conduct experiments on CRRank using a real-world dataset of taxi trajectories. Experiments verify the utility of CRRank. ",discovery important crossroads road network use massive taxi trajectories major problem road network analysis discovery important crossroads provide useful information transport plan however none exist approach address problem identify network wide important crossroads real road network paper propose novel data drive base approach name crrank rank important crossroads key innovation model trip network reflect real travel demand tripartite graph instead solely analysis topology road network compute importance score crossroads accurately propose hit like rank algorithm procedure score propagation tripartite graph perform conduct experiment crrank use real world dataset taxi trajectories experiment verify utility crrank,94,6,1407.2506.txt
http://arxiv.org/abs/1407.2524,"An improved analysis of the M\""omke-Svensson algorithm for graph-TSP on   subquartic graphs","  M\""omke and Svensson presented a beautiful new approach for the traveling salesman problem on a graph metric (graph-TSP), which yields a $4/3$-approximation guarantee on subcubic graphs as well as a substantial improvement over the $3/2$-approximation guarantee of Christofides' algorithm on general graphs. The crux of their approach is to compute an upper bound on the minimum cost of a circulation in a particular network, $C(G,T)$, where $G$ is the input graph and $T$ is a carefully chosen spanning tree. The cost of this circulation is directly related to the number of edges in a tour output by their algorithm. Mucha subsequently improved the analysis of the circulation cost, proving that M\""omke and Svensson's algorithm for graph-TSP has an approximation ratio of at most $13/9$ on general graphs.   This analysis of the circulation is local, and vertices with degree four and five can contribute the most to its cost. Thus, hypothetically, there could exist a subquartic graph (a graph with degree at most four at each vertex) for which Mucha's analysis of the M\""omke-Svensson algorithm is tight. We show that this is not the case and that M\""omke and Svensson's algorithm for graph-TSP has an approximation guarantee of at most $25/18$ on subquartic graphs. To prove this, we present different methods to upper bound the minimum cost of a circulation on the network $C(G,T)$. Our approximation guarantee holds for all graphs that have an optimal solution to a standard linear programming relaxation of graph-TSP with subquartic support. ",Computer Science - Data Structures and Algorithms ; ,"Newman, Alantha ; ","An improved analysis of the M\""omke-Svensson algorithm for graph-TSP on   subquartic graphs  M\""omke and Svensson presented a beautiful new approach for the traveling salesman problem on a graph metric (graph-TSP), which yields a $4/3$-approximation guarantee on subcubic graphs as well as a substantial improvement over the $3/2$-approximation guarantee of Christofides' algorithm on general graphs. The crux of their approach is to compute an upper bound on the minimum cost of a circulation in a particular network, $C(G,T)$, where $G$ is the input graph and $T$ is a carefully chosen spanning tree. The cost of this circulation is directly related to the number of edges in a tour output by their algorithm. Mucha subsequently improved the analysis of the circulation cost, proving that M\""omke and Svensson's algorithm for graph-TSP has an approximation ratio of at most $13/9$ on general graphs.   This analysis of the circulation is local, and vertices with degree four and five can contribute the most to its cost. Thus, hypothetically, there could exist a subquartic graph (a graph with degree at most four at each vertex) for which Mucha's analysis of the M\""omke-Svensson algorithm is tight. We show that this is not the case and that M\""omke and Svensson's algorithm for graph-TSP has an approximation guarantee of at most $25/18$ on subquartic graphs. To prove this, we present different methods to upper bound the minimum cost of a circulation on the network $C(G,T)$. Our approximation guarantee holds for all graphs that have an optimal solution to a standard linear programming relaxation of graph-TSP with subquartic support. ",improve analysis omke svensson algorithm graph tsp subquartic graph omke svensson present beautiful new approach travel salesman problem graph metric graph tsp yield approximation guarantee subcubic graph well substantial improvement approximation guarantee christofides algorithm general graph crux approach compute upper bind minimum cost circulation particular network input graph carefully choose span tree cost circulation directly relate number edge tour output algorithm mucha subsequently improve analysis circulation cost prove omke svensson algorithm graph tsp approximation ratio general graph analysis circulation local vertices degree four five contribute cost thus hypothetically could exist subquartic graph graph degree four vertex mucha analysis omke svensson algorithm tight show case omke svensson algorithm graph tsp approximation guarantee subquartic graph prove present different methods upper bind minimum cost circulation network approximation guarantee hold graph optimal solution standard linear program relaxation graph tsp subquartic support,137,3,1407.2524.txt
http://arxiv.org/abs/1407.2988,Proving differential privacy in Hoare logic,"  Differential privacy is a rigorous, worst-case notion of privacy-preserving computation. Informally, a probabilistic program is differentially private if the participation of a single individual in the input database has a limited effect on the program's distribution on outputs. More technically, differential privacy is a quantitative 2-safety property that bounds the distance between the output distributions of a probabilistic program on adjacent inputs. Like many 2-safety properties, differential privacy lies outside the scope of traditional verification techniques. Existing approaches to enforce privacy are based on intricate, non-conventional type systems, or customized relational logics. These approaches are difficult to implement and often cumbersome to use.   We present an alternative approach that verifies differential privacy by standard, non-relational reasoning on non-probabilistic programs. Our approach transforms a probabilistic program into a non-probabilistic program which simulates two executions of the original program. We prove that if the target program is correct with respect to a Hoare specification, then the original probabilistic program is differentially private. We provide a variety of examples from the differential privacy literature to demonstrate the utility of our approach. Finally, we compare our approach with existing verification techniques for privacy. ",Computer Science - Logic in Computer Science ; Computer Science - Cryptography and Security ; ,"Barthe, Gilles ; Gaboardi, Marco ; Arias, Emilio Jesús Gallego ; Hsu, Justin ; Kunz, César ; Strub, Pierre-Yves ; ","Proving differential privacy in Hoare logic  Differential privacy is a rigorous, worst-case notion of privacy-preserving computation. Informally, a probabilistic program is differentially private if the participation of a single individual in the input database has a limited effect on the program's distribution on outputs. More technically, differential privacy is a quantitative 2-safety property that bounds the distance between the output distributions of a probabilistic program on adjacent inputs. Like many 2-safety properties, differential privacy lies outside the scope of traditional verification techniques. Existing approaches to enforce privacy are based on intricate, non-conventional type systems, or customized relational logics. These approaches are difficult to implement and often cumbersome to use.   We present an alternative approach that verifies differential privacy by standard, non-relational reasoning on non-probabilistic programs. Our approach transforms a probabilistic program into a non-probabilistic program which simulates two executions of the original program. We prove that if the target program is correct with respect to a Hoare specification, then the original probabilistic program is differentially private. We provide a variety of examples from the differential privacy literature to demonstrate the utility of our approach. Finally, we compare our approach with existing verification techniques for privacy. ",prove differential privacy hoare logic differential privacy rigorous worst case notion privacy preserve computation informally probabilistic program differentially private participation single individual input database limit effect program distribution output technically differential privacy quantitative safety property bound distance output distributions probabilistic program adjacent input like many safety properties differential privacy lie outside scope traditional verification techniques exist approach enforce privacy base intricate non conventional type systems customize relational logics approach difficult implement often cumbersome use present alternative approach verify differential privacy standard non relational reason non probabilistic program approach transform probabilistic program non probabilistic program simulate two executions original program prove target program correct respect hoare specification original probabilistic program differentially private provide variety examples differential privacy literature demonstrate utility approach finally compare approach exist verification techniques privacy,127,8,1407.2988.txt
http://arxiv.org/abs/1407.3556,Optimal Spectrum Management in Two-User Interference Channels,"  In this work, we address the problem of optimal spectrum management in continuous frequency domain in multiuser interference channels. The objective is to maximize the weighted sum of user capacities. Our main results are as follows: (i) For frequency-selective channels, we prove that in an optimal solution, each user uses maximum power; this result also generalizes to the cases where the objective is to maximize the weighted product (i.e., proportional fairness) of user capacities. (ii) For the special case of two users in flat channels, we solve the problem optimally. ",Computer Science - Information Theory ; ,"Hamedazimi, Navid ; Gupta, Himanshu ; ","Optimal Spectrum Management in Two-User Interference Channels  In this work, we address the problem of optimal spectrum management in continuous frequency domain in multiuser interference channels. The objective is to maximize the weighted sum of user capacities. Our main results are as follows: (i) For frequency-selective channels, we prove that in an optimal solution, each user uses maximum power; this result also generalizes to the cases where the objective is to maximize the weighted product (i.e., proportional fairness) of user capacities. (ii) For the special case of two users in flat channels, we solve the problem optimally. ",optimal spectrum management two user interference channel work address problem optimal spectrum management continuous frequency domain multiuser interference channel objective maximize weight sum user capacities main result follow frequency selective channel prove optimal solution user use maximum power result also generalize case objective maximize weight product proportional fairness user capacities ii special case two users flat channel solve problem optimally,60,12,1407.3556.txt
http://arxiv.org/abs/1407.3764,Perfect sampling algorithm for Schur processes,"  We describe random generation algorithms for a large class of random combinatorial objects called Schur processes, which are sequences of random (integer) partitions subject to certain interlacing conditions. This class contains several fundamental combinatorial objects as special cases, such as plane partitions, tilings of Aztec diamonds, pyramid partitions and more generally steep domino tilings of the plane. Our algorithm, which is of polynomial complexity, is both exact (i.e. the output follows exactly the target probability law, which is either Boltzmann or uniform in our case), and entropy optimal (i.e. it reads a minimal number of random bits as an input).   The algorithm encompasses previous growth procedures for special Schur processes related to the primal and dual RSK algorithm, as well as the famous domino shuffling algorithm for domino tilings of the Aztec diamond. It can be easily adapted to deal with symmetric Schur processes and general Schur processes involving infinitely many parameters. It is more concrete and easier to implement than Borodin's algorithm, and it is entropy optimal.   At a technical level, it relies on unified bijective proofs of the different types of Cauchy and Littlewood identities for Schur functions, and on an adaptation of Fomin's growth diagram description of the RSK algorithm to that setting. Simulations performed with this algorithm suggest interesting limit shape phenomena for the corresponding tiling models, some of which are new. ","Mathematics - Probability ; Condensed Matter - Statistical Mechanics ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; 05A17, 05E10, 60C05, 60J10, 68U20, 82B20 ; ","Betea, Dan ; Boutillier, Cédric ; Bouttier, Jérémie ; Chapuy, Guillaume ; Corteel, Sylvie ; Vuletić, Mirjana ; ","Perfect sampling algorithm for Schur processes  We describe random generation algorithms for a large class of random combinatorial objects called Schur processes, which are sequences of random (integer) partitions subject to certain interlacing conditions. This class contains several fundamental combinatorial objects as special cases, such as plane partitions, tilings of Aztec diamonds, pyramid partitions and more generally steep domino tilings of the plane. Our algorithm, which is of polynomial complexity, is both exact (i.e. the output follows exactly the target probability law, which is either Boltzmann or uniform in our case), and entropy optimal (i.e. it reads a minimal number of random bits as an input).   The algorithm encompasses previous growth procedures for special Schur processes related to the primal and dual RSK algorithm, as well as the famous domino shuffling algorithm for domino tilings of the Aztec diamond. It can be easily adapted to deal with symmetric Schur processes and general Schur processes involving infinitely many parameters. It is more concrete and easier to implement than Borodin's algorithm, and it is entropy optimal.   At a technical level, it relies on unified bijective proofs of the different types of Cauchy and Littlewood identities for Schur functions, and on an adaptation of Fomin's growth diagram description of the RSK algorithm to that setting. Simulations performed with this algorithm suggest interesting limit shape phenomena for the corresponding tiling models, some of which are new. ",perfect sample algorithm schur process describe random generation algorithms large class random combinatorial object call schur process sequence random integer partition subject certain interlace condition class contain several fundamental combinatorial object special case plane partition tile aztec diamonds pyramid partition generally steep domino tile plane algorithm polynomial complexity exact output follow exactly target probability law either boltzmann uniform case entropy optimal read minimal number random bits input algorithm encompass previous growth procedures special schur process relate primal dual rsk algorithm well famous domino shuffle algorithm domino tile aztec diamond easily adapt deal symmetric schur process general schur process involve infinitely many parameters concrete easier implement borodin algorithm entropy optimal technical level rely unify bijective proof different type cauchy littlewood identities schur function adaptation fomin growth diagram description rsk algorithm set simulations perform algorithm suggest interest limit shape phenomena correspond tile model new,142,4,1407.3764.txt
http://arxiv.org/abs/1407.3975,A Generalized Write Channel Model for Bit-Patterned Media Recording,"  In this paper, we propose a generalized write channel model for bit-patterned media recording by considering all sources of errors causing some extra disturbances during write process, in addition to data dependent write synchronization errors. We investigate information-theoretic bounds for this new model according to various input distributions and also compare it numerically to the last proposed model. ",Computer Science - Information Theory ; ,"Naseri, Sima ; Yazdani, Somaie ; Razeghi, Behrooz ; Hodtani, Ghosheh Abed ; ","A Generalized Write Channel Model for Bit-Patterned Media Recording  In this paper, we propose a generalized write channel model for bit-patterned media recording by considering all sources of errors causing some extra disturbances during write process, in addition to data dependent write synchronization errors. We investigate information-theoretic bounds for this new model according to various input distributions and also compare it numerically to the last proposed model. ",generalize write channel model bite pattern media record paper propose generalize write channel model bite pattern media record consider source errors cause extra disturbances write process addition data dependent write synchronization errors investigate information theoretic bound new model accord various input distributions also compare numerically last propose model,48,12,1407.3975.txt
http://arxiv.org/abs/1407.4066,Minors and dimension,"  It has been known for 30 years that posets with bounded height and with cover graphs of bounded maximum degree have bounded dimension. Recently, Streib and Trotter proved that dimension is bounded for posets with bounded height and planar cover graphs, and Joret et al. proved that dimension is bounded for posets with bounded height and with cover graphs of bounded tree-width. In this paper, it is proved that posets of bounded height whose cover graphs exclude a fixed topological minor have bounded dimension. This generalizes all the aforementioned results and verifies a conjecture of Joret et al. The proof relies on the Robertson-Seymour and Grohe-Marx graph structure theorems. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 06A07, 05C35 ; ","Walczak, Bartosz ; ","Minors and dimension  It has been known for 30 years that posets with bounded height and with cover graphs of bounded maximum degree have bounded dimension. Recently, Streib and Trotter proved that dimension is bounded for posets with bounded height and planar cover graphs, and Joret et al. proved that dimension is bounded for posets with bounded height and with cover graphs of bounded tree-width. In this paper, it is proved that posets of bounded height whose cover graphs exclude a fixed topological minor have bounded dimension. This generalizes all the aforementioned results and verifies a conjecture of Joret et al. The proof relies on the Robertson-Seymour and Grohe-Marx graph structure theorems. ",minors dimension know years posets bound height cover graph bound maximum degree bound dimension recently streib trotter prove dimension bound posets bound height planar cover graph joret et al prove dimension bound posets bound height cover graph bound tree width paper prove posets bound height whose cover graph exclude fix topological minor bound dimension generalize aforementioned result verify conjecture joret et al proof rely robertson seymour grohe marx graph structure theorems,71,3,1407.4066.txt
http://arxiv.org/abs/1407.4723,Toward Selectivity Based Keyword Extraction for Croatian News,"  Preliminary report on network based keyword extraction for Croatian is an unsupervised method for keyword extraction from the complex network. We build our approach with a new network measure the node selectivity, motivated by the research of the graph based centrality approaches. The node selectivity is defined as the average weight distribution on the links of the single node. We extract nodes (keyword candidates) based on the selectivity value. Furthermore, we expand extracted nodes to word-tuples ranked with the highest in/out selectivity values. Selectivity based extraction does not require linguistic knowledge while it is purely derived from statistical and structural information en-compassed in the source text which is reflected into the structure of the network. Obtained sets are evaluated on a manually annotated keywords: for the set of extracted keyword candidates average F1 score is 24,63%, and average F2 score is 21,19%; for the exacted words-tuples candidates average F1 score is 25,9% and average F2 score is 24,47%. ",Computer Science - Computation and Language ; Computer Science - Information Retrieval ; Computer Science - Social and Information Networks ; ,"Beliga, Slobodan ; Meštrović, Ana ; Martinčić-Ipšić, Sanda ; ","Toward Selectivity Based Keyword Extraction for Croatian News  Preliminary report on network based keyword extraction for Croatian is an unsupervised method for keyword extraction from the complex network. We build our approach with a new network measure the node selectivity, motivated by the research of the graph based centrality approaches. The node selectivity is defined as the average weight distribution on the links of the single node. We extract nodes (keyword candidates) based on the selectivity value. Furthermore, we expand extracted nodes to word-tuples ranked with the highest in/out selectivity values. Selectivity based extraction does not require linguistic knowledge while it is purely derived from statistical and structural information en-compassed in the source text which is reflected into the structure of the network. Obtained sets are evaluated on a manually annotated keywords: for the set of extracted keyword candidates average F1 score is 24,63%, and average F2 score is 21,19%; for the exacted words-tuples candidates average F1 score is 25,9% and average F2 score is 24,47%. ",toward selectivity base keyword extraction croatian news preliminary report network base keyword extraction croatian unsupervised method keyword extraction complex network build approach new network measure node selectivity motivate research graph base centrality approach node selectivity define average weight distribution link single node extract nod keyword candidates base selectivity value furthermore expand extract nod word tuples rank highest selectivity value selectivity base extraction require linguistic knowledge purely derive statistical structural information en compass source text reflect structure network obtain set evaluate manually annotate keywords set extract keyword candidates average score average score exact word tuples candidates average score average score,99,11,1407.4723.txt
http://arxiv.org/abs/1407.4729,Sparse Partially Linear Additive Models,"  The generalized partially linear additive model (GPLAM) is a flexible and interpretable approach to building predictive models. It combines features in an additive manner, allowing each to have either a linear or nonlinear effect on the response. However, the choice of which features to treat as linear or nonlinear is typically assumed known. Thus, to make a GPLAM a viable approach in situations in which little is known $a~priori$ about the features, one must overcome two primary model selection challenges: deciding which features to include in the model and determining which of these features to treat nonlinearly. We introduce the sparse partially linear additive model (SPLAM), which combines model fitting and $both$ of these model selection challenges into a single convex optimization problem. SPLAM provides a bridge between the lasso and sparse additive models. Through a statistical oracle inequality and thorough simulation, we demonstrate that SPLAM can outperform other methods across a broad spectrum of statistical regimes, including the high-dimensional ($p\gg N$) setting. We develop efficient algorithms that are applied to real data sets with half a million samples and over 45,000 features with excellent predictive performance. ",Statistics - Methodology ; Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Lou, Yin ; Bien, Jacob ; Caruana, Rich ; Gehrke, Johannes ; ","Sparse Partially Linear Additive Models  The generalized partially linear additive model (GPLAM) is a flexible and interpretable approach to building predictive models. It combines features in an additive manner, allowing each to have either a linear or nonlinear effect on the response. However, the choice of which features to treat as linear or nonlinear is typically assumed known. Thus, to make a GPLAM a viable approach in situations in which little is known $a~priori$ about the features, one must overcome two primary model selection challenges: deciding which features to include in the model and determining which of these features to treat nonlinearly. We introduce the sparse partially linear additive model (SPLAM), which combines model fitting and $both$ of these model selection challenges into a single convex optimization problem. SPLAM provides a bridge between the lasso and sparse additive models. Through a statistical oracle inequality and thorough simulation, we demonstrate that SPLAM can outperform other methods across a broad spectrum of statistical regimes, including the high-dimensional ($p\gg N$) setting. We develop efficient algorithms that are applied to real data sets with half a million samples and over 45,000 features with excellent predictive performance. ",sparse partially linear additive model generalize partially linear additive model gplam flexible interpretable approach build predictive model combine feature additive manner allow either linear nonlinear effect response however choice feature treat linear nonlinear typically assume know thus make gplam viable approach situations little know priori feature one must overcome two primary model selection challenge decide feature include model determine feature treat nonlinearly introduce sparse partially linear additive model splam combine model fit model selection challenge single convex optimization problem splam provide bridge lasso sparse additive model statistical oracle inequality thorough simulation demonstrate splam outperform methods across broad spectrum statistical regimes include high dimensional gg set develop efficient algorithms apply real data set half million sample feature excellent predictive performance,119,9,1407.4729.txt
http://arxiv.org/abs/1407.4908,Integrating R and Hadoop for Big Data Analysis,"  Analyzing and working with big data could be very diffi cult using classical means like relational database management systems or desktop software packages for statistics and visualization. Instead, big data requires large clusters with hundreds or even thousands of computing nodes. Offi cial statistics is increasingly considering big data for deriving new statistics because big data sources could produce more relevant and timely statistics than traditional sources. One of the software tools successfully and wide spread used for storage and processing of big data sets on clusters of commodity hardware is Hadoop. Hadoop framework contains libraries, a distributed fi le-system (HDFS), a resource-management platform and implements a version of the MapReduce programming model for large scale data processing. In this paper we investigate the possibilities of integrating Hadoop with R which is a popular software used for statistical computing and data visualization. We present three ways of integrating them: R with Streaming, Rhipe and RHadoop and we emphasize the advantages and disadvantages of each solution. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Oancea, Bogdan ; Dragoescu, Raluca Mariana ; ","Integrating R and Hadoop for Big Data Analysis  Analyzing and working with big data could be very diffi cult using classical means like relational database management systems or desktop software packages for statistics and visualization. Instead, big data requires large clusters with hundreds or even thousands of computing nodes. Offi cial statistics is increasingly considering big data for deriving new statistics because big data sources could produce more relevant and timely statistics than traditional sources. One of the software tools successfully and wide spread used for storage and processing of big data sets on clusters of commodity hardware is Hadoop. Hadoop framework contains libraries, a distributed fi le-system (HDFS), a resource-management platform and implements a version of the MapReduce programming model for large scale data processing. In this paper we investigate the possibilities of integrating Hadoop with R which is a popular software used for statistical computing and data visualization. We present three ways of integrating them: R with Streaming, Rhipe and RHadoop and we emphasize the advantages and disadvantages of each solution. ",integrate hadoop big data analysis analyze work big data could diffi cult use classical mean like relational database management systems desktop software package statistics visualization instead big data require large cluster hundreds even thousands compute nod offi cial statistics increasingly consider big data derive new statistics big data source could produce relevant timely statistics traditional source one software tool successfully wide spread use storage process big data set cluster commodity hardware hadoop hadoop framework contain libraries distribute fi le system hdfs resource management platform implement version mapreduce program model large scale data process paper investigate possibilities integrate hadoop popular software use statistical compute data visualization present three ways integrate stream rhipe rhadoop emphasize advantage disadvantage solution,116,10,1407.4908.txt
http://arxiv.org/abs/1407.5117,Implementing Transitive Credit with JSON-LD,"  Science and engineering research increasingly relies on activities that facilitate research but are not currently rewarded or recognized, such as: data sharing; developing common data resources, software and methodologies; and annotating data and publications. To promote and advance these activities, we must develop mechanisms for assigning credit, facilitate the appropriate attribution of research outcomes, devise incentives for activities that facilitate research, and allocate funds to maximize return on investment. In this article, we focus on addressing the issue of assigning credit for both direct and indirect contributions, specifically by using JSON-LD to implement a prototype transitive credit system. ",Computer Science - Computers and Society ; Computer Science - Digital Libraries ; ,"Katz, Daniel S. ; Smith, Arfon M. ; ","Implementing Transitive Credit with JSON-LD  Science and engineering research increasingly relies on activities that facilitate research but are not currently rewarded or recognized, such as: data sharing; developing common data resources, software and methodologies; and annotating data and publications. To promote and advance these activities, we must develop mechanisms for assigning credit, facilitate the appropriate attribution of research outcomes, devise incentives for activities that facilitate research, and allocate funds to maximize return on investment. In this article, we focus on addressing the issue of assigning credit for both direct and indirect contributions, specifically by using JSON-LD to implement a prototype transitive credit system. ",implement transitive credit json ld science engineer research increasingly rely activities facilitate research currently reward recognize data share develop common data resources software methodologies annotate data publications promote advance activities must develop mechanisms assign credit facilitate appropriate attribution research outcomes devise incentives activities facilitate research allocate fund maximize return investment article focus address issue assign credit direct indirect contributions specifically use json ld implement prototype transitive credit system,68,10,1407.5117.txt
http://arxiv.org/abs/1407.5218,"Abstractions, Algorithms and Data Structures for Structural   Bioinformatics in PyCogent","  To facilitate flexible and efficient structural bioinformatics analyses, new functionality for three-dimensional structure processing and analysis has been introduced into PyCogent -- a popular feature-rich framework for sequence-based bioinformatics, but one which has lacked equally powerful tools for handling stuctural/coordinate-based data. Extensible Python modules have been developed, which provide object-oriented abstractions (based on a hierarchical representation of macromolecules), efficient data structures (e.g. kD-trees), fast implementations of common algorithms (e.g. surface-area calculations), read/write support for Protein Data Bank-related file formats and wrappers for external command-line applications (e.g. Stride). Integration of this code into PyCogent is symbiotic, allowing sequence-based work to benefit from structure-derived data and, reciprocally, enabling structural studies to leverage PyCogent's versatile tools for phylogenetic and evolutionary analyses. ",Quantitative Biology - Biomolecules ; Computer Science - Data Structures and Algorithms ; Computer Science - Software Engineering ; ,"Cieslik, Marcin ; Derewenda, Zygmunt ; Mura, Cameron ; ","Abstractions, Algorithms and Data Structures for Structural   Bioinformatics in PyCogent  To facilitate flexible and efficient structural bioinformatics analyses, new functionality for three-dimensional structure processing and analysis has been introduced into PyCogent -- a popular feature-rich framework for sequence-based bioinformatics, but one which has lacked equally powerful tools for handling stuctural/coordinate-based data. Extensible Python modules have been developed, which provide object-oriented abstractions (based on a hierarchical representation of macromolecules), efficient data structures (e.g. kD-trees), fast implementations of common algorithms (e.g. surface-area calculations), read/write support for Protein Data Bank-related file formats and wrappers for external command-line applications (e.g. Stride). Integration of this code into PyCogent is symbiotic, allowing sequence-based work to benefit from structure-derived data and, reciprocally, enabling structural studies to leverage PyCogent's versatile tools for phylogenetic and evolutionary analyses. ",abstractions algorithms data structure structural bioinformatics pycogent facilitate flexible efficient structural bioinformatics analyse new functionality three dimensional structure process analysis introduce pycogent popular feature rich framework sequence base bioinformatics one lack equally powerful tool handle stuctural coordinate base data extensible python modules develop provide object orient abstractions base hierarchical representation macromolecules efficient data structure kd tree fast implementations common algorithms surface area calculations read write support protein data bank relate file format wrappers external command line applications stride integration code pycogent symbiotic allow sequence base work benefit structure derive data reciprocally enable structural study leverage pycogent versatile tool phylogenetic evolutionary analyse,101,10,1407.5218.txt
http://arxiv.org/abs/1407.5374,Acyclic Edge Coloring through the Lov\'asz Local Lemma,"  We give a probabilistic analysis of a Moser-type algorithm for the Lov\'{a}sz Local Lemma (LLL), adjusted to search for acyclic edge colorings of a graph. We thus improve the best known upper bound to acyclic chromatic index, also obtained by analyzing a similar algorithm, but through the entropic method (basically counting argument). Specifically we show that a graph with maximum degree $\Delta$ has an acyclic proper edge coloring with at most $\lceil 3.74(\Delta-1)\rceil+1 $ colors, whereas, previously, the best bound was $4(\Delta-1)$. The main contribution of this work is that it comprises a probabilistic analysis of a Moser-type algorithm applied to events pertaining to dependent variables. ",Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; Mathematics - Probability ; ,"Giotis, Ioannis ; Kirousis, Lefteris ; Psaromiligkos, Kostas I. ; Thilikos, Dimitrios M. ; ","Acyclic Edge Coloring through the Lov\'asz Local Lemma  We give a probabilistic analysis of a Moser-type algorithm for the Lov\'{a}sz Local Lemma (LLL), adjusted to search for acyclic edge colorings of a graph. We thus improve the best known upper bound to acyclic chromatic index, also obtained by analyzing a similar algorithm, but through the entropic method (basically counting argument). Specifically we show that a graph with maximum degree $\Delta$ has an acyclic proper edge coloring with at most $\lceil 3.74(\Delta-1)\rceil+1 $ colors, whereas, previously, the best bound was $4(\Delta-1)$. The main contribution of this work is that it comprises a probabilistic analysis of a Moser-type algorithm applied to events pertaining to dependent variables. ",acyclic edge color lov asz local lemma give probabilistic analysis moser type algorithm lov sz local lemma lll adjust search acyclic edge color graph thus improve best know upper bind acyclic chromatic index also obtain analyze similar algorithm entropic method basically count argument specifically show graph maximum degree delta acyclic proper edge color lceil delta rceil color whereas previously best bind delta main contribution work comprise probabilistic analysis moser type algorithm apply events pertain dependent variables,76,13,1407.5374.txt
http://arxiv.org/abs/1407.5536,Multichannel Compressive Sensing MRI Using Noiselet Encoding,"  The incoherence between measurement and sparsifying transform matrices and the restricted isometry property (RIP) of measurement matrix are two of the key factors in determining the performance of compressive sensing (CS). In CS-MRI, the randomly under-sampled Fourier matrix is used as the measurement matrix and the wavelet transform is usually used as sparsifying transform matrix. However, the incoherence between the randomly under-sampled Fourier matrix and the wavelet matrix is not optimal, which can deteriorate the performance of CS-MRI. Using the mathematical result that noiselets are maximally incoherent with wavelets, this paper introduces the noiselet unitary bases as the measurement matrix to improve the incoherence and RIP in CS-MRI, and presents a method to design the pulse sequence for the noiselet encoding. This novel encoding scheme is combined with the multichannel compressive sensing (MCS) framework to take the advantage of multichannel data acquisition used in MRI scanners. An empirical RIP analysis is presented to compare the multichannel noiselet and multichannel Fourier measurement matrices in MCS. Simulations are presented in the MCS framework to compare the performance of noiselet encoding reconstructions and Fourier encoding reconstructions at different acceleration factors. The comparisons indicate that multichannel noiselet measurement matrix has better RIP than that of its Fourier counterpart, and that noiselet encoded MCS-MRI outperforms Fourier encoded MCS-MRI in preserving image resolution and can achieve higher acceleration factors. To demonstrate the feasibility of the proposed noiselet encoding scheme, two pulse sequences with tailored spatially selective RF excitation pulses was designed and implemented on a 3T scanner to acquire the data in the noiselet domain from a phantom and a human brain. ",Physics - Medical Physics ; Computer Science - Computer Vision and Pattern Recognition ; ,"Pawar, Kamlesh ; Egan, Gary F. ; Zhang, Jingxin ; ","Multichannel Compressive Sensing MRI Using Noiselet Encoding  The incoherence between measurement and sparsifying transform matrices and the restricted isometry property (RIP) of measurement matrix are two of the key factors in determining the performance of compressive sensing (CS). In CS-MRI, the randomly under-sampled Fourier matrix is used as the measurement matrix and the wavelet transform is usually used as sparsifying transform matrix. However, the incoherence between the randomly under-sampled Fourier matrix and the wavelet matrix is not optimal, which can deteriorate the performance of CS-MRI. Using the mathematical result that noiselets are maximally incoherent with wavelets, this paper introduces the noiselet unitary bases as the measurement matrix to improve the incoherence and RIP in CS-MRI, and presents a method to design the pulse sequence for the noiselet encoding. This novel encoding scheme is combined with the multichannel compressive sensing (MCS) framework to take the advantage of multichannel data acquisition used in MRI scanners. An empirical RIP analysis is presented to compare the multichannel noiselet and multichannel Fourier measurement matrices in MCS. Simulations are presented in the MCS framework to compare the performance of noiselet encoding reconstructions and Fourier encoding reconstructions at different acceleration factors. The comparisons indicate that multichannel noiselet measurement matrix has better RIP than that of its Fourier counterpart, and that noiselet encoded MCS-MRI outperforms Fourier encoded MCS-MRI in preserving image resolution and can achieve higher acceleration factors. To demonstrate the feasibility of the proposed noiselet encoding scheme, two pulse sequences with tailored spatially selective RF excitation pulses was designed and implemented on a 3T scanner to acquire the data in the noiselet domain from a phantom and a human brain. ",multichannel compressive sense mri use noiselet encode incoherence measurement sparsifying transform matrices restrict isometry property rip measurement matrix two key factor determine performance compressive sense cs cs mri randomly sample fourier matrix use measurement matrix wavelet transform usually use sparsifying transform matrix however incoherence randomly sample fourier matrix wavelet matrix optimal deteriorate performance cs mri use mathematical result noiselets maximally incoherent wavelets paper introduce noiselet unitary base measurement matrix improve incoherence rip cs mri present method design pulse sequence noiselet encode novel encode scheme combine multichannel compressive sense mcs framework take advantage multichannel data acquisition use mri scanners empirical rip analysis present compare multichannel noiselet multichannel fourier measurement matrices mcs simulations present mcs framework compare performance noiselet encode reconstructions fourier encode reconstructions different acceleration factor comparisons indicate multichannel noiselet measurement matrix better rip fourier counterpart noiselet encode mcs mri outperform fourier encode mcs mri preserve image resolution achieve higher acceleration factor demonstrate feasibility propose noiselet encode scheme two pulse sequence tailor spatially selective rf excitation pulse design implement scanner acquire data noiselet domain phantom human brain,176,7,1407.5536.txt
http://arxiv.org/abs/1407.5965,Optimization Techniques on Riemannian Manifolds,"  The techniques and analysis presented in this paper provide new methods to solve optimization problems posed on Riemannian manifolds. A new point of view is offered for the solution of constrained optimization problems. Some classical optimization techniques on Euclidean space are generalized to Riemannian manifolds. Several algorithms are presented and their convergence properties are analyzed employing the Riemannian structure of the manifold. Specifically, two apparently new algorithms, which can be thought of as Newton's method and the conjugate gradient method on Riemannian manifolds, are presented and shown to possess, respectively, quadratic and superlinear convergence. Examples of each method on certain Riemannian manifolds are given with the results of numerical experiments. Rayleigh's quotient defined on the sphere is one example. It is shown that Newton's method applied to this function converges cubically, and that the Rayleigh quotient iteration is an efficient approximation of Newton's method. The Riemannian version of the conjugate gradient method applied to this function gives a new algorithm for finding the eigenvectors corresponding to the extreme eigenvalues of a symmetric matrix. Another example arises from extremizing the function $\mathop{\rm tr} {\Theta}^{\scriptscriptstyle\rm T}Q{\Theta}N$ on the special orthogonal group. In a similar example, it is shown that Newton's method applied to the sum of the squares of the off-diagonal entries of a symmetric matrix converges cubically. ",Mathematics - Optimization and Control ; Computer Science - Computational Geometry ; Computer Science - Numerical Analysis ; Mathematics - Differential Geometry ; Mathematics - Dynamical Systems ; ,"Smith, Steven Thomas ; ","Optimization Techniques on Riemannian Manifolds  The techniques and analysis presented in this paper provide new methods to solve optimization problems posed on Riemannian manifolds. A new point of view is offered for the solution of constrained optimization problems. Some classical optimization techniques on Euclidean space are generalized to Riemannian manifolds. Several algorithms are presented and their convergence properties are analyzed employing the Riemannian structure of the manifold. Specifically, two apparently new algorithms, which can be thought of as Newton's method and the conjugate gradient method on Riemannian manifolds, are presented and shown to possess, respectively, quadratic and superlinear convergence. Examples of each method on certain Riemannian manifolds are given with the results of numerical experiments. Rayleigh's quotient defined on the sphere is one example. It is shown that Newton's method applied to this function converges cubically, and that the Rayleigh quotient iteration is an efficient approximation of Newton's method. The Riemannian version of the conjugate gradient method applied to this function gives a new algorithm for finding the eigenvectors corresponding to the extreme eigenvalues of a symmetric matrix. Another example arises from extremizing the function $\mathop{\rm tr} {\Theta}^{\scriptscriptstyle\rm T}Q{\Theta}N$ on the special orthogonal group. In a similar example, it is shown that Newton's method applied to the sum of the squares of the off-diagonal entries of a symmetric matrix converges cubically. ",optimization techniques riemannian manifold techniques analysis present paper provide new methods solve optimization problems pose riemannian manifold new point view offer solution constrain optimization problems classical optimization techniques euclidean space generalize riemannian manifold several algorithms present convergence properties analyze employ riemannian structure manifold specifically two apparently new algorithms think newton method conjugate gradient method riemannian manifold present show possess respectively quadratic superlinear convergence examples method certain riemannian manifold give result numerical experiment rayleigh quotient define sphere one example show newton method apply function converge cubically rayleigh quotient iteration efficient approximation newton method riemannian version conjugate gradient method apply function give new algorithm find eigenvectors correspond extreme eigenvalues symmetric matrix another example arise extremizing function mathop rm tr theta scriptscriptstyle rm theta special orthogonal group similar example show newton method apply sum square diagonal entries symmetric matrix converge cubically,138,7,1407.5965.txt
http://arxiv.org/abs/1407.6116,A Genetic Algorithm for Software Design Migration from Structured to   Object Oriented Paradigm,"  The potential benefit of migrating software design from Structured to Object Oriented Paradigm is manifolded including modularity, manageability and extendability. This design migration should be automated as it will reduce the time required in manual process. Our previous work has addressed this issue in terms of optimal graph clustering problem formulated by a quadratic Integer Program (IP). However, it has been realized that solution to the IP is computationally hard and thus heuristic based methods are required to get a near optimal solution. This paper presents a Genetic Algorithm (GA) for optimal clustering with an objective of maximizing intra-cluster edges whereas minimizing the inter-cluster ones. The proposed algorithm relies on fitness based parent selection and cross-overing cluster elements to reach an optimal solution step by step. The scheme was implemented and tested against a set of real and synthetic data. The experimental results show that GA outperforms our previous works based on Greedy and Monte Carlo approaches by 40% and 49.5%. ",Computer Science - Software Engineering ; Computer Science - Neural and Evolutionary Computing ; ,"Selim, Md. ; Siddik, Saeed ; Gias, Alim Ul ; Abdullah-Al-Wadud, M. ; Khaled, Shah Mostafa ; ","A Genetic Algorithm for Software Design Migration from Structured to   Object Oriented Paradigm  The potential benefit of migrating software design from Structured to Object Oriented Paradigm is manifolded including modularity, manageability and extendability. This design migration should be automated as it will reduce the time required in manual process. Our previous work has addressed this issue in terms of optimal graph clustering problem formulated by a quadratic Integer Program (IP). However, it has been realized that solution to the IP is computationally hard and thus heuristic based methods are required to get a near optimal solution. This paper presents a Genetic Algorithm (GA) for optimal clustering with an objective of maximizing intra-cluster edges whereas minimizing the inter-cluster ones. The proposed algorithm relies on fitness based parent selection and cross-overing cluster elements to reach an optimal solution step by step. The scheme was implemented and tested against a set of real and synthetic data. The experimental results show that GA outperforms our previous works based on Greedy and Monte Carlo approaches by 40% and 49.5%. ",genetic algorithm software design migration structure object orient paradigm potential benefit migrate software design structure object orient paradigm manifold include modularity manageability extendability design migration automate reduce time require manual process previous work address issue term optimal graph cluster problem formulate quadratic integer program ip however realize solution ip computationally hard thus heuristic base methods require get near optimal solution paper present genetic algorithm ga optimal cluster objective maximize intra cluster edge whereas minimize inter cluster ones propose algorithm rely fitness base parent selection cross overing cluster elements reach optimal solution step step scheme implement test set real synthetic data experimental result show ga outperform previous work base greedy monte carlo approach,112,11,1407.6116.txt
http://arxiv.org/abs/1407.6169,Multiplicative Complexity of Vector Valued Boolean Functions,"  We consider the multiplicative complexity of Boolean functions with multiple bits of output, studying how large a multiplicative complexity is necessary and sufficient to provide a desired nonlinearity. For so-called $\Sigma\Pi\Sigma$ circuits, we show that there is a tight connection between error correcting codes and circuits computing functions with high nonlinearity. Combining this with known coding theory results, we show that functions with $n$ inputs and $n$ outputs with the highest possible nonlinearity must have at least $2.32n$ AND gates. We further show that one cannot prove stronger lower bounds by only appealing to the nonlinearity of a function; we show a bilinear circuit computing a function with almost optimal nonlinearity with the number of AND gates being exactly the length of such a shortest code.   Additionally we provide a function which, for general circuits, has multiplicative complexity at least $2n-3$.   Finally we study the multiplicative complexity of ""almost all"" functions. We show that every function with $n$ bits of input and $m$ bits of output can be computed using at most $2.5(1+o(1))\sqrt{m2^n}$ AND gates. ",Computer Science - Computational Complexity ; ,"Find, Magnus Gausdal ; Boyar, Joan ; ","Multiplicative Complexity of Vector Valued Boolean Functions  We consider the multiplicative complexity of Boolean functions with multiple bits of output, studying how large a multiplicative complexity is necessary and sufficient to provide a desired nonlinearity. For so-called $\Sigma\Pi\Sigma$ circuits, we show that there is a tight connection between error correcting codes and circuits computing functions with high nonlinearity. Combining this with known coding theory results, we show that functions with $n$ inputs and $n$ outputs with the highest possible nonlinearity must have at least $2.32n$ AND gates. We further show that one cannot prove stronger lower bounds by only appealing to the nonlinearity of a function; we show a bilinear circuit computing a function with almost optimal nonlinearity with the number of AND gates being exactly the length of such a shortest code.   Additionally we provide a function which, for general circuits, has multiplicative complexity at least $2n-3$.   Finally we study the multiplicative complexity of ""almost all"" functions. We show that every function with $n$ bits of input and $m$ bits of output can be computed using at most $2.5(1+o(1))\sqrt{m2^n}$ AND gates. ",multiplicative complexity vector value boolean function consider multiplicative complexity boolean function multiple bits output study large multiplicative complexity necessary sufficient provide desire nonlinearity call sigma pi sigma circuit show tight connection error correct cod circuit compute function high nonlinearity combine know cod theory result show function input output highest possible nonlinearity must least gate show one cannot prove stronger lower bound appeal nonlinearity function show bilinear circuit compute function almost optimal nonlinearity number gate exactly length shortest code additionally provide function general circuit multiplicative complexity least finally study multiplicative complexity almost function show every function bits input bits output compute use sqrt gate,103,7,1407.6169.txt
http://arxiv.org/abs/1407.6845,Higher-Order Approximate Relational Refinement Types for Mechanism   Design and Differential Privacy,"  Mechanism design is the study of algorithm design in which the inputs to the algorithm are controlled by strategic agents, who must be incentivized to faithfully report them. Unlike typical programmatic properties, it is not sufficient for algorithms to merely satisfy the property---incentive properties are only useful if the strategic agents also believe this fact.   Verification is an attractive way to convince agents that the incentive properties actually hold, but mechanism design poses several unique challenges: interesting properties can be sophisticated relational properties of probabilistic computations involving expected values, and mechanisms may rely on other probabilistic properties, like differential privacy, to achieve their goals.   We introduce a relational refinement type system, called $\mathsf{HOARe}^2$, for verifying mechanism design and differential privacy. We show that $\mathsf{HOARe}^2$ is sound w.r.t. a denotational semantics, and correctly models $(\epsilon,\delta)$-differential privacy; moreover, we show that it subsumes DFuzz, an existing linear dependent type system for differential privacy. Finally, we develop an SMT-based implementation of $\mathsf{HOARe}^2$ and use it to verify challenging examples of mechanism design, including auctions and aggregative games, and new proposed examples from differential privacy. ",Computer Science - Programming Languages ; Computer Science - Computer Science and Game Theory ; ,"Barthe, Gilles ; Gaboardi, Marco ; Arias, Emilio Jesús Gallego ; Hsu, Justin ; Roth, Aaron ; Strub, Pierre-Yves ; ","Higher-Order Approximate Relational Refinement Types for Mechanism   Design and Differential Privacy  Mechanism design is the study of algorithm design in which the inputs to the algorithm are controlled by strategic agents, who must be incentivized to faithfully report them. Unlike typical programmatic properties, it is not sufficient for algorithms to merely satisfy the property---incentive properties are only useful if the strategic agents also believe this fact.   Verification is an attractive way to convince agents that the incentive properties actually hold, but mechanism design poses several unique challenges: interesting properties can be sophisticated relational properties of probabilistic computations involving expected values, and mechanisms may rely on other probabilistic properties, like differential privacy, to achieve their goals.   We introduce a relational refinement type system, called $\mathsf{HOARe}^2$, for verifying mechanism design and differential privacy. We show that $\mathsf{HOARe}^2$ is sound w.r.t. a denotational semantics, and correctly models $(\epsilon,\delta)$-differential privacy; moreover, we show that it subsumes DFuzz, an existing linear dependent type system for differential privacy. Finally, we develop an SMT-based implementation of $\mathsf{HOARe}^2$ and use it to verify challenging examples of mechanism design, including auctions and aggregative games, and new proposed examples from differential privacy. ",higher order approximate relational refinement type mechanism design differential privacy mechanism design study algorithm design input algorithm control strategic agents must incentivized faithfully report unlike typical programmatic properties sufficient algorithms merely satisfy property incentive properties useful strategic agents also believe fact verification attractive way convince agents incentive properties actually hold mechanism design pose several unique challenge interest properties sophisticate relational properties probabilistic computations involve expect value mechanisms may rely probabilistic properties like differential privacy achieve goals introduce relational refinement type system call mathsf hoare verify mechanism design differential privacy show mathsf hoare sound denotational semantics correctly model epsilon delta differential privacy moreover show subsume dfuzz exist linear dependent type system differential privacy finally develop smt base implementation mathsf hoare use verify challenge examples mechanism design include auction aggregative game new propose examples differential privacy,134,8,1407.6845.txt
http://arxiv.org/abs/1407.7274,Isomorphism within Naive Type Theory,"  We provide a treatment of isomorphism within a set-theoretic formulation of dependent type theory. Type expressions are assigned their natural set-theoretic compositional meaning. Types are divided into small and large types --- sets and proper classes respectively. Each proper class, such as ""group"" or ""topological space"", has an associated notion of isomorphism in correspondence with standard definitions. Isomorphism is handled by definging a groupoid structure on the space of all definable values. The values are simultaneously objects (oids) and morphism --- they are ""morphoids"". Soundness can then be proved for simple and natural inference rules deriving isomorphisms and for the substitution of isomorphics. ",Computer Science - Logic in Computer Science ; ,"McAllester, David ; ","Isomorphism within Naive Type Theory  We provide a treatment of isomorphism within a set-theoretic formulation of dependent type theory. Type expressions are assigned their natural set-theoretic compositional meaning. Types are divided into small and large types --- sets and proper classes respectively. Each proper class, such as ""group"" or ""topological space"", has an associated notion of isomorphism in correspondence with standard definitions. Isomorphism is handled by definging a groupoid structure on the space of all definable values. The values are simultaneously objects (oids) and morphism --- they are ""morphoids"". Soundness can then be proved for simple and natural inference rules deriving isomorphisms and for the substitution of isomorphics. ",isomorphism within naive type theory provide treatment isomorphism within set theoretic formulation dependent type theory type expressions assign natural set theoretic compositional mean type divide small large type set proper class respectively proper class group topological space associate notion isomorphism correspondence standard definitions isomorphism handle definging groupoid structure space definable value value simultaneously object oids morphism morphoids soundness prove simple natural inference rule derive isomorphisms substitution isomorphics,67,8,1407.7274.txt
http://arxiv.org/abs/1407.7459,A note on multipivot Quicksort,"  We analyse a generalisation of the Quicksort algorithm, where k uniformly at random chosen pivots are used for partitioning an array of n distinct keys. Specifically, the expected cost of this scheme is obtained, under the assumption of linearity of the cost needed for the partition process. The integration constants of the expected cost are computed using Vandermonde matrices. ","Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; 68P10, 68W20 ; ","Iliopoulos, Vasileios ; ","A note on multipivot Quicksort  We analyse a generalisation of the Quicksort algorithm, where k uniformly at random chosen pivots are used for partitioning an array of n distinct keys. Specifically, the expected cost of this scheme is obtained, under the assumption of linearity of the cost needed for the partition process. The integration constants of the expected cost are computed using Vandermonde matrices. ",note multipivot quicksort analyse generalisation quicksort algorithm uniformly random choose pivot use partition array distinct key specifically expect cost scheme obtain assumption linearity cost need partition process integration constants expect cost compute use vandermonde matrices,35,9,1407.7459.txt
http://arxiv.org/abs/1408.0135,"New data, new possibilities: Exploring the insides of Altmetric.com","  This paper analyzes Altmetric.com, one of the most important altmetric data providers currently used. We have analyzed a set of publications with DOI number indexed in the Web of Science during the period 2011-2013 and collected their data with the Altmetric API. 19% of the original set of papers was retrieved from Altmetric.com including some altmetric data. We identified 16 different social media sources from which Altmetric.com retrieves data. However five of them cover 95.5% of the total set. Twitter (87.1%) and Mendeley (64.8%) have the highest coverage. We conclude that Altmetric.com is a transparent, rich and accurate tool for altmetric data. Nevertheless, there are still potential limitations on its exhaustiveness as well as on the selection of social media sources that need further research. ",Computer Science - Digital Libraries ; ,"Robinson-García, Nicolás ; Torres-Salinas, Daniel ; Zahedi, Zohreh ; Costas, Rodrigo ; ","New data, new possibilities: Exploring the insides of Altmetric.com  This paper analyzes Altmetric.com, one of the most important altmetric data providers currently used. We have analyzed a set of publications with DOI number indexed in the Web of Science during the period 2011-2013 and collected their data with the Altmetric API. 19% of the original set of papers was retrieved from Altmetric.com including some altmetric data. We identified 16 different social media sources from which Altmetric.com retrieves data. However five of them cover 95.5% of the total set. Twitter (87.1%) and Mendeley (64.8%) have the highest coverage. We conclude that Altmetric.com is a transparent, rich and accurate tool for altmetric data. Nevertheless, there are still potential limitations on its exhaustiveness as well as on the selection of social media sources that need further research. ",new data new possibilities explore insides altmetric com paper analyze altmetric com one important altmetric data providers currently use analyze set publications doi number index web science period collect data altmetric api original set paper retrieve altmetric com include altmetric data identify different social media source altmetric com retrieve data however five cover total set twitter mendeley highest coverage conclude altmetric com transparent rich accurate tool altmetric data nevertheless still potential limitations exhaustiveness well selection social media source need research,80,10,1408.0135.txt
http://arxiv.org/abs/1408.0652,Precision of Pulse-Coupled Oscillator Synchronization on FPGA-Based   Radios,"  The precision of synchronization algorithms based on the theory of pulse-coupled oscillators is evaluated on FPGA-based radios for the first time. Measurements show that such algorithms can reach precision in the low microsecond range when being implemented in the physical layer. Furthermore, we propose an algorithm extension accounting for phase rate deviations of the hardware and show that an improved precision below one microsecond is possible with this extension in the given setup. The resulting algorithm can thus be applied in ad hoc wireless systems for fully distributed synchronization of transmission slots or sleep cycles, in particular, if centralized synchronization is impossible. ",Computer Science - Other Computer Science ; ,"Brandner, Günther ; Klinglmayr, Johannes ; Schilcher, Udo ; Egarter, Dominik ; Bettstetter, Christian ; ","Precision of Pulse-Coupled Oscillator Synchronization on FPGA-Based   Radios  The precision of synchronization algorithms based on the theory of pulse-coupled oscillators is evaluated on FPGA-based radios for the first time. Measurements show that such algorithms can reach precision in the low microsecond range when being implemented in the physical layer. Furthermore, we propose an algorithm extension accounting for phase rate deviations of the hardware and show that an improved precision below one microsecond is possible with this extension in the given setup. The resulting algorithm can thus be applied in ad hoc wireless systems for fully distributed synchronization of transmission slots or sleep cycles, in particular, if centralized synchronization is impossible. ",precision pulse couple oscillator synchronization fpga base radio precision synchronization algorithms base theory pulse couple oscillators evaluate fpga base radio first time measurements show algorithms reach precision low microsecond range implement physical layer furthermore propose algorithm extension account phase rate deviations hardware show improve precision one microsecond possible extension give setup result algorithm thus apply ad hoc wireless systems fully distribute synchronization transmission slot sleep cycle particular centralize synchronization impossible,70,11,1408.0652.txt
http://arxiv.org/abs/1408.0807,Polynomial size linear programs for problems in P,"  A perfect matching in an undirected graph $G=(V,E)$ is a set of vertex disjoint edges from $E$ that include all vertices in $V$. The perfect matching problem is to decide if $G$ has such a matching. Recently Rothvo{\ss} proved the striking result that the Edmonds' matching polytope has exponential extension complexity. Here for each $n=|V|$ we describe a perfect matching polytope that is different from Edmonds' polytope and define a weaker notion of extended formulation. We show that the new polytope has a weak extended formulation (WEF) $Q$ of polynomial size. For each graph $G$ with $n$ vertices we can readily construct an objective function so that solving the resulting linear program over $Q$ decides whether or not $G$ has a perfect matching. The construction is uniform in the sense that, for each $n$, a single polytope is defined for the class of all graphs with $n$ nodes. The method extends to solve poly time optimization problems, such as the weighted matching problem. In this case a logarithmic (in the weight of the optimum solution) number of optimizations are made over the constructed WEF.   The method described in the paper involves construction of a compiler that converts an algorithm given in a prescribed pseudocode into a polytope. It can therefore be used to construct a polytope for any decision problem in {\bf P} which can be solved by a given algorithm. Compared with earlier results of Dobkin-Lipton-Reiss and Valiant our method allows the construction of explicit linear programs directly from algorithms written for a standard register model, without intermediate transformations. We apply our results to obtain polynomial upper bounds on the non-negative rank of certain slack matrices related to membership testing of languages in {\bf P/Poly}. ",Computer Science - Discrete Mathematics ; ,"Avis, David ; Bremner, David ; Tiwary, Hans Raj ; Watanabe, Osamu ; ","Polynomial size linear programs for problems in P  A perfect matching in an undirected graph $G=(V,E)$ is a set of vertex disjoint edges from $E$ that include all vertices in $V$. The perfect matching problem is to decide if $G$ has such a matching. Recently Rothvo{\ss} proved the striking result that the Edmonds' matching polytope has exponential extension complexity. Here for each $n=|V|$ we describe a perfect matching polytope that is different from Edmonds' polytope and define a weaker notion of extended formulation. We show that the new polytope has a weak extended formulation (WEF) $Q$ of polynomial size. For each graph $G$ with $n$ vertices we can readily construct an objective function so that solving the resulting linear program over $Q$ decides whether or not $G$ has a perfect matching. The construction is uniform in the sense that, for each $n$, a single polytope is defined for the class of all graphs with $n$ nodes. The method extends to solve poly time optimization problems, such as the weighted matching problem. In this case a logarithmic (in the weight of the optimum solution) number of optimizations are made over the constructed WEF.   The method described in the paper involves construction of a compiler that converts an algorithm given in a prescribed pseudocode into a polytope. It can therefore be used to construct a polytope for any decision problem in {\bf P} which can be solved by a given algorithm. Compared with earlier results of Dobkin-Lipton-Reiss and Valiant our method allows the construction of explicit linear programs directly from algorithms written for a standard register model, without intermediate transformations. We apply our results to obtain polynomial upper bounds on the non-negative rank of certain slack matrices related to membership testing of languages in {\bf P/Poly}. ",polynomial size linear program problems perfect match undirected graph set vertex disjoint edge include vertices perfect match problem decide match recently rothvo ss prove strike result edmonds match polytope exponential extension complexity describe perfect match polytope different edmonds polytope define weaker notion extend formulation show new polytope weak extend formulation wef polynomial size graph vertices readily construct objective function solve result linear program decide whether perfect match construction uniform sense single polytope define class graph nod method extend solve poly time optimization problems weight match problem case logarithmic weight optimum solution number optimizations make construct wef method describe paper involve construction compiler convert algorithm give prescribe pseudocode polytope therefore use construct polytope decision problem bf solve give algorithm compare earlier result dobkin lipton reiss valiant method allow construction explicit linear program directly algorithms write standard register model without intermediate transformations apply result obtain polynomial upper bound non negative rank certain slack matrices relate membership test languages bf poly,158,8,1408.0807.txt
http://arxiv.org/abs/1408.0848,Multilayer bootstrap networks,"  Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear network from bottom up for unsupervised nonlinear dimensionality reduction. Each layer of the network is a nonparametric density estimator. It consists of a group of k-centroids clusterings. Each clustering randomly selects data points with randomly selected features as its centroids, and learns a one-hot encoder by one-nearest-neighbor optimization. Geometrically, the nonparametric density estimator at each layer projects the input data space to a uniformly-distributed discrete feature space, where the similarity of two data points in the discrete feature space is measured by the number of the nearest centroids they share in common. The multilayer network gradually reduces the nonlinear variations of data from bottom up by building a vast number of hierarchical trees implicitly on the original data space. Theoretically, the estimation error caused by the nonparametric density estimator is proportional to the correlation between the clusterings, both of which are reduced by the randomization steps. ",Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; Statistics - Machine Learning ; ,"Zhang, Xiao-Lei ; ","Multilayer bootstrap networks  Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear network from bottom up for unsupervised nonlinear dimensionality reduction. Each layer of the network is a nonparametric density estimator. It consists of a group of k-centroids clusterings. Each clustering randomly selects data points with randomly selected features as its centroids, and learns a one-hot encoder by one-nearest-neighbor optimization. Geometrically, the nonparametric density estimator at each layer projects the input data space to a uniformly-distributed discrete feature space, where the similarity of two data points in the discrete feature space is measured by the number of the nearest centroids they share in common. The multilayer network gradually reduces the nonlinear variations of data from bottom up by building a vast number of hierarchical trees implicitly on the original data space. Theoretically, the estimation error caused by the nonparametric density estimator is proportional to the correlation between the clusterings, both of which are reduced by the randomization steps. ",multilayer bootstrap network multilayer bootstrap network build gradually narrow multilayer nonlinear network bottom unsupervised nonlinear dimensionality reduction layer network nonparametric density estimator consist group centroids cluster cluster randomly select data point randomly select feature centroids learn one hot encoder one nearest neighbor optimization geometrically nonparametric density estimator layer project input data space uniformly distribute discrete feature space similarity two data point discrete feature space measure number nearest centroids share common multilayer network gradually reduce nonlinear variations data bottom build vast number hierarchical tree implicitly original data space theoretically estimation error cause nonparametric density estimator proportional correlation cluster reduce randomization step,100,6,1408.0848.txt
http://arxiv.org/abs/1408.0948,A special role of Boolean quadratic polytopes among other combinatorial   polytopes,"  We consider several families of combinatorial polytopes associated with the following NP-complete problems: maximum cut, Boolean quadratic programming, quadratic linear ordering, quadratic assignment, set partition, set packing, stable set, 3-assignment. For comparing two families of polytopes we use the following method. We say that a family $P$ is affinely reduced to a family $Q$ if for every polytope $p\in P$ there exists $q\in Q$ such that $p$ is affinely equivalent to $q$ or to a face of $q$, where $\dim q = O((\dim p)^k)$ for some constant $k$. Under this comparison the above-mentioned families are splitted into two equivalence classes. We show also that these two classes are simpler (in the above sence) than the families of poytopes of the following problems: set covering, traveling salesman, 0-1 knapsack problem, 3-satisfiability, cubic subgraph, partial ordering. In particular, Boolean quadratic polytopes appear as faces of polytopes in every of the mentioned families. ",Computer Science - Computational Complexity ; Mathematics - Combinatorics ; ,"Maksimenko, Aleksandr ; ","A special role of Boolean quadratic polytopes among other combinatorial   polytopes  We consider several families of combinatorial polytopes associated with the following NP-complete problems: maximum cut, Boolean quadratic programming, quadratic linear ordering, quadratic assignment, set partition, set packing, stable set, 3-assignment. For comparing two families of polytopes we use the following method. We say that a family $P$ is affinely reduced to a family $Q$ if for every polytope $p\in P$ there exists $q\in Q$ such that $p$ is affinely equivalent to $q$ or to a face of $q$, where $\dim q = O((\dim p)^k)$ for some constant $k$. Under this comparison the above-mentioned families are splitted into two equivalence classes. We show also that these two classes are simpler (in the above sence) than the families of poytopes of the following problems: set covering, traveling salesman, 0-1 knapsack problem, 3-satisfiability, cubic subgraph, partial ordering. In particular, Boolean quadratic polytopes appear as faces of polytopes in every of the mentioned families. ",special role boolean quadratic polytopes among combinatorial polytopes consider several families combinatorial polytopes associate follow np complete problems maximum cut boolean quadratic program quadratic linear order quadratic assignment set partition set pack stable set assignment compare two families polytopes use follow method say family affinely reduce family every polytope exist affinely equivalent face dim dim constant comparison mention families splitted two equivalence class show also two class simpler sence families poytopes follow problems set cover travel salesman knapsack problem satisfiability cubic subgraph partial order particular boolean quadratic polytopes appear face polytopes every mention families,94,8,1408.0948.txt
http://arxiv.org/abs/1408.1025,Stable Throughput Region of Cognitive-Relay Networks with Imperfect   Sensing and Finite Relaying Buffer,"  In this letter, we obtain the stable throughput region for a cognitive relaying scheme with a finite relaying buffer and imperfect sensing. The analysis investigates the effect of the secondary user's finite relaying capabilities under different scenarios of primary, secondary and relaying links outages. Furthermore, we demonstrate the effect of miss detection and false alarm probabilities on the achievable throughput for the primary and secondary users. ",Computer Science - Networking and Internet Architecture ; ,"Alaa, Ahmed M. ; ","Stable Throughput Region of Cognitive-Relay Networks with Imperfect   Sensing and Finite Relaying Buffer  In this letter, we obtain the stable throughput region for a cognitive relaying scheme with a finite relaying buffer and imperfect sensing. The analysis investigates the effect of the secondary user's finite relaying capabilities under different scenarios of primary, secondary and relaying links outages. Furthermore, we demonstrate the effect of miss detection and false alarm probabilities on the achievable throughput for the primary and secondary users. ",stable throughput region cognitive relay network imperfect sense finite relay buffer letter obtain stable throughput region cognitive relay scheme finite relay buffer imperfect sense analysis investigate effect secondary user finite relay capabilities different scenarios primary secondary relay link outages furthermore demonstrate effect miss detection false alarm probabilities achievable throughput primary secondary users,52,7,1408.1025.txt
http://arxiv.org/abs/1408.1118,Spoke-Darts for High-Dimensional Blue-Noise Sampling,"  Blue noise sampling has proved useful for many graphics applications, but remains underexplored in high-dimensional spaces due to the difficulty of generating distributions and proving properties about them. We present a blue noise sampling method with good quality and performance across different dimensions. The method, spoke-dart sampling, shoots rays from prior samples and selects samples from these rays. It combines the advantages of two major high-dimensional sampling methods: the locality of advancing front with the dimensionality-reduction of hyperplanes, specifically line sampling. We prove that the output sampling is saturated with high probability, with bounds on distances between pairs of samples and between any domain point and its nearest sample. We demonstrate spoke-dart applications for approximate Delaunay graph construction, global optimization, and robotic motion planning. Both the blue-noise quality of the output distribution and the adaptability of the intermediate processes of our method are useful in these applications. ",Computer Science - Graphics ; ,"Mitchell, Scott A. ; Ebeida, Mohamed S. ; Awad, Muhammad A. ; Park, Chonhyon ; Patney, Anjul ; Rushdi, Ahmad A. ; Swiler, Laura P. ; Manocha, Dinesh ; Wei, Li-Yi ; ","Spoke-Darts for High-Dimensional Blue-Noise Sampling  Blue noise sampling has proved useful for many graphics applications, but remains underexplored in high-dimensional spaces due to the difficulty of generating distributions and proving properties about them. We present a blue noise sampling method with good quality and performance across different dimensions. The method, spoke-dart sampling, shoots rays from prior samples and selects samples from these rays. It combines the advantages of two major high-dimensional sampling methods: the locality of advancing front with the dimensionality-reduction of hyperplanes, specifically line sampling. We prove that the output sampling is saturated with high probability, with bounds on distances between pairs of samples and between any domain point and its nearest sample. We demonstrate spoke-dart applications for approximate Delaunay graph construction, global optimization, and robotic motion planning. Both the blue-noise quality of the output distribution and the adaptability of the intermediate processes of our method are useful in these applications. ",speak dart high dimensional blue noise sample blue noise sample prove useful many graphics applications remain underexplored high dimensional space due difficulty generate distributions prove properties present blue noise sample method good quality performance across different dimension method speak dart sample shoot ray prior sample select sample ray combine advantage two major high dimensional sample methods locality advance front dimensionality reduction hyperplanes specifically line sample prove output sample saturate high probability bound distance pair sample domain point nearest sample demonstrate speak dart applications approximate delaunay graph construction global optimization robotic motion plan blue noise quality output distribution adaptability intermediate process method useful applications,103,12,1408.1118.txt
http://arxiv.org/abs/1408.1390,On optimal approximability results for computing the strong metric   dimension,"  The strong metric dimension of a graph was first introduced by Seb\""{o} and Tannier (Mathematics of Operations Research, 29(2), 383-393, 2004) as an alternative to the (weak) metric dimension of graphs previously introduced independently by Slater (Proc. 6th Southeastern Conference on Combinatorics, Graph Theory, and Computing, 549-559, 1975) and by Harary and Melter (Ars Combinatoria, 2, 191-195, 1976), and has since been investigated in several research papers. However, the exact worst-case computational complexity of computing the strong metric dimension has remained open beyond being NP-complete. In this communication, we show that the problem of computing the strong metric dimension of a graph of $n$ nodes admits a polynomial-time $2$-approximation, admits a $O^\ast\big(2^{\,0.287\,n}\big)$-time exact computation algorithm, admits a $O\big(1.2738^k+n\,k\big)$-time exact computation algorithm if the strong metric dimension is at most $k$, does not admit a polynomial time $(2-\varepsilon)$-approximation algorithm assuming the unique games conjecture is true, does not admit a polynomial time $(10\sqrt{5}-21-\varepsilon)$-approximation algorithm assuming P$\neq$NP, does not admit a $O^\ast\big(2^{o(n)}\big)$-time exact computation algorithm assuming the exponential time hypothesis is true, and does not admit a $O^\ast\big(n^{o(k)}\big)$-time exact computation algorithm if the strong metric dimension is at most $k$ assuming the exponential time hypothesis is true. ","Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; 68Q17, 68Q25, 68R10 ; G.2.2 ; F.2.2 ; ","DasGupta, Bhaskar ; Mobasheri, Nasim ; ","On optimal approximability results for computing the strong metric   dimension  The strong metric dimension of a graph was first introduced by Seb\""{o} and Tannier (Mathematics of Operations Research, 29(2), 383-393, 2004) as an alternative to the (weak) metric dimension of graphs previously introduced independently by Slater (Proc. 6th Southeastern Conference on Combinatorics, Graph Theory, and Computing, 549-559, 1975) and by Harary and Melter (Ars Combinatoria, 2, 191-195, 1976), and has since been investigated in several research papers. However, the exact worst-case computational complexity of computing the strong metric dimension has remained open beyond being NP-complete. In this communication, we show that the problem of computing the strong metric dimension of a graph of $n$ nodes admits a polynomial-time $2$-approximation, admits a $O^\ast\big(2^{\,0.287\,n}\big)$-time exact computation algorithm, admits a $O\big(1.2738^k+n\,k\big)$-time exact computation algorithm if the strong metric dimension is at most $k$, does not admit a polynomial time $(2-\varepsilon)$-approximation algorithm assuming the unique games conjecture is true, does not admit a polynomial time $(10\sqrt{5}-21-\varepsilon)$-approximation algorithm assuming P$\neq$NP, does not admit a $O^\ast\big(2^{o(n)}\big)$-time exact computation algorithm assuming the exponential time hypothesis is true, and does not admit a $O^\ast\big(n^{o(k)}\big)$-time exact computation algorithm if the strong metric dimension is at most $k$ assuming the exponential time hypothesis is true. ",optimal approximability result compute strong metric dimension strong metric dimension graph first introduce seb tannier mathematics operations research alternative weak metric dimension graph previously introduce independently slater proc th southeastern conference combinatorics graph theory compute harary melter ars combinatoria since investigate several research paper however exact worst case computational complexity compute strong metric dimension remain open beyond np complete communication show problem compute strong metric dimension graph nod admit polynomial time approximation admit ast big big time exact computation algorithm admit big big time exact computation algorithm strong metric dimension admit polynomial time varepsilon approximation algorithm assume unique game conjecture true admit polynomial time sqrt varepsilon approximation algorithm assume neq np admit ast big big time exact computation algorithm assume exponential time hypothesis true admit ast big big time exact computation algorithm strong metric dimension assume exponential time hypothesis true,140,1,1408.1390.txt
http://arxiv.org/abs/1408.1868,On the structure of classical realizability models of ZF,"  The technique of ""classical realizability"" is an extension of the method of ""forcing""; it permits to extend the Curry-Howard correspondence between proofs and programs, to Zermelo-Fraenkel set theory and to build new models of ZF, called ""realizability models"". The structure of these models is, in general, much more complicated than that of the particular case of ""forcing models"". We show here that the class of constructible sets of any realizability model is an elementary extension of the constructibles of the ground model (a trivial fact in the case of forcing, since these classes are identical). It follows that Shoenfield absoluteness theorem applies to realizability models. ",Computer Science - Logic in Computer Science ; Mathematics - Logic ; 03E40 ; F.4.1 ; ,"Krivine, Jean-Louis ; ","On the structure of classical realizability models of ZF  The technique of ""classical realizability"" is an extension of the method of ""forcing""; it permits to extend the Curry-Howard correspondence between proofs and programs, to Zermelo-Fraenkel set theory and to build new models of ZF, called ""realizability models"". The structure of these models is, in general, much more complicated than that of the particular case of ""forcing models"". We show here that the class of constructible sets of any realizability model is an elementary extension of the constructibles of the ground model (a trivial fact in the case of forcing, since these classes are identical). It follows that Shoenfield absoluteness theorem applies to realizability models. ",structure classical realizability model zf technique classical realizability extension method force permit extend curry howard correspondence proof program zermelo fraenkel set theory build new model zf call realizability model structure model general much complicate particular case force model show class constructible set realizability model elementary extension constructibles grind model trivial fact case force since class identical follow shoenfield absoluteness theorem apply realizability model,63,8,1408.1868.txt
http://arxiv.org/abs/1408.2071,Near-Constant-Time Distributed Algorithms on a Congested Clique,"  This paper presents constant-time and near-constant-time distributed algorithms for a variety of problems in the congested clique model. We show how to compute a 3-ruling set in expected $O(\log \log \log n)$ rounds and using this, we obtain a constant-approximation to metric facility location, also in expected $O(\log \log \log n)$ rounds. In addition, assuming an input metric space of constant doubling dimension, we obtain constant-round algorithms to compute constant-factor approximations to the minimum spanning tree and the metric facility location problems. These results significantly improve on the running time of the fastest known algorithms for these problems in the congested clique setting. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Hegeman, James W. ; Pemmaraju, Sriram V. ; Sardeshmukh, Vivek B. ; ","Near-Constant-Time Distributed Algorithms on a Congested Clique  This paper presents constant-time and near-constant-time distributed algorithms for a variety of problems in the congested clique model. We show how to compute a 3-ruling set in expected $O(\log \log \log n)$ rounds and using this, we obtain a constant-approximation to metric facility location, also in expected $O(\log \log \log n)$ rounds. In addition, assuming an input metric space of constant doubling dimension, we obtain constant-round algorithms to compute constant-factor approximations to the minimum spanning tree and the metric facility location problems. These results significantly improve on the running time of the fastest known algorithms for these problems in the congested clique setting. ",near constant time distribute algorithms congest clique paper present constant time near constant time distribute algorithms variety problems congest clique model show compute rule set expect log log log round use obtain constant approximation metric facility location also expect log log log round addition assume input metric space constant double dimension obtain constant round algorithms compute constant factor approximations minimum span tree metric facility location problems result significantly improve run time fastest know algorithms problems congest clique set,78,1,1408.2071.txt
http://arxiv.org/abs/1408.2467,Matrix Completion under Interval Uncertainty,"  Matrix completion under interval uncertainty can be cast as matrix completion with element-wise box constraints. We present an efficient alternating-direction parallel coordinate-descent method for the problem. We show that the method outperforms any other known method on a benchmark in image in-painting in terms of signal-to-noise ratio, and that it provides high-quality solutions for an instance of collaborative filtering with 100,198,805 recommendations within 5 minutes. ",Mathematics - Optimization and Control ; Computer Science - Artificial Intelligence ; Computer Science - Information Retrieval ; ,"Marecek, Jakub ; Richtarik, Peter ; Takac, Martin ; ","Matrix Completion under Interval Uncertainty  Matrix completion under interval uncertainty can be cast as matrix completion with element-wise box constraints. We present an efficient alternating-direction parallel coordinate-descent method for the problem. We show that the method outperforms any other known method on a benchmark in image in-painting in terms of signal-to-noise ratio, and that it provides high-quality solutions for an instance of collaborative filtering with 100,198,805 recommendations within 5 minutes. ",matrix completion interval uncertainty matrix completion interval uncertainty cast matrix completion element wise box constraints present efficient alternate direction parallel coordinate descent method problem show method outperform know method benchmark image paint term signal noise ratio provide high quality solutions instance collaborative filter recommendations within minutes,46,7,1408.2467.txt
http://arxiv.org/abs/1408.3030,Distributed Graph Automata and Verification of Distributed Algorithms,"  Combining ideas from distributed algorithms and alternating automata, we introduce a new class of finite graph automata that recognize precisely the languages of finite graphs definable in monadic second-order logic. By restricting transitions to be nondeterministic or deterministic, we also obtain two strictly weaker variants of our automata for which the emptiness problem is decidable. As an application, we suggest how suitable graph automata might be useful in formal verification of distributed algorithms, using Floyd-Hoare logic. ","Computer Science - Formal Languages and Automata Theory ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Logic in Computer Science ; ","Reiter, Fabian ; ","Distributed Graph Automata and Verification of Distributed Algorithms  Combining ideas from distributed algorithms and alternating automata, we introduce a new class of finite graph automata that recognize precisely the languages of finite graphs definable in monadic second-order logic. By restricting transitions to be nondeterministic or deterministic, we also obtain two strictly weaker variants of our automata for which the emptiness problem is decidable. As an application, we suggest how suitable graph automata might be useful in formal verification of distributed algorithms, using Floyd-Hoare logic. ",distribute graph automata verification distribute algorithms combine ideas distribute algorithms alternate automata introduce new class finite graph automata recognize precisely languages finite graph definable monadic second order logic restrict transition nondeterministic deterministic also obtain two strictly weaker variants automata emptiness problem decidable application suggest suitable graph automata might useful formal verification distribute algorithms use floyd hoare logic,57,14,1408.3030.txt
http://arxiv.org/abs/1408.3190,On the neighbour sum distinguishing index of planar graphs,"  Let $c$ be a proper edge colouring of a graph $G=(V,E)$ with integers $1,2,\ldots,k$. Then $k\geq \Delta(G)$, while by Vizing's theorem, no more than $k=\Delta(G)+1$ is necessary for constructing such $c$. On the course of investigating irregularities in graphs, it has been moreover conjectured that only slightly larger $k$, i.e., $k=\Delta(G)+2$ enables enforcing additional strong feature of $c$, namely that it attributes distinct sums of incident colours to adjacent vertices in $G$ if only this graph has no isolated edges and is not isomorphic to $C_5$. We prove the conjecture is valid for planar graphs of sufficiently large maximum degree. In fact even stronger statement holds, as the necessary number of colours stemming from the result of Vizing is proved to be sufficient for this family of graphs. Specifically, our main result states that every planar graph $G$ of maximum degree at least $28$ which contains no isolated edges admits a proper edge colouring $c:E\to\{1,2,\ldots,\Delta(G)+1\}$ such that $\sum_{e\ni u}c(e)\neq \sum_{e\ni v}c(e)$ for every edge $uv$ of $G$. ","Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; 05C78, 05C15 ; ","Bonamy, Marthe ; Przybyło, Jakub ; ","On the neighbour sum distinguishing index of planar graphs  Let $c$ be a proper edge colouring of a graph $G=(V,E)$ with integers $1,2,\ldots,k$. Then $k\geq \Delta(G)$, while by Vizing's theorem, no more than $k=\Delta(G)+1$ is necessary for constructing such $c$. On the course of investigating irregularities in graphs, it has been moreover conjectured that only slightly larger $k$, i.e., $k=\Delta(G)+2$ enables enforcing additional strong feature of $c$, namely that it attributes distinct sums of incident colours to adjacent vertices in $G$ if only this graph has no isolated edges and is not isomorphic to $C_5$. We prove the conjecture is valid for planar graphs of sufficiently large maximum degree. In fact even stronger statement holds, as the necessary number of colours stemming from the result of Vizing is proved to be sufficient for this family of graphs. Specifically, our main result states that every planar graph $G$ of maximum degree at least $28$ which contains no isolated edges admits a proper edge colouring $c:E\to\{1,2,\ldots,\Delta(G)+1\}$ such that $\sum_{e\ni u}c(e)\neq \sum_{e\ni v}c(e)$ for every edge $uv$ of $G$. ",neighbour sum distinguish index planar graph let proper edge colour graph integers ldots geq delta vizing theorem delta necessary construct course investigate irregularities graph moreover conjecture slightly larger delta enable enforce additional strong feature namely attribute distinct sum incident colour adjacent vertices graph isolate edge isomorphic prove conjecture valid planar graph sufficiently large maximum degree fact even stronger statement hold necessary number colour stem result vizing prove sufficient family graph specifically main result state every planar graph maximum degree least contain isolate edge admit proper edge colour ldots delta sum ni neq sum ni every edge uv,97,3,1408.3190.txt
http://arxiv.org/abs/1408.3310,An algorithm for canonical forms of finite subsets of $\mathbb{Z}^d$ up   to affinities,"  In this paper we describe an algorithm for the computation of canonical forms of finite subsets of $\mathbb{Z}^d$, up to affinities over $\mathbb{Z}$. For fixed dimension $d$, this algorithm has worst-case asymptotic complexity $O(n \log^2 n \, s\,\mu(s))$, where $n$ is the number of points in the given subset, $s$ is an upper bound to the size of the binary representation of any of the $n$ points, and $\mu(s)$ is an upper bound to the number of operations required to multiply two $s$-bit numbers. In particular, the problem is fixed-parameter tractable with respect to the dimension $d$. This problem arises e.g. in the context of computation of invariants of finitely presented groups with abelianized group isomorphic to $\mathbb{Z}^d$. In that context one needs to decide whether two Laurent polynomials in $d$ indeterminates, considered as elements of the group ring over the abelianized group, are equivalent with respect to a change of basis. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; Mathematics - Group Theory ; 52C07 ; ,"Paolini, Giovanni ; ","An algorithm for canonical forms of finite subsets of $\mathbb{Z}^d$ up   to affinities  In this paper we describe an algorithm for the computation of canonical forms of finite subsets of $\mathbb{Z}^d$, up to affinities over $\mathbb{Z}$. For fixed dimension $d$, this algorithm has worst-case asymptotic complexity $O(n \log^2 n \, s\,\mu(s))$, where $n$ is the number of points in the given subset, $s$ is an upper bound to the size of the binary representation of any of the $n$ points, and $\mu(s)$ is an upper bound to the number of operations required to multiply two $s$-bit numbers. In particular, the problem is fixed-parameter tractable with respect to the dimension $d$. This problem arises e.g. in the context of computation of invariants of finitely presented groups with abelianized group isomorphic to $\mathbb{Z}^d$. In that context one needs to decide whether two Laurent polynomials in $d$ indeterminates, considered as elements of the group ring over the abelianized group, are equivalent with respect to a change of basis. ",algorithm canonical form finite subsets mathbb affinities paper describe algorithm computation canonical form finite subsets mathbb affinities mathbb fix dimension algorithm worst case asymptotic complexity log mu number point give subset upper bind size binary representation point mu upper bind number operations require multiply two bite number particular problem fix parameter tractable respect dimension problem arise context computation invariants finitely present group abelianized group isomorphic mathbb context one need decide whether two laurent polynomials indeterminates consider elements group ring abelianized group equivalent respect change basis,85,4,1408.3310.txt
http://arxiv.org/abs/1408.3743,Parallel generator of $q$-valued pseudorandom sequences based on   arithmetic polynomials,"  A new method for parallel generation of $q$-valued pseudorandom sequence based on the presentation of systems generating logical formulae by means of arithmetic polynomials is proposed. Fragment consisting of $k$-elements of $q$-valued pseudorandom sequence may be obtained by means of single calculation of a single recursion numerical formula. It is mentioned that the method of the ""arithmetization"" of generation may be used and further developed in order to protect the encryption gears from cryptographic onset, resulting in the initiating of mass hardware failures. The achieved results may be widely applied to the realization of perspective high-performance cryptographic facilities for information protection. ","Computer Science - Cryptography and Security ; 94A55, 68W10, 03B50, 11A07, 11B50, 94A60 ; ","Finko, Oleg ; Samoylenko, Dmitriy ; Dichenko, Sergey ; Eliseev, Nikolay ; ","Parallel generator of $q$-valued pseudorandom sequences based on   arithmetic polynomials  A new method for parallel generation of $q$-valued pseudorandom sequence based on the presentation of systems generating logical formulae by means of arithmetic polynomials is proposed. Fragment consisting of $k$-elements of $q$-valued pseudorandom sequence may be obtained by means of single calculation of a single recursion numerical formula. It is mentioned that the method of the ""arithmetization"" of generation may be used and further developed in order to protect the encryption gears from cryptographic onset, resulting in the initiating of mass hardware failures. The achieved results may be widely applied to the realization of perspective high-performance cryptographic facilities for information protection. ",parallel generator value pseudorandom sequence base arithmetic polynomials new method parallel generation value pseudorandom sequence base presentation systems generate logical formulae mean arithmetic polynomials propose fragment consist elements value pseudorandom sequence may obtain mean single calculation single recursion numerical formula mention method arithmetization generation may use develop order protect encryption gear cryptographic onset result initiate mass hardware failures achieve result may widely apply realization perspective high performance cryptographic facilities information protection,71,11,1408.3743.txt
http://arxiv.org/abs/1408.3869,Treewidth of graphs with balanced separations,  We prove that if every subgraph of a graph $G$ has a balanced separation of order at most $a$ then $G$ has treewidth at most $15a$. This establishes a linear dependence between the treewidth and the separation number. ,Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Dvorak, Zdenek ; Norin, Sergey ; ",Treewidth of graphs with balanced separations  We prove that if every subgraph of a graph $G$ has a balanced separation of order at most $a$ then $G$ has treewidth at most $15a$. This establishes a linear dependence between the treewidth and the separation number. ,treewidth graph balance separations prove every subgraph graph balance separation order treewidth establish linear dependence treewidth separation number,18,3,1408.3869.txt
http://arxiv.org/abs/1408.3877,FESTUNG: A MATLAB / GNU Octave toolbox for the discontinuous Galerkin   method. Part I: Diffusion operator,"  This is the first in a series of papers on implementing a discontinuous Galerkin method as a MATLAB / GNU Octave toolbox. The main goal is the development of techniques that deliver optimized computational performance combined with a compact, user-friendly interface. Our implementation relies on fully vectorized matrix / vector operations and is carefully documented; in addition, a direct mapping between discretization terms and code routines is maintained throughout. The present work focuses on a two-dimensional time-dependent diffusion equation with space / time-varying coefficients. The spatial discretization is based on the local discontinuous Galerkin formulation and is locally mass conservative. Approximations of orders zero through four based on orthogonal polynomials have been implemented; more spaces of arbitrary type and order can be easily accommodated by the code structure. Time discretization is performed using an implicit Euler method. ","Mathematics - Numerical Analysis ; Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Numerical Analysis ; ","Frank, Florian ; Reuter, Balthasar ; Aizinger, Vadym ; Knabner, Peter ; ","FESTUNG: A MATLAB / GNU Octave toolbox for the discontinuous Galerkin   method. Part I: Diffusion operator  This is the first in a series of papers on implementing a discontinuous Galerkin method as a MATLAB / GNU Octave toolbox. The main goal is the development of techniques that deliver optimized computational performance combined with a compact, user-friendly interface. Our implementation relies on fully vectorized matrix / vector operations and is carefully documented; in addition, a direct mapping between discretization terms and code routines is maintained throughout. The present work focuses on a two-dimensional time-dependent diffusion equation with space / time-varying coefficients. The spatial discretization is based on the local discontinuous Galerkin formulation and is locally mass conservative. Approximations of orders zero through four based on orthogonal polynomials have been implemented; more spaces of arbitrary type and order can be easily accommodated by the code structure. Time discretization is performed using an implicit Euler method. ",festung matlab gnu octave toolbox discontinuous galerkin method part diffusion operator first series paper implement discontinuous galerkin method matlab gnu octave toolbox main goal development techniques deliver optimize computational performance combine compact user friendly interface implementation rely fully vectorized matrix vector operations carefully document addition direct map discretization term code routines maintain throughout present work focus two dimensional time dependent diffusion equation space time vary coefficients spatial discretization base local discontinuous galerkin formulation locally mass conservative approximations order zero four base orthogonal polynomials implement space arbitrary type order easily accommodate code structure time discretization perform use implicit euler method,99,11,1408.3877.txt
http://arxiv.org/abs/1408.3976,Static Analysis for Extracting Permission Checks of a Large Scale   Framework: The Challenges And Solutions for Analyzing Android,"  A common security architecture is based on the protection of certain resources by permission checks (used e.g., in Android and Blackberry). It has some limitations, for instance, when applications are granted more permissions than they actually need, which facilitates all kinds of malicious usage (e.g., through code injection). The analysis of permission-based framework requires a precise mapping between API methods of the framework and the permissions they require. In this paper, we show that naive static analysis fails miserably when applied with off-the-shelf components on the Android framework. We then present an advanced class-hierarchy and field-sensitive set of analyses to extract this mapping. Those static analyses are capable of analyzing the Android framework. They use novel domain specific optimizations dedicated to Android. ",Computer Science - Software Engineering ; ,"Bartel, Alexandre ; Klein, Jacques ; Monperrus, Martin ; Traon, Yves Le ; ","Static Analysis for Extracting Permission Checks of a Large Scale   Framework: The Challenges And Solutions for Analyzing Android  A common security architecture is based on the protection of certain resources by permission checks (used e.g., in Android and Blackberry). It has some limitations, for instance, when applications are granted more permissions than they actually need, which facilitates all kinds of malicious usage (e.g., through code injection). The analysis of permission-based framework requires a precise mapping between API methods of the framework and the permissions they require. In this paper, we show that naive static analysis fails miserably when applied with off-the-shelf components on the Android framework. We then present an advanced class-hierarchy and field-sensitive set of analyses to extract this mapping. Those static analyses are capable of analyzing the Android framework. They use novel domain specific optimizations dedicated to Android. ",static analysis extract permission check large scale framework challenge solutions analyze android common security architecture base protection certain resources permission check use android blackberry limitations instance applications grant permissions actually need facilitate kinds malicious usage code injection analysis permission base framework require precise map api methods framework permissions require paper show naive static analysis fail miserably apply shelf components android framework present advance class hierarchy field sensitive set analyse extract map static analyse capable analyze android framework use novel domain specific optimizations dedicate android,84,9,1408.3976.txt
http://arxiv.org/abs/1408.4528,Laplace Functional Ordering of Point Processes in Large-scale Wireless   Networks,"  Stochastic orders on point processes are partial orders which capture notions like being larger or more variable. Laplace functional ordering of point processes is a useful stochastic order for comparing spatial deployments of wireless networks. It is shown that the ordering of point processes is preserved under independent operations such as marking, thinning, clustering, superposition, and random translation. Laplace functional ordering can be used to establish comparisons of several performance metrics such as coverage probability, achievable rate, and resource allocation even when closed form expressions of such metrics are unavailable. Applications in several network scenarios are also provided where tradeoffs between coverage and interference as well as fairness and peakyness are studied. Monte-Carlo simulations are used to supplement our analytical results. ",Computer Science - Information Theory ; Computer Science - Networking and Internet Architecture ; ,"Lee, Junghoon ; Tepedelenlioglu, Cihan ; ","Laplace Functional Ordering of Point Processes in Large-scale Wireless   Networks  Stochastic orders on point processes are partial orders which capture notions like being larger or more variable. Laplace functional ordering of point processes is a useful stochastic order for comparing spatial deployments of wireless networks. It is shown that the ordering of point processes is preserved under independent operations such as marking, thinning, clustering, superposition, and random translation. Laplace functional ordering can be used to establish comparisons of several performance metrics such as coverage probability, achievable rate, and resource allocation even when closed form expressions of such metrics are unavailable. Applications in several network scenarios are also provided where tradeoffs between coverage and interference as well as fairness and peakyness are studied. Monte-Carlo simulations are used to supplement our analytical results. ",laplace functional order point process large scale wireless network stochastic order point process partial order capture notions like larger variable laplace functional order point process useful stochastic order compare spatial deployments wireless network show order point process preserve independent operations mark thin cluster superposition random translation laplace functional order use establish comparisons several performance metrics coverage probability achievable rate resource allocation even close form expressions metrics unavailable applications several network scenarios also provide tradeoffs coverage interference well fairness peakyness study monte carlo simulations use supplement analytical result,87,6,1408.4528.txt
http://arxiv.org/abs/1408.6321,Crossing Minimization for 1-page and 2-page Drawings of Graphs with   Bounded Treewidth,"  We investigate crossing minimization for 1-page and 2-page book drawings. We show that computing the 1-page crossing number is fixed-parameter tractable with respect to the number of crossings, that testing 2-page planarity is fixed-parameter tractable with respect to treewidth, and that computing the 2-page crossing number is fixed-parameter tractable with respect to the sum of the number of crossings and the treewidth of the input graph. We prove these results via Courcelle's theorem on the fixed-parameter tractability of properties expressible in monadic second order logic for graphs of bounded treewidth. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Bannister, Michael J. ; Eppstein, David ; ","Crossing Minimization for 1-page and 2-page Drawings of Graphs with   Bounded Treewidth  We investigate crossing minimization for 1-page and 2-page book drawings. We show that computing the 1-page crossing number is fixed-parameter tractable with respect to the number of crossings, that testing 2-page planarity is fixed-parameter tractable with respect to treewidth, and that computing the 2-page crossing number is fixed-parameter tractable with respect to the sum of the number of crossings and the treewidth of the input graph. We prove these results via Courcelle's theorem on the fixed-parameter tractability of properties expressible in monadic second order logic for graphs of bounded treewidth. ",cross minimization page page draw graph bound treewidth investigate cross minimization page page book draw show compute page cross number fix parameter tractable respect number cross test page planarity fix parameter tractable respect treewidth compute page cross number fix parameter tractable respect sum number cross treewidth input graph prove result via courcelle theorem fix parameter tractability properties expressible monadic second order logic graph bound treewidth,65,3,1408.6321.txt
http://arxiv.org/abs/1408.6771,Flat Foldings of Plane Graphs with Prescribed Angles and Edge Lengths,"  When can a plane graph with prescribed edge lengths and prescribed angles (from among $\{0,180^\circ, 360^\circ$\}) be folded flat to lie in an infinitesimally thin line, without crossings? This problem generalizes the classic theory of single-vertex flat origami with prescribed mountain-valley assignment, which corresponds to the case of a cycle graph. We characterize such flat-foldable plane graphs by two obviously necessary but also sufficient conditions, proving a conjecture made in 2001: the angles at each vertex should sum to $360^\circ$, and every face of the graph must itself be flat foldable. This characterization leads to a linear-time algorithm for testing flat foldability of plane graphs with prescribed edge lengths and angles, and a polynomial-time algorithm for counting the number of distinct folded states. ",Computer Science - Computational Geometry ; Computer Science - Data Structures and Algorithms ; F.2.2 ; ,"Abel, Zachary ; Demaine, Erik D. ; Demaine, Martin L. ; Eppstein, David ; Lubiw, Anna ; Uehara, Ryuhei ; ","Flat Foldings of Plane Graphs with Prescribed Angles and Edge Lengths  When can a plane graph with prescribed edge lengths and prescribed angles (from among $\{0,180^\circ, 360^\circ$\}) be folded flat to lie in an infinitesimally thin line, without crossings? This problem generalizes the classic theory of single-vertex flat origami with prescribed mountain-valley assignment, which corresponds to the case of a cycle graph. We characterize such flat-foldable plane graphs by two obviously necessary but also sufficient conditions, proving a conjecture made in 2001: the angles at each vertex should sum to $360^\circ$, and every face of the graph must itself be flat foldable. This characterization leads to a linear-time algorithm for testing flat foldability of plane graphs with prescribed edge lengths and angles, and a polynomial-time algorithm for counting the number of distinct folded states. ",flat fold plane graph prescribe angle edge lengths plane graph prescribe edge lengths prescribe angle among circ circ fold flat lie infinitesimally thin line without cross problem generalize classic theory single vertex flat origami prescribe mountain valley assignment correspond case cycle graph characterize flat foldable plane graph two obviously necessary also sufficient condition prove conjecture make angle vertex sum circ every face graph must flat foldable characterization lead linear time algorithm test flat foldability plane graph prescribe edge lengths angle polynomial time algorithm count number distinct fold state,88,3,1408.6771.txt
http://arxiv.org/abs/1408.6923,GPGPU Computing,"  Since the first idea of using GPU to general purpose computing, things have evolved over the years and now there are several approaches to GPU programming. GPU computing practically began with the introduction of CUDA (Compute Unified Device Architecture) by NVIDIA and Stream by AMD. These are APIs designed by the GPU vendors to be used together with the hardware that they provide. A new emerging standard, OpenCL (Open Computing Language) tries to unify different GPU general computing API implementations and provides a framework for writing programs executed across heterogeneous platforms consisting of both CPUs and GPUs. OpenCL provides parallel computing using task-based and data-based parallelism. In this paper we will focus on the CUDA parallel computing architecture and programming model introduced by NVIDIA. We will present the benefits of the CUDA programming model. We will also compare the two main approaches, CUDA and AMD APP (STREAM) and the new framwork, OpenCL that tries to unify the GPGPU computing models. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Oancea, Bogdan ; Andrei, Tudorel ; Dragoescu, Raluca Mariana ; ","GPGPU Computing  Since the first idea of using GPU to general purpose computing, things have evolved over the years and now there are several approaches to GPU programming. GPU computing practically began with the introduction of CUDA (Compute Unified Device Architecture) by NVIDIA and Stream by AMD. These are APIs designed by the GPU vendors to be used together with the hardware that they provide. A new emerging standard, OpenCL (Open Computing Language) tries to unify different GPU general computing API implementations and provides a framework for writing programs executed across heterogeneous platforms consisting of both CPUs and GPUs. OpenCL provides parallel computing using task-based and data-based parallelism. In this paper we will focus on the CUDA parallel computing architecture and programming model introduced by NVIDIA. We will present the benefits of the CUDA programming model. We will also compare the two main approaches, CUDA and AMD APP (STREAM) and the new framwork, OpenCL that tries to unify the GPGPU computing models. ",gpgpu compute since first idea use gpu general purpose compute things evolve years several approach gpu program gpu compute practically begin introduction cuda compute unify device architecture nvidia stream amd apis design gpu vendors use together hardware provide new emerge standard opencl open compute language try unify different gpu general compute api implementations provide framework write program execute across heterogeneous platforms consist cpus gpus opencl provide parallel compute use task base data base parallelism paper focus cuda parallel compute architecture program model introduce nvidia present benefit cuda program model also compare two main approach cuda amd app stream new framwork opencl try unify gpgpu compute model,106,4,1408.6923.txt
http://arxiv.org/abs/1409.0264,Nash Equilbria for Quadratic Voting,"  Voters making a binary decision purchase votes from a centralized clearing house, paying the square of the number of votes purchased. The net payoff to an agent with utility $u$ who purchases $v$ votes is $\Psi (S_{n+1})u-v^{2}$, where $\Psi$ is a monotone function taking values between -1 and +1 and $S_{n+1}$ is the sum of all votes purchased by the $n+1$ voters participating in the election. The utilities of the voters are assumed to arise by random sampling from a probability distribution $F_{U}$ with compact support; each voter knows her own utility, but not those of the other voters, although she does know the sampling distribution $F_{U}$. Nash equilibria for this game are described. These results imply that the expected inefficiency of any Nash equilibrium decays like $1/n$. ","Computer Science - Computer Science and Game Theory ; Mathematics - Probability ; 91B12 (Primary), 91B52, 60F99 (Secondary) ; ","Lalley, Steven P. ; Weyl, E. Glen ; ","Nash Equilbria for Quadratic Voting  Voters making a binary decision purchase votes from a centralized clearing house, paying the square of the number of votes purchased. The net payoff to an agent with utility $u$ who purchases $v$ votes is $\Psi (S_{n+1})u-v^{2}$, where $\Psi$ is a monotone function taking values between -1 and +1 and $S_{n+1}$ is the sum of all votes purchased by the $n+1$ voters participating in the election. The utilities of the voters are assumed to arise by random sampling from a probability distribution $F_{U}$ with compact support; each voter knows her own utility, but not those of the other voters, although she does know the sampling distribution $F_{U}$. Nash equilibria for this game are described. These results imply that the expected inefficiency of any Nash equilibrium decays like $1/n$. ",nash equilbria quadratic vote voters make binary decision purchase vote centralize clear house pay square number vote purchase net payoff agent utility purchase vote psi psi monotone function take value sum vote purchase voters participate election utilities voters assume arise random sample probability distribution compact support voter know utility voters although know sample distribution nash equilibria game describe result imply expect inefficiency nash equilibrium decay like,66,12,1409.0264.txt
http://arxiv.org/abs/1409.0375,Polynomial solvability of $NP$-complete problems,"  ${ NP}$-complete problem ""Hamiltonian cycle""\ for graph $G=(V,E)$ is extended to the ""Hamiltonian Complement of the Graph""\ problem of finding the minimal cardinality set $H$ containing additional edges so that graph $G=(V,E\cup H)$ is Hamiltonian. The solving of ""Hamiltonian Complement of a Graph""\ problem is reduced to the linear programming problem {\bf P}, which has an optimal integer solution. The optimal integer solution of {\bf P} is found for any its optimal solution by solving the linear assignment problem {\bf L}. The existence of polynomial algorithms for problems {\bf P} and {\bf L} proves the polynomial solvability of ${ NP}$-complete problems. ",Computer Science - Computational Complexity ; 05C85 ; ,"Panyukov, Anatoly ; ","Polynomial solvability of $NP$-complete problems  ${ NP}$-complete problem ""Hamiltonian cycle""\ for graph $G=(V,E)$ is extended to the ""Hamiltonian Complement of the Graph""\ problem of finding the minimal cardinality set $H$ containing additional edges so that graph $G=(V,E\cup H)$ is Hamiltonian. The solving of ""Hamiltonian Complement of a Graph""\ problem is reduced to the linear programming problem {\bf P}, which has an optimal integer solution. The optimal integer solution of {\bf P} is found for any its optimal solution by solving the linear assignment problem {\bf L}. The existence of polynomial algorithms for problems {\bf P} and {\bf L} proves the polynomial solvability of ${ NP}$-complete problems. ",polynomial solvability np complete problems np complete problem hamiltonian cycle graph extend hamiltonian complement graph problem find minimal cardinality set contain additional edge graph cup hamiltonian solve hamiltonian complement graph problem reduce linear program problem bf optimal integer solution optimal integer solution bf find optimal solution solve linear assignment problem bf existence polynomial algorithms problems bf bf prove polynomial solvability np complete problems,63,3,1409.0375.txt
http://arxiv.org/abs/1409.1467,Evaluation of Position-related Information in Multipath Components for   Indoor Positioning,"  Location awareness is a key factor for a wealth of wireless indoor applications. Its provision requires the careful fusion of diverse information sources. For agents that use radio signals for localization, this information may either come from signal transmissions with respect to fixed anchors, from cooperative transmissions inbetween agents, or from radar-like monostatic transmissions. Using a-priori knowledge of a floor plan of the environment, specular multipath components can be exploited, based on a geometric-stochastic channel model. In this paper, a unified framework is presented for the quantification of this type of position-related information, using the concept of equivalent Fisher information. We derive analytical results for the Cram\'er-Rao lower bound of multipath-assisted positioning, considering bistatic transmissions between agents and fixed anchors, monostatic transmissions from agents, cooperative measurements inbetween agents, and combinations thereof, including the effect of clock offsets. Awareness of this information enables highly accurate and robust indoor positioning. Computational results show the applicability of the framework for the characterization of the localization capabilities of a given environment, quantifying the influence of different system setups, signal parameters, and the impact of path overlap. ",Computer Science - Information Theory ; ,"Leitinger, Erik ; Meissner, Paul ; Rüdisser, Christoph ; Dumphart, Gregor ; Witrisal, Klaus ; ","Evaluation of Position-related Information in Multipath Components for   Indoor Positioning  Location awareness is a key factor for a wealth of wireless indoor applications. Its provision requires the careful fusion of diverse information sources. For agents that use radio signals for localization, this information may either come from signal transmissions with respect to fixed anchors, from cooperative transmissions inbetween agents, or from radar-like monostatic transmissions. Using a-priori knowledge of a floor plan of the environment, specular multipath components can be exploited, based on a geometric-stochastic channel model. In this paper, a unified framework is presented for the quantification of this type of position-related information, using the concept of equivalent Fisher information. We derive analytical results for the Cram\'er-Rao lower bound of multipath-assisted positioning, considering bistatic transmissions between agents and fixed anchors, monostatic transmissions from agents, cooperative measurements inbetween agents, and combinations thereof, including the effect of clock offsets. Awareness of this information enables highly accurate and robust indoor positioning. Computational results show the applicability of the framework for the characterization of the localization capabilities of a given environment, quantifying the influence of different system setups, signal parameters, and the impact of path overlap. ",evaluation position relate information multipath components indoor position location awareness key factor wealth wireless indoor applications provision require careful fusion diverse information source agents use radio signal localization information may either come signal transmissions respect fix anchor cooperative transmissions inbetween agents radar like monostatic transmissions use priori knowledge floor plan environment specular multipath components exploit base geometric stochastic channel model paper unify framework present quantification type position relate information use concept equivalent fisher information derive analytical result cram er rao lower bind multipath assist position consider bistatic transmissions agents fix anchor monostatic transmissions agents cooperative measurements inbetween agents combinations thereof include effect clock offset awareness information enable highly accurate robust indoor position computational result show applicability framework characterization localization capabilities give environment quantify influence different system setups signal parameters impact path overlap,132,9,1409.1467.txt
http://arxiv.org/abs/1409.1714,A level set based method for fixing overhangs in 3D printing,"  3D printers based on the Fused Decomposition Modeling create objects layer-by-layer dropping fused material. As a consequence, strong overhangs cannot be printed because the new-come material does not find a suitable support over the last deposed layer. In these cases, one can add some support structures (scaffolds) which make the object printable, to be removed at the end. In this paper we propose a level set method to create object-dependent support structures, specifically conceived to reduce both the amount of additional material and the printing time. We also review some open problems about 3D printing which can be of interests for the mathematical community. ","Mathematics - Numerical Analysis ; Computer Science - Graphics ; 65D17, 35F21 ; ","Cacace, Simone ; Cristiani, Emiliano ; Rocchi, Leonardo ; ","A level set based method for fixing overhangs in 3D printing  3D printers based on the Fused Decomposition Modeling create objects layer-by-layer dropping fused material. As a consequence, strong overhangs cannot be printed because the new-come material does not find a suitable support over the last deposed layer. In these cases, one can add some support structures (scaffolds) which make the object printable, to be removed at the end. In this paper we propose a level set method to create object-dependent support structures, specifically conceived to reduce both the amount of additional material and the printing time. We also review some open problems about 3D printing which can be of interests for the mathematical community. ",level set base method fix overhang print printers base fuse decomposition model create object layer layer drop fuse material consequence strong overhang cannot print new come material find suitable support last depose layer case one add support structure scaffold make object printable remove end paper propose level set method create object dependent support structure specifically conceive reduce amount additional material print time also review open problems print interest mathematical community,70,11,1409.1714.txt
http://arxiv.org/abs/1409.2193,An Epistemic Strategy Logic,"  This paper presents an extension of temporal epistemic logic with operators that quantify over agent strategies. Unlike previous work on alternating temporal epistemic logic, the semantics works with systems whose states explicitly encode the strategy being used by each of the agents. This provides a natural way to express what agents would know were they to be aware of some of the strategies being used by other agents. A number of examples that rely upon the ability to express an agent's knowledge about the strategies being used by other agents are presented to motivate the framework, including reasoning about game theoretic equilibria, knowledge-based programs, and information theoretic computer security policies. Relationships to several variants of alternating temporal epistemic logic are discussed. The computational complexity of model checking the logic and several of its fragments are also characterized. ",Computer Science - Logic in Computer Science ; ,"Huang, Xiaowei ; van der Meyden, Ron ; ","An Epistemic Strategy Logic  This paper presents an extension of temporal epistemic logic with operators that quantify over agent strategies. Unlike previous work on alternating temporal epistemic logic, the semantics works with systems whose states explicitly encode the strategy being used by each of the agents. This provides a natural way to express what agents would know were they to be aware of some of the strategies being used by other agents. A number of examples that rely upon the ability to express an agent's knowledge about the strategies being used by other agents are presented to motivate the framework, including reasoning about game theoretic equilibria, knowledge-based programs, and information theoretic computer security policies. Relationships to several variants of alternating temporal epistemic logic are discussed. The computational complexity of model checking the logic and several of its fragments are also characterized. ",epistemic strategy logic paper present extension temporal epistemic logic operators quantify agent strategies unlike previous work alternate temporal epistemic logic semantics work systems whose state explicitly encode strategy use agents provide natural way express agents would know aware strategies use agents number examples rely upon ability express agent knowledge strategies use agents present motivate framework include reason game theoretic equilibria knowledge base program information theoretic computer security policies relationships several variants alternate temporal epistemic logic discuss computational complexity model check logic several fragment also characterize,85,8,1409.2193.txt
http://arxiv.org/abs/1409.2248,Secure pseudo-random linear binary sequences generators based on   arithmetic polynoms,"  We present a new approach to constructing of pseudo-random binary sequences (PRS) generators for the purpose of cryptographic data protection, secured from the perpetrator's attacks, caused by generation of masses of hardware errors and faults. The new method is based on use of linear polynomial arithmetic for the realization of systems of boolean characteristic functions of PRS' generators. ""Arithmetizatio"" of systems of logic formulas has allowed to apply mathematical apparatus of residue systems for multisequencing of the process of PRS generation and organizing control of computing errors, caused by hardware faults. This has guaranteed high security of PRS generator's functioning and, consequently, security of tools for cryptographic data protection based on those PRSs. ","Computer Science - Cryptography and Security ; 94C10, 94A60, 11K45, 11A07 ; ","Finko, Oleg ; Dichenko, Sergey ; ","Secure pseudo-random linear binary sequences generators based on   arithmetic polynoms  We present a new approach to constructing of pseudo-random binary sequences (PRS) generators for the purpose of cryptographic data protection, secured from the perpetrator's attacks, caused by generation of masses of hardware errors and faults. The new method is based on use of linear polynomial arithmetic for the realization of systems of boolean characteristic functions of PRS' generators. ""Arithmetizatio"" of systems of logic formulas has allowed to apply mathematical apparatus of residue systems for multisequencing of the process of PRS generation and organizing control of computing errors, caused by hardware faults. This has guaranteed high security of PRS generator's functioning and, consequently, security of tools for cryptographic data protection based on those PRSs. ",secure pseudo random linear binary sequence generators base arithmetic polynoms present new approach construct pseudo random binary sequence prs generators purpose cryptographic data protection secure perpetrator attack cause generation mass hardware errors fault new method base use linear polynomial arithmetic realization systems boolean characteristic function prs generators arithmetizatio systems logic formulas allow apply mathematical apparatus residue systems multisequencing process prs generation organize control compute errors cause hardware fault guarantee high security prs generator function consequently security tool cryptographic data protection base prss,82,11,1409.2248.txt
http://arxiv.org/abs/1409.2612,A simple proof of the completeness of APAL,  We provide a simple proof of the completeness of arbitrary public announcement logic APAL. The proof is an improvement over the proof found in the publication Knowable as Known after an Announcement. ,Computer Science - Logic in Computer Science ; ,"Balbiani, Philippe ; van Ditmarsch, Hans ; ",A simple proof of the completeness of APAL  We provide a simple proof of the completeness of arbitrary public announcement logic APAL. The proof is an improvement over the proof found in the publication Knowable as Known after an Announcement. ,simple proof completeness apal provide simple proof completeness arbitrary public announcement logic apal proof improvement proof find publication knowable know announcement,21,8,1409.2612.txt
http://arxiv.org/abs/1409.2908,A Framework for Practical Parallel Fast Matrix Multiplication,"  Matrix multiplication is a fundamental computation in many scientific disciplines. In this paper, we show that novel fast matrix multiplication algorithms can significantly outperform vendor implementations of the classical algorithm and Strassen's fast algorithm on modest problem sizes and shapes. Furthermore, we show that the best choice of fast algorithm depends not only on the size of the matrices but also the shape. We develop a code generation tool to automatically implement multiple sequential and shared-memory parallel variants of each fast algorithm, including our novel parallelization scheme. This allows us to rapidly benchmark over 20 fast algorithms on several problem sizes. Furthermore, we discuss a number of practical implementation issues for these algorithms on shared-memory machines that can direct further research on making fast algorithms practical. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Mathematical Software ; Computer Science - Numerical Analysis ; G.4 ; ","Benson, Austin R. ; Ballard, Grey ; ","A Framework for Practical Parallel Fast Matrix Multiplication  Matrix multiplication is a fundamental computation in many scientific disciplines. In this paper, we show that novel fast matrix multiplication algorithms can significantly outperform vendor implementations of the classical algorithm and Strassen's fast algorithm on modest problem sizes and shapes. Furthermore, we show that the best choice of fast algorithm depends not only on the size of the matrices but also the shape. We develop a code generation tool to automatically implement multiple sequential and shared-memory parallel variants of each fast algorithm, including our novel parallelization scheme. This allows us to rapidly benchmark over 20 fast algorithms on several problem sizes. Furthermore, we discuss a number of practical implementation issues for these algorithms on shared-memory machines that can direct further research on making fast algorithms practical. ",framework practical parallel fast matrix multiplication matrix multiplication fundamental computation many scientific discipline paper show novel fast matrix multiplication algorithms significantly outperform vendor implementations classical algorithm strassen fast algorithm modest problem size shape furthermore show best choice fast algorithm depend size matrices also shape develop code generation tool automatically implement multiple sequential share memory parallel variants fast algorithm include novel parallelization scheme allow us rapidly benchmark fast algorithms several problem size furthermore discuss number practical implementation issue algorithms share memory machine direct research make fast algorithms practical,87,4,1409.2908.txt
http://arxiv.org/abs/1409.3176,Test Case Purification for Improving Fault Localization,"  Finding and fixing bugs are time-consuming activities in software development. Spectrum-based fault localization aims to identify the faulty position in source code based on the execution trace of test cases. Failing test cases and their assertions form test oracles for the failing behavior of the system under analysis. In this paper, we propose a novel concept of spectrum driven test case purification for improving fault localization. The goal of test case purification is to separate existing test cases into small fractions (called purified test cases) and to enhance the test oracles to further localize faults. Combining with an original fault localization technique (e.g., Tarantula), test case purification results in better ranking the program statements. Our experiments on 1800 faults in six open-source Java programs show that test case purification can effectively improve existing fault localization techniques. ",Computer Science - Software Engineering ; ,"Xuan, Jifeng ; Monperrus, Martin ; ","Test Case Purification for Improving Fault Localization  Finding and fixing bugs are time-consuming activities in software development. Spectrum-based fault localization aims to identify the faulty position in source code based on the execution trace of test cases. Failing test cases and their assertions form test oracles for the failing behavior of the system under analysis. In this paper, we propose a novel concept of spectrum driven test case purification for improving fault localization. The goal of test case purification is to separate existing test cases into small fractions (called purified test cases) and to enhance the test oracles to further localize faults. Combining with an original fault localization technique (e.g., Tarantula), test case purification results in better ranking the program statements. Our experiments on 1800 faults in six open-source Java programs show that test case purification can effectively improve existing fault localization techniques. ",test case purification improve fault localization find fix bug time consume activities software development spectrum base fault localization aim identify faulty position source code base execution trace test case fail test case assertions form test oracles fail behavior system analysis paper propose novel concept spectrum drive test case purification improve fault localization goal test case purification separate exist test case small fraction call purify test case enhance test oracles localize fault combine original fault localization technique tarantula test case purification result better rank program statements experiment fault six open source java program show test case purification effectively improve exist fault localization techniques,102,12,1409.3176.txt
http://arxiv.org/abs/1409.3562,Strong converse exponent for classical-quantum channel coding,"  We determine the exact strong converse exponent of classical-quantum channel coding, for every rate above the Holevo capacity. Our form of the exponent is an exact analogue of Arimoto's, given as a transform of the Renyi capacities with parameters alpha>1. It is important to note that, unlike in the classical case, there are many inequivalent ways to define the Renyi divergence of states, and hence the R\'enyi capacities of channels. Our exponent is in terms of the Renyi capacities corresponding to a version of the Renyi divergences that has been introduced recently in [M\""uller-Lennert, Dupuis, Szehr, Fehr and Tomamichel, J. Math. Phys. 54, 122203, (2013)], and [Wilde, Winter, Yang, Commun. Math. Phys. 331, (2014)]. Our result adds to the growing body of evidence that this new version is the natural definition for the purposes of strong converse problems. ",Quantum Physics ; Computer Science - Information Theory ; Mathematical Physics ; ,"Mosonyi, Milan ; Ogawa, Tomohiro ; ","Strong converse exponent for classical-quantum channel coding  We determine the exact strong converse exponent of classical-quantum channel coding, for every rate above the Holevo capacity. Our form of the exponent is an exact analogue of Arimoto's, given as a transform of the Renyi capacities with parameters alpha>1. It is important to note that, unlike in the classical case, there are many inequivalent ways to define the Renyi divergence of states, and hence the R\'enyi capacities of channels. Our exponent is in terms of the Renyi capacities corresponding to a version of the Renyi divergences that has been introduced recently in [M\""uller-Lennert, Dupuis, Szehr, Fehr and Tomamichel, J. Math. Phys. 54, 122203, (2013)], and [Wilde, Winter, Yang, Commun. Math. Phys. 331, (2014)]. Our result adds to the growing body of evidence that this new version is the natural definition for the purposes of strong converse problems. ",strong converse exponent classical quantum channel cod determine exact strong converse exponent classical quantum channel cod every rate holevo capacity form exponent exact analogue arimoto give transform renyi capacities parameters alpha important note unlike classical case many inequivalent ways define renyi divergence state hence enyi capacities channel exponent term renyi capacities correspond version renyi divergences introduce recently uller lennert dupuis szehr fehr tomamichel math phys wilde winter yang commun math phys result add grow body evidence new version natural definition purpose strong converse problems,84,5,1409.3562.txt
http://arxiv.org/abs/1409.3600,Select with Small Groups,"  We revisit the selection problem, namely that of computing the $i$th order statistic of $n$ given elements, in particular the classical deterministic algorithm by grouping and partition due to Blum, Floyd, Pratt, Rivest, and Tarjan (1973). While the original algorithm uses groups of odd size at least $5$ and runs in linear time, it has been perpetuated in the literature that using smaller group sizes will force the worst-case running time to become superlinear, namely $\Omega(n \log{n})$. We first point out that the arguments existent in the literature justifying the superlinear worst-case running time fall short of proving this claim. We further prove that it is possible to use group size smaller than $5$ while maintaining the worst case linear running time. To this end we introduce three simple variants of the classical algorithm, the repeated step algorithm, the shifting target algorithm, and the hyperpair algorithm, all running in linear time. ",Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; ,"Chen, Ke ; Dumitrescu, Adrian ; ","Select with Small Groups  We revisit the selection problem, namely that of computing the $i$th order statistic of $n$ given elements, in particular the classical deterministic algorithm by grouping and partition due to Blum, Floyd, Pratt, Rivest, and Tarjan (1973). While the original algorithm uses groups of odd size at least $5$ and runs in linear time, it has been perpetuated in the literature that using smaller group sizes will force the worst-case running time to become superlinear, namely $\Omega(n \log{n})$. We first point out that the arguments existent in the literature justifying the superlinear worst-case running time fall short of proving this claim. We further prove that it is possible to use group size smaller than $5$ while maintaining the worst case linear running time. To this end we introduce three simple variants of the classical algorithm, the repeated step algorithm, the shifting target algorithm, and the hyperpair algorithm, all running in linear time. ",select small group revisit selection problem namely compute th order statistic give elements particular classical deterministic algorithm group partition due blum floyd pratt rivest tarjan original algorithm use group odd size least run linear time perpetuate literature use smaller group size force worst case run time become superlinear namely omega log first point arguments existent literature justify superlinear worst case run time fall short prove claim prove possible use group size smaller maintain worst case linear run time end introduce three simple variants classical algorithm repeat step algorithm shift target algorithm hyperpair algorithm run linear time,96,1,1409.3600.txt
http://arxiv.org/abs/1409.3954,MIMO-MC Radar: A MIMO Radar Approach Based on Matrix Completion,"  In a typical MIMO radar scenario, transmit nodes transmit orthogonal waveforms, while each receive node performs matched filtering with the known set of transmit waveforms, and forwards the results to the fusion center. Based on the data it receives from multiple antennas, the fusion center formulates a matrix, which, in conjunction with standard array processing schemes, such as MUSIC, leads to target detection and parameter estimation. In MIMO radars with compressive sensing (MIMO-CS), the data matrix is formulated by each receive node forwarding a small number of compressively obtained samples. In this paper, it is shown that under certain conditions, in both sampling cases, the data matrix at the fusion center is low-rank, and thus can be recovered based on knowledge of a small subset of its entries via matrix completion (MC) techniques. Leveraging the low-rank property of that matrix, we propose a new MIMO radar approach, termed, MIMO-MC radar, in which each receive node either performs matched filtering with a small number of randomly selected dictionary waveforms or obtains sub-Nyquist samples of the received signal at random sampling instants, and forwards the results to a fusion center. Based on the received samples, and with knowledge of the sampling scheme, the fusion center partially fills the data matrix and subsequently applies MC techniques to estimate the full matrix. MIMO-MC radars share the advantages of the recently proposed MIMO-CS radars, i.e., high resolution with reduced amounts of data, but unlike MIMO-CS radars do not require grid discretization. The MIMO-MC radar concept is illustrated through a linear uniform array configuration, and its target estimation performance is demonstrated via simulations. ",Computer Science - Information Theory ; Statistics - Applications ; ,"Sun, Shunqiao ; Bajwa, Waheed U. ; Petropulu, Athina P. ; ","MIMO-MC Radar: A MIMO Radar Approach Based on Matrix Completion  In a typical MIMO radar scenario, transmit nodes transmit orthogonal waveforms, while each receive node performs matched filtering with the known set of transmit waveforms, and forwards the results to the fusion center. Based on the data it receives from multiple antennas, the fusion center formulates a matrix, which, in conjunction with standard array processing schemes, such as MUSIC, leads to target detection and parameter estimation. In MIMO radars with compressive sensing (MIMO-CS), the data matrix is formulated by each receive node forwarding a small number of compressively obtained samples. In this paper, it is shown that under certain conditions, in both sampling cases, the data matrix at the fusion center is low-rank, and thus can be recovered based on knowledge of a small subset of its entries via matrix completion (MC) techniques. Leveraging the low-rank property of that matrix, we propose a new MIMO radar approach, termed, MIMO-MC radar, in which each receive node either performs matched filtering with a small number of randomly selected dictionary waveforms or obtains sub-Nyquist samples of the received signal at random sampling instants, and forwards the results to a fusion center. Based on the received samples, and with knowledge of the sampling scheme, the fusion center partially fills the data matrix and subsequently applies MC techniques to estimate the full matrix. MIMO-MC radars share the advantages of the recently proposed MIMO-CS radars, i.e., high resolution with reduced amounts of data, but unlike MIMO-CS radars do not require grid discretization. The MIMO-MC radar concept is illustrated through a linear uniform array configuration, and its target estimation performance is demonstrated via simulations. ",mimo mc radar mimo radar approach base matrix completion typical mimo radar scenario transmit nod transmit orthogonal waveforms receive node perform match filter know set transmit waveforms forward result fusion center base data receive multiple antennas fusion center formulate matrix conjunction standard array process scheme music lead target detection parameter estimation mimo radars compressive sense mimo cs data matrix formulate receive node forward small number compressively obtain sample paper show certain condition sample case data matrix fusion center low rank thus recover base knowledge small subset entries via matrix completion mc techniques leverage low rank property matrix propose new mimo radar approach term mimo mc radar receive node either perform match filter small number randomly select dictionary waveforms obtain sub nyquist sample receive signal random sample instants forward result fusion center base receive sample knowledge sample scheme fusion center partially fill data matrix subsequently apply mc techniques estimate full matrix mimo mc radars share advantage recently propose mimo cs radars high resolution reduce amount data unlike mimo cs radars require grid discretization mimo mc radar concept illustrate linear uniform array configuration target estimation performance demonstrate via simulations,187,12,1409.3954.txt
http://arxiv.org/abs/1409.4575,Stable Cosparse Recovery via \ell_p-analysis Optimization,"  In this paper we study the $\ell_p$-analysis optimization ($0<p\leq1$) problem for cosparse signal recovery. We establish a bound for recovery error via the restricted $p$-isometry property over any subspace. We further prove that the nonconvex $\ell_q$-analysis optimization can do recovery with a lower sample complexity and in a wider range of cosparsity than its convex counterpart. In addition, we develop an iteratively reweighted method to solve the optimization problem under a variational framework. Empirical results of preliminary computational experiments illustrate that the nonconvex method outperforms its convex counterpart. ",Computer Science - Information Theory ; ,"Zhang, Shubao ; Qian, Hui ; Gong, Xiaojin ; Zhou, Jianying ; ","Stable Cosparse Recovery via \ell_p-analysis Optimization  In this paper we study the $\ell_p$-analysis optimization ($0<p\leq1$) problem for cosparse signal recovery. We establish a bound for recovery error via the restricted $p$-isometry property over any subspace. We further prove that the nonconvex $\ell_q$-analysis optimization can do recovery with a lower sample complexity and in a wider range of cosparsity than its convex counterpart. In addition, we develop an iteratively reweighted method to solve the optimization problem under a variational framework. Empirical results of preliminary computational experiments illustrate that the nonconvex method outperforms its convex counterpart. ",stable cosparse recovery via ell analysis optimization paper study ell analysis optimization leq problem cosparse signal recovery establish bind recovery error via restrict isometry property subspace prove nonconvex ell analysis optimization recovery lower sample complexity wider range cosparsity convex counterpart addition develop iteratively reweighted method solve optimization problem variational framework empirical result preliminary computational experiment illustrate nonconvex method outperform convex counterpart,61,9,1409.4575.txt
http://arxiv.org/abs/1409.4711,Doing-it-All with Bounded Work and Communication,"  We consider the Do-All problem, where $p$ cooperating processors need to complete $t$ similar and independent tasks in an adversarial setting. Here we deal with a synchronous message passing system with processors that are subject to crash failures. Efficiency of algorithms in this setting is measured in terms of work complexity (also known as total available processor steps) and communication complexity (total number of point-to-point messages). When work and communication are considered to be comparable resources, then the overall efficiency is meaningfully expressed in terms of effort defined as work + communication. We develop and analyze a constructive algorithm that has work $O( t + p \log p\, (\sqrt{p\log p}+\sqrt{t\log t}\, ) )$ and a nonconstructive algorithm that has work $O(t +p \log^2 p)$. The latter result is close to the lower bound $\Omega(t + p \log p/ \log \log p)$ on work. The effort of each of these algorithms is proportional to its work when the number of crashes is bounded above by $c\,p$, for some positive constant $c < 1$. We also present a nonconstructive algorithm that has effort $O(t + p ^{1.77})$. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Chlebus, Bogdan S. ; Gąsieniec, Leszek ; Kowalski, Dariusz R. ; Schwarzmann, Alexander A. ; ","Doing-it-All with Bounded Work and Communication  We consider the Do-All problem, where $p$ cooperating processors need to complete $t$ similar and independent tasks in an adversarial setting. Here we deal with a synchronous message passing system with processors that are subject to crash failures. Efficiency of algorithms in this setting is measured in terms of work complexity (also known as total available processor steps) and communication complexity (total number of point-to-point messages). When work and communication are considered to be comparable resources, then the overall efficiency is meaningfully expressed in terms of effort defined as work + communication. We develop and analyze a constructive algorithm that has work $O( t + p \log p\, (\sqrt{p\log p}+\sqrt{t\log t}\, ) )$ and a nonconstructive algorithm that has work $O(t +p \log^2 p)$. The latter result is close to the lower bound $\Omega(t + p \log p/ \log \log p)$ on work. The effort of each of these algorithms is proportional to its work when the number of crashes is bounded above by $c\,p$, for some positive constant $c < 1$. We also present a nonconstructive algorithm that has effort $O(t + p ^{1.77})$. ",bound work communication consider problem cooperate processors need complete similar independent task adversarial set deal synchronous message pass system processors subject crash failures efficiency algorithms set measure term work complexity also know total available processor step communication complexity total number point point message work communication consider comparable resources overall efficiency meaningfully express term effort define work communication develop analyze constructive algorithm work log sqrt log sqrt log nonconstructive algorithm work log latter result close lower bind omega log log log work effort algorithms proportional work number crash bound positive constant also present nonconstructive algorithm effort,95,1,1409.4711.txt
http://arxiv.org/abs/1409.6182,A Benchmark Suite for Template Detection and Content Extraction,"  Template detection and content extraction are two of the main areas of information retrieval applied to the Web. They perform different analyses over the structure and content of webpages to extract some part of the document. However, their objective is different. While template detection identifies the template of a webpage (usually comparing with other webpages of the same website), content extraction identifies the main content of the webpage discarding the other part. Therefore, they are somehow complementary, because the main content is not part of the template. It has been measured that templates represent between 40% and 50% of data on the Web. Therefore, identifying templates is essential for indexing tasks because templates usually contain irrelevant information such as advertisements, menus and banners. Processing and storing this information is likely to lead to a waste of resources (storage space, bandwidth, etc.). Similarly, identifying the main content is essential for many information retrieval tasks. In this paper, we present a benchmark suite to test different approaches for template detection and content extraction. The suite is public, and it contains real heterogeneous webpages that have been labelled so that different techniques can be suitable (and automatically) compared. ",Computer Science - Information Retrieval ; ,"Alarte, Julián ; Insa, David ; Silva, Josep ; Tamarit, Salvador ; ","A Benchmark Suite for Template Detection and Content Extraction  Template detection and content extraction are two of the main areas of information retrieval applied to the Web. They perform different analyses over the structure and content of webpages to extract some part of the document. However, their objective is different. While template detection identifies the template of a webpage (usually comparing with other webpages of the same website), content extraction identifies the main content of the webpage discarding the other part. Therefore, they are somehow complementary, because the main content is not part of the template. It has been measured that templates represent between 40% and 50% of data on the Web. Therefore, identifying templates is essential for indexing tasks because templates usually contain irrelevant information such as advertisements, menus and banners. Processing and storing this information is likely to lead to a waste of resources (storage space, bandwidth, etc.). Similarly, identifying the main content is essential for many information retrieval tasks. In this paper, we present a benchmark suite to test different approaches for template detection and content extraction. The suite is public, and it contains real heterogeneous webpages that have been labelled so that different techniques can be suitable (and automatically) compared. ",benchmark suite template detection content extraction template detection content extraction two main areas information retrieval apply web perform different analyse structure content webpages extract part document however objective different template detection identify template webpage usually compare webpages website content extraction identify main content webpage discard part therefore somehow complementary main content part template measure templates represent data web therefore identify templates essential index task templates usually contain irrelevant information advertisements menus banners process store information likely lead waste resources storage space bandwidth etc similarly identify main content essential many information retrieval task paper present benchmark suite test different approach template detection content extraction suite public contain real heterogeneous webpages label different techniques suitable automatically compare,115,10,1409.6182.txt
http://arxiv.org/abs/1409.6193,Estimating topological properties of weighted networks from limited   information,"  A fundamental problem in studying and modeling economic and financial systems is represented by privacy issues, which put severe limitations on the amount of accessible information. Here we introduce a novel, highly nontrivial method to reconstruct the structural properties of complex weighted networks of this kind using only partial information: the total number of nodes and links, and the values of the strength for all nodes. The latter are used as fitness to estimate the unknown node degrees through a standard configuration model. Then, these estimated degrees and the strengths are used to calibrate an enhanced configuration model in order to generate ensembles of networks intended to represent the real system. The method, which is tested on real economic and financial networks, while drastically reducing the amount of information needed to infer network properties, turns out to be remarkably effective$-$thus representing a valuable tool for gaining insights on privacy-protected socioeconomic systems. ",Physics - Physics and Society ; Condensed Matter - Statistical Mechanics ; Computer Science - Social and Information Networks ; Quantitative Finance - Statistical Finance ; ,"Cimini, Giulio ; Squartini, Tiziano ; Gabrielli, Andrea ; Garlaschelli, Diego ; ","Estimating topological properties of weighted networks from limited   information  A fundamental problem in studying and modeling economic and financial systems is represented by privacy issues, which put severe limitations on the amount of accessible information. Here we introduce a novel, highly nontrivial method to reconstruct the structural properties of complex weighted networks of this kind using only partial information: the total number of nodes and links, and the values of the strength for all nodes. The latter are used as fitness to estimate the unknown node degrees through a standard configuration model. Then, these estimated degrees and the strengths are used to calibrate an enhanced configuration model in order to generate ensembles of networks intended to represent the real system. The method, which is tested on real economic and financial networks, while drastically reducing the amount of information needed to infer network properties, turns out to be remarkably effective$-$thus representing a valuable tool for gaining insights on privacy-protected socioeconomic systems. ",estimate topological properties weight network limit information fundamental problem study model economic financial systems represent privacy issue put severe limitations amount accessible information introduce novel highly nontrivial method reconstruct structural properties complex weight network kind use partial information total number nod link value strength nod latter use fitness estimate unknown node degrees standard configuration model estimate degrees strengths use calibrate enhance configuration model order generate ensembles network intend represent real system method test real economic financial network drastically reduce amount information need infer network properties turn remarkably effective thus represent valuable tool gain insights privacy protect socioeconomic systems,98,6,1409.6193.txt
http://arxiv.org/abs/1409.6777,Impossibility of Classically Simulating One-Clean-Qubit Computation,"  Deterministic quantum computation with one quantum bit (DQC1) is a restricted model of quantum computing where the input state is the completely mixed state except for a single clean qubit, and only a single output qubit is measured at the end of the computing. It is proved that the restriction of quantum computation to the DQC1 model does not change the complexity classes NQP and SBQP. As a main consequence, it follows that the DQC1 model cannot be efficiently simulated by classical computers unless the polynomial-time hierarchy collapses to the second level (more precisely, to AM), which answers the long-standing open problem posed by Knill and Laflamme under the very plausible complexity assumption. The argument developed in this paper also weakens the complexity assumption necessary for the existing impossibility results on classical simulation of various sub-universal quantum computing models, such as the IQP model and the Boson sampling. ",Quantum Physics ; Computer Science - Computational Complexity ; ,"Fujii, Keisuke ; Kobayashi, Hirotada ; Morimae, Tomoyuki ; Nishimura, Harumichi ; Tamate, Shuhei ; Tani, Seiichiro ; ","Impossibility of Classically Simulating One-Clean-Qubit Computation  Deterministic quantum computation with one quantum bit (DQC1) is a restricted model of quantum computing where the input state is the completely mixed state except for a single clean qubit, and only a single output qubit is measured at the end of the computing. It is proved that the restriction of quantum computation to the DQC1 model does not change the complexity classes NQP and SBQP. As a main consequence, it follows that the DQC1 model cannot be efficiently simulated by classical computers unless the polynomial-time hierarchy collapses to the second level (more precisely, to AM), which answers the long-standing open problem posed by Knill and Laflamme under the very plausible complexity assumption. The argument developed in this paper also weakens the complexity assumption necessary for the existing impossibility results on classical simulation of various sub-universal quantum computing models, such as the IQP model and the Boson sampling. ",impossibility classically simulate one clean qubit computation deterministic quantum computation one quantum bite dqc restrict model quantum compute input state completely mix state except single clean qubit single output qubit measure end compute prove restriction quantum computation dqc model change complexity class nqp sbqp main consequence follow dqc model cannot efficiently simulate classical computers unless polynomial time hierarchy collapse second level precisely answer long stand open problem pose knill laflamme plausible complexity assumption argument develop paper also weaken complexity assumption necessary exist impossibility result classical simulation various sub universal quantum compute model iqp model boson sample,96,4,1409.6777.txt
http://arxiv.org/abs/1409.7579,Field evidence of social influence in the expression of political   preferences: the case of secessionist flags in Barcelona,"  Different models of social influence have explored the dynamics of social contagion, imitation, and diffusion of different types of traits, opinions, and conducts. However, few behavioral data indicating social influence dynamics have been obtained from direct observation in `natural' social contexts. The present research provides that kind of evidence in the case of the public expression of political preferences in the city of Barcelona, where thousands of citizens supporting the secession of Catalonia from Spain have placed a Catalan flag in their balconies. We present two different studies. 1) In July 2013 we registered the number of flags in 26% of the the city. We find that there is a large dispersion in the density of flags in districts with similar density of pro-independence voters. However, we find that the density of flags tends to be fostered in those electoral district where there is a clear majority of pro-independence vote, while it is inhibited in the opposite cases. 2) During 17 days around Catalonia's 2013 National Holiday we observed the position at balcony resolution of the flags displayed in the facades of 82 blocks. We compare the clustering of flags on the facades observed each day to equivalent random distributions and find that successive hangings of flags are not independent events but that a local influence mechanism is favoring their clustering. We also find that except for the National Holiday day the density of flags tends to be fostered in those facades where there is a clear majority of pro-independence vote. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Parravano, Antonio ; Noguera, José A. ; Tena, Jordi ; Hermida, Paula ; ","Field evidence of social influence in the expression of political   preferences: the case of secessionist flags in Barcelona  Different models of social influence have explored the dynamics of social contagion, imitation, and diffusion of different types of traits, opinions, and conducts. However, few behavioral data indicating social influence dynamics have been obtained from direct observation in `natural' social contexts. The present research provides that kind of evidence in the case of the public expression of political preferences in the city of Barcelona, where thousands of citizens supporting the secession of Catalonia from Spain have placed a Catalan flag in their balconies. We present two different studies. 1) In July 2013 we registered the number of flags in 26% of the the city. We find that there is a large dispersion in the density of flags in districts with similar density of pro-independence voters. However, we find that the density of flags tends to be fostered in those electoral district where there is a clear majority of pro-independence vote, while it is inhibited in the opposite cases. 2) During 17 days around Catalonia's 2013 National Holiday we observed the position at balcony resolution of the flags displayed in the facades of 82 blocks. We compare the clustering of flags on the facades observed each day to equivalent random distributions and find that successive hangings of flags are not independent events but that a local influence mechanism is favoring their clustering. We also find that except for the National Holiday day the density of flags tends to be fostered in those facades where there is a clear majority of pro-independence vote. ",field evidence social influence expression political preferences case secessionist flag barcelona different model social influence explore dynamics social contagion imitation diffusion different type traits opinions conduct however behavioral data indicate social influence dynamics obtain direct observation natural social contexts present research provide kind evidence case public expression political preferences city barcelona thousands citizens support secession catalonia spain place catalan flag balconies present two different study july register number flag city find large dispersion density flag district similar density pro independence voters however find density flag tend foster electoral district clear majority pro independence vote inhibit opposite case days around catalonia national holiday observe position balcony resolution flag display facades block compare cluster flag facades observe day equivalent random distributions find successive hang flag independent events local influence mechanism favor cluster also find except national holiday day density flag tend foster facades clear majority pro independence vote,146,0,1409.7579.txt
http://arxiv.org/abs/1409.8061,A New DoF Upper Bound and Its Achievability for $K$-User MIMO Y Channels,"  This work is to study the degrees of freedom (DoF) for the $K$-user MIMO Y channel. Previously, two transmission frameworks have been proposed for the DoF analysis when $N \geq 2M$, where $M$ and $N$ denote the number of antennas at each source node and the relay node respectively. The first method is named as signal group based alignment proposed by Hua et al. in [1]. The second is named as signal pattern approach introduced by Wang et al. in [2]. But both of them only studied certain antenna configurations. The maximum achievable DoF in the general case still remains unknown. In this work, we first derive a new upper bound of the DoF using the genie-aided approach. Then, we propose a more general transmission framework, generalized signal alignment (GSA), and show that the previous two methods are both special cases of GSA. With GSA, we prove that the new DoF upper bound is achievable when $\frac{N}{M} \in \left(0,2+\frac{4}{K(K-1)}\right] \cup \left[K-2, +\infty\right)$. The DoF analysis in this paper provides a major step forward towards the fundamental capacity limit of the $K$-user MIMO Y channel. It also offers a new approach of integrating interference alignment with physical layer network coding. ",Computer Science - Information Theory ; ,"Liu, Kangqi ; Tao, Meixia ; ","A New DoF Upper Bound and Its Achievability for $K$-User MIMO Y Channels  This work is to study the degrees of freedom (DoF) for the $K$-user MIMO Y channel. Previously, two transmission frameworks have been proposed for the DoF analysis when $N \geq 2M$, where $M$ and $N$ denote the number of antennas at each source node and the relay node respectively. The first method is named as signal group based alignment proposed by Hua et al. in [1]. The second is named as signal pattern approach introduced by Wang et al. in [2]. But both of them only studied certain antenna configurations. The maximum achievable DoF in the general case still remains unknown. In this work, we first derive a new upper bound of the DoF using the genie-aided approach. Then, we propose a more general transmission framework, generalized signal alignment (GSA), and show that the previous two methods are both special cases of GSA. With GSA, we prove that the new DoF upper bound is achievable when $\frac{N}{M} \in \left(0,2+\frac{4}{K(K-1)}\right] \cup \left[K-2, +\infty\right)$. The DoF analysis in this paper provides a major step forward towards the fundamental capacity limit of the $K$-user MIMO Y channel. It also offers a new approach of integrating interference alignment with physical layer network coding. ",new dof upper bind achievability user mimo channel work study degrees freedom dof user mimo channel previously two transmission frameworks propose dof analysis geq denote number antennas source node relay node respectively first method name signal group base alignment propose hua et al second name signal pattern approach introduce wang et al study certain antenna configurations maximum achievable dof general case still remain unknown work first derive new upper bind dof use genie aid approach propose general transmission framework generalize signal alignment gsa show previous two methods special case gsa gsa prove new dof upper bind achievable frac leave frac right cup leave infty right dof analysis paper provide major step forward towards fundamental capacity limit user mimo channel also offer new approach integrate interference alignment physical layer network cod,130,9,1409.8061.txt
http://arxiv.org/abs/1409.8230,RENOIR - A Dataset for Real Low-Light Image Noise Reduction,"  Image denoising algorithms are evaluated using images corrupted by artificial noise, which may lead to incorrect conclusions about their performances on real noise. In this paper we introduce a dataset of color images corrupted by natural noise due to low-light conditions, together with spatially and intensity-aligned low noise images of the same scenes. We also introduce a method for estimating the true noise level in our images, since even the low noise images contain small amounts of noise. We evaluate the accuracy of our noise estimation method on real and artificial noise, and investigate the Poisson-Gaussian noise model. Finally, we use our dataset to evaluate six denoising algorithms: Active Random Field, BM3D, Bilevel-MRF, Multi-Layer Perceptron, and two versions of NL-means. We show that while the Multi-Layer Perceptron, Bilevel-MRF, and NL-means with soft threshold outperform BM3D on gray images with synthetic noise, they lag behind on our dataset. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Anaya, Josue ; Barbu, Adrian ; ","RENOIR - A Dataset for Real Low-Light Image Noise Reduction  Image denoising algorithms are evaluated using images corrupted by artificial noise, which may lead to incorrect conclusions about their performances on real noise. In this paper we introduce a dataset of color images corrupted by natural noise due to low-light conditions, together with spatially and intensity-aligned low noise images of the same scenes. We also introduce a method for estimating the true noise level in our images, since even the low noise images contain small amounts of noise. We evaluate the accuracy of our noise estimation method on real and artificial noise, and investigate the Poisson-Gaussian noise model. Finally, we use our dataset to evaluate six denoising algorithms: Active Random Field, BM3D, Bilevel-MRF, Multi-Layer Perceptron, and two versions of NL-means. We show that while the Multi-Layer Perceptron, Bilevel-MRF, and NL-means with soft threshold outperform BM3D on gray images with synthetic noise, they lag behind on our dataset. ",renoir dataset real low light image noise reduction image denoising algorithms evaluate use image corrupt artificial noise may lead incorrect conclusions performances real noise paper introduce dataset color image corrupt natural noise due low light condition together spatially intensity align low noise image scenes also introduce method estimate true noise level image since even low noise image contain small amount noise evaluate accuracy noise estimation method real artificial noise investigate poisson gaussian noise model finally use dataset evaluate six denoising algorithms active random field bm bilevel mrf multi layer perceptron two versions nl mean show multi layer perceptron bilevel mrf nl mean soft threshold outperform bm gray image synthetic noise lag behind dataset,113,11,1409.8230.txt
http://arxiv.org/abs/1409.8498,Non-myopic learning in repeated stochastic games,"  In repeated stochastic games (RSGs), an agent must quickly adapt to the behavior of previously unknown associates, who may themselves be learning. This machine-learning problem is particularly challenging due, in part, to the presence of multiple (even infinite) equilibria and inherently large strategy spaces. In this paper, we introduce a method to reduce the strategy space of two-player general-sum RSGs to a handful of expert strategies. This process, called Mega, effectually reduces an RSG to a bandit problem. We show that the resulting strategy space preserves several important properties of the original RSG, thus enabling a learner to produce robust strategies within a reasonably small number of interactions. To better establish strengths and weaknesses of this approach, we empirically evaluate the resulting learning system against other algorithms in three different RSGs. ",Computer Science - Computer Science and Game Theory ; Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; ,"Crandall, Jacob W. ; ","Non-myopic learning in repeated stochastic games  In repeated stochastic games (RSGs), an agent must quickly adapt to the behavior of previously unknown associates, who may themselves be learning. This machine-learning problem is particularly challenging due, in part, to the presence of multiple (even infinite) equilibria and inherently large strategy spaces. In this paper, we introduce a method to reduce the strategy space of two-player general-sum RSGs to a handful of expert strategies. This process, called Mega, effectually reduces an RSG to a bandit problem. We show that the resulting strategy space preserves several important properties of the original RSG, thus enabling a learner to produce robust strategies within a reasonably small number of interactions. To better establish strengths and weaknesses of this approach, we empirically evaluate the resulting learning system against other algorithms in three different RSGs. ",non myopic learn repeat stochastic game repeat stochastic game rsgs agent must quickly adapt behavior previously unknown associate may learn machine learn problem particularly challenge due part presence multiple even infinite equilibria inherently large strategy space paper introduce method reduce strategy space two player general sum rsgs handful expert strategies process call mega effectually reduce rsg bandit problem show result strategy space preserve several important properties original rsg thus enable learner produce robust strategies within reasonably small number interactions better establish strengths weaknesses approach empirically evaluate result learn system algorithms three different rsgs,93,8,1409.8498.txt
http://arxiv.org/abs/1409.8580,Interference Functionals in Poisson Networks,"  We propose and prove a theorem that allows the calculation of a class of functionals on Poisson point processes that have the form of expected values of sum-products of functions. In proving the theorem, we present a variant of the Campbell-Mecke theorem from stochastic geometry. We proceed to apply our result in the calculation of expected values involving interference in wireless Poisson networks. Based on this, we derive outage probabilities for transmissions in a Poisson network with Nakagami fading. Our results extend the stochastic geometry toolbox used for the mathematical analysis of interference-limited wireless networks. ",Computer Science - Information Theory ; ,"Schilcher, Udo ; Toumpis, Stavros ; Haenggi, Martin ; Crismani, Alessandro ; Brandner, Günther ; Bettstetter, Christian ; ","Interference Functionals in Poisson Networks  We propose and prove a theorem that allows the calculation of a class of functionals on Poisson point processes that have the form of expected values of sum-products of functions. In proving the theorem, we present a variant of the Campbell-Mecke theorem from stochastic geometry. We proceed to apply our result in the calculation of expected values involving interference in wireless Poisson networks. Based on this, we derive outage probabilities for transmissions in a Poisson network with Nakagami fading. Our results extend the stochastic geometry toolbox used for the mathematical analysis of interference-limited wireless networks. ",interference functionals poisson network propose prove theorem allow calculation class functionals poisson point process form expect value sum products function prove theorem present variant campbell mecke theorem stochastic geometry proceed apply result calculation expect value involve interference wireless poisson network base derive outage probabilities transmissions poisson network nakagami fade result extend stochastic geometry toolbox use mathematical analysis interference limit wireless network,61,6,1409.8580.txt
http://arxiv.org/abs/1410.0446,Identification of Dynamic functional brain network states Through Tensor   Decomposition,"  With the advances in high resolution neuroimaging, there has been a growing interest in the detection of functional brain connectivity. Complex network theory has been proposed as an attractive mathematical representation of functional brain networks. However, most of the current studies of functional brain networks have focused on the computation of graph theoretic indices for static networks, i.e. long-time averages of connectivity networks. It is well-known that functional connectivity is a dynamic process and the construction and reorganization of the networks is key to understanding human cognition. Therefore, there is a growing need to track dynamic functional brain networks and identify time intervals over which the network is quasi-stationary. In this paper, we present a tensor decomposition based method to identify temporally invariant 'network states' and find a common topographic representation for each state. The proposed methods are applied to electroencephalogram (EEG) data during the study of error-related negativity (ERN). ",Computer Science - Neural and Evolutionary Computing ; Computer Science - Machine Learning ; Quantitative Biology - Neurons and Cognition ; ,"Mahyari, Arash Golibagh ; Aviyente, Selin ; ","Identification of Dynamic functional brain network states Through Tensor   Decomposition  With the advances in high resolution neuroimaging, there has been a growing interest in the detection of functional brain connectivity. Complex network theory has been proposed as an attractive mathematical representation of functional brain networks. However, most of the current studies of functional brain networks have focused on the computation of graph theoretic indices for static networks, i.e. long-time averages of connectivity networks. It is well-known that functional connectivity is a dynamic process and the construction and reorganization of the networks is key to understanding human cognition. Therefore, there is a growing need to track dynamic functional brain networks and identify time intervals over which the network is quasi-stationary. In this paper, we present a tensor decomposition based method to identify temporally invariant 'network states' and find a common topographic representation for each state. The proposed methods are applied to electroencephalogram (EEG) data during the study of error-related negativity (ERN). ",identification dynamic functional brain network state tensor decomposition advance high resolution neuroimaging grow interest detection functional brain connectivity complex network theory propose attractive mathematical representation functional brain network however current study functional brain network focus computation graph theoretic indices static network long time average connectivity network well know functional connectivity dynamic process construction reorganization network key understand human cognition therefore grow need track dynamic functional brain network identify time intervals network quasi stationary paper present tensor decomposition base method identify temporally invariant network state find common topographic representation state propose methods apply electroencephalogram eeg data study error relate negativity ern,100,6,1410.0446.txt
http://arxiv.org/abs/1410.0736,HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale   Visual Recognition,"  In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65%, 3.1% and 1.1%, respectively. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; Statistics - Machine Learning ; ,"Yan, Zhicheng ; Zhang, Hao ; Piramuthu, Robinson ; Jagadeesh, Vignesh ; DeCoste, Dennis ; Di, Wei ; Yu, Yizhou ; ","HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale   Visual Recognition  In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65%, 3.1% and 1.1%, respectively. ",hd cnn hierarchical deep convolutional neural network large scale visual recognition image classification visual separability different object categories highly uneven categories difficult distinguish others difficult categories demand dedicate classifiers however exist deep convolutional neural network cnn train flat way classifiers efforts make leverage hierarchical structure categories paper introduce hierarchical deep cnns hd cnns embed deep cnns category hierarchy hd cnn separate easy class use coarse category classifier distinguish difficult class use fine category classifiers hd cnn train component wise pretraining follow global finetuning multinomial logistic loss regularize coarse category consistency term addition conditional executions fine category classifiers layer parameter compression make hd cnns scalable large scale visual recognition achieve state art result cifar large scale imagenet class benchmark datasets experiment build three different hd cnns lower top error standard cnns respectively,131,6,1410.0736.txt
http://arxiv.org/abs/1410.2670,Entropy NAND: Early Functional Completeness in Entropy Networks,"  An observer increases in relative entropy as it receives information from what it is observing. In a system of only an observer and the observed, an increase in the relative entropy of the observer is a decrease in the relative entropy of the observed. Linking together these directional entropy disequilibriums we show that NAND and NOR functionality arise in such networks at very low levels of complexity. ","Computer Science - Information Theory ; 37A35, 81P15 ; F.1.1 ; F.1.2 ; ","Jesse, Forrest Fabian ; ","Entropy NAND: Early Functional Completeness in Entropy Networks  An observer increases in relative entropy as it receives information from what it is observing. In a system of only an observer and the observed, an increase in the relative entropy of the observer is a decrease in the relative entropy of the observed. Linking together these directional entropy disequilibriums we show that NAND and NOR functionality arise in such networks at very low levels of complexity. ",entropy nand early functional completeness entropy network observer increase relative entropy receive information observe system observer observe increase relative entropy observer decrease relative entropy observe link together directional entropy disequilibriums show nand functionality arise network low level complexity,38,6,1410.2670.txt
http://arxiv.org/abs/1410.2752,Spatial Straight Line Linkages by Factorization of Motion Polynomials,"  We use the recently introduced factorization of motion polynomials for constructing overconstrained spatial linkages with a straight line trajectory. Unlike previous examples, the end-effector motion is not translational and the link graph is a cycle. In particular, we obtain a number of linkages with four revolute and two prismatic joints and a remarkable linkage with seven revolute joints one of whose joints performs a Darboux motion. ",Mathematics - Metric Geometry ; Computer Science - Robotics ; Mathematics - Rings and Algebras ; 70B10 ; ,"Li, Zijia ; Schicho, Josef ; Schröcker, Hans-Peter ; ","Spatial Straight Line Linkages by Factorization of Motion Polynomials  We use the recently introduced factorization of motion polynomials for constructing overconstrained spatial linkages with a straight line trajectory. Unlike previous examples, the end-effector motion is not translational and the link graph is a cycle. In particular, we obtain a number of linkages with four revolute and two prismatic joints and a remarkable linkage with seven revolute joints one of whose joints performs a Darboux motion. ",spatial straight line linkages factorization motion polynomials use recently introduce factorization motion polynomials construct overconstrained spatial linkages straight line trajectory unlike previous examples end effector motion translational link graph cycle particular obtain number linkages four revolute two prismatic joint remarkable linkage seven revolute joint one whose joint perform darboux motion,50,4,1410.2752.txt
http://arxiv.org/abs/1410.3247,An easy subexponential bound for online chain partitioning,  Bosek and Krawczyk exhibited an online algorithm for partitioning an online poset of width $w$ into $w^{14\lg w}$ chains. We improve this to $w^{6.5 \lg w + 7}$ with a simpler and shorter proof by combining the work of Bosek & Krawczyk with work of Kierstead & Smith on First-Fit chain partitioning of ladder-free posets. We also provide examples illustrating the limits of our approach. ,Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; 68W27 ; ,"Bosek, Bartłomiej ; Kierstead, Hal A. ; Krawczyk, Tomasz ; Matecki, Grzegorz ; Smith, Matthew E. ; ",An easy subexponential bound for online chain partitioning  Bosek and Krawczyk exhibited an online algorithm for partitioning an online poset of width $w$ into $w^{14\lg w}$ chains. We improve this to $w^{6.5 \lg w + 7}$ with a simpler and shorter proof by combining the work of Bosek & Krawczyk with work of Kierstead & Smith on First-Fit chain partitioning of ladder-free posets. We also provide examples illustrating the limits of our approach. ,easy subexponential bind online chain partition bosek krawczyk exhibit online algorithm partition online poset width lg chain improve lg simpler shorter proof combine work bosek krawczyk work kierstead smith first fit chain partition ladder free posets also provide examples illustrate limit approach,42,4,1410.3247.txt
http://arxiv.org/abs/1410.3351,Ricci Curvature and the Manifold Learning Problem,"  Consider a sample of $n$ points taken i.i.d from a submanifold $\Sigma$ of Euclidean space. We show that there is a way to estimate the Ricci curvature of $\Sigma$ with respect to the induced metric from the sample. Our method is grounded in the notions of Carr\'e du Champ for diffusion semi-groups, the theory of Empirical processes and local Principal Component Analysis. ","Mathematics - Differential Geometry ; Computer Science - Machine Learning ; Mathematics - Metric Geometry ; Statistics - Machine Learning ; 68T05, 58J35 ; ","Ache, Antonio G. ; Warren, Micah W. ; ","Ricci Curvature and the Manifold Learning Problem  Consider a sample of $n$ points taken i.i.d from a submanifold $\Sigma$ of Euclidean space. We show that there is a way to estimate the Ricci curvature of $\Sigma$ with respect to the induced metric from the sample. Our method is grounded in the notions of Carr\'e du Champ for diffusion semi-groups, the theory of Empirical processes and local Principal Component Analysis. ",ricci curvature manifold learn problem consider sample point take submanifold sigma euclidean space show way estimate ricci curvature sigma respect induce metric sample method ground notions carr du champ diffusion semi group theory empirical process local principal component analysis,39,12,1410.3351.txt
http://arxiv.org/abs/1410.3764,A lazy approach to on-line bipartite matching,"  We present a new approach, called a lazy matching, to the problem of on-line matching on bipartite graphs. Imagine that one side of a graph is given and the vertices of the other side are arriving on-line. Originally, incoming vertex is either irrevocably matched to an another element or stays forever unmatched. A lazy algorithm is allowed to match a new vertex to a group of elements (possibly empty) and afterwords, forced against next vertices, may give up parts of the group. The restriction is that all the time each element is in at most one group. We present an optimal lazy algorithm (deterministic) and prove that its competitive ratio equals $1-\pi/\cosh(\frac{\sqrt{3}}{2}\pi)\approx 0.588$. The lazy approach allows us to break the barrier of $1/2$, which is the best competitive ratio that can be guaranteed by any deterministic algorithm in the classical on-line matching. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; ,"Kozik, Jakub ; Matecki, Grzegorz ; ","A lazy approach to on-line bipartite matching  We present a new approach, called a lazy matching, to the problem of on-line matching on bipartite graphs. Imagine that one side of a graph is given and the vertices of the other side are arriving on-line. Originally, incoming vertex is either irrevocably matched to an another element or stays forever unmatched. A lazy algorithm is allowed to match a new vertex to a group of elements (possibly empty) and afterwords, forced against next vertices, may give up parts of the group. The restriction is that all the time each element is in at most one group. We present an optimal lazy algorithm (deterministic) and prove that its competitive ratio equals $1-\pi/\cosh(\frac{\sqrt{3}}{2}\pi)\approx 0.588$. The lazy approach allows us to break the barrier of $1/2$, which is the best competitive ratio that can be guaranteed by any deterministic algorithm in the classical on-line matching. ",lazy approach line bipartite match present new approach call lazy match problem line match bipartite graph imagine one side graph give vertices side arrive line originally incoming vertex either irrevocably match another element stay forever unmatched lazy algorithm allow match new vertex group elements possibly empty afterwords force next vertices may give part group restriction time element one group present optimal lazy algorithm deterministic prove competitive ratio equal pi cosh frac sqrt pi approx lazy approach allow us break barrier best competitive ratio guarantee deterministic algorithm classical line match,89,4,1410.3764.txt
http://arxiv.org/abs/1410.3877,Selection-based Approach to Cooperative Interval Games,"  Cooperative interval games are a generalized model of cooperative games in which the worth of every coalition corresponds to a closed interval representing the possible outcomes of its cooperation. Selections are all possible outcomes of the interval game with no additional uncertainty.   We introduce new selection-based classes of interval games and prove their characterization theorems and relations to existing classes based on the interval weakly better operator. We show new results regarding the core and imputations and examine a problem of equivalence for two different versions of the core, the main stability solution of cooperative games. Finally, we introduce the definition of strong imputation and strong core as universal solution concepts of interval games. ","Mathematics - Optimization and Control ; Computer Science - Computer Science and Game Theory ; 65G99, 91A12 ; ","Bok, Jan ; Hladík, Milan ; ","Selection-based Approach to Cooperative Interval Games  Cooperative interval games are a generalized model of cooperative games in which the worth of every coalition corresponds to a closed interval representing the possible outcomes of its cooperation. Selections are all possible outcomes of the interval game with no additional uncertainty.   We introduce new selection-based classes of interval games and prove their characterization theorems and relations to existing classes based on the interval weakly better operator. We show new results regarding the core and imputations and examine a problem of equivalence for two different versions of the core, the main stability solution of cooperative games. Finally, we introduce the definition of strong imputation and strong core as universal solution concepts of interval games. ",selection base approach cooperative interval game cooperative interval game generalize model cooperative game worth every coalition correspond close interval represent possible outcomes cooperation selections possible outcomes interval game additional uncertainty introduce new selection base class interval game prove characterization theorems relations exist class base interval weakly better operator show new result regard core imputations examine problem equivalence two different versions core main stability solution cooperative game finally introduce definition strong imputation strong core universal solution concepts interval game,78,8,1410.3877.txt
http://arxiv.org/abs/1410.4060,Decoupling Multivariate Polynomials Using First-Order Information,"  We present a method to decompose a set of multivariate real polynomials into linear combinations of univariate polynomials in linear forms of the input variables. The method proceeds by collecting the first-order information of the polynomials in a set of operating points, which is captured by the Jacobian matrix evaluated at the operating points. The polyadic canonical decomposition of the three-way tensor of Jacobian matrices directly returns the unknown linear relations, as well as the necessary information to reconstruct the univariate polynomials. The conditions under which this decoupling procedure works are discussed, and the method is illustrated on several numerical examples. ",Mathematics - Numerical Analysis ; Computer Science - Numerical Analysis ; ,"Dreesen, Philippe ; Ishteva, Mariya ; Schoukens, Johan ; ","Decoupling Multivariate Polynomials Using First-Order Information  We present a method to decompose a set of multivariate real polynomials into linear combinations of univariate polynomials in linear forms of the input variables. The method proceeds by collecting the first-order information of the polynomials in a set of operating points, which is captured by the Jacobian matrix evaluated at the operating points. The polyadic canonical decomposition of the three-way tensor of Jacobian matrices directly returns the unknown linear relations, as well as the necessary information to reconstruct the univariate polynomials. The conditions under which this decoupling procedure works are discussed, and the method is illustrated on several numerical examples. ",decouple multivariate polynomials use first order information present method decompose set multivariate real polynomials linear combinations univariate polynomials linear form input variables method proceed collect first order information polynomials set operate point capture jacobian matrix evaluate operate point polyadic canonical decomposition three way tensor jacobian matrices directly return unknown linear relations well necessary information reconstruct univariate polynomials condition decouple procedure work discuss method illustrate several numerical examples,67,4,1410.4060.txt
http://arxiv.org/abs/1410.4536,Numerical Optimization for Symmetric Tensor Decomposition,"  We consider the problem of decomposing a real-valued symmetric tensor as the sum of outer products of real-valued vectors. Algebraic methods exist for computing complex-valued decompositions of symmetric tensors, but here we focus on real-valued decompositions, both unconstrained and nonnegative, for problems with low-rank structure. We discuss when solutions exist and how to formulate the mathematical program. Numerical results show the properties of the proposed formulations (including one that ignores symmetry) on a set of test problems and illustrate that these straightforward formulations can be effective even though the problem is nonconvex. ",Mathematics - Numerical Analysis ; Computer Science - Numerical Analysis ; ,"Kolda, Tamara G. ; ","Numerical Optimization for Symmetric Tensor Decomposition  We consider the problem of decomposing a real-valued symmetric tensor as the sum of outer products of real-valued vectors. Algebraic methods exist for computing complex-valued decompositions of symmetric tensors, but here we focus on real-valued decompositions, both unconstrained and nonnegative, for problems with low-rank structure. We discuss when solutions exist and how to formulate the mathematical program. Numerical results show the properties of the proposed formulations (including one that ignores symmetry) on a set of test problems and illustrate that these straightforward formulations can be effective even though the problem is nonconvex. ",numerical optimization symmetric tensor decomposition consider problem decompose real value symmetric tensor sum outer products real value vectors algebraic methods exist compute complex value decompositions symmetric tensors focus real value decompositions unconstrained nonnegative problems low rank structure discuss solutions exist formulate mathematical program numerical result show properties propose formulations include one ignore symmetry set test problems illustrate straightforward formulations effective even though problem nonconvex,64,7,1410.4536.txt
http://arxiv.org/abs/1410.4617,A Cut Principle for Information Flow,"  We view a distributed system as a graph of active locations with unidirectional channels between them, through which they pass messages. In this context, the graph structure of a system constrains the propagation of information through it.   Suppose a set of channels is a cut set between an information source and a potential sink. We prove that, if there is no disclosure from the source to the cut set, then there can be no disclosure to the sink. We introduce a new formalization of partial disclosure, called *blur operators*, and show that the same cut property is preserved for disclosure to within a blur operator. This cut-blur property also implies a compositional principle, which ensures limited disclosure for a class of systems that differ only beyond the cut. ",Computer Science - Cryptography and Security ; ,"Guttman, Joshua D. ; Rowe, Paul D. ; ","A Cut Principle for Information Flow  We view a distributed system as a graph of active locations with unidirectional channels between them, through which they pass messages. In this context, the graph structure of a system constrains the propagation of information through it.   Suppose a set of channels is a cut set between an information source and a potential sink. We prove that, if there is no disclosure from the source to the cut set, then there can be no disclosure to the sink. We introduce a new formalization of partial disclosure, called *blur operators*, and show that the same cut property is preserved for disclosure to within a blur operator. This cut-blur property also implies a compositional principle, which ensures limited disclosure for a class of systems that differ only beyond the cut. ",cut principle information flow view distribute system graph active locations unidirectional channel pass message context graph structure system constrain propagation information suppose set channel cut set information source potential sink prove disclosure source cut set disclosure sink introduce new formalization partial disclosure call blur operators show cut property preserve disclosure within blur operator cut blur property also imply compositional principle ensure limit disclosure class systems differ beyond cut,68,3,1410.4617.txt
http://arxiv.org/abs/1410.5131,An Algebra of Reversible Computation,"  Process algebra ACP based on the interleaving semantics can not be reversed. We design a reversible version of APTC called RAPTC. It has algebraic laws of reversible choice, sequence, parallelism, communication, silent step and abstraction, and also the soundness and completeness modulo strongly forward-reverse truly concurrent bisimulations and weakly forward-reverse truly concurrent bisimulations. ",Computer Science - Logic in Computer Science ; ,"Wang, Yong ; ","An Algebra of Reversible Computation  Process algebra ACP based on the interleaving semantics can not be reversed. We design a reversible version of APTC called RAPTC. It has algebraic laws of reversible choice, sequence, parallelism, communication, silent step and abstraction, and also the soundness and completeness modulo strongly forward-reverse truly concurrent bisimulations and weakly forward-reverse truly concurrent bisimulations. ",algebra reversible computation process algebra acp base interleave semantics reverse design reversible version aptc call raptc algebraic laws reversible choice sequence parallelism communication silent step abstraction also soundness completeness modulo strongly forward reverse truly concurrent bisimulations weakly forward reverse truly concurrent bisimulations,42,4,1410.5131.txt
http://arxiv.org/abs/1410.5920,Active Regression by Stratification,"  We propose a new active learning algorithm for parametric linear regression with random design. We provide finite sample convergence guarantees for general distributions in the misspecified model. This is the first active learner for this setting that provably can improve over passive learning. Unlike other learning settings (such as classification), in regression the passive learning rate of $O(1/\epsilon)$ cannot in general be improved upon. Nonetheless, the so-called `constant' in the rate of convergence, which is characterized by a distribution-dependent risk, can be improved in many cases. For a given distribution, achieving the optimal risk requires prior knowledge of the distribution. Following the stratification technique advocated in Monte-Carlo function integration, our active learner approaches the optimal risk using piecewise constant approximations. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; ,"Sabato, Sivan ; Munos, Remi ; ","Active Regression by Stratification  We propose a new active learning algorithm for parametric linear regression with random design. We provide finite sample convergence guarantees for general distributions in the misspecified model. This is the first active learner for this setting that provably can improve over passive learning. Unlike other learning settings (such as classification), in regression the passive learning rate of $O(1/\epsilon)$ cannot in general be improved upon. Nonetheless, the so-called `constant' in the rate of convergence, which is characterized by a distribution-dependent risk, can be improved in many cases. For a given distribution, achieving the optimal risk requires prior knowledge of the distribution. Following the stratification technique advocated in Monte-Carlo function integration, our active learner approaches the optimal risk using piecewise constant approximations. ",active regression stratification propose new active learn algorithm parametric linear regression random design provide finite sample convergence guarantee general distributions misspecified model first active learner set provably improve passive learn unlike learn settings classification regression passive learn rate epsilon cannot general improve upon nonetheless call constant rate convergence characterize distribution dependent risk improve many case give distribution achieve optimal risk require prior knowledge distribution follow stratification technique advocate monte carlo function integration active learner approach optimal risk use piecewise constant approximations,81,12,1410.5920.txt
http://arxiv.org/abs/1410.6516,Coalition Structure Generation on Graphs,"  Two fundamental algorithm-design paradigms are Tree Search and Dynamic Programming. The techniques used therein have been shown to complement one another when solving the complete set partitioning problem, also known as the coalition structure generation problem [5]. Inspired by this observation, we develop in this paper an algorithm to solve the coalition structure generation problem on graphs, where the goal is to identifying an optimal partition of a graph into connected subgraphs. More specifically, we develop a new depth-first search algorithm, and combine it with an existing dynamic programming algorithm due to Vinyals et al. [9]. The resulting hybrid algorithm is empirically shown to significantly outperform both its constituent parts when the subset-evaluation function happens to have certain intuitive properties. ",Computer Science - Multiagent Systems ; ,"Rahwan, Talal ; Michalak, Tomasz P. ; ","Coalition Structure Generation on Graphs  Two fundamental algorithm-design paradigms are Tree Search and Dynamic Programming. The techniques used therein have been shown to complement one another when solving the complete set partitioning problem, also known as the coalition structure generation problem [5]. Inspired by this observation, we develop in this paper an algorithm to solve the coalition structure generation problem on graphs, where the goal is to identifying an optimal partition of a graph into connected subgraphs. More specifically, we develop a new depth-first search algorithm, and combine it with an existing dynamic programming algorithm due to Vinyals et al. [9]. The resulting hybrid algorithm is empirically shown to significantly outperform both its constituent parts when the subset-evaluation function happens to have certain intuitive properties. ",coalition structure generation graph two fundamental algorithm design paradigms tree search dynamic program techniques use therein show complement one another solve complete set partition problem also know coalition structure generation problem inspire observation develop paper algorithm solve coalition structure generation problem graph goal identify optimal partition graph connect subgraphs specifically develop new depth first search algorithm combine exist dynamic program algorithm due vinyals et al result hybrid algorithm empirically show significantly outperform constituent part subset evaluation function happen certain intuitive properties,81,3,1410.6516.txt
http://arxiv.org/abs/1410.7082,Complexity of LP in Terms of the Face Lattice,"  Let $X$ be a finite set in $Z^d$. We consider the problem of optimizing linear function $f(x) = c^T x$ on $X$, where $c\in Z^d$ is an input vector. We call it a problem $X$. A problem $X$ is related with linear program $\max\limits_{x \in P} f(x)$, where polytope $P$ is a convex hull of $X$. The key parameters for evaluating the complexity of a problem $X$ are the dimension $d$, the cardinality $|X|$, and the encoding size $S(X) = \log_2 \left(\max\limits_{x\in X} \|x\|_{\infty}\right)$. We show that if the (time and space) complexity of some algorithm $A$ for solving a problem $X$ is defined only in terms of combinatorial structure of $P$ and the size $S(X)$, then for every $d$ and $n$ there exists polynomially (in $d$, $\log n$, and $S$) solvable problem $Y$ with $\dim Y = d$, $|Y| = n$, such that the algorithm $A$ requires exponential time or space for solving $Y$. ",Computer Science - Computational Complexity ; Mathematics - Combinatorics ; ,"Maksimenko, Aleksandr ; ","Complexity of LP in Terms of the Face Lattice  Let $X$ be a finite set in $Z^d$. We consider the problem of optimizing linear function $f(x) = c^T x$ on $X$, where $c\in Z^d$ is an input vector. We call it a problem $X$. A problem $X$ is related with linear program $\max\limits_{x \in P} f(x)$, where polytope $P$ is a convex hull of $X$. The key parameters for evaluating the complexity of a problem $X$ are the dimension $d$, the cardinality $|X|$, and the encoding size $S(X) = \log_2 \left(\max\limits_{x\in X} \|x\|_{\infty}\right)$. We show that if the (time and space) complexity of some algorithm $A$ for solving a problem $X$ is defined only in terms of combinatorial structure of $P$ and the size $S(X)$, then for every $d$ and $n$ there exists polynomially (in $d$, $\log n$, and $S$) solvable problem $Y$ with $\dim Y = d$, $|Y| = n$, such that the algorithm $A$ requires exponential time or space for solving $Y$. ",complexity lp term face lattice let finite set consider problem optimize linear function input vector call problem problem relate linear program max limit polytope convex hull key parameters evaluate complexity problem dimension cardinality encode size log leave max limit infty right show time space complexity algorithm solve problem define term combinatorial structure size every exist polynomially log solvable problem dim algorithm require exponential time space solve,66,1,1410.7082.txt
http://arxiv.org/abs/1410.7694,Dynamic Analysis of Digital Chaotic Maps via State-Mapping Networks,"  Chaotic dynamics is widely used to design pseudo-random number generators and for other applications such as secure communications and encryption. This paper aims to study the dynamics of discrete-time chaotic maps in the digital (i.e., finite-precision) domain. Differing from the traditional approaches treating a digital chaotic map as a black box with different explanations according to the test results of the output, the dynamical properties of such chaotic maps are first explored with a fixed-point arithmetic, using the Logistic map and the Tent map as two representative examples, from a new perspective with the corresponding state-mapping networks (SMNs). In an SMN, every possible value in the digital domain is considered as a node and the mapping relationship between any pair of nodes is a directed edge. The scale-free properties of the Logistic map's SMN are proved. The analytic results are further extended to the scenario of floating-point arithmetic and for other chaotic maps. Understanding the network structure of a chaotic map's SMN in digital computers can facilitate counteracting the undesirable degeneration of chaotic dynamics in finite-precision domains, helping also classify and improve the randomness of pseudo-random number sequences generated by iterating chaotic maps. ","Computer Science - Cryptography and Security ; Nonlinear Sciences - Chaotic Dynamics ; 37D45, 05C82 ; ","Li, Chengqing ; Feng, Bingbing ; Li, Shujun ; Kurths, Juergen ; Chen, Guanrong ; ","Dynamic Analysis of Digital Chaotic Maps via State-Mapping Networks  Chaotic dynamics is widely used to design pseudo-random number generators and for other applications such as secure communications and encryption. This paper aims to study the dynamics of discrete-time chaotic maps in the digital (i.e., finite-precision) domain. Differing from the traditional approaches treating a digital chaotic map as a black box with different explanations according to the test results of the output, the dynamical properties of such chaotic maps are first explored with a fixed-point arithmetic, using the Logistic map and the Tent map as two representative examples, from a new perspective with the corresponding state-mapping networks (SMNs). In an SMN, every possible value in the digital domain is considered as a node and the mapping relationship between any pair of nodes is a directed edge. The scale-free properties of the Logistic map's SMN are proved. The analytic results are further extended to the scenario of floating-point arithmetic and for other chaotic maps. Understanding the network structure of a chaotic map's SMN in digital computers can facilitate counteracting the undesirable degeneration of chaotic dynamics in finite-precision domains, helping also classify and improve the randomness of pseudo-random number sequences generated by iterating chaotic maps. ",dynamic analysis digital chaotic map via state map network chaotic dynamics widely use design pseudo random number generators applications secure communications encryption paper aim study dynamics discrete time chaotic map digital finite precision domain differ traditional approach treat digital chaotic map black box different explanations accord test result output dynamical properties chaotic map first explore fix point arithmetic use logistic map tent map two representative examples new perspective correspond state map network smns smn every possible value digital domain consider node map relationship pair nod direct edge scale free properties logistic map smn prove analytic result extend scenario float point arithmetic chaotic map understand network structure chaotic map smn digital computers facilitate counteract undesirable degeneration chaotic dynamics finite precision domains help also classify improve randomness pseudo random number sequence generate iterate chaotic map,133,6,1410.7694.txt
http://arxiv.org/abs/1410.8663,On the Inequalities of Projected Volumes and the Constructible Region,"  We study the following geometry problem: given a $2^n-1$ dimensional vector $\pi=\{\pi_S\}_{S\subseteq [n], S\ne \emptyset}$, is there an object $T\subseteq\mathbb{R}^n$ such that $\log(\mathsf{vol}(T_S))= \pi_S$, for all $S\subseteq [n]$, where $T_S$ is the projection of $T$ to the subspace spanned by the axes in $S$? If $\pi$ does correspond to an object in $\mathbb{R}^n$, we say that $\pi$ is {\em constructible}. We use $\Psi_n$ to denote the constructible region, i.e., the set of all constructible vectors in $\mathbb{R}^{2^n-1}$. In 1995, Bollob\'{a}s and Thomason showed that $\Psi_n$ is contained in a polyhedral cone, defined a class of so called uniform cover inequalities. We propose a new set of natural inequalities, called nonuniform-cover inequalities, which generalize the BT inequalities. We show that any linear inequality that all points in $\Psi_n$ satisfy must be a nonuniform-cover inequality. Based on this result and an example by Bollob\'{a}s and Thomason, we show that constructible region $\Psi_n$ is not even convex, and thus cannot be fully characterized by linear inequalities. We further show that some subclasses of the nonuniform-cover inequalities are not correct by various combinatorial constructions, which refutes a previous conjecture about $\Psi_n$. Finally, we conclude with an interesting conjecture regarding the convex hull of $\Psi_n$. ",Computer Science - Discrete Mathematics ; ,"Tan, Zihan ; Zeng, Liwei ; ","On the Inequalities of Projected Volumes and the Constructible Region  We study the following geometry problem: given a $2^n-1$ dimensional vector $\pi=\{\pi_S\}_{S\subseteq [n], S\ne \emptyset}$, is there an object $T\subseteq\mathbb{R}^n$ such that $\log(\mathsf{vol}(T_S))= \pi_S$, for all $S\subseteq [n]$, where $T_S$ is the projection of $T$ to the subspace spanned by the axes in $S$? If $\pi$ does correspond to an object in $\mathbb{R}^n$, we say that $\pi$ is {\em constructible}. We use $\Psi_n$ to denote the constructible region, i.e., the set of all constructible vectors in $\mathbb{R}^{2^n-1}$. In 1995, Bollob\'{a}s and Thomason showed that $\Psi_n$ is contained in a polyhedral cone, defined a class of so called uniform cover inequalities. We propose a new set of natural inequalities, called nonuniform-cover inequalities, which generalize the BT inequalities. We show that any linear inequality that all points in $\Psi_n$ satisfy must be a nonuniform-cover inequality. Based on this result and an example by Bollob\'{a}s and Thomason, we show that constructible region $\Psi_n$ is not even convex, and thus cannot be fully characterized by linear inequalities. We further show that some subclasses of the nonuniform-cover inequalities are not correct by various combinatorial constructions, which refutes a previous conjecture about $\Psi_n$. Finally, we conclude with an interesting conjecture regarding the convex hull of $\Psi_n$. ",inequalities project volumes constructible region study follow geometry problem give dimensional vector pi pi subseteq ne emptyset object subseteq mathbb log mathsf vol pi subseteq projection subspace span ax pi correspond object mathbb say pi em constructible use psi denote constructible region set constructible vectors mathbb bollob thomason show psi contain polyhedral cone define class call uniform cover inequalities propose new set natural inequalities call nonuniform cover inequalities generalize bt inequalities show linear inequality point psi satisfy must nonuniform cover inequality base result example bollob thomason show constructible region psi even convex thus cannot fully characterize linear inequalities show subclasses nonuniform cover inequalities correct various combinatorial constructions refute previous conjecture psi finally conclude interest conjecture regard convex hull psi,119,4,1410.8663.txt
http://arxiv.org/abs/1411.0187,Construction of Capacity-Achieving Lattice Codes: Polar Lattices,"  In this paper, we propose a new class of lattices constructed from polar codes, namely polar lattices, to achieve the capacity $\frac{1}{2}\log(1+\SNR)$ of the additive white Gaussian-noise (AWGN) channel. Our construction follows the multilevel approach of Forney \textit{et al.}, where we construct a capacity-achieving polar code on each level. The component polar codes are shown to be naturally nested, thereby fulfilling the requirement of the multilevel lattice construction. We prove that polar lattices are \emph{AWGN-good}. Furthermore, using the technique of source polarization, we propose discrete Gaussian shaping over the polar lattice to satisfy the power constraint. Both the construction and shaping are explicit, and the overall complexity of encoding and decoding is $O(N\log N)$ for any fixed target error probability. ",Computer Science - Information Theory ; ,"Liu, Ling ; Yan, Yanfei ; Ling, Cong ; Wu, Xiaofu ; ","Construction of Capacity-Achieving Lattice Codes: Polar Lattices  In this paper, we propose a new class of lattices constructed from polar codes, namely polar lattices, to achieve the capacity $\frac{1}{2}\log(1+\SNR)$ of the additive white Gaussian-noise (AWGN) channel. Our construction follows the multilevel approach of Forney \textit{et al.}, where we construct a capacity-achieving polar code on each level. The component polar codes are shown to be naturally nested, thereby fulfilling the requirement of the multilevel lattice construction. We prove that polar lattices are \emph{AWGN-good}. Furthermore, using the technique of source polarization, we propose discrete Gaussian shaping over the polar lattice to satisfy the power constraint. Both the construction and shaping are explicit, and the overall complexity of encoding and decoding is $O(N\log N)$ for any fixed target error probability. ",construction capacity achieve lattice cod polar lattices paper propose new class lattices construct polar cod namely polar lattices achieve capacity frac log snr additive white gaussian noise awgn channel construction follow multilevel approach forney textit et al construct capacity achieve polar code level component polar cod show naturally nest thereby fulfil requirement multilevel lattice construction prove polar lattices emph awgn good furthermore use technique source polarization propose discrete gaussian shape polar lattice satisfy power constraint construction shape explicit overall complexity encode decode log fix target error probability,87,5,1411.0187.txt
http://arxiv.org/abs/1411.0440,Modelling serendipity in a computational context,"  We understand the term serendipity to describe a creative process that develops, in context, with the active participation of a creative agent, but not entirely within that agent's control. While a system cannot be made to perform serendipitously on demand, nevertheless, we argue that its \emph{serendipity potential} can be increased by means of a suitable system architecture and other design choices. We distil a unified description of serendipitous occurrences from historical theorisations of serendipity and creativity. This takes the form of a framework with six phases: \emph{perception}, \emph{attention}, \emph{interest}, \emph{explanation}, \emph{bridge}, and \emph{valuation}. We then use this framework to organise a survey of literature in cognitive science, philosophy, and computing, which yields practical definitions of the six phases, along with heuristics for implementation. We use the resulting model to evaluate the serendipity potential of four existing systems developed by others, and two systems previously developed by two of us. Whereas most existing research that considers serendipity in a computing context deals with serendipity as a service, we relate theories of serendipity to artificial intelligence practice and the development of autonomous systems. We outline representative directions for future applications of our model in the domains of automated programming, recommender systems, and computational creativity. We conclude that it is feasible to equip computational systems with the potential for serendipity, and that this could be beneficial in varied artificial intelligence applications, particularly those designed to operate responsively in real-world contexts. ",Computer Science - Artificial Intelligence ; I.2.11 ; D.2.2 ; ,"Corneli, Joseph ; Jordanous, Anna ; Guckelsberger, Christian ; Pease, Alison ; Colton, Simon ; ","Modelling serendipity in a computational context  We understand the term serendipity to describe a creative process that develops, in context, with the active participation of a creative agent, but not entirely within that agent's control. While a system cannot be made to perform serendipitously on demand, nevertheless, we argue that its \emph{serendipity potential} can be increased by means of a suitable system architecture and other design choices. We distil a unified description of serendipitous occurrences from historical theorisations of serendipity and creativity. This takes the form of a framework with six phases: \emph{perception}, \emph{attention}, \emph{interest}, \emph{explanation}, \emph{bridge}, and \emph{valuation}. We then use this framework to organise a survey of literature in cognitive science, philosophy, and computing, which yields practical definitions of the six phases, along with heuristics for implementation. We use the resulting model to evaluate the serendipity potential of four existing systems developed by others, and two systems previously developed by two of us. Whereas most existing research that considers serendipity in a computing context deals with serendipity as a service, we relate theories of serendipity to artificial intelligence practice and the development of autonomous systems. We outline representative directions for future applications of our model in the domains of automated programming, recommender systems, and computational creativity. We conclude that it is feasible to equip computational systems with the potential for serendipity, and that this could be beneficial in varied artificial intelligence applications, particularly those designed to operate responsively in real-world contexts. ",model serendipity computational context understand term serendipity describe creative process develop context active participation creative agent entirely within agent control system cannot make perform serendipitously demand nevertheless argue emph serendipity potential increase mean suitable system architecture design choices distil unify description serendipitous occurrences historical theorisations serendipity creativity take form framework six phase emph perception emph attention emph interest emph explanation emph bridge emph valuation use framework organise survey literature cognitive science philosophy compute yield practical definitions six phase along heuristics implementation use result model evaluate serendipity potential four exist systems develop others two systems previously develop two us whereas exist research consider serendipity compute context deal serendipity service relate theories serendipity artificial intelligence practice development autonomous systems outline representative directions future applications model domains automate program recommender systems computational creativity conclude feasible equip computational systems potential serendipity could beneficial vary artificial intelligence applications particularly design operate responsively real world contexts,150,11,1411.0440.txt
http://arxiv.org/abs/1411.0998,Jointly Private Convex Programming,"  In this paper we present an extremely general method for approximately solving a large family of convex programs where the solution can be divided between different agents, subject to joint differential privacy. This class includes multi-commodity flow problems, general allocation problems, and multi-dimensional knapsack problems, among other examples. The accuracy of our algorithm depends on the \emph{number} of constraints that bind between individuals, but crucially, is \emph{nearly independent} of the number of primal variables and hence the number of agents who make up the problem. As the number of agents in a problem grows, the error we introduce often becomes negligible.   We also consider the setting where agents are strategic and have preferences over their part of the solution. For any convex program in this class that maximizes \emph{social welfare}, there is a generic reduction that makes the corresponding optimization \emph{approximately dominant strategy truthful} by charging agents prices for resources as a function of the approximately optimal dual variables, which are themselves computed under differential privacy. Our results substantially expand the class of problems that are known to be solvable under both privacy and incentive constraints. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computer Science and Game Theory ; ,"Hsu, Justin ; Huang, Zhiyi ; Roth, Aaron ; Wu, Zhiwei Steven ; ","Jointly Private Convex Programming  In this paper we present an extremely general method for approximately solving a large family of convex programs where the solution can be divided between different agents, subject to joint differential privacy. This class includes multi-commodity flow problems, general allocation problems, and multi-dimensional knapsack problems, among other examples. The accuracy of our algorithm depends on the \emph{number} of constraints that bind between individuals, but crucially, is \emph{nearly independent} of the number of primal variables and hence the number of agents who make up the problem. As the number of agents in a problem grows, the error we introduce often becomes negligible.   We also consider the setting where agents are strategic and have preferences over their part of the solution. For any convex program in this class that maximizes \emph{social welfare}, there is a generic reduction that makes the corresponding optimization \emph{approximately dominant strategy truthful} by charging agents prices for resources as a function of the approximately optimal dual variables, which are themselves computed under differential privacy. Our results substantially expand the class of problems that are known to be solvable under both privacy and incentive constraints. ",jointly private convex program paper present extremely general method approximately solve large family convex program solution divide different agents subject joint differential privacy class include multi commodity flow problems general allocation problems multi dimensional knapsack problems among examples accuracy algorithm depend emph number constraints bind individuals crucially emph nearly independent number primal variables hence number agents make problem number agents problem grow error introduce often become negligible also consider set agents strategic preferences part solution convex program class maximize emph social welfare generic reduction make correspond optimization emph approximately dominant strategy truthful charge agents price resources function approximately optimal dual variables compute differential privacy result substantially expand class problems know solvable privacy incentive constraints,114,8,1411.0998.txt
http://arxiv.org/abs/1411.1124,Nearly Linear-Time Packing and Covering LP Solvers,"  Packing and covering linear programs (PC-LPs) form an important class of linear programs (LPs) across computer science, operations research, and optimization. In 1993, Luby and Nisan constructed an iterative algorithm for approximately solving PC-LPs in nearly linear time, where the time complexity scales nearly linearly in $N$, the number of nonzero entries of the matrix, and polynomially in $\varepsilon$, the (multiplicative) approximation error. Unfortunately, all existing nearly linear-time algorithms for solving PC-LPs require time at least proportional to $\varepsilon^{-2}$.   In this paper, we break this longstanding barrier by designing a packing solver that runs in time $\tilde{O}(N \varepsilon^{-1})$ and covering LP solver that runs in time $\tilde{O}(N \varepsilon^{-1.5})$. Our packing solver can be extended to run in time $\tilde{O}(N \varepsilon^{-1})$ for a class of well-behaved covering programs. In a follow-up work, Wang et al. showed that all covering LPs can be converted into well-behaved ones by a reduction that blows up the problem size only logarithmically.   At high level, these two algorithms can be described as linear couplings of several first-order descent steps. This is an application of our linear coupling technique to problems that are not amenable to blackbox applications known iterative algorithms in convex optimization. ",Computer Science - Data Structures and Algorithms ; Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; Mathematics - Optimization and Control ; ,"Allen-Zhu, Zeyuan ; Orecchia, Lorenzo ; ","Nearly Linear-Time Packing and Covering LP Solvers  Packing and covering linear programs (PC-LPs) form an important class of linear programs (LPs) across computer science, operations research, and optimization. In 1993, Luby and Nisan constructed an iterative algorithm for approximately solving PC-LPs in nearly linear time, where the time complexity scales nearly linearly in $N$, the number of nonzero entries of the matrix, and polynomially in $\varepsilon$, the (multiplicative) approximation error. Unfortunately, all existing nearly linear-time algorithms for solving PC-LPs require time at least proportional to $\varepsilon^{-2}$.   In this paper, we break this longstanding barrier by designing a packing solver that runs in time $\tilde{O}(N \varepsilon^{-1})$ and covering LP solver that runs in time $\tilde{O}(N \varepsilon^{-1.5})$. Our packing solver can be extended to run in time $\tilde{O}(N \varepsilon^{-1})$ for a class of well-behaved covering programs. In a follow-up work, Wang et al. showed that all covering LPs can be converted into well-behaved ones by a reduction that blows up the problem size only logarithmically.   At high level, these two algorithms can be described as linear couplings of several first-order descent steps. This is an application of our linear coupling technique to problems that are not amenable to blackbox applications known iterative algorithms in convex optimization. ",nearly linear time pack cover lp solvers pack cover linear program pc lps form important class linear program lps across computer science operations research optimization luby nisan construct iterative algorithm approximately solve pc lps nearly linear time time complexity scale nearly linearly number nonzero entries matrix polynomially varepsilon multiplicative approximation error unfortunately exist nearly linear time algorithms solve pc lps require time least proportional varepsilon paper break longstanding barrier design pack solver run time tilde varepsilon cover lp solver run time tilde varepsilon pack solver extend run time tilde varepsilon class well behave cover program follow work wang et al show cover lps convert well behave ones reduction blow problem size logarithmically high level two algorithms describe linear couple several first order descent step application linear couple technique problems amenable blackbox applications know iterative algorithms convex optimization,137,1,1411.1124.txt
http://arxiv.org/abs/1411.1420,Eigenvectors of Orthogonally Decomposable Functions,"  The Eigendecomposition of quadratic forms (symmetric matrices) guaranteed by the spectral theorem is a foundational result in applied mathematics. Motivated by a shared structure found in inferential problems of recent interest---namely orthogonal tensor decompositions, Independent Component Analysis (ICA), topic models, spectral clustering, and Gaussian mixture learning---we generalize the eigendecomposition from quadratic forms to a broad class of ""orthogonally decomposable"" functions. We identify a key role of convexity in our extension, and we generalize two traditional characterizations of eigenvectors: First, the eigenvectors of a quadratic form arise from the optima structure of the quadratic form on the sphere. Second, the eigenvectors are the fixed points of the power iteration.   In our setting, we consider a simple first order generalization of the power method which we call gradient iteration. It leads to efficient and easily implementable methods for basis recovery. It includes influential Machine Learning methods such as cumulant-based FastICA and the tensor power iteration for orthogonally decomposable tensors as special cases.   We provide a complete theoretical analysis of gradient iteration using the structure theory of discrete dynamical systems to show almost sure convergence and fast (super-linear) convergence rates. The analysis also extends to the case when the observed function is only approximately orthogonally decomposable, with bounds that are polynomial in dimension and other relevant parameters, such as perturbation size. Our perturbation results can be considered as a non-linear version of the classical Davis-Kahan theorem for perturbations of eigenvectors of symmetric matrices. ",Computer Science - Machine Learning ; ,"Belkin, Mikhail ; Rademacher, Luis ; Voss, James ; ","Eigenvectors of Orthogonally Decomposable Functions  The Eigendecomposition of quadratic forms (symmetric matrices) guaranteed by the spectral theorem is a foundational result in applied mathematics. Motivated by a shared structure found in inferential problems of recent interest---namely orthogonal tensor decompositions, Independent Component Analysis (ICA), topic models, spectral clustering, and Gaussian mixture learning---we generalize the eigendecomposition from quadratic forms to a broad class of ""orthogonally decomposable"" functions. We identify a key role of convexity in our extension, and we generalize two traditional characterizations of eigenvectors: First, the eigenvectors of a quadratic form arise from the optima structure of the quadratic form on the sphere. Second, the eigenvectors are the fixed points of the power iteration.   In our setting, we consider a simple first order generalization of the power method which we call gradient iteration. It leads to efficient and easily implementable methods for basis recovery. It includes influential Machine Learning methods such as cumulant-based FastICA and the tensor power iteration for orthogonally decomposable tensors as special cases.   We provide a complete theoretical analysis of gradient iteration using the structure theory of discrete dynamical systems to show almost sure convergence and fast (super-linear) convergence rates. The analysis also extends to the case when the observed function is only approximately orthogonally decomposable, with bounds that are polynomial in dimension and other relevant parameters, such as perturbation size. Our perturbation results can be considered as a non-linear version of the classical Davis-Kahan theorem for perturbations of eigenvectors of symmetric matrices. ",eigenvectors orthogonally decomposable function eigendecomposition quadratic form symmetric matrices guarantee spectral theorem foundational result apply mathematics motivate share structure find inferential problems recent interest namely orthogonal tensor decompositions independent component analysis ica topic model spectral cluster gaussian mixture learn generalize eigendecomposition quadratic form broad class orthogonally decomposable function identify key role convexity extension generalize two traditional characterizations eigenvectors first eigenvectors quadratic form arise optima structure quadratic form sphere second eigenvectors fix point power iteration set consider simple first order generalization power method call gradient iteration lead efficient easily implementable methods basis recovery include influential machine learn methods cumulant base fastica tensor power iteration orthogonally decomposable tensors special case provide complete theoretical analysis gradient iteration use structure theory discrete dynamical systems show almost sure convergence fast super linear convergence rat analysis also extend case observe function approximately orthogonally decomposable bound polynomial dimension relevant parameters perturbation size perturbation result consider non linear version classical davis kahan theorem perturbations eigenvectors symmetric matrices,159,7,1411.1420.txt
http://arxiv.org/abs/1411.1751,Playing the Wrong Game: Bounding Externalities in Diverse Populations of   Agents,"  The robustness of multiagent systems can be affected by mistakes or behavioral biases (e.g., risk-aversion, altruism, toll-sensitivity), with some agents playing the ""wrong game."" This can change the set of equilibria, and may in turn harm or improve the social welfare of agents in the system. We are interested in bounding what we call the biased price of anarchy (BPoA) in populations with diverse agent behaviors, which is the ratio between welfare in the ""wrong"" equilibrium and optimal welfare.   We study nonatomic routing games, and derive an externality bound that depends on a key topological parameter of the underlying network.   We then prove two general BPoA bounds for games with diverse populations: one that relies on the network structure and the average bias of all agents in the population, and one that is independent of the structure but depends on the maximal bias. Both types of bounds can be combined with known results to derive concrete BPoA bounds for a variety of specific behaviors (e.g., varied levels of risk-aversion). ",Computer Science - Computer Science and Game Theory ; Computer Science - Multiagent Systems ; ,"Meir, Reshef ; Parkes, David ; ","Playing the Wrong Game: Bounding Externalities in Diverse Populations of   Agents  The robustness of multiagent systems can be affected by mistakes or behavioral biases (e.g., risk-aversion, altruism, toll-sensitivity), with some agents playing the ""wrong game."" This can change the set of equilibria, and may in turn harm or improve the social welfare of agents in the system. We are interested in bounding what we call the biased price of anarchy (BPoA) in populations with diverse agent behaviors, which is the ratio between welfare in the ""wrong"" equilibrium and optimal welfare.   We study nonatomic routing games, and derive an externality bound that depends on a key topological parameter of the underlying network.   We then prove two general BPoA bounds for games with diverse populations: one that relies on the network structure and the average bias of all agents in the population, and one that is independent of the structure but depends on the maximal bias. Both types of bounds can be combined with known results to derive concrete BPoA bounds for a variety of specific behaviors (e.g., varied levels of risk-aversion). ",play wrong game bound externalities diverse populations agents robustness multiagent systems affect mistake behavioral bias risk aversion altruism toll sensitivity agents play wrong game change set equilibria may turn harm improve social welfare agents system interest bound call bias price anarchy bpoa populations diverse agent behaviors ratio welfare wrong equilibrium optimal welfare study nonatomic rout game derive externality bind depend key topological parameter underlie network prove two general bpoa bound game diverse populations one rely network structure average bias agents population one independent structure depend maximal bias type bound combine know result derive concrete bpoa bound variety specific behaviors vary level risk aversion,103,8,1411.1751.txt
http://arxiv.org/abs/1411.2419,Beta-expansions of rational numbers in quadratic Pisot bases,"  We study rational numbers with purely periodic R\'enyi $\beta$-expansions. For bases $\beta$ satisfying $\beta^2=a\beta+b$ with $b$ dividing $a$, we give a necessary and sufficient condition for $\gamma(\beta)=1$, i.e., that all rational numbers $p/q\in[0,1)$ with $\gcd(q,b)=1$ have a purely periodic $\beta$-expansion. A simple algorithm for determining the value of $\gamma(\beta)$ for all quadratic Pisot numbers $\beta$ is described. ",Mathematics - Dynamical Systems ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; Mathematics - Number Theory ; 11A63 (11R06 37B10) ; ,"Hejda, Tomáš ; Steiner, Wolfgang ; ","Beta-expansions of rational numbers in quadratic Pisot bases  We study rational numbers with purely periodic R\'enyi $\beta$-expansions. For bases $\beta$ satisfying $\beta^2=a\beta+b$ with $b$ dividing $a$, we give a necessary and sufficient condition for $\gamma(\beta)=1$, i.e., that all rational numbers $p/q\in[0,1)$ with $\gcd(q,b)=1$ have a purely periodic $\beta$-expansion. A simple algorithm for determining the value of $\gamma(\beta)$ for all quadratic Pisot numbers $\beta$ is described. ",beta expansions rational number quadratic pisot base study rational number purely periodic enyi beta expansions base beta satisfy beta beta divide give necessary sufficient condition gamma beta rational number gcd purely periodic beta expansion simple algorithm determine value gamma beta quadratic pisot number beta describe,45,4,1411.2419.txt
http://arxiv.org/abs/1411.3010,Computational Complexity of Functions,"  Below is a translation from my Russian paper. I added references, unavailable to me in Moscow. Similar results have been also given in [Schnorr Stumpf 75] (see also [Lynch 75]). Earlier relevant work (classical theorems like Compression, Speed-up, etc.) was done in [Tseitin 56, Rabin 59, Hartmanis Stearns 65, Blum 67, Trakhtenbrot 67, Meyer Fischer 72].   I translated only the part with the statement of the results. Instead of the proof part I appended a later (1979, unpublished) proof sketch of a slightly tighter version. The improvement is based on the results of [Meyer Winklmann 78, Sipser 78]. Meyer and Winklmann extended earlier versions to machines with a separate input and working tape, thus allowing complexities smaller than the input length (down to its log). Sipser showed the space-bounded Halting Problem to require only additive constant overhead. The proof in the appendix below employs both advances to extend the original proofs to machines with a fixed alphabet and a separate input and working space. The extension has no (even logarithmic) restrictions on complexity and no overhead (beyond an additive constant). The sketch is very brief and a more detailed exposition is expected later: [Seiferas Meyer]. ",Computer Science - Computational Complexity ; ,"Levin, Leonid A. ; ","Computational Complexity of Functions  Below is a translation from my Russian paper. I added references, unavailable to me in Moscow. Similar results have been also given in [Schnorr Stumpf 75] (see also [Lynch 75]). Earlier relevant work (classical theorems like Compression, Speed-up, etc.) was done in [Tseitin 56, Rabin 59, Hartmanis Stearns 65, Blum 67, Trakhtenbrot 67, Meyer Fischer 72].   I translated only the part with the statement of the results. Instead of the proof part I appended a later (1979, unpublished) proof sketch of a slightly tighter version. The improvement is based on the results of [Meyer Winklmann 78, Sipser 78]. Meyer and Winklmann extended earlier versions to machines with a separate input and working tape, thus allowing complexities smaller than the input length (down to its log). Sipser showed the space-bounded Halting Problem to require only additive constant overhead. The proof in the appendix below employs both advances to extend the original proofs to machines with a fixed alphabet and a separate input and working space. The extension has no (even logarithmic) restrictions on complexity and no overhead (beyond an additive constant). The sketch is very brief and a more detailed exposition is expected later: [Seiferas Meyer]. ",computational complexity function translation russian paper add reference unavailable moscow similar result also give schnorr stumpf see also lynch earlier relevant work classical theorems like compression speed etc do tseitin rabin hartmanis stearns blum trakhtenbrot meyer fischer translate part statement result instead proof part append later unpublished proof sketch slightly tighter version improvement base result meyer winklmann sipser meyer winklmann extend earlier versions machine separate input work tape thus allow complexities smaller input length log sipser show space bound halt problem require additive constant overhead proof appendix employ advance extend original proof machine fix alphabet separate input work space extension even logarithmic restrictions complexity overhead beyond additive constant sketch brief detail exposition expect later seiferas meyer,116,8,1411.3010.txt
http://arxiv.org/abs/1411.3140,Social media fingerprints of unemployment,"  Recent wide-spread adoption of electronic and pervasive technologies has enabled the study of human behavior at an unprecedented level, uncovering universal patterns underlying human activity, mobility, and inter-personal communication. In the present work, we investigate whether deviations from these universal patterns may reveal information about the socio-economical status of geographical regions. We quantify the extent to which deviations in diurnal rhythm, mobility patterns, and communication styles across regions relate to their unemployment incidence. For this we examine a country-scale publicly articulated social media dataset, where we quantify individual behavioral features from over 145 million geo-located messages distributed among more than 340 different Spanish economic regions, inferred by computing communities of cohesive mobility fluxes. We find that regions exhibiting more diverse mobility fluxes, earlier diurnal rhythms, and more correct grammatical styles display lower unemployment rates. As a result, we provide a simple model able to produce accurate, easily interpretable reconstruction of regional unemployment incidence from their social-media digital fingerprints alone. Our results show that cost-effective economical indicators can be built based on publicly-available social media datasets. ","Physics - Physics and Society ; Computer Science - Social and Information Networks ; Physics - Data Analysis, Statistics and Probability ; ","Llorente, Alejandro ; Garcia-Herranz, Manuel ; Cebrian, Manuel ; Moro, Esteban ; ","Social media fingerprints of unemployment  Recent wide-spread adoption of electronic and pervasive technologies has enabled the study of human behavior at an unprecedented level, uncovering universal patterns underlying human activity, mobility, and inter-personal communication. In the present work, we investigate whether deviations from these universal patterns may reveal information about the socio-economical status of geographical regions. We quantify the extent to which deviations in diurnal rhythm, mobility patterns, and communication styles across regions relate to their unemployment incidence. For this we examine a country-scale publicly articulated social media dataset, where we quantify individual behavioral features from over 145 million geo-located messages distributed among more than 340 different Spanish economic regions, inferred by computing communities of cohesive mobility fluxes. We find that regions exhibiting more diverse mobility fluxes, earlier diurnal rhythms, and more correct grammatical styles display lower unemployment rates. As a result, we provide a simple model able to produce accurate, easily interpretable reconstruction of regional unemployment incidence from their social-media digital fingerprints alone. Our results show that cost-effective economical indicators can be built based on publicly-available social media datasets. ",social media fingerprint unemployment recent wide spread adoption electronic pervasive technologies enable study human behavior unprecedented level uncover universal pattern underlie human activity mobility inter personal communication present work investigate whether deviations universal pattern may reveal information socio economical status geographical regions quantify extent deviations diurnal rhythm mobility pattern communication style across regions relate unemployment incidence examine country scale publicly articulate social media dataset quantify individual behavioral feature million geo locate message distribute among different spanish economic regions infer compute communities cohesive mobility flux find regions exhibit diverse mobility flux earlier diurnal rhythms correct grammatical style display lower unemployment rat result provide simple model able produce accurate easily interpretable reconstruction regional unemployment incidence social media digital fingerprint alone result show cost effective economical indicators build base publicly available social media datasets,131,0,1411.3140.txt
http://arxiv.org/abs/1411.3444,Growing Scale-free Networks by a Mediation-Driven Attachment Rule,"  We propose a model that generates a new class of networks exhibiting power-law degree distribution with a spectrum of exponents depending on the number of links ($m$) with which incoming nodes join the existing network. Unlike the Barab\'{a}si-Albert (BA) model, each new node first picks an existing node at random, and connects not with this but with $m$ of its neighbors also picked at random. Counterintuitively enough, such a mediation-driven attachment rule results not only in preferential but super-preferential attachment, albeit in disguise. We show that for small $m$, the dynamics of our model is governed by winners take all phenomenon, and for higher $m$ it is governed by winners take some. Besides, we show that the mean of the inverse harmonic mean of degrees of the neighborhood of all existing nodes is a measure that can well qualify how straight the degree distribution is. ",Physics - Physics and Society ; Condensed Matter - Statistical Mechanics ; Computer Science - Social and Information Networks ; ,"Hassan, Kamrul ; Islam, Liana ; ","Growing Scale-free Networks by a Mediation-Driven Attachment Rule  We propose a model that generates a new class of networks exhibiting power-law degree distribution with a spectrum of exponents depending on the number of links ($m$) with which incoming nodes join the existing network. Unlike the Barab\'{a}si-Albert (BA) model, each new node first picks an existing node at random, and connects not with this but with $m$ of its neighbors also picked at random. Counterintuitively enough, such a mediation-driven attachment rule results not only in preferential but super-preferential attachment, albeit in disguise. We show that for small $m$, the dynamics of our model is governed by winners take all phenomenon, and for higher $m$ it is governed by winners take some. Besides, we show that the mean of the inverse harmonic mean of degrees of the neighborhood of all existing nodes is a measure that can well qualify how straight the degree distribution is. ",grow scale free network mediation drive attachment rule propose model generate new class network exhibit power law degree distribution spectrum exponents depend number link incoming nod join exist network unlike barab si albert ba model new node first pick exist node random connect neighbor also pick random counterintuitively enough mediation drive attachment rule result preferential super preferential attachment albeit disguise show small dynamics model govern winners take phenomenon higher govern winners take besides show mean inverse harmonic mean degrees neighborhood exist nod measure well qualify straight degree distribution,88,6,1411.3444.txt
http://arxiv.org/abs/1411.3517,Derandomized Graph Product Results using the Low Degree Long Code,"  In this paper, we address the question of whether the recent derandomization results obtained by the use of the low-degree long code can be extended to other product settings. We consider two settings: (1) the graph product results of Alon, Dinur, Friedgut and Sudakov [GAFA, 2004] and (2) the ""majority is stablest"" type of result obtained by Dinur, Mossel and Regev [SICOMP, 2009] and Dinur and Shinkar [In Proc. APPROX, 2010] while studying the hardness of approximate graph coloring.   In our first result, we show that there exists a considerably smaller subgraph of $K_3^{\otimes R}$ which exhibits the following property (shown for $K_3^{\otimes R}$ by Alon et al.): independent sets close in size to the maximum independent set are well approximated by dictators.   The ""majority is stablest"" type of result of Dinur et al. and Dinur and Shinkar shows that if there exist two sets of vertices $A$ and $B$ in $K_3^{\otimes R}$ with very few edges with one endpoint in $A$ and another in $B$, then it must be the case that the two sets $A$ and $B$ share a single influential coordinate. In our second result, we show that a similar ""majority is stablest"" statement holds good for a considerably smaller subgraph of $K_3^{\otimes R}$. Furthermore using this result, we give a more efficient reduction from Unique Games to the graph coloring problem, leading to improved hardness of approximation results for coloring. ",Computer Science - Computational Complexity ; ,"Dinur, Irit ; Harsha, Prahladh ; Srinivasan, Srikanth ; Varma, Girish ; ","Derandomized Graph Product Results using the Low Degree Long Code  In this paper, we address the question of whether the recent derandomization results obtained by the use of the low-degree long code can be extended to other product settings. We consider two settings: (1) the graph product results of Alon, Dinur, Friedgut and Sudakov [GAFA, 2004] and (2) the ""majority is stablest"" type of result obtained by Dinur, Mossel and Regev [SICOMP, 2009] and Dinur and Shinkar [In Proc. APPROX, 2010] while studying the hardness of approximate graph coloring.   In our first result, we show that there exists a considerably smaller subgraph of $K_3^{\otimes R}$ which exhibits the following property (shown for $K_3^{\otimes R}$ by Alon et al.): independent sets close in size to the maximum independent set are well approximated by dictators.   The ""majority is stablest"" type of result of Dinur et al. and Dinur and Shinkar shows that if there exist two sets of vertices $A$ and $B$ in $K_3^{\otimes R}$ with very few edges with one endpoint in $A$ and another in $B$, then it must be the case that the two sets $A$ and $B$ share a single influential coordinate. In our second result, we show that a similar ""majority is stablest"" statement holds good for a considerably smaller subgraph of $K_3^{\otimes R}$. Furthermore using this result, we give a more efficient reduction from Unique Games to the graph coloring problem, leading to improved hardness of approximation results for coloring. ",derandomized graph product result use low degree long code paper address question whether recent derandomization result obtain use low degree long code extend product settings consider two settings graph product result alon dinur friedgut sudakov gafa majority stablest type result obtain dinur mossel regev sicomp dinur shinkar proc approx study hardness approximate graph color first result show exist considerably smaller subgraph otimes exhibit follow property show otimes alon et al independent set close size maximum independent set well approximate dictators majority stablest type result dinur et al dinur shinkar show exist two set vertices otimes edge one endpoint another must case two set share single influential coordinate second result show similar majority stablest statement hold good considerably smaller subgraph otimes furthermore use result give efficient reduction unique game graph color problem lead improve hardness approximation result color,137,13,1411.3517.txt
http://arxiv.org/abs/1411.4097,A Game Theoretic Model for the Formation of Navigable Small-World   Networks --- the Tradeoff between Distance and Reciprocity,"  Kleinberg proposed a family of small-world networks to explain the navigability of large-scale real-world social networks. However, the underlying mechanism that drives real networks to be navigable is not yet well understood. In this paper, we present a game theoretic model for the formation of navigable small world networks. We model the network formation as a Distance-Reciprocity Balanced (DRB) game in which people seek for both high reciprocity and long-distance relationships. We show that the game has only two Nash equilibria: One is the navigable small-world network, and the other is the random network in which each node connects with each other node with equal probability. We further show that the navigable small world is very stable --- (a) no collusion of any size would benefit from deviating from it; and (b) after an arbitrary deviations of a large random set of nodes, the network would return to the navigable small world as soon as every node takes one best-response step. In contrast, for the random network, a small group collusion or random perturbations is guaranteed to move the network to the navigable network as soon as every node takes one best-response step. Moreover, we show that navigable small world has much better social welfare than the random network, and provide the price-of-anarchy and price-of-stability results of the game. Our empirical evaluation demonstrates that the system always converges to the navigable network even when limited or no information about other players' strategies is available, and the DRB game simulated on the real-world network leads to navigability characteristic that is very close to that of the real network. Our theoretical and empirical analyses provide important new insight on the connection between distance, reciprocity and navigability in social networks. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Yang, Zhi ; Chen, Wei ; ","A Game Theoretic Model for the Formation of Navigable Small-World   Networks --- the Tradeoff between Distance and Reciprocity  Kleinberg proposed a family of small-world networks to explain the navigability of large-scale real-world social networks. However, the underlying mechanism that drives real networks to be navigable is not yet well understood. In this paper, we present a game theoretic model for the formation of navigable small world networks. We model the network formation as a Distance-Reciprocity Balanced (DRB) game in which people seek for both high reciprocity and long-distance relationships. We show that the game has only two Nash equilibria: One is the navigable small-world network, and the other is the random network in which each node connects with each other node with equal probability. We further show that the navigable small world is very stable --- (a) no collusion of any size would benefit from deviating from it; and (b) after an arbitrary deviations of a large random set of nodes, the network would return to the navigable small world as soon as every node takes one best-response step. In contrast, for the random network, a small group collusion or random perturbations is guaranteed to move the network to the navigable network as soon as every node takes one best-response step. Moreover, we show that navigable small world has much better social welfare than the random network, and provide the price-of-anarchy and price-of-stability results of the game. Our empirical evaluation demonstrates that the system always converges to the navigable network even when limited or no information about other players' strategies is available, and the DRB game simulated on the real-world network leads to navigability characteristic that is very close to that of the real network. Our theoretical and empirical analyses provide important new insight on the connection between distance, reciprocity and navigability in social networks. ",game theoretic model formation navigable small world network tradeoff distance reciprocity kleinberg propose family small world network explain navigability large scale real world social network however underlie mechanism drive real network navigable yet well understand paper present game theoretic model formation navigable small world network model network formation distance reciprocity balance drb game people seek high reciprocity long distance relationships show game two nash equilibria one navigable small world network random network node connect node equal probability show navigable small world stable collusion size would benefit deviate arbitrary deviations large random set nod network would return navigable small world soon every node take one best response step contrast random network small group collusion random perturbations guarantee move network navigable network soon every node take one best response step moreover show navigable small world much better social welfare random network provide price anarchy price stability result game empirical evaluation demonstrate system always converge navigable network even limit information players strategies available drb game simulate real world network lead navigability characteristic close real network theoretical empirical analyse provide important new insight connection distance reciprocity navigability social network,185,6,1411.4097.txt
http://arxiv.org/abs/1411.4498,Scalable Wake-up of Multi-Channel Single-Hop Radio Networks,"  We consider single-hop radio networks with multiple channels as a model of wireless networks. There are $n$ stations connected to $b$ radio channels that do not provide collision detection. A station uses all the channels concurrently and independently. Some $k$ stations may become active spontaneously at arbitrary times. The goal is to wake up the network, which occurs when all the stations hear a successful transmission on some channel. Duration of a waking-up execution is measured starting from the first spontaneous activation. We present a deterministic algorithm for the general problem that wakes up the network in $O(k\log^{1/b} k\log n)$ time, where $k$ is unknown. We give a deterministic scalable algorithm for the special case when $b>d \log \log n$, for some constant $d>1$, which wakes up the network in $O(\frac{k}{b}\log n\log(b\log n))$ time, with $k$ unknown. This algorithm misses time optimality by at most a factor of $O(\log n(\log b +\log\log n))$, because any deterministic algorithm requires $\Omega(\frac{k}{b}\log \frac{n}{k})$ time. We give a randomized algorithm that wakes up the network within $O(k^{1/b}\ln \frac{1}{\epsilon})$ rounds with a probability that is at least $1-\epsilon$, for any $0<\epsilon<1$, where $k$ is known. We also consider a model of jamming, in which each channel in any round may be jammed to prevent a successful transmission, which happens with some known parameter probability $p$, independently across all channels and rounds. For this model, we give two deterministic algorithms for unknown~$k$: one wakes up the network in time $O(\log^{-1}(\frac{1}{p})\, k\log n\log^{1/b} k)$, and the other in time $O(\log^{-1}(\frac{1}{p}) \, \frac{k}{b} \log n\log(b\log n))$ but assuming the inequality $b>\log(128b\log n)$, both with a probability that is at least $1-1/\mbox{poly}(n)$. ",Computer Science - Data Structures and Algorithms ; ,"Chlebus, Bogdan S. ; De Marco, Gianluca ; Kowalski, Dariusz R. ; ","Scalable Wake-up of Multi-Channel Single-Hop Radio Networks  We consider single-hop radio networks with multiple channels as a model of wireless networks. There are $n$ stations connected to $b$ radio channels that do not provide collision detection. A station uses all the channels concurrently and independently. Some $k$ stations may become active spontaneously at arbitrary times. The goal is to wake up the network, which occurs when all the stations hear a successful transmission on some channel. Duration of a waking-up execution is measured starting from the first spontaneous activation. We present a deterministic algorithm for the general problem that wakes up the network in $O(k\log^{1/b} k\log n)$ time, where $k$ is unknown. We give a deterministic scalable algorithm for the special case when $b>d \log \log n$, for some constant $d>1$, which wakes up the network in $O(\frac{k}{b}\log n\log(b\log n))$ time, with $k$ unknown. This algorithm misses time optimality by at most a factor of $O(\log n(\log b +\log\log n))$, because any deterministic algorithm requires $\Omega(\frac{k}{b}\log \frac{n}{k})$ time. We give a randomized algorithm that wakes up the network within $O(k^{1/b}\ln \frac{1}{\epsilon})$ rounds with a probability that is at least $1-\epsilon$, for any $0<\epsilon<1$, where $k$ is known. We also consider a model of jamming, in which each channel in any round may be jammed to prevent a successful transmission, which happens with some known parameter probability $p$, independently across all channels and rounds. For this model, we give two deterministic algorithms for unknown~$k$: one wakes up the network in time $O(\log^{-1}(\frac{1}{p})\, k\log n\log^{1/b} k)$, and the other in time $O(\log^{-1}(\frac{1}{p}) \, \frac{k}{b} \log n\log(b\log n))$ but assuming the inequality $b>\log(128b\log n)$, both with a probability that is at least $1-1/\mbox{poly}(n)$. ",scalable wake multi channel single hop radio network consider single hop radio network multiple channel model wireless network station connect radio channel provide collision detection station use channel concurrently independently station may become active spontaneously arbitrary time goal wake network occur station hear successful transmission channel duration wake execution measure start first spontaneous activation present deterministic algorithm general problem wake network log log time unknown give deterministic scalable algorithm special case log log constant wake network frac log log log time unknown algorithm miss time optimality factor log log log log deterministic algorithm require omega frac log frac time give randomize algorithm wake network within ln frac epsilon round probability least epsilon epsilon know also consider model jam channel round may jam prevent successful transmission happen know parameter probability independently across channel round model give two deterministic algorithms unknown one wake network time log frac log log time log frac frac log log log assume inequality log log probability least mbox poly,162,1,1411.4498.txt
http://arxiv.org/abs/1411.4696,Security Analysis of the Unrestricted Identity-Based Aggregate Signature   Scheme,"  Aggregate signatures allow anyone to combine different signatures signed by different signers on different messages into a single short signature. An ideal aggregate signature scheme is an identity-based aggregate signature (IBAS) scheme that supports full aggregation since it can reduce the total transmitted data by using an identity string as a public key and anyone can freely aggregate different signatures. Constructing a secure IBAS scheme that supports full aggregation in bilinear maps is an important open problem. Recently, Yuan {\it et al.} proposed an IBAS scheme with full aggregation in bilinear maps and claimed its security in the random oracle model under the computational Diffie-Hellman assumption. In this paper, we show that there exists an efficient forgery attacker on their IBAS scheme and their security proof has a serious flaw. ",Computer Science - Cryptography and Security ; ,"Lee, Kwangsu ; Lee, Dong Hoon ; ","Security Analysis of the Unrestricted Identity-Based Aggregate Signature   Scheme  Aggregate signatures allow anyone to combine different signatures signed by different signers on different messages into a single short signature. An ideal aggregate signature scheme is an identity-based aggregate signature (IBAS) scheme that supports full aggregation since it can reduce the total transmitted data by using an identity string as a public key and anyone can freely aggregate different signatures. Constructing a secure IBAS scheme that supports full aggregation in bilinear maps is an important open problem. Recently, Yuan {\it et al.} proposed an IBAS scheme with full aggregation in bilinear maps and claimed its security in the random oracle model under the computational Diffie-Hellman assumption. In this paper, we show that there exists an efficient forgery attacker on their IBAS scheme and their security proof has a serious flaw. ",security analysis unrestricted identity base aggregate signature scheme aggregate signatures allow anyone combine different signatures sign different signers different message single short signature ideal aggregate signature scheme identity base aggregate signature ibas scheme support full aggregation since reduce total transmit data use identity string public key anyone freely aggregate different signatures construct secure ibas scheme support full aggregation bilinear map important open problem recently yuan et al propose ibas scheme full aggregation bilinear map claim security random oracle model computational diffie hellman assumption paper show exist efficient forgery attacker ibas scheme security proof serious flaw,95,12,1411.4696.txt
http://arxiv.org/abs/1411.4823,Automated Reasoning in Deontic Logic,"  Deontic logic is a very well researched branch of mathematical logic and philosophy. Various kinds of deontic logics are discussed for different application domains like argumentation theory, legal reasoning, and acts in multi-agent systems. In this paper, we show how standard deontic logic can be stepwise transformed into description logic and DL- clauses, such that it can be processed by Hyper, a high performance theorem prover which uses a hypertableau calculus. Two use cases, one from multi-agent research and one from the development of normative system are investigated. ",Computer Science - Artificial Intelligence ; ,"Furbach, Ulrich ; Schon, Claudia ; Stolzenburg, Frieder ; ","Automated Reasoning in Deontic Logic  Deontic logic is a very well researched branch of mathematical logic and philosophy. Various kinds of deontic logics are discussed for different application domains like argumentation theory, legal reasoning, and acts in multi-agent systems. In this paper, we show how standard deontic logic can be stepwise transformed into description logic and DL- clauses, such that it can be processed by Hyper, a high performance theorem prover which uses a hypertableau calculus. Two use cases, one from multi-agent research and one from the development of normative system are investigated. ",automate reason deontic logic deontic logic well research branch mathematical logic philosophy various kinds deontic logics discuss different application domains like argumentation theory legal reason act multi agent systems paper show standard deontic logic stepwise transform description logic dl clauses process hyper high performance theorem prover use hypertableau calculus two use case one multi agent research one development normative system investigate,61,2,1411.4823.txt
http://arxiv.org/abs/1411.4840,Dual-induced multifractality in online viewing activity,"  Although recent studies have found that the long-term correlations relating to the fat-tailed distribution of inter-event times exist in human activity, and that these correlations indicate the presence of fractality, the property of fractality and its origin have not been analyzed. We use both DFA and MFDFA to analyze the time series in online viewing activity separating from Movielens and Netflix. We find long-term correlations at both the individual and communal level, and that the extent of correlation at the individual level is determined by the activity level. These long-term correlations also indicate that there is fractality in the pattern of online viewing. And, we firstly find a multifractality that results from the combined effect of the fat-tailed distribution of inter-event times (i.e., the times between successive viewing actions of individual) and the long-term correlations in online viewing activity and verify this finding using three synthesized series. Therefore, it can be concluded that the multifractality in online viewing activity is caused by both the fat-tailed distribution of inter-event times and the long-term correlations, and that this enlarges the generic property of human activity to include not just physical space, but also cyberspace. ","Physics - Physics and Society ; Computer Science - Social and Information Networks ; Physics - Data Analysis, Statistics and Probability ; ","Qin, Yu-Hao ; Zhao, Zhi-Dan ; Cai, Shi-Min ; Gao, Liang ; Stanley, H. Eugene ; ","Dual-induced multifractality in online viewing activity  Although recent studies have found that the long-term correlations relating to the fat-tailed distribution of inter-event times exist in human activity, and that these correlations indicate the presence of fractality, the property of fractality and its origin have not been analyzed. We use both DFA and MFDFA to analyze the time series in online viewing activity separating from Movielens and Netflix. We find long-term correlations at both the individual and communal level, and that the extent of correlation at the individual level is determined by the activity level. These long-term correlations also indicate that there is fractality in the pattern of online viewing. And, we firstly find a multifractality that results from the combined effect of the fat-tailed distribution of inter-event times (i.e., the times between successive viewing actions of individual) and the long-term correlations in online viewing activity and verify this finding using three synthesized series. Therefore, it can be concluded that the multifractality in online viewing activity is caused by both the fat-tailed distribution of inter-event times and the long-term correlations, and that this enlarges the generic property of human activity to include not just physical space, but also cyberspace. ",dual induce multifractality online view activity although recent study find long term correlations relate fat tail distribution inter event time exist human activity correlations indicate presence fractality property fractality origin analyze use dfa mfdfa analyze time series online view activity separate movielens netflix find long term correlations individual communal level extent correlation individual level determine activity level long term correlations also indicate fractality pattern online view firstly find multifractality result combine effect fat tail distribution inter event time time successive view action individual long term correlations online view activity verify find use three synthesize series therefore conclude multifractality online view activity cause fat tail distribution inter event time long term correlations enlarge generic property human activity include physical space also cyberspace,121,11,1411.4840.txt
http://arxiv.org/abs/1411.5123,Deterministic Edge Connectivity in Near-Linear Time,"  We present a deterministic near-linear time algorithm that computes the edge-connectivity and finds a minimum cut for a simple undirected unweighted graph G with n vertices and m edges. This is the first o(mn) time deterministic algorithm for the problem. In near-linear time we can also construct the classic cactus representation of all minimum cuts.   The previous fastest deterministic algorithm by Gabow from STOC'91 took ~O(m+k^2 n), where k is the edge connectivity, but k could be Omega(n).   At STOC'96 Karger presented a randomized near linear time Monte Carlo algorithm for the minimum cut problem. As he points out, there is no better way of certifying the minimality of the returned cut than to use Gabow's slower deterministic algorithm and compare sizes.   Our main technical contribution is a near-linear time algorithm that contract vertex sets of a simple input graph G with minimum degree d, producing a multigraph with ~O(m/d) edges which preserves all minimum cuts of G with at least 2 vertices on each side.   In our deterministic near-linear time algorithm, we will decompose the problem via low-conductance cuts found using PageRank a la Brin and Page (1998), as analyzed by Andersson, Chung, and Lang at FOCS'06. Normally such algorithms for low-conductance cuts are randomized Monte Carlo algorithms, because they rely on guessing a good start vertex. However, in our case, we have so much structure that no guessing is needed. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; ,"Kawarabayashi, Ken-ichi ; Thorup, Mikkel ; ","Deterministic Edge Connectivity in Near-Linear Time  We present a deterministic near-linear time algorithm that computes the edge-connectivity and finds a minimum cut for a simple undirected unweighted graph G with n vertices and m edges. This is the first o(mn) time deterministic algorithm for the problem. In near-linear time we can also construct the classic cactus representation of all minimum cuts.   The previous fastest deterministic algorithm by Gabow from STOC'91 took ~O(m+k^2 n), where k is the edge connectivity, but k could be Omega(n).   At STOC'96 Karger presented a randomized near linear time Monte Carlo algorithm for the minimum cut problem. As he points out, there is no better way of certifying the minimality of the returned cut than to use Gabow's slower deterministic algorithm and compare sizes.   Our main technical contribution is a near-linear time algorithm that contract vertex sets of a simple input graph G with minimum degree d, producing a multigraph with ~O(m/d) edges which preserves all minimum cuts of G with at least 2 vertices on each side.   In our deterministic near-linear time algorithm, we will decompose the problem via low-conductance cuts found using PageRank a la Brin and Page (1998), as analyzed by Andersson, Chung, and Lang at FOCS'06. Normally such algorithms for low-conductance cuts are randomized Monte Carlo algorithms, because they rely on guessing a good start vertex. However, in our case, we have so much structure that no guessing is needed. ",deterministic edge connectivity near linear time present deterministic near linear time algorithm compute edge connectivity find minimum cut simple undirected unweighted graph vertices edge first mn time deterministic algorithm problem near linear time also construct classic cactus representation minimum cut previous fastest deterministic algorithm gabow stoc take edge connectivity could omega stoc karger present randomize near linear time monte carlo algorithm minimum cut problem point better way certify minimality return cut use gabow slower deterministic algorithm compare size main technical contribution near linear time algorithm contract vertex set simple input graph minimum degree produce multigraph edge preserve minimum cut least vertices side deterministic near linear time algorithm decompose problem via low conductance cut find use pagerank la brin page analyze andersson chung lang focs normally algorithms low conductance cut randomize monte carlo algorithms rely guess good start vertex however case much structure guess need,144,1,1411.5123.txt
http://arxiv.org/abs/1411.5166,Subtyping in Java is a Fractal,"  While developing their software, professional object-oriented (OO) software developers keep in their minds an image of the subtyping relation between types in their software. The goal of this paper is to present an observation about the graph of the subtyping relation in Java, namely the observation that, after the addition of generics---and of wildcards, in particular---to Java, the graph of the subtyping relation is no longer a simple directed-acyclic graph (DAG), as in pre-generics Java, but is rather a fractal. Further, this observation equally applies to other mainstream nominally-typed OO languages (such as C#, C++ and Scala) where generics and wildcards (or some other form of 'variance annotations') are standard features. Accordingly, the shape of the subtyping relation in these OO languages is more complex than a tree or a simple DAG, and indeed is also a fractal. Given the popularity of fractals, the fractal observation may help OO software developers keep a useful and intuitive mental image of their software's subtyping relation, even if it is a little more frightening, and more amazing one than before. With proper support from IDEs, the fractal observation can help OO developers in resolving type errors they may find in their code in lesser time, and with more confidence. ",Computer Science - Programming Languages ; Mathematics - Geometric Topology ; ,"AbdelGawad, Moez A. ; ","Subtyping in Java is a Fractal  While developing their software, professional object-oriented (OO) software developers keep in their minds an image of the subtyping relation between types in their software. The goal of this paper is to present an observation about the graph of the subtyping relation in Java, namely the observation that, after the addition of generics---and of wildcards, in particular---to Java, the graph of the subtyping relation is no longer a simple directed-acyclic graph (DAG), as in pre-generics Java, but is rather a fractal. Further, this observation equally applies to other mainstream nominally-typed OO languages (such as C#, C++ and Scala) where generics and wildcards (or some other form of 'variance annotations') are standard features. Accordingly, the shape of the subtyping relation in these OO languages is more complex than a tree or a simple DAG, and indeed is also a fractal. Given the popularity of fractals, the fractal observation may help OO software developers keep a useful and intuitive mental image of their software's subtyping relation, even if it is a little more frightening, and more amazing one than before. With proper support from IDEs, the fractal observation can help OO developers in resolving type errors they may find in their code in lesser time, and with more confidence. ",subtyping java fractal develop software professional object orient oo software developers keep mind image subtyping relation type software goal paper present observation graph subtyping relation java namely observation addition generics wildcards particular java graph subtyping relation longer simple direct acyclic graph dag pre generics java rather fractal observation equally apply mainstream nominally type oo languages scala generics wildcards form variance annotations standard feature accordingly shape subtyping relation oo languages complex tree simple dag indeed also fractal give popularity fractals fractal observation may help oo software developers keep useful intuitive mental image software subtyping relation even little frighten amaze one proper support ides fractal observation help oo developers resolve type errors may find code lesser time confidence,116,8,1411.5166.txt
http://arxiv.org/abs/1411.5735,"Minimization of Transformed $L_1$ Penalty: Theory, Difference of Convex   Function Algorithm, and Robust Application in Compressed Sensing","  We study the minimization problem of a non-convex sparsity promoting penalty function, the transformed $l_1$ (TL1), and its application in compressed sensing (CS). The TL1 penalty interpolates $l_0$ and $l_1$ norms through a nonnegative parameter $a \in (0,+\infty)$, similar to $l_p$ with $p \in (0,1]$, and is known to satisfy unbiasedness, sparsity and Lipschitz continuity properties. We first consider the constrained minimization problem and discuss the exact recovery of $l_0$ norm minimal solution based on the null space property (NSP). We then prove the stable recovery of $l_0$ norm minimal solution if the sensing matrix $A$ satisfies a restricted isometry property (RIP). Next, we present difference of convex algorithms for TL1 (DCATL1) in computing TL1-regularized constrained and unconstrained problems in CS. The inner loop concerns an $l_1$ minimization problem on which we employ the Alternating Direction Method of Multipliers (ADMM). For the unconstrained problem, we prove convergence of DCATL1 to a stationary point satisfying the first order optimality condition. In numerical experiments, we identify the optimal value $a=1$, and compare DCATL1 with other CS algorithms on two classes of sensing matrices: Gaussian random matrices and over-sampled discrete cosine transform matrices (DCT). We find that for both classes of sensing matrices, the performance of DCATL1 algorithm (initiated with $l_1$ minimization) always ranks near the top (if not the top), and is the most robust choice insensitive to the conditioning of the sensing matrix $A$. DCATL1 is also competitive in comparison with DCA on other non-convex penalty functions commonly used in statistics with two hyperparameters. ",Computer Science - Information Theory ; ,"Zhang, Shuai ; Xin, Jack ; ","Minimization of Transformed $L_1$ Penalty: Theory, Difference of Convex   Function Algorithm, and Robust Application in Compressed Sensing  We study the minimization problem of a non-convex sparsity promoting penalty function, the transformed $l_1$ (TL1), and its application in compressed sensing (CS). The TL1 penalty interpolates $l_0$ and $l_1$ norms through a nonnegative parameter $a \in (0,+\infty)$, similar to $l_p$ with $p \in (0,1]$, and is known to satisfy unbiasedness, sparsity and Lipschitz continuity properties. We first consider the constrained minimization problem and discuss the exact recovery of $l_0$ norm minimal solution based on the null space property (NSP). We then prove the stable recovery of $l_0$ norm minimal solution if the sensing matrix $A$ satisfies a restricted isometry property (RIP). Next, we present difference of convex algorithms for TL1 (DCATL1) in computing TL1-regularized constrained and unconstrained problems in CS. The inner loop concerns an $l_1$ minimization problem on which we employ the Alternating Direction Method of Multipliers (ADMM). For the unconstrained problem, we prove convergence of DCATL1 to a stationary point satisfying the first order optimality condition. In numerical experiments, we identify the optimal value $a=1$, and compare DCATL1 with other CS algorithms on two classes of sensing matrices: Gaussian random matrices and over-sampled discrete cosine transform matrices (DCT). We find that for both classes of sensing matrices, the performance of DCATL1 algorithm (initiated with $l_1$ minimization) always ranks near the top (if not the top), and is the most robust choice insensitive to the conditioning of the sensing matrix $A$. DCATL1 is also competitive in comparison with DCA on other non-convex penalty functions commonly used in statistics with two hyperparameters. ",minimization transform penalty theory difference convex function algorithm robust application compress sense study minimization problem non convex sparsity promote penalty function transform tl application compress sense cs tl penalty interpolate norms nonnegative parameter infty similar know satisfy unbiasedness sparsity lipschitz continuity properties first consider constrain minimization problem discuss exact recovery norm minimal solution base null space property nsp prove stable recovery norm minimal solution sense matrix satisfy restrict isometry property rip next present difference convex algorithms tl dcatl compute tl regularize constrain unconstrained problems cs inner loop concern minimization problem employ alternate direction method multipliers admm unconstrained problem prove convergence dcatl stationary point satisfy first order optimality condition numerical experiment identify optimal value compare dcatl cs algorithms two class sense matrices gaussian random matrices sample discrete cosine transform matrices dct find class sense matrices performance dcatl algorithm initiate minimization always rank near top top robust choice insensitive condition sense matrix dcatl also competitive comparison dca non convex penalty function commonly use statistics two hyperparameters,164,7,1411.5735.txt
http://arxiv.org/abs/1411.5878,Salient Object Detection: A Survey,"  Detecting and segmenting salient objects in natural scenes, often referred to as salient object detection, has attracted a lot of interest in computer vision. While many models have been proposed and several applications have emerged, yet a deep understanding of achievements and issues is lacking. We aim to provide a comprehensive review of the recent progress in salient object detection and situate this field among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 228 publications, we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics in salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance and suggest future research directions. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Artificial Intelligence ; Quantitative Biology - Neurons and Cognition ; ,"Borji, Ali ; Cheng, Ming-Ming ; Hou, Qibin ; Jiang, Huaizu ; Li, Jia ; ","Salient Object Detection: A Survey  Detecting and segmenting salient objects in natural scenes, often referred to as salient object detection, has attracted a lot of interest in computer vision. While many models have been proposed and several applications have emerged, yet a deep understanding of achievements and issues is lacking. We aim to provide a comprehensive review of the recent progress in salient object detection and situate this field among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 228 publications, we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics in salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance and suggest future research directions. ",salient object detection survey detect segment salient object natural scenes often refer salient object detection attract lot interest computer vision many model propose several applications emerge yet deep understand achievements issue lack aim provide comprehensive review recent progress salient object detection situate field among closely relate areas generic scene segmentation object proposal generation saliency fixation prediction cover publications survey root key concepts task ii core techniques main model trend iii datasets evaluation metrics salient object detection also discuss open problems evaluation metrics dataset bias model performance suggest future research directions,90,2,1411.5878.txt
http://arxiv.org/abs/1411.6057,Mesoscopic analysis of online social networks - The role of negative   ties,"  A class of networks are those with both positive and negative links. In this manuscript, we studied the interplay between positive and negative ties on mesoscopic level of these networks, i.e., their community structure. A community is considered as a tightly interconnected group of actors; therefore, it does not borrow any assumption from balance theory and merely uses the well-known assumption in the community detection literature. We found that if one detects the communities based on only positive relations (by ignoring the negative ones), the majority of negative relations are already placed between the communities. In other words, negative ties do not have a major role in detecting communities of studied signed networks. Moreover, regarding the internal negative ties, we proved that most unbalanced communities are maximally balanced, and hence they cannot be partitioned into k nonempty sub-clusters with higher balancedness (k >= 2). Furthermore, we showed that although the mediator triad ++- (hostile-mediator-hostile) is underrepresented, it constitutes a considerable portion of triadic relations among communities. Hence, mediator triads should not be ignored by community detection and clustering algorithms. As a result, if one uses a clustering algorithm that operates merely based on social balance, mesoscopic structure of signed networks significantly remains hidden. ","Physics - Physics and Society ; Computer Science - Social and Information Networks ; Physics - Data Analysis, Statistics and Probability ; ","Esmailian, Pouya ; Abtahi, Seyed Ebrahim ; Jalili, Mahdi ; ","Mesoscopic analysis of online social networks - The role of negative   ties  A class of networks are those with both positive and negative links. In this manuscript, we studied the interplay between positive and negative ties on mesoscopic level of these networks, i.e., their community structure. A community is considered as a tightly interconnected group of actors; therefore, it does not borrow any assumption from balance theory and merely uses the well-known assumption in the community detection literature. We found that if one detects the communities based on only positive relations (by ignoring the negative ones), the majority of negative relations are already placed between the communities. In other words, negative ties do not have a major role in detecting communities of studied signed networks. Moreover, regarding the internal negative ties, we proved that most unbalanced communities are maximally balanced, and hence they cannot be partitioned into k nonempty sub-clusters with higher balancedness (k >= 2). Furthermore, we showed that although the mediator triad ++- (hostile-mediator-hostile) is underrepresented, it constitutes a considerable portion of triadic relations among communities. Hence, mediator triads should not be ignored by community detection and clustering algorithms. As a result, if one uses a clustering algorithm that operates merely based on social balance, mesoscopic structure of signed networks significantly remains hidden. ",mesoscopic analysis online social network role negative tie class network positive negative link manuscript study interplay positive negative tie mesoscopic level network community structure community consider tightly interconnect group actors therefore borrow assumption balance theory merely use well know assumption community detection literature find one detect communities base positive relations ignore negative ones majority negative relations already place communities word negative tie major role detect communities study sign network moreover regard internal negative tie prove unbalance communities maximally balance hence cannot partition nonempty sub cluster higher balancedness furthermore show although mediator triad hostile mediator hostile underrepresented constitute considerable portion triadic relations among communities hence mediator triads ignore community detection cluster algorithms result one use cluster algorithm operate merely base social balance mesoscopic structure sign network significantly remain hide,128,6,1411.6057.txt
http://arxiv.org/abs/1411.6538,Efficient storage of Pareto points in biobjective mixed integer   programming,"  In biobjective mixed integer linear programs (BOMILPs), two linear objectives are minimized over a polyhedron while restricting some of the variables to be integer. Since many of the techniques for finding or approximating the Pareto set of a BOMILP use and update a subset of nondominated solutions, it is highly desirable to efficiently store this subset. We present a new data structure, a variant of a binary tree that takes as input points and line segments in $\R^2$ and stores the nondominated subset of this input. When used within an exact solution procedure, such as branch-and-bound (BB), at termination this structure contains the set of Pareto optimal solutions.   We compare the efficiency of our structure in storing solutions to that of a dynamic list which updates via pairwise comparison. Then we use our data structure in two biobjective BB techniques available in the literature and solve three classes of instances of BOMILP, one of which is generated by us. The first experiment shows that our data structure handles up to $10^7$ points or segments much more efficiently than a dynamic list. The second experiment shows that our data structure handles points and segments much more efficiently than a list when used in a BB. ","Computer Science - Data Structures and Algorithms ; Mathematics - Optimization and Control ; 90C29, 68P05, 90C11, ; E.1.7 ; ","Adelgren, Nathan ; Belotti, Pietro ; Gupte, Akshay ; ","Efficient storage of Pareto points in biobjective mixed integer   programming  In biobjective mixed integer linear programs (BOMILPs), two linear objectives are minimized over a polyhedron while restricting some of the variables to be integer. Since many of the techniques for finding or approximating the Pareto set of a BOMILP use and update a subset of nondominated solutions, it is highly desirable to efficiently store this subset. We present a new data structure, a variant of a binary tree that takes as input points and line segments in $\R^2$ and stores the nondominated subset of this input. When used within an exact solution procedure, such as branch-and-bound (BB), at termination this structure contains the set of Pareto optimal solutions.   We compare the efficiency of our structure in storing solutions to that of a dynamic list which updates via pairwise comparison. Then we use our data structure in two biobjective BB techniques available in the literature and solve three classes of instances of BOMILP, one of which is generated by us. The first experiment shows that our data structure handles up to $10^7$ points or segments much more efficiently than a dynamic list. The second experiment shows that our data structure handles points and segments much more efficiently than a list when used in a BB. ",efficient storage pareto point biobjective mix integer program biobjective mix integer linear program bomilps two linear objectives minimize polyhedron restrict variables integer since many techniques find approximate pareto set bomilp use update subset nondominated solutions highly desirable efficiently store subset present new data structure variant binary tree take input point line segment store nondominated subset input use within exact solution procedure branch bind bb termination structure contain set pareto optimal solutions compare efficiency structure store solutions dynamic list update via pairwise comparison use data structure two biobjective bb techniques available literature solve three class instance bomilp one generate us first experiment show data structure handle point segment much efficiently dynamic list second experiment show data structure handle point segment much efficiently list use bb,124,10,1411.6538.txt
http://arxiv.org/abs/1411.6907,Spatiotemporal Modeling of a Pervasive Game,"  Given pervasive games that maintain a virtual spatiotemporal model of the physical world, game designers must contend with space and time in the virtual and physical, but an integrated conceptual model is lacking. Because the problem domains of GIS and Pervasive Games overlap, Peuquet's Triad Representational Framework is exapted, from the domain of GIS, and applied to Pervasive Games. Using Dix et al.'s three types of space and Langran's notion of time, virtual time and space are then be mapped to the physical world and vice versa. The approach is evaluated using the pervasive game called Codename: Heroes, as case study. ",Computer Science - Other Computer Science ; ,"Nevelsteen, Kim J. L. ; ","Spatiotemporal Modeling of a Pervasive Game  Given pervasive games that maintain a virtual spatiotemporal model of the physical world, game designers must contend with space and time in the virtual and physical, but an integrated conceptual model is lacking. Because the problem domains of GIS and Pervasive Games overlap, Peuquet's Triad Representational Framework is exapted, from the domain of GIS, and applied to Pervasive Games. Using Dix et al.'s three types of space and Langran's notion of time, virtual time and space are then be mapped to the physical world and vice versa. The approach is evaluated using the pervasive game called Codename: Heroes, as case study. ",spatiotemporal model pervasive game give pervasive game maintain virtual spatiotemporal model physical world game designers must contend space time virtual physical integrate conceptual model lack problem domains gi pervasive game overlap peuquet triad representational framework exapted domain gi apply pervasive game use dix et al three type space langran notion time virtual time space map physical world vice versa approach evaluate use pervasive game call codename heroes case study,69,8,1411.6907.txt
http://arxiv.org/abs/1411.7086,Discrete Sampling: A graph theoretic approach to Orthogonal   Interpolation,"  We study the problem of finding unitary submatrices of the $N \times N$ discrete Fourier transform matrix, in the context of interpolating a discrete bandlimited signal using an orthogonal basis. This problem is related to a diverse set of questions on idempotents on $\mathbb{Z}_N$ and tiling $\mathbb{Z}_N$. In this work, we establish a graph-theoretic approach and connections to the problem of finding maximum cliques. We identify the key properties of these graphs that make the interpolation problem tractable when $N$ is a prime power, and we identify the challenges in generalizing to arbitrary $N$. Finally, we investigate some connections between graph properties and the spectral-tile direction of the Fuglede conjecture. ",Computer Science - Information Theory ; ,"Siripuram, Aditya ; Wu, William ; Osgood, Brad ; ","Discrete Sampling: A graph theoretic approach to Orthogonal   Interpolation  We study the problem of finding unitary submatrices of the $N \times N$ discrete Fourier transform matrix, in the context of interpolating a discrete bandlimited signal using an orthogonal basis. This problem is related to a diverse set of questions on idempotents on $\mathbb{Z}_N$ and tiling $\mathbb{Z}_N$. In this work, we establish a graph-theoretic approach and connections to the problem of finding maximum cliques. We identify the key properties of these graphs that make the interpolation problem tractable when $N$ is a prime power, and we identify the challenges in generalizing to arbitrary $N$. Finally, we investigate some connections between graph properties and the spectral-tile direction of the Fuglede conjecture. ",discrete sample graph theoretic approach orthogonal interpolation study problem find unitary submatrices time discrete fourier transform matrix context interpolate discrete bandlimited signal use orthogonal basis problem relate diverse set question idempotents mathbb tile mathbb work establish graph theoretic approach connections problem find maximum cliques identify key properties graph make interpolation problem tractable prime power identify challenge generalize arbitrary finally investigate connections graph properties spectral tile direction fuglede conjecture,68,3,1411.7086.txt
http://arxiv.org/abs/1411.7087,Consistency proof of a fragment of PV with substitution in bounded   arithmetic,"  This paper presents proof that Buss's $S^2_2$ can prove the consistency of a fragment of Cook and Urquhart's $\mathrm{PV}$ from which induction has been removed but substitution has been retained.   This result improves Beckmann's result, which proves the consistency of such a system without substitution in bounded arithmetic $S^1_2$.   Our proof relies on the notion of ""computation"" of the terms of $\mathrm{PV}$.   In our work, we first prove that, in the system under consideration, if an equation is proved and either its left- or right-hand side is computed, then there is a corresponding computation for its right- or left-hand side, respectively.   By carefully computing the bound of the size of the computation, the proof of this theorem inside a bounded arithmetic is obtained, from which the consistency of the system is readily proven.   This result apparently implies the separation of bounded arithmetic because Buss and Ignjatovi\'c stated that it is not possible to prove the consistency of a fragment of $\mathrm{PV}$ without induction but with substitution in Buss's $S^1_2$.   However, their proof actually shows that it is not possible to prove the consistency of the system, which is obtained by the addition of propositional logic and other axioms to a system such as ours.   On the other hand, the system that we have considered is strictly equational, which is a property on which our proof relies. ","Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03F03, 03D15 ; F.4.1 ; ","Yamagata, Yoriyuki ; ","Consistency proof of a fragment of PV with substitution in bounded   arithmetic  This paper presents proof that Buss's $S^2_2$ can prove the consistency of a fragment of Cook and Urquhart's $\mathrm{PV}$ from which induction has been removed but substitution has been retained.   This result improves Beckmann's result, which proves the consistency of such a system without substitution in bounded arithmetic $S^1_2$.   Our proof relies on the notion of ""computation"" of the terms of $\mathrm{PV}$.   In our work, we first prove that, in the system under consideration, if an equation is proved and either its left- or right-hand side is computed, then there is a corresponding computation for its right- or left-hand side, respectively.   By carefully computing the bound of the size of the computation, the proof of this theorem inside a bounded arithmetic is obtained, from which the consistency of the system is readily proven.   This result apparently implies the separation of bounded arithmetic because Buss and Ignjatovi\'c stated that it is not possible to prove the consistency of a fragment of $\mathrm{PV}$ without induction but with substitution in Buss's $S^1_2$.   However, their proof actually shows that it is not possible to prove the consistency of the system, which is obtained by the addition of propositional logic and other axioms to a system such as ours.   On the other hand, the system that we have considered is strictly equational, which is a property on which our proof relies. ",consistency proof fragment pv substitution bound arithmetic paper present proof bus prove consistency fragment cook urquhart mathrm pv induction remove substitution retain result improve beckmann result prove consistency system without substitution bound arithmetic proof rely notion computation term mathrm pv work first prove system consideration equation prove either leave right hand side compute correspond computation right leave hand side respectively carefully compute bind size computation proof theorem inside bound arithmetic obtain consistency system readily prove result apparently imply separation bound arithmetic bus ignjatovi state possible prove consistency fragment mathrm pv without induction substitution bus however proof actually show possible prove consistency system obtain addition propositional logic axioms system hand system consider strictly equational property proof rely,116,8,1411.7087.txt
http://arxiv.org/abs/1411.7346,A Chasm Between Identity and Equivalence Testing with Conditional   Queries,"  A recent model for property testing of probability distributions (Chakraborty et al., ITCS 2013, Canonne et al., SICOMP 2015) enables tremendous savings in the sample complexity of testing algorithms, by allowing them to condition the sampling on subsets of the domain. In particular, Canonne, Ron, and Servedio (SICOMP 2015) showed that, in this setting, testing identity of an unknown distribution $D$ (whether $D=D^\ast$ for an explicitly known $D^\ast$) can be done with a constant number of queries, independent of the support size $n$ -- in contrast to the required $\Omega(\sqrt{n})$ in the standard sampling model. It was unclear whether the same stark contrast exists for the case of testing equivalence, where both distributions are unknown. While Canonne et al. established a $\mathrm{poly}(\log n)$-query upper bound for equivalence testing, very recently brought down to $\tilde O(\log\log n)$ by Falahatgar et al. (COLT 2015), whether a dependence on the domain size $n$ is necessary was still open, and explicitly posed by Fischer at the Bertinoro Workshop on Sublinear Algorithms (2014). We show that any testing algorithm for equivalence must make $\Omega(\sqrt{\log\log n})$ queries in the conditional sampling model. This demonstrates a gap between identity and equivalence testing, absent in the standard sampling model (where both problems have sampling complexity $n^{\Theta(1)}$).   We also obtain results on the query complexity of uniformity testing and support-size estimation with conditional samples. We answer a question of Chakraborty et al. (ITCS 2013) showing that non-adaptive uniformity testing indeed requires $\Omega(\log n)$ queries in the conditional model. For the related problem of support-size estimation, we provide both adaptive and non-adaptive algorithms, with query complexities $\mathrm{poly}(\log\log n)$ and $\mathrm{poly}(\log n)$, respectively. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; Computer Science - Machine Learning ; Mathematics - Probability ; Mathematics - Statistics Theory ; ,"Acharya, Jayadev ; Canonne, Clément L. ; Kamath, Gautam ; ","A Chasm Between Identity and Equivalence Testing with Conditional   Queries  A recent model for property testing of probability distributions (Chakraborty et al., ITCS 2013, Canonne et al., SICOMP 2015) enables tremendous savings in the sample complexity of testing algorithms, by allowing them to condition the sampling on subsets of the domain. In particular, Canonne, Ron, and Servedio (SICOMP 2015) showed that, in this setting, testing identity of an unknown distribution $D$ (whether $D=D^\ast$ for an explicitly known $D^\ast$) can be done with a constant number of queries, independent of the support size $n$ -- in contrast to the required $\Omega(\sqrt{n})$ in the standard sampling model. It was unclear whether the same stark contrast exists for the case of testing equivalence, where both distributions are unknown. While Canonne et al. established a $\mathrm{poly}(\log n)$-query upper bound for equivalence testing, very recently brought down to $\tilde O(\log\log n)$ by Falahatgar et al. (COLT 2015), whether a dependence on the domain size $n$ is necessary was still open, and explicitly posed by Fischer at the Bertinoro Workshop on Sublinear Algorithms (2014). We show that any testing algorithm for equivalence must make $\Omega(\sqrt{\log\log n})$ queries in the conditional sampling model. This demonstrates a gap between identity and equivalence testing, absent in the standard sampling model (where both problems have sampling complexity $n^{\Theta(1)}$).   We also obtain results on the query complexity of uniformity testing and support-size estimation with conditional samples. We answer a question of Chakraborty et al. (ITCS 2013) showing that non-adaptive uniformity testing indeed requires $\Omega(\log n)$ queries in the conditional model. For the related problem of support-size estimation, we provide both adaptive and non-adaptive algorithms, with query complexities $\mathrm{poly}(\log\log n)$ and $\mathrm{poly}(\log n)$, respectively. ",chasm identity equivalence test conditional query recent model property test probability distributions chakraborty et al itcs canonne et al sicomp enable tremendous save sample complexity test algorithms allow condition sample subsets domain particular canonne ron servedio sicomp show set test identity unknown distribution whether ast explicitly know ast do constant number query independent support size contrast require omega sqrt standard sample model unclear whether stark contrast exist case test equivalence distributions unknown canonne et al establish mathrm poly log query upper bind equivalence test recently bring tilde log log falahatgar et al colt whether dependence domain size necessary still open explicitly pose fischer bertinoro workshop sublinear algorithms show test algorithm equivalence must make omega sqrt log log query conditional sample model demonstrate gap identity equivalence test absent standard sample model problems sample complexity theta also obtain result query complexity uniformity test support size estimation conditional sample answer question chakraborty et al itcs show non adaptive uniformity test indeed require omega log query conditional model relate problem support size estimation provide adaptive non adaptive algorithms query complexities mathrm poly log log mathrm poly log respectively,184,1,1411.7346.txt
http://arxiv.org/abs/1411.7632,Semidefinite Programming Approach to Gaussian Sequential Rate-Distortion   Trade-offs,"  Sequential rate-distortion (SRD) theory provides a framework for studying the fundamental trade-off between data-rate and data-quality in real-time communication systems. In this paper, we consider the SRD problem for multi-dimensional time-varying Gauss-Markov processes under mean-square distortion criteria. We first revisit the sensor-estimator separation principle, which asserts that considered SRD problem is equivalent to a joint sensor and estimator design problem in which data-rate of the sensor output is minimized while the estimator's performance satisfies the distortion criteria. We then show that the optimal joint design can be performed by semidefinite programming. A semidefinite representation of the corresponding SRD function is obtained. Implications of the obtained result in the context of zero-delay source coding theory and applications to networked control theory are also discussed. ",Mathematics - Optimization and Control ; Computer Science - Information Theory ; ,"Tanaka, Takashi ; Kim, Kwang-Ki K. ; Parrilo, Pablo A. ; Mitter, Sanjoy K. ; ","Semidefinite Programming Approach to Gaussian Sequential Rate-Distortion   Trade-offs  Sequential rate-distortion (SRD) theory provides a framework for studying the fundamental trade-off between data-rate and data-quality in real-time communication systems. In this paper, we consider the SRD problem for multi-dimensional time-varying Gauss-Markov processes under mean-square distortion criteria. We first revisit the sensor-estimator separation principle, which asserts that considered SRD problem is equivalent to a joint sensor and estimator design problem in which data-rate of the sensor output is minimized while the estimator's performance satisfies the distortion criteria. We then show that the optimal joint design can be performed by semidefinite programming. A semidefinite representation of the corresponding SRD function is obtained. Implications of the obtained result in the context of zero-delay source coding theory and applications to networked control theory are also discussed. ",semidefinite program approach gaussian sequential rate distortion trade off sequential rate distortion srd theory provide framework study fundamental trade data rate data quality real time communication systems paper consider srd problem multi dimensional time vary gauss markov process mean square distortion criteria first revisit sensor estimator separation principle assert consider srd problem equivalent joint sensor estimator design problem data rate sensor output minimize estimator performance satisfy distortion criteria show optimal joint design perform semidefinite program semidefinite representation correspond srd function obtain implications obtain result context zero delay source cod theory applications network control theory also discuss,96,2,1411.7632.txt
http://arxiv.org/abs/1411.7895,Influence of sociodemographic characteristics on human mobility,"  Human mobility has been traditionally studied using surveys that deliver snapshots of population displacement patterns. The growing accessibility to ICT information from portable digital media has recently opened the possibility of exploring human behavior at high spatio-temporal resolutions. Mobile phone records, geolocated tweets, check-ins from Foursquare or geotagged photos, have contributed to this purpose at different scales, from cities to countries, in different world areas. Many previous works lacked, however, details on the individuals' attributes such as age or gender. In this work, we analyze credit-card records from Barcelona and Madrid and by examining the geolocated credit-card transactions of individuals living in the two provinces, we find that the mobility patterns vary according to gender, age and occupation. Differences in distance traveled and travel purpose are observed between younger and older people, but, curiously, either between males and females of similar age. While mobility displays some generic features, here we show that sociodemographic characteristics play a relevant role and must be taken into account for mobility and epidemiological modelization. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Lenormand, Maxime ; Louail, Thomas ; Cantu-Ros, Oliva G. ; Picornell, Miguel ; Herranz, Ricardo ; Arias, Juan Murillo ; Barthelemy, Marc ; Miguel, Maxi San ; Ramasco, Jose J. ; ","Influence of sociodemographic characteristics on human mobility  Human mobility has been traditionally studied using surveys that deliver snapshots of population displacement patterns. The growing accessibility to ICT information from portable digital media has recently opened the possibility of exploring human behavior at high spatio-temporal resolutions. Mobile phone records, geolocated tweets, check-ins from Foursquare or geotagged photos, have contributed to this purpose at different scales, from cities to countries, in different world areas. Many previous works lacked, however, details on the individuals' attributes such as age or gender. In this work, we analyze credit-card records from Barcelona and Madrid and by examining the geolocated credit-card transactions of individuals living in the two provinces, we find that the mobility patterns vary according to gender, age and occupation. Differences in distance traveled and travel purpose are observed between younger and older people, but, curiously, either between males and females of similar age. While mobility displays some generic features, here we show that sociodemographic characteristics play a relevant role and must be taken into account for mobility and epidemiological modelization. ",influence sociodemographic characteristics human mobility human mobility traditionally study use survey deliver snapshots population displacement pattern grow accessibility ict information portable digital media recently open possibility explore human behavior high spatio temporal resolutions mobile phone record geolocated tweet check ins foursquare geotagged photos contribute purpose different scale cities countries different world areas many previous work lack however detail individuals attribute age gender work analyze credit card record barcelona madrid examine geolocated credit card transactions individuals live two provinces find mobility pattern vary accord gender age occupation differences distance travel travel purpose observe younger older people curiously either males females similar age mobility display generic feature show sociodemographic characteristics play relevant role must take account mobility epidemiological modelization,117,0,1411.7895.txt
http://arxiv.org/abs/1412.0291,Bits from Biology for Computational Intelligence,"  Computational intelligence is broadly defined as biologically-inspired computing. Usually, inspiration is drawn from neural systems. This article shows how to analyze neural systems using information theory to obtain constraints that help identify the algorithms run by such systems and the information they represent. Algorithms and representations identified information-theoretically may then guide the design of biologically inspired computing systems (BICS). The material covered includes the necessary introduction to information theory and the estimation of information theoretic quantities from neural data. We then show how to analyze the information encoded in a system about its environment, and also discuss recent methodological developments on the question of how much information each agent carries about the environment either uniquely, or redundantly or synergistically together with others. Last, we introduce the framework of local information dynamics, where information processing is decomposed into component processes of information storage, transfer, and modification -- locally in space and time. We close by discussing example applications of these measures to neural data and other complex systems. ","Quantitative Biology - Neurons and Cognition ; Computer Science - Information Theory ; Physics - Data Analysis, Statistics and Probability ; ","Wibral, Michael ; Lizier, Joseph T. ; Priesemann, Viola ; ","Bits from Biology for Computational Intelligence  Computational intelligence is broadly defined as biologically-inspired computing. Usually, inspiration is drawn from neural systems. This article shows how to analyze neural systems using information theory to obtain constraints that help identify the algorithms run by such systems and the information they represent. Algorithms and representations identified information-theoretically may then guide the design of biologically inspired computing systems (BICS). The material covered includes the necessary introduction to information theory and the estimation of information theoretic quantities from neural data. We then show how to analyze the information encoded in a system about its environment, and also discuss recent methodological developments on the question of how much information each agent carries about the environment either uniquely, or redundantly or synergistically together with others. Last, we introduce the framework of local information dynamics, where information processing is decomposed into component processes of information storage, transfer, and modification -- locally in space and time. We close by discussing example applications of these measures to neural data and other complex systems. ",bits biology computational intelligence computational intelligence broadly define biologically inspire compute usually inspiration draw neural systems article show analyze neural systems use information theory obtain constraints help identify algorithms run systems information represent algorithms representations identify information theoretically may guide design biologically inspire compute systems bics material cover include necessary introduction information theory estimation information theoretic quantities neural data show analyze information encode system environment also discuss recent methodological developments question much information agent carry environment either uniquely redundantly synergistically together others last introduce framework local information dynamics information process decompose component process information storage transfer modification locally space time close discuss example applications measure neural data complex systems,109,11,1412.0291.txt
http://arxiv.org/abs/1412.0340,Approximate MAP Estimation for Pairwise Potentials via Baker's Technique,"  The theoretical models providing mathematical abstractions for several significant optimization problems in machine learning, combinatorial optimization, computer vision and statistical physics have intrinsic similarities. We propose a unified framework to model these computation tasks where the structures of these optimization problems are encoded by functions attached on the vertices and edges of a graph. We show that computing MAX 2-CSP admits polynomial-time approximation scheme (PTAS) on planar graphs, graphs with bounded local treewidth, $H$-minor-free graphs, geometric graphs with bounded density and graphs embeddable with bounded number of crossings per edge. This implies computing MAX-CUT, MAX-DICUT and MAX $k$-CUT admits PTASs on all these classes of graphs. Our method also gives the first PTAS for computing the ground state of ferromagnetic Edwards-Anderson model without external magnetic field on $d$-dimensional lattice graphs. These results are widely applicable in vision, graphics and machine learning. ",Computer Science - Data Structures and Algorithms ; ,"Wang, Yi-Kai ; ","Approximate MAP Estimation for Pairwise Potentials via Baker's Technique  The theoretical models providing mathematical abstractions for several significant optimization problems in machine learning, combinatorial optimization, computer vision and statistical physics have intrinsic similarities. We propose a unified framework to model these computation tasks where the structures of these optimization problems are encoded by functions attached on the vertices and edges of a graph. We show that computing MAX 2-CSP admits polynomial-time approximation scheme (PTAS) on planar graphs, graphs with bounded local treewidth, $H$-minor-free graphs, geometric graphs with bounded density and graphs embeddable with bounded number of crossings per edge. This implies computing MAX-CUT, MAX-DICUT and MAX $k$-CUT admits PTASs on all these classes of graphs. Our method also gives the first PTAS for computing the ground state of ferromagnetic Edwards-Anderson model without external magnetic field on $d$-dimensional lattice graphs. These results are widely applicable in vision, graphics and machine learning. ",approximate map estimation pairwise potentials via baker technique theoretical model provide mathematical abstractions several significant optimization problems machine learn combinatorial optimization computer vision statistical physics intrinsic similarities propose unify framework model computation task structure optimization problems encode function attach vertices edge graph show compute max csp admit polynomial time approximation scheme ptas planar graph graph bound local treewidth minor free graph geometric graph bound density graph embeddable bound number cross per edge imply compute max cut max dicut max cut admit ptass class graph method also give first ptas compute grind state ferromagnetic edwards anderson model without external magnetic field dimensional lattice graph result widely applicable vision graphics machine learn,110,3,1412.0340.txt
http://arxiv.org/abs/1412.1538,Krylov Subspace Methods in Dynamical Sampling,"  Let $B$ be an unknown linear evolution process on $\mathbb C^d\simeq l^2(\mathbb Z_d)$ driving an unknown initial state $x$ and producing the states $\{B^\ell x, \ell = 0,1,\ldots\}$ at different time levels. The problem under consideration in this paper is to find as much information as possible about $B$ and $x$ from the measurements $Y=\{x(i)$, $Bx(i)$, $\dots$, $B^{\ell_i}x(i): i \in \Omega\subset \mathbb Z^d\}$. If $B$ is a ""low-pass"" convolution operator, we show that we can recover both $B$ and $x$, almost surely, as long as we double the amount of temporal samples needed in \cite{ADK13} to recover the signal propagated by a known operator $B$. For a general operator $B$, we can recover parts or even all of its spectrum from $Y$. As a special case of our method, we derive the centuries old Prony's method \cite{BDVMC08, P795, PP13} which recovers a vector with an $s$-sparse Fourier transform from $2s$ of its consecutive components. ","Computer Science - Information Theory ; 94A20, 94A12, 42C15, 15A29 ; ","Aldroubi, Akram ; Krishtal, Ilya ; ","Krylov Subspace Methods in Dynamical Sampling  Let $B$ be an unknown linear evolution process on $\mathbb C^d\simeq l^2(\mathbb Z_d)$ driving an unknown initial state $x$ and producing the states $\{B^\ell x, \ell = 0,1,\ldots\}$ at different time levels. The problem under consideration in this paper is to find as much information as possible about $B$ and $x$ from the measurements $Y=\{x(i)$, $Bx(i)$, $\dots$, $B^{\ell_i}x(i): i \in \Omega\subset \mathbb Z^d\}$. If $B$ is a ""low-pass"" convolution operator, we show that we can recover both $B$ and $x$, almost surely, as long as we double the amount of temporal samples needed in \cite{ADK13} to recover the signal propagated by a known operator $B$. For a general operator $B$, we can recover parts or even all of its spectrum from $Y$. As a special case of our method, we derive the centuries old Prony's method \cite{BDVMC08, P795, PP13} which recovers a vector with an $s$-sparse Fourier transform from $2s$ of its consecutive components. ",krylov subspace methods dynamical sample let unknown linear evolution process mathbb simeq mathbb drive unknown initial state produce state ell ell ldots different time level problem consideration paper find much information possible measurements bx dot ell omega subset mathbb low pass convolution operator show recover almost surely long double amount temporal sample need cite adk recover signal propagate know operator general operator recover part even spectrum special case method derive centuries old prony method cite bdvmc pp recover vector sparse fourier transform consecutive components,84,9,1412.1538.txt
http://arxiv.org/abs/1412.1547,Efficient algorithms to decide tightness,"  Tightness is a generalisation of the notion of convexity: a space is tight if and only if it is ""as convex as possible"", given its topological constraints. For a simplicial complex, deciding tightness has a straightforward exponential time algorithm, but efficient methods to decide tightness are only known in the trivial setting of triangulated surfaces.   In this article, we present a new polynomial time procedure to decide tightness for triangulations of $3$-manifolds -- a problem which previously was thought to be hard. Furthermore, we describe an algorithm to decide general tightness in the case of $4$-dimensional combinatorial manifolds which is fixed parameter tractable in the treewidth of the $1$-skeletons of their vertex links, and we present an algorithm to decide $\mathbb{F}_2$-tightness for weak pseudomanifolds $M$ of arbitrary but fixed dimension which is fixed parameter tractable in the treewidth of the dual graph of $M$. ",Computer Science - Computational Geometry ; Mathematics - Combinatorics ; F.2.2 ; G.2.1 ; ,"Bagchi, Bhaskar ; Burton, Benjamin A. ; Datta, Basudeb ; Singh, Nitin ; Spreer, Jonathan ; ","Efficient algorithms to decide tightness  Tightness is a generalisation of the notion of convexity: a space is tight if and only if it is ""as convex as possible"", given its topological constraints. For a simplicial complex, deciding tightness has a straightforward exponential time algorithm, but efficient methods to decide tightness are only known in the trivial setting of triangulated surfaces.   In this article, we present a new polynomial time procedure to decide tightness for triangulations of $3$-manifolds -- a problem which previously was thought to be hard. Furthermore, we describe an algorithm to decide general tightness in the case of $4$-dimensional combinatorial manifolds which is fixed parameter tractable in the treewidth of the $1$-skeletons of their vertex links, and we present an algorithm to decide $\mathbb{F}_2$-tightness for weak pseudomanifolds $M$ of arbitrary but fixed dimension which is fixed parameter tractable in the treewidth of the dual graph of $M$. ",efficient algorithms decide tightness tightness generalisation notion convexity space tight convex possible give topological constraints simplicial complex decide tightness straightforward exponential time algorithm efficient methods decide tightness know trivial set triangulate surface article present new polynomial time procedure decide tightness triangulations manifold problem previously think hard furthermore describe algorithm decide general tightness case dimensional combinatorial manifold fix parameter tractable treewidth skeletons vertex link present algorithm decide mathbb tightness weak pseudomanifolds arbitrary fix dimension fix parameter tractable treewidth dual graph,79,8,1412.1547.txt
http://arxiv.org/abs/1412.1695,Convolutional codes from unit schemes,"  Convolutional codes are constructed, designed and analysed using row and/or block structures of unit algebraic schemes. Infinite series of such codes and of codes with specific properties are derived. Properties are shown algebraically and algebraic decoding methods are derived. For a given rate and given error-correction capability at each component, convolutional codes with these specifications and with efficient decoding algorithms are constructed. Explicit prototype examples are given but in general large lengths and large error capability are achievable. Convolutional codes with efficient decoding algorithms at or near the maximum free distances attainable for the parameters are constructible. Unit memory convolutional codes of maximum possible free distance are designed with practical algebraic decoding algorithms.   LDPC (low density parity check) convolutional codes with efficient decoding schemes are constructed and analysed by the methods. Self-dual and dual-containing convolutional codes may also be designed by the methods; dual-containing codes enables the construction of quantum codes. ","Mathematics - Rings and Algebras ; Computer Science - Discrete Mathematics ; Computer Science - Information Theory ; 94B10, 11T71, 16S99 ; ","Hurley, Ted ; ","Convolutional codes from unit schemes  Convolutional codes are constructed, designed and analysed using row and/or block structures of unit algebraic schemes. Infinite series of such codes and of codes with specific properties are derived. Properties are shown algebraically and algebraic decoding methods are derived. For a given rate and given error-correction capability at each component, convolutional codes with these specifications and with efficient decoding algorithms are constructed. Explicit prototype examples are given but in general large lengths and large error capability are achievable. Convolutional codes with efficient decoding algorithms at or near the maximum free distances attainable for the parameters are constructible. Unit memory convolutional codes of maximum possible free distance are designed with practical algebraic decoding algorithms.   LDPC (low density parity check) convolutional codes with efficient decoding schemes are constructed and analysed by the methods. Self-dual and dual-containing convolutional codes may also be designed by the methods; dual-containing codes enables the construction of quantum codes. ",convolutional cod unit scheme convolutional cod construct design analyse use row block structure unit algebraic scheme infinite series cod cod specific properties derive properties show algebraically algebraic decode methods derive give rate give error correction capability component convolutional cod specifications efficient decode algorithms construct explicit prototype examples give general large lengths large error capability achievable convolutional cod efficient decode algorithms near maximum free distance attainable parameters constructible unit memory convolutional cod maximum possible free distance design practical algebraic decode algorithms ldpc low density parity check convolutional cod efficient decode scheme construct analyse methods self dual dual contain convolutional cod may also design methods dual contain cod enable construction quantum cod,110,5,1412.1695.txt
http://arxiv.org/abs/1412.1866,Integer-Programming Ensemble of Temporal-Relations Classifiers,"  The extraction and understanding of temporal events and their relations are major challenges in natural language processing. Processing text on a sentence-by-sentence or expression-by-expression basis often fails, in part due to the challenge of capturing the global consistency of the text. We present an ensemble method, which reconciles the outputs of multiple classifiers of temporal expressions across the text using integer programming. Computational experiments show that the ensemble improves upon the best individual results from two recent challenges, SemEval-2013 TempEval-3 (Temporal Annotation) and SemEval-2016 Task 12 (Clinical TempEval). ",Computer Science - Computation and Language ; Computer Science - Machine Learning ; Mathematics - Optimization and Control ; ,"Kerr, Catherine ; Hoare, Terri ; Carroll, Paula ; Marecek, Jakub ; ","Integer-Programming Ensemble of Temporal-Relations Classifiers  The extraction and understanding of temporal events and their relations are major challenges in natural language processing. Processing text on a sentence-by-sentence or expression-by-expression basis often fails, in part due to the challenge of capturing the global consistency of the text. We present an ensemble method, which reconciles the outputs of multiple classifiers of temporal expressions across the text using integer programming. Computational experiments show that the ensemble improves upon the best individual results from two recent challenges, SemEval-2013 TempEval-3 (Temporal Annotation) and SemEval-2016 Task 12 (Clinical TempEval). ",integer program ensemble temporal relations classifiers extraction understand temporal events relations major challenge natural language process process text sentence sentence expression expression basis often fail part due challenge capture global consistency text present ensemble method reconcile output multiple classifiers temporal expressions across text use integer program computational experiment show ensemble improve upon best individual result two recent challenge semeval tempeval temporal annotation semeval task clinical tempeval,66,8,1412.1866.txt
http://arxiv.org/abs/1412.2114,"Chases and Escapes, and Optimization Problems","  We propose a new approach for solving combinatorial optimization problem by utilizing the mechanism of chases and escapes, which has a long history in mathematics. In addition to the well-used steepest descent and neighboring search, we perform a chase and escape game on the ""landscape"" of the cost function. We have created a concrete algorithm for the Traveling Salesman Problem. Our preliminary test indicates a possibility that this new fusion of chases and escapes problem into combinatorial optimization search is fruitful. ",Computer Science - Artificial Intelligence ; ,"Ohira, Toru ; ","Chases and Escapes, and Optimization Problems  We propose a new approach for solving combinatorial optimization problem by utilizing the mechanism of chases and escapes, which has a long history in mathematics. In addition to the well-used steepest descent and neighboring search, we perform a chase and escape game on the ""landscape"" of the cost function. We have created a concrete algorithm for the Traveling Salesman Problem. Our preliminary test indicates a possibility that this new fusion of chases and escapes problem into combinatorial optimization search is fruitful. ",chase escape optimization problems propose new approach solve combinatorial optimization problem utilize mechanism chase escape long history mathematics addition well use steepest descent neighbor search perform chase escape game landscape cost function create concrete algorithm travel salesman problem preliminary test indicate possibility new fusion chase escape problem combinatorial optimization search fruitful,51,12,1412.2114.txt
http://arxiv.org/abs/1412.2231,Generalized Singular Value Thresholding,"  This work studies the Generalized Singular Value Thresholding (GSVT) operator ${\text{Prox}}_{g}^{{\sigma}}(\cdot)$, \begin{equation*}   {\text{Prox}}_{g}^{{\sigma}}(B)=\arg\min\limits_{X}\sum_{i=1}^{m}g(\sigma_{i}(X)) + \frac{1}{2}||X-B||_{F}^{2}, \end{equation*} associated with a nonconvex function $g$ defined on the singular values of $X$. We prove that GSVT can be obtained by performing the proximal operator of $g$ (denoted as $\text{Prox}_g(\cdot)$) on the singular values since $\text{Prox}_g(\cdot)$ is monotone when $g$ is lower bounded. If the nonconvex $g$ satisfies some conditions (many popular nonconvex surrogate functions, e.g., $\ell_p$-norm, $0<p<1$, of $\ell_0$-norm are special cases), a general solver to find $\text{Prox}_g(b)$ is proposed for any $b\geq0$. GSVT greatly generalizes the known Singular Value Thresholding (SVT) which is a basic subroutine in many convex low rank minimization methods. We are able to solve the nonconvex low rank minimization problem by using GSVT in place of SVT. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Machine Learning ; Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; ,"Lu, Canyi ; Zhu, Changbo ; Xu, Chunyan ; Yan, Shuicheng ; Lin, Zhouchen ; ","Generalized Singular Value Thresholding  This work studies the Generalized Singular Value Thresholding (GSVT) operator ${\text{Prox}}_{g}^{{\sigma}}(\cdot)$, \begin{equation*}   {\text{Prox}}_{g}^{{\sigma}}(B)=\arg\min\limits_{X}\sum_{i=1}^{m}g(\sigma_{i}(X)) + \frac{1}{2}||X-B||_{F}^{2}, \end{equation*} associated with a nonconvex function $g$ defined on the singular values of $X$. We prove that GSVT can be obtained by performing the proximal operator of $g$ (denoted as $\text{Prox}_g(\cdot)$) on the singular values since $\text{Prox}_g(\cdot)$ is monotone when $g$ is lower bounded. If the nonconvex $g$ satisfies some conditions (many popular nonconvex surrogate functions, e.g., $\ell_p$-norm, $0<p<1$, of $\ell_0$-norm are special cases), a general solver to find $\text{Prox}_g(b)$ is proposed for any $b\geq0$. GSVT greatly generalizes the known Singular Value Thresholding (SVT) which is a basic subroutine in many convex low rank minimization methods. We are able to solve the nonconvex low rank minimization problem by using GSVT in place of SVT. ",generalize singular value thresholding work study generalize singular value thresholding gsvt operator text prox sigma cdot begin equation text prox sigma arg min limit sum sigma frac end equation associate nonconvex function define singular value prove gsvt obtain perform proximal operator denote text prox cdot singular value since text prox cdot monotone lower bound nonconvex satisfy condition many popular nonconvex surrogate function ell norm ell norm special case general solver find text prox propose geq gsvt greatly generalize know singular value thresholding svt basic subroutine many convex low rank minimization methods able solve nonconvex low rank minimization problem use gsvt place svt,102,7,1412.2231.txt
http://arxiv.org/abs/1412.2526,Exploiting Packing Components in General-Purpose Integer Programming   Solvers,"  The problem of packing boxes into a large box is often a part of a larger problem. For example in furniture supply chain applications, one needs to decide what trucks to use to transport furniture between production sites and distribution centers and stores, such that the furniture fits inside. Such problems are often formulated and sometimes solved using general-purpose integer programming solvers.   This chapter studies the problem of identifying a compact formulation of the multi-dimensional packing component in a general instance of integer linear programming, reformulating it using the discretisation of Allen--Burke--Marecek, and and solving the extended reformulation. Results on instances of up to 10000000 boxes are reported. ",Mathematics - Optimization and Control ; Computer Science - Data Structures and Algorithms ; ,"Marecek, Jakub ; ","Exploiting Packing Components in General-Purpose Integer Programming   Solvers  The problem of packing boxes into a large box is often a part of a larger problem. For example in furniture supply chain applications, one needs to decide what trucks to use to transport furniture between production sites and distribution centers and stores, such that the furniture fits inside. Such problems are often formulated and sometimes solved using general-purpose integer programming solvers.   This chapter studies the problem of identifying a compact formulation of the multi-dimensional packing component in a general instance of integer linear programming, reformulating it using the discretisation of Allen--Burke--Marecek, and and solving the extended reformulation. Results on instances of up to 10000000 boxes are reported. ",exploit pack components general purpose integer program solvers problem pack box large box often part larger problem example furniture supply chain applications one need decide truck use transport furniture production sit distribution center store furniture fit inside problems often formulate sometimes solve use general purpose integer program solvers chapter study problem identify compact formulation multi dimensional pack component general instance integer linear program reformulate use discretisation allen burke marecek solve extend reformulation result instance box report,76,8,1412.2526.txt
http://arxiv.org/abs/1412.2684,HyperSpectral classification with adaptively weighted L1-norm   regularization and spatial postprocessing,"  Sparse regression methods have been proven effective in a wide range of signal processing problems such as image compression, speech coding, channel equalization, linear regression and classification. In this paper a new convex method of hyperspectral image classification is developed based on the sparse unmixing algorithm SUnSAL for which a pixel adaptive L1-norm regularization term is introduced. To further enhance class separability, the algorithm is kernelized using an RBF kernel and the final results are improved by a combination of spatial pre and post-processing operations. It is shown that the proposed method is competitive with state of the art algorithms such as SVM-CK, KSOMP-CK and KSSP-CK. ",Mathematics - Optimization and Control ; Computer Science - Computer Vision and Pattern Recognition ; ,"Aldea, Victor Stefan ; ","HyperSpectral classification with adaptively weighted L1-norm   regularization and spatial postprocessing  Sparse regression methods have been proven effective in a wide range of signal processing problems such as image compression, speech coding, channel equalization, linear regression and classification. In this paper a new convex method of hyperspectral image classification is developed based on the sparse unmixing algorithm SUnSAL for which a pixel adaptive L1-norm regularization term is introduced. To further enhance class separability, the algorithm is kernelized using an RBF kernel and the final results are improved by a combination of spatial pre and post-processing operations. It is shown that the proposed method is competitive with state of the art algorithms such as SVM-CK, KSOMP-CK and KSSP-CK. ",hyperspectral classification adaptively weight norm regularization spatial postprocessing sparse regression methods prove effective wide range signal process problems image compression speech cod channel equalization linear regression classification paper new convex method hyperspectral image classification develop base sparse unmixing algorithm sunsal pixel adaptive norm regularization term introduce enhance class separability algorithm kernelized use rbf kernel final result improve combination spatial pre post process operations show propose method competitive state art algorithms svm ck ksomp ck kssp ck,76,9,1412.2684.txt
http://arxiv.org/abs/1412.2877,Autonomous Load Disaggregation Approach based on Active Power   Measurements,"  With the help of smart metering valuable information of the appliance usage can be retrieved. In detail, non-intrusive load monitoring (NILM), also called load disaggregation, tries to identify appliances in the power draw of an household. In this paper an unsupervised load disaggregation approach is proposed that works without a priori knowledge about appliances. The proposed algorithm works autonomously in real time. The number of used appliances and the corresponding appliance models are learned in operation and are progressively updated. The proposed algorithm is considering each useful and suitable detected power state. The algorithm tries to detect power states corresponding to on/off appliances as well as to multi-state appliances based on active power measurements in 1s resolution. We evaluated the novel introduced load disaggregation approach on real world data by testing the possibility to disaggregate energy demand on appliance level. ",Computer Science - Other Computer Science ; ,"Egarter, Dominik ; Elmenreich, Wilfried ; ","Autonomous Load Disaggregation Approach based on Active Power   Measurements  With the help of smart metering valuable information of the appliance usage can be retrieved. In detail, non-intrusive load monitoring (NILM), also called load disaggregation, tries to identify appliances in the power draw of an household. In this paper an unsupervised load disaggregation approach is proposed that works without a priori knowledge about appliances. The proposed algorithm works autonomously in real time. The number of used appliances and the corresponding appliance models are learned in operation and are progressively updated. The proposed algorithm is considering each useful and suitable detected power state. The algorithm tries to detect power states corresponding to on/off appliances as well as to multi-state appliances based on active power measurements in 1s resolution. We evaluated the novel introduced load disaggregation approach on real world data by testing the possibility to disaggregate energy demand on appliance level. ",autonomous load disaggregation approach base active power measurements help smart meter valuable information appliance usage retrieve detail non intrusive load monitor nilm also call load disaggregation try identify appliances power draw household paper unsupervised load disaggregation approach propose work without priori knowledge appliances propose algorithm work autonomously real time number use appliances correspond appliance model learn operation progressively update propose algorithm consider useful suitable detect power state algorithm try detect power state correspond appliances well multi state appliances base active power measurements resolution evaluate novel introduce load disaggregation approach real world data test possibility disaggregate energy demand appliance level,99,4,1412.2877.txt
http://arxiv.org/abs/1412.3347,Computational Aspects of the Colorful Carath\'eodory Theorem,"  Let $C_1,\dots,C_{d+1}\subset \mathbb{R}^d$ be $d+1$ point sets, each containing the origin in its convex hull. We call these sets color classes, and we call a sequence $p_1, \dots, p_{d+1}$ with $p_i \in C_i$, for $i = 1, \dots, d+1$, a colorful choice. The colorful Carath\'eodory theorem guarantees the existence of a colorful choice that also contains the origin in its convex hull. The computational complexity of finding such a colorful choice (CCP) is unknown. This is particularly interesting in the light of polynomial-time reductions from several related problems, such as computing centerpoints, to CCP.   We define a novel notion of approximation that is compatible with the polynomial-time reductions to CCP: a sequence that contains at most $k$ points from each color class is called a $k$-colorful choice. We present an algorithm that for any fixed $\varepsilon > 0$, outputs an $\lceil \epsilon d\rceil$-colorful choice containing the origin in its convex hull in polynomial time.   Furthermore, we consider a related problem of CCP: in the nearest colorful polytope problem (NCP), we are given sets $C_1,\dots,C_n\subset\mathbb{R}^d$ that do not necessarily contain the origin in their convex hulls. The goal is to find a colorful choice whose convex hull minimizes the distance to the origin. We show that computing a local optimum for NCP is PLS-complete, while computing a global optimum is NP-hard. ",Computer Science - Computational Geometry ; ,"Mulzer, Wolfgang ; Stein, Yannik ; ","Computational Aspects of the Colorful Carath\'eodory Theorem  Let $C_1,\dots,C_{d+1}\subset \mathbb{R}^d$ be $d+1$ point sets, each containing the origin in its convex hull. We call these sets color classes, and we call a sequence $p_1, \dots, p_{d+1}$ with $p_i \in C_i$, for $i = 1, \dots, d+1$, a colorful choice. The colorful Carath\'eodory theorem guarantees the existence of a colorful choice that also contains the origin in its convex hull. The computational complexity of finding such a colorful choice (CCP) is unknown. This is particularly interesting in the light of polynomial-time reductions from several related problems, such as computing centerpoints, to CCP.   We define a novel notion of approximation that is compatible with the polynomial-time reductions to CCP: a sequence that contains at most $k$ points from each color class is called a $k$-colorful choice. We present an algorithm that for any fixed $\varepsilon > 0$, outputs an $\lceil \epsilon d\rceil$-colorful choice containing the origin in its convex hull in polynomial time.   Furthermore, we consider a related problem of CCP: in the nearest colorful polytope problem (NCP), we are given sets $C_1,\dots,C_n\subset\mathbb{R}^d$ that do not necessarily contain the origin in their convex hulls. The goal is to find a colorful choice whose convex hull minimizes the distance to the origin. We show that computing a local optimum for NCP is PLS-complete, while computing a global optimum is NP-hard. ",computational aspects colorful carath eodory theorem let dot subset mathbb point set contain origin convex hull call set color class call sequence dot dot colorful choice colorful carath eodory theorem guarantee existence colorful choice also contain origin convex hull computational complexity find colorful choice ccp unknown particularly interest light polynomial time reductions several relate problems compute centerpoints ccp define novel notion approximation compatible polynomial time reductions ccp sequence contain point color class call colorful choice present algorithm fix varepsilon output lceil epsilon rceil colorful choice contain origin convex hull polynomial time furthermore consider relate problem ccp nearest colorful polytope problem ncp give set dot subset mathbb necessarily contain origin convex hull goal find colorful choice whose convex hull minimize distance origin show compute local optimum ncp pls complete compute global optimum np hard,133,4,1412.3347.txt
http://arxiv.org/abs/1412.3701,How Much Lookahead is Needed to Win Infinite Games?,"  Delay games are two-player games of infinite duration in which one player may delay her moves to obtain a lookahead on her opponent's moves. For $\omega$-regular winning conditions it is known that such games can be solved in doubly-exponential time and that doubly-exponential lookahead is sufficient.   We improve upon both results by giving an exponential time algorithm and an exponential upper bound on the necessary lookahead. This is complemented by showing EXPTIME-hardness of the solution problem and tight exponential lower bounds on the lookahead. Both lower bounds already hold for safety conditions. Furthermore, solving delay games with reachability conditions is shown to be PSPACE-complete.   This is a corrected version of the paper https://arxiv.org/abs/1412.3701v4 published originally on August 26, 2016. ",Computer Science - Computer Science and Game Theory ; Computer Science - Formal Languages and Automata Theory ; ,"Klein, Felix ; Zimmermann, Martin ; ","How Much Lookahead is Needed to Win Infinite Games?  Delay games are two-player games of infinite duration in which one player may delay her moves to obtain a lookahead on her opponent's moves. For $\omega$-regular winning conditions it is known that such games can be solved in doubly-exponential time and that doubly-exponential lookahead is sufficient.   We improve upon both results by giving an exponential time algorithm and an exponential upper bound on the necessary lookahead. This is complemented by showing EXPTIME-hardness of the solution problem and tight exponential lower bounds on the lookahead. Both lower bounds already hold for safety conditions. Furthermore, solving delay games with reachability conditions is shown to be PSPACE-complete.   This is a corrected version of the paper https://arxiv.org/abs/1412.3701v4 published originally on August 26, 2016. ",much lookahead need win infinite game delay game two player game infinite duration one player may delay move obtain lookahead opponent move omega regular win condition know game solve doubly exponential time doubly exponential lookahead sufficient improve upon result give exponential time algorithm exponential upper bind necessary lookahead complement show exptime hardness solution problem tight exponential lower bound lookahead lower bound already hold safety condition furthermore solve delay game reachability condition show pspace complete correct version paper https arxiv org abs publish originally august,84,8,1412.3701.txt
http://arxiv.org/abs/1412.4171,Dynamics of Information Diffusion and Social Sensing,"  Statistical inference using social sensors is an area that has witnessed remarkable progress and is relevant in applications including localizing events for targeted advertising, marketing, localization of natural disasters and predicting sentiment of investors in financial markets. This chapter presents a tutorial description of four important aspects of sensing-based information diffusion in social networks from a communications/signal processing perspective. First, diffusion models for information exchange in large scale social networks together with social sensing via social media networks such as Twitter is considered. Second, Bayesian social learning models and risk averse social learning is considered with applications in finance and online reputation systems. Third, the principle of revealed preferences arising in micro-economics theory is used to parse datasets to determine if social sensors are utility maximizers and then determine their utility functions. Finally, the interaction of social sensors with YouTube channel owners is studied using time series analysis methods. All four topics are explained in the context of actual experimental datasets from health networks, social media and psychological experiments. Also, algorithms are given that exploit the above models to infer underlying events based on social sensing. The overview, insights, models and algorithms presented in this chapter stem from recent developments in network science, economics and signal processing. At a deeper level, this chapter considers mean field dynamics of networks, risk averse Bayesian social learning filtering and quickest change detection, data incest in decision making over a directed acyclic graph of social sensors, inverse optimization problems for utility function estimation (revealed preferences) and statistical modeling of interacting social sensors in YouTube social networks. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Krishnamurthy, Vikram ; Hoiles, William ; ","Dynamics of Information Diffusion and Social Sensing  Statistical inference using social sensors is an area that has witnessed remarkable progress and is relevant in applications including localizing events for targeted advertising, marketing, localization of natural disasters and predicting sentiment of investors in financial markets. This chapter presents a tutorial description of four important aspects of sensing-based information diffusion in social networks from a communications/signal processing perspective. First, diffusion models for information exchange in large scale social networks together with social sensing via social media networks such as Twitter is considered. Second, Bayesian social learning models and risk averse social learning is considered with applications in finance and online reputation systems. Third, the principle of revealed preferences arising in micro-economics theory is used to parse datasets to determine if social sensors are utility maximizers and then determine their utility functions. Finally, the interaction of social sensors with YouTube channel owners is studied using time series analysis methods. All four topics are explained in the context of actual experimental datasets from health networks, social media and psychological experiments. Also, algorithms are given that exploit the above models to infer underlying events based on social sensing. The overview, insights, models and algorithms presented in this chapter stem from recent developments in network science, economics and signal processing. At a deeper level, this chapter considers mean field dynamics of networks, risk averse Bayesian social learning filtering and quickest change detection, data incest in decision making over a directed acyclic graph of social sensors, inverse optimization problems for utility function estimation (revealed preferences) and statistical modeling of interacting social sensors in YouTube social networks. ",dynamics information diffusion social sense statistical inference use social sensors area witness remarkable progress relevant applications include localize events target advertise market localization natural disasters predict sentiment investors financial market chapter present tutorial description four important aspects sense base information diffusion social network communications signal process perspective first diffusion model information exchange large scale social network together social sense via social media network twitter consider second bayesian social learn model risk averse social learn consider applications finance online reputation systems third principle reveal preferences arise micro economics theory use parse datasets determine social sensors utility maximizers determine utility function finally interaction social sensors youtube channel owners study use time series analysis methods four topics explain context actual experimental datasets health network social media psychological experiment also algorithms give exploit model infer underlie events base social sense overview insights model algorithms present chapter stem recent developments network science economics signal process deeper level chapter consider mean field dynamics network risk averse bayesian social learn filter quickest change detection data incest decision make direct acyclic graph social sensors inverse optimization problems utility function estimation reveal preferences statistical model interact social sensors youtube social network,192,6,1412.4171.txt
http://arxiv.org/abs/1412.4198,An Ordinal Minimax Theorem,"  In the early 1950s Lloyd Shapley proposed an ordinal and set-valued solution concept for zero-sum games called \emph{weak saddle}. We show that all weak saddles of a given zero-sum game are interchangeable and equivalent. As a consequence, every such game possesses a unique set-based value. ",Computer Science - Computer Science and Game Theory ; ,"Brandt, Felix ; Brill, Markus ; Suksompong, Warut ; ","An Ordinal Minimax Theorem  In the early 1950s Lloyd Shapley proposed an ordinal and set-valued solution concept for zero-sum games called \emph{weak saddle}. We show that all weak saddles of a given zero-sum game are interchangeable and equivalent. As a consequence, every such game possesses a unique set-based value. ",ordinal minimax theorem early lloyd shapley propose ordinal set value solution concept zero sum game call emph weak saddle show weak saddle give zero sum game interchangeable equivalent consequence every game possess unique set base value,36,8,1412.4198.txt
http://arxiv.org/abs/1412.4586,Generalized Vietoris Bisimulations,"  We introduce and study bisimulations for coalgebras on Stone spaces [14]. Our notion of bisimulation is sound and complete for behavioural equivalence, and generalizes Vietoris bisimulations [4]. The main result of our paper is that bisimulation for a $\mathbf{Stone}$ coalgebra is the topological closure of bisimulation for the underlying $\mathbf{Set}$ coalgebra. ",Computer Science - Logic in Computer Science ; Mathematics - Logic ; ,"Enqvist, Sebastian ; Sourabh, Sumit ; ","Generalized Vietoris Bisimulations  We introduce and study bisimulations for coalgebras on Stone spaces [14]. Our notion of bisimulation is sound and complete for behavioural equivalence, and generalizes Vietoris bisimulations [4]. The main result of our paper is that bisimulation for a $\mathbf{Stone}$ coalgebra is the topological closure of bisimulation for the underlying $\mathbf{Set}$ coalgebra. ",generalize vietoris bisimulations introduce study bisimulations coalgebras stone space notion bisimulation sound complete behavioural equivalence generalize vietoris bisimulations main result paper bisimulation mathbf stone coalgebra topological closure bisimulation underlie mathbf set coalgebra,32,4,1412.4586.txt
http://arxiv.org/abs/1412.5204,How many queries are needed to distinguish a truncated random   permutation from a random function?,"  An oracle chooses a function $f$ from the set of $n$ bits strings to itself, which is either a randomly chosen permutation or a randomly chosen function. When queried by an $n$-bit string $w$, the oracle computes $f(w)$, truncates the $m$ last bits, and returns only the first $n-m$ bits of $f(w)$. How many queries does a querying adversary need to submit in order to distinguish the truncated permutation from the (truncated) function?   In 1998, Hall et al. showed an algorithm for determining (with high probability) whether or not $f$ is a permutation, using $O(2^{\frac{m+n}{2}})$ queries. They also showed that if $m < n/7$, a smaller number of queries will not suffice. For $m > n/7$, their method gives a weaker bound. In this note, we first show how a modification of the approximation method used by Hall et al. can solve the problem completely. It extends the result to practically any $m$, showing that $\Omega(2^{\frac{m+n}{2}})$ queries are needed to get a non-negligible distinguishing advantage. However, more surprisingly, a better bound for the distinguishing advantage can be obtained from a result of Stam published, in a different context, already in 1978. We also show that, at least in some cases, Stam's bound is tight. ",Computer Science - Cryptography and Security ; ,"Gilboa, Shoni ; Gueron, Shay ; Morris, Ben ; ","How many queries are needed to distinguish a truncated random   permutation from a random function?  An oracle chooses a function $f$ from the set of $n$ bits strings to itself, which is either a randomly chosen permutation or a randomly chosen function. When queried by an $n$-bit string $w$, the oracle computes $f(w)$, truncates the $m$ last bits, and returns only the first $n-m$ bits of $f(w)$. How many queries does a querying adversary need to submit in order to distinguish the truncated permutation from the (truncated) function?   In 1998, Hall et al. showed an algorithm for determining (with high probability) whether or not $f$ is a permutation, using $O(2^{\frac{m+n}{2}})$ queries. They also showed that if $m < n/7$, a smaller number of queries will not suffice. For $m > n/7$, their method gives a weaker bound. In this note, we first show how a modification of the approximation method used by Hall et al. can solve the problem completely. It extends the result to practically any $m$, showing that $\Omega(2^{\frac{m+n}{2}})$ queries are needed to get a non-negligible distinguishing advantage. However, more surprisingly, a better bound for the distinguishing advantage can be obtained from a result of Stam published, in a different context, already in 1978. We also show that, at least in some cases, Stam's bound is tight. ",many query need distinguish truncate random permutation random function oracle choose function set bits string either randomly choose permutation randomly choose function query bite string oracle compute truncate last bits return first bits many query query adversary need submit order distinguish truncate permutation truncate function hall et al show algorithm determine high probability whether permutation use frac query also show smaller number query suffice method give weaker bind note first show modification approximation method use hall et al solve problem completely extend result practically show omega frac query need get non negligible distinguish advantage however surprisingly better bind distinguish advantage obtain result stam publish different context already also show least case stam bind tight,114,1,1412.5204.txt
http://arxiv.org/abs/1412.5374,Maximal Correlation Secrecy,"  This paper shows that the Hirschfeld-Gebelein-R\'enyi maximal correlation between the message and the ciphertext provides good secrecy guarantees for cryptosystems that use short keys. We first establish a bound on the eavesdropper's advantage in guessing functions of the message in terms of maximal correlation and the R\'enyi entropy of the message. This result implies that maximal correlation is stronger than the notion of entropic security introduced by Russell and Wang. We then show that a small maximal correlation $\rho$ can be achieved via a randomly generated cipher with key length $\approx2\log(1/\rho)$, independent of the message length, and by a stream cipher with key length $2\log(1/\rho)+\log n+2$ for a message of length $n$. We establish a converse showing that these ciphers are close to optimal. This is in contrast to entropic security for which there is a gap between the lower and upper bounds. Finally, we show that a small maximal correlation implies secrecy with respect to several mutual information based criteria but is not necessarily implied by them. Hence, maximal correlation is a stronger and more practically relevant measure of secrecy than mutual information. ",Computer Science - Information Theory ; Computer Science - Cryptography and Security ; ,"Li, Cheuk Ting ; Gamal, Abbas El ; ","Maximal Correlation Secrecy  This paper shows that the Hirschfeld-Gebelein-R\'enyi maximal correlation between the message and the ciphertext provides good secrecy guarantees for cryptosystems that use short keys. We first establish a bound on the eavesdropper's advantage in guessing functions of the message in terms of maximal correlation and the R\'enyi entropy of the message. This result implies that maximal correlation is stronger than the notion of entropic security introduced by Russell and Wang. We then show that a small maximal correlation $\rho$ can be achieved via a randomly generated cipher with key length $\approx2\log(1/\rho)$, independent of the message length, and by a stream cipher with key length $2\log(1/\rho)+\log n+2$ for a message of length $n$. We establish a converse showing that these ciphers are close to optimal. This is in contrast to entropic security for which there is a gap between the lower and upper bounds. Finally, we show that a small maximal correlation implies secrecy with respect to several mutual information based criteria but is not necessarily implied by them. Hence, maximal correlation is a stronger and more practically relevant measure of secrecy than mutual information. ",maximal correlation secrecy paper show hirschfeld gebelein enyi maximal correlation message ciphertext provide good secrecy guarantee cryptosystems use short key first establish bind eavesdropper advantage guess function message term maximal correlation enyi entropy message result imply maximal correlation stronger notion entropic security introduce russell wang show small maximal correlation rho achieve via randomly generate cipher key length approx log rho independent message length stream cipher key length log rho log message length establish converse show cipher close optimal contrast entropic security gap lower upper bound finally show small maximal correlation imply secrecy respect several mutual information base criteria necessarily imply hence maximal correlation stronger practically relevant measure secrecy mutual information,110,4,1412.5374.txt
http://arxiv.org/abs/1412.5466,Enumerative Coding for Line Polar Grassmannians with applications to   codes,"  A $k$-polar Grassmannian is the geometry having as pointset the set of all $k$-dimensional subspaces of a vector space $V$ which are totally isotropic for a given non-degenerate bilinear form $\mu$ defined on $V.$ Hence it can be regarded as a subgeometry of the ordinary $k$-Grassmannian. In this paper we deal with orthogonal line Grassmannians and with symplectic line Grassmannians, i.e. we assume $k=2$ and $\mu$ a non-degenerate symmetric or alternating form. We will provide a method to efficiently enumerate the pointsets of both orthogonal and symplectic line Grassmannians. This has several nice applications; among them, we shall discuss an efficient encoding/decoding/error correction strategy for line polar Grassmann codes of both types. ","Computer Science - Information Theory ; Mathematics - Combinatorics ; 14M15, 94B27, 94B05 ; ","Cardinali, Ilaria ; Giuzzi, Luca ; ","Enumerative Coding for Line Polar Grassmannians with applications to   codes  A $k$-polar Grassmannian is the geometry having as pointset the set of all $k$-dimensional subspaces of a vector space $V$ which are totally isotropic for a given non-degenerate bilinear form $\mu$ defined on $V.$ Hence it can be regarded as a subgeometry of the ordinary $k$-Grassmannian. In this paper we deal with orthogonal line Grassmannians and with symplectic line Grassmannians, i.e. we assume $k=2$ and $\mu$ a non-degenerate symmetric or alternating form. We will provide a method to efficiently enumerate the pointsets of both orthogonal and symplectic line Grassmannians. This has several nice applications; among them, we shall discuss an efficient encoding/decoding/error correction strategy for line polar Grassmann codes of both types. ",enumerative cod line polar grassmannians applications cod polar grassmannian geometry pointset set dimensional subspaces vector space totally isotropic give non degenerate bilinear form mu define hence regard subgeometry ordinary grassmannian paper deal orthogonal line grassmannians symplectic line grassmannians assume mu non degenerate symmetric alternate form provide method efficiently enumerate pointsets orthogonal symplectic line grassmannians several nice applications among shall discuss efficient encode decode error correction strategy line polar grassmann cod type,71,5,1412.5466.txt
http://arxiv.org/abs/1412.5669,The Timestamp of Timed Automata,"  Given a member A of the class of non-deterministic timed automata with silent transitions (eNTA), we show how one can effectively compute its timestamp: the set of all pairs of time values and the corresponding actions of all observable timed traces of A, and also a deterministic timed automaton with the same timestamp as that of A. The timestamp is eventually periodic and is constructed via a finite periodic augmented region automaton. A consequence of this construction is the periodicity of the language of timed automata with respect to suffixes. Applications include the decidability of the 1-bounded language inclusion problem for the class eNTA, and a partial method, not bounded by time or number of steps, for the general language non-inclusion problem for eNTA. ",Computer Science - Formal Languages and Automata Theory ; F.1.1 ; D.2.4 ; ,"Rosenmann, Amnon ; ","The Timestamp of Timed Automata  Given a member A of the class of non-deterministic timed automata with silent transitions (eNTA), we show how one can effectively compute its timestamp: the set of all pairs of time values and the corresponding actions of all observable timed traces of A, and also a deterministic timed automaton with the same timestamp as that of A. The timestamp is eventually periodic and is constructed via a finite periodic augmented region automaton. A consequence of this construction is the periodicity of the language of timed automata with respect to suffixes. Applications include the decidability of the 1-bounded language inclusion problem for the class eNTA, and a partial method, not bounded by time or number of steps, for the general language non-inclusion problem for eNTA. ",timestamp time automata give member class non deterministic time automata silent transition enta show one effectively compute timestamp set pair time value correspond action observable time trace also deterministic time automaton timestamp timestamp eventually periodic construct via finite periodic augment region automaton consequence construction periodicity language time automata respect suffix applications include decidability bound language inclusion problem class enta partial method bound time number step general language non inclusion problem enta,71,14,1412.5669.txt
http://arxiv.org/abs/1412.5718,Revisiting Non-Progressive Influence Models: Scalable Influence   Maximization,"  While influence maximization in social networks has been studied extensively in computer science community for the last decade the focus has been on the progressive influence models, such as independent cascade (IC) and Linear threshold (LT) models, which cannot capture the reversibility of choices. In this paper, we present the Heat Conduction (HC) model which is a non-progressive influence model with real-world interpretations. We show that HC unifies, generalizes, and extends the existing nonprogressive models, such as the Voter model [1] and non-progressive LT [2]. We then prove that selecting the optimal seed set of influential nodes is NP-hard for HC but by establishing the submodularity of influence spread, we can tackle the influence maximization problem with a scalable and provably near-optimal greedy algorithm. We are the first to present a scalable solution for influence maximization under nonprogressive LT model, as a special case of the HC model. In sharp contrast to the other greedy influence maximization methods, our fast and efficient C2GREEDY algorithm benefits from two analytically computable steps: closed-form computation for finding the influence spread as well as the greedy seed selection. Through extensive experiments on several large real and synthetic networks, we show that C2GREEDY outperforms the state-of-the-art methods, in terms of both influence spread and scalability. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Golnari, Golshan ; Asiaee, Amir ; Banerjee, Arindam ; Zhang, Zhi-Li ; ","Revisiting Non-Progressive Influence Models: Scalable Influence   Maximization  While influence maximization in social networks has been studied extensively in computer science community for the last decade the focus has been on the progressive influence models, such as independent cascade (IC) and Linear threshold (LT) models, which cannot capture the reversibility of choices. In this paper, we present the Heat Conduction (HC) model which is a non-progressive influence model with real-world interpretations. We show that HC unifies, generalizes, and extends the existing nonprogressive models, such as the Voter model [1] and non-progressive LT [2]. We then prove that selecting the optimal seed set of influential nodes is NP-hard for HC but by establishing the submodularity of influence spread, we can tackle the influence maximization problem with a scalable and provably near-optimal greedy algorithm. We are the first to present a scalable solution for influence maximization under nonprogressive LT model, as a special case of the HC model. In sharp contrast to the other greedy influence maximization methods, our fast and efficient C2GREEDY algorithm benefits from two analytically computable steps: closed-form computation for finding the influence spread as well as the greedy seed selection. Through extensive experiments on several large real and synthetic networks, we show that C2GREEDY outperforms the state-of-the-art methods, in terms of both influence spread and scalability. ",revisit non progressive influence model scalable influence maximization influence maximization social network study extensively computer science community last decade focus progressive influence model independent cascade ic linear threshold lt model cannot capture reversibility choices paper present heat conduction hc model non progressive influence model real world interpretations show hc unify generalize extend exist nonprogressive model voter model non progressive lt prove select optimal seed set influential nod np hard hc establish submodularity influence spread tackle influence maximization problem scalable provably near optimal greedy algorithm first present scalable solution influence maximization nonprogressive lt model special case hc model sharp contrast greedy influence maximization methods fast efficient greedy algorithm benefit two analytically computable step close form computation find influence spread well greedy seed selection extensive experiment several large real synthetic network show greedy outperform state art methods term influence spread scalability,139,0,1412.5718.txt
http://arxiv.org/abs/1412.5831,Deducing Truth from Correlation,"  This work is motivated by a question at the heart of unsupervised learning approaches: Assume we are collecting a number K of (subjective) opinions about some event E from K different agents. Can we infer E from them? Prima facie this seems impossible, since the agents may be lying. We model this task by letting the events be distributed according to some distribution p and the task is to estimate p under unknown noise. Again, this is impossible without additional assumptions. We report here the finding of very natural such assumptions - the availability of multiple copies of the true data, each under independent and invertible (in the sense of matrices) noise, is already sufficient: If the true distribution and the observations are modeled on the same finite alphabet, then the number of such copies needed to determine p to the highest possible precision is exactly three! This result can be seen as a counterpart to independent component analysis. Therefore, we call our approach 'dependent component analysis'. In addition, we present generalizations of the model to different alphabet sizes at in- and output. A second result is found: the 'activation' of invertibility through multiple parallel uses. ",Computer Science - Information Theory ; ,"Nötzel, Janis ; Swetly, Walter ; ","Deducing Truth from Correlation  This work is motivated by a question at the heart of unsupervised learning approaches: Assume we are collecting a number K of (subjective) opinions about some event E from K different agents. Can we infer E from them? Prima facie this seems impossible, since the agents may be lying. We model this task by letting the events be distributed according to some distribution p and the task is to estimate p under unknown noise. Again, this is impossible without additional assumptions. We report here the finding of very natural such assumptions - the availability of multiple copies of the true data, each under independent and invertible (in the sense of matrices) noise, is already sufficient: If the true distribution and the observations are modeled on the same finite alphabet, then the number of such copies needed to determine p to the highest possible precision is exactly three! This result can be seen as a counterpart to independent component analysis. Therefore, we call our approach 'dependent component analysis'. In addition, we present generalizations of the model to different alphabet sizes at in- and output. A second result is found: the 'activation' of invertibility through multiple parallel uses. ",deduce truth correlation work motivate question heart unsupervised learn approach assume collect number subjective opinions event different agents infer prima facie seem impossible since agents may lie model task let events distribute accord distribution task estimate unknown noise impossible without additional assumptions report find natural assumptions availability multiple copy true data independent invertible sense matrices noise already sufficient true distribution observations model finite alphabet number copy need determine highest possible precision exactly three result see counterpart independent component analysis therefore call approach dependent component analysis addition present generalizations model different alphabet size output second result find activation invertibility multiple parallel use,101,7,1412.5831.txt
http://arxiv.org/abs/1412.5902,"Nearest Descent, In-Tree, and Clustering","  In this paper, we propose a physically inspired graph-theoretical clustering method, which first makes the data points organized into an attractive graph, called In-Tree, via a physically inspired rule, called Nearest Descent (ND). In particular, the rule of ND works to select the nearest node in the descending direction of potential as the parent node of each node, which is in essence different from the classical Gradient Descent or Steepest Descent. The constructed In-Tree proves a very good candidate for clustering due to its particular features and properties. In the In-Tree, the original clustering problem is reduced to a problem of removing a very few of undesired edges from this graph. Pleasingly, the undesired edges in In-Tree are so distinguishable that they can be easily determined in either automatic or interactive way, which is in stark contrast to the cases in the widely used Minimal Spanning Tree and k-nearest-neighbor graph. The cluster number in the proposed method can be easily determined based on some intermediate plots, and the cluster assignment for each node is easily made by quickly searching its root node in each sub-graph (also an In-Tree). The proposed method is extensively evaluated on both synthetic and real-world datasets. Overall, the proposed clustering method is a density-based one, but shows significant differences and advantages in comparison to the traditional ones. The proposed method is simple yet efficient and reliable, and is applicable to various datasets with diverse shapes, attributes and any high dimensionality ",Computer Science - Machine Learning ; Computer Science - Computer Vision and Pattern Recognition ; ,"Qiu, Teng ; Yang, Kaifu ; Li, Chaoyi ; Li, Yongjie ; ","Nearest Descent, In-Tree, and Clustering  In this paper, we propose a physically inspired graph-theoretical clustering method, which first makes the data points organized into an attractive graph, called In-Tree, via a physically inspired rule, called Nearest Descent (ND). In particular, the rule of ND works to select the nearest node in the descending direction of potential as the parent node of each node, which is in essence different from the classical Gradient Descent or Steepest Descent. The constructed In-Tree proves a very good candidate for clustering due to its particular features and properties. In the In-Tree, the original clustering problem is reduced to a problem of removing a very few of undesired edges from this graph. Pleasingly, the undesired edges in In-Tree are so distinguishable that they can be easily determined in either automatic or interactive way, which is in stark contrast to the cases in the widely used Minimal Spanning Tree and k-nearest-neighbor graph. The cluster number in the proposed method can be easily determined based on some intermediate plots, and the cluster assignment for each node is easily made by quickly searching its root node in each sub-graph (also an In-Tree). The proposed method is extensively evaluated on both synthetic and real-world datasets. Overall, the proposed clustering method is a density-based one, but shows significant differences and advantages in comparison to the traditional ones. The proposed method is simple yet efficient and reliable, and is applicable to various datasets with diverse shapes, attributes and any high dimensionality ",nearest descent tree cluster paper propose physically inspire graph theoretical cluster method first make data point organize attractive graph call tree via physically inspire rule call nearest descent nd particular rule nd work select nearest node descend direction potential parent node node essence different classical gradient descent steepest descent construct tree prove good candidate cluster due particular feature properties tree original cluster problem reduce problem remove undesired edge graph pleasingly undesired edge tree distinguishable easily determine either automatic interactive way stark contrast case widely use minimal span tree nearest neighbor graph cluster number propose method easily determine base intermediate plot cluster assignment node easily make quickly search root node sub graph also tree propose method extensively evaluate synthetic real world datasets overall propose cluster method density base one show significant differences advantage comparison traditional ones propose method simple yet efficient reliable applicable various datasets diverse shape attribute high dimensionality,149,3,1412.5902.txt
http://arxiv.org/abs/1412.6466,Finding 2-Edge and 2-Vertex Strongly Connected Components in Quadratic   Time,"  We present faster algorithms for computing the 2-edge and 2-vertex strongly connected components of a directed graph, which are straightforward generalizations of strongly connected components. While in undirected graphs the 2-edge and 2-vertex connected components can be found in linear time, in directed graphs only rather simple $O(m n)$-time algorithms were known. We use a hierarchical sparsification technique to obtain algorithms that run in time $O(n^2)$. For 2-edge strongly connected components our algorithm gives the first running time improvement in 20 years. Additionally we present an $O(m^2 / \log{n})$-time algorithm for 2-edge strongly connected components, and thus improve over the $O(m n)$ running time also when $m = O(n)$. Our approach extends to k-edge and k-vertex strongly connected components for any constant k with a running time of $O(n^2 \log^2 n)$ for edges and $O(n^3)$ for vertices. ",Computer Science - Data Structures and Algorithms ; ,"Henzinger, Monika ; Krinninger, Sebastian ; Loitzenbauer, Veronika ; ","Finding 2-Edge and 2-Vertex Strongly Connected Components in Quadratic   Time  We present faster algorithms for computing the 2-edge and 2-vertex strongly connected components of a directed graph, which are straightforward generalizations of strongly connected components. While in undirected graphs the 2-edge and 2-vertex connected components can be found in linear time, in directed graphs only rather simple $O(m n)$-time algorithms were known. We use a hierarchical sparsification technique to obtain algorithms that run in time $O(n^2)$. For 2-edge strongly connected components our algorithm gives the first running time improvement in 20 years. Additionally we present an $O(m^2 / \log{n})$-time algorithm for 2-edge strongly connected components, and thus improve over the $O(m n)$ running time also when $m = O(n)$. Our approach extends to k-edge and k-vertex strongly connected components for any constant k with a running time of $O(n^2 \log^2 n)$ for edges and $O(n^3)$ for vertices. ",find edge vertex strongly connect components quadratic time present faster algorithms compute edge vertex strongly connect components direct graph straightforward generalizations strongly connect components undirected graph edge vertex connect components find linear time direct graph rather simple time algorithms know use hierarchical sparsification technique obtain algorithms run time edge strongly connect components algorithm give first run time improvement years additionally present log time algorithm edge strongly connect components thus improve run time also approach extend edge vertex strongly connect components constant run time log edge vertices,86,1,1412.6466.txt
http://arxiv.org/abs/1412.6622,Deep metric learning using Triplet network,"  Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning. ",Computer Science - Machine Learning ; Computer Science - Computer Vision and Pattern Recognition ; Statistics - Machine Learning ; ,"Hoffer, Elad ; Ailon, Nir ; ","Deep metric learning using Triplet network  Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning. ",deep metric learn use triplet network deep learn prove successful set model learn useful semantic representations data however mostly implicitly learn part classification task paper propose triplet network model aim learn useful representations distance comparisons similar model define wang et al tailor make learn rank image information retrieval demonstrate use various datasets model learn better representation immediate competitor siamese network also discuss future possible usage framework unsupervised learn,68,6,1412.6622.txt
http://arxiv.org/abs/1412.6761,New results on classical and quantum counter automata,"  We show that one-way quantum one-counter automaton with zero-error is more powerful than its probabilistic counterpart on promise problems. Then, we obtain a similar separation result between Las Vegas one-way probabilistic one-counter automaton and one-way deterministic one-counter automaton.   We also obtain new results on classical counter automata regarding language recognition. It was conjectured that one-way probabilistic one blind-counter automata cannot recognize Kleene closure of equality language [A. Yakaryilmaz: Superiority of one-way and realtime quantum machines. RAIRO - Theor. Inf. and Applic. 46(4): 615-641 (2012)]. We show that this conjecture is false, and also show several separation results for blind/non-blind counter automata. ",Computer Science - Formal Languages and Automata Theory ; Computer Science - Computational Complexity ; Quantum Physics ; ,"Nakanishi, Masaki ; Yakaryılmaz, Abuzer ; Gainutdinova, Aida ; ","New results on classical and quantum counter automata  We show that one-way quantum one-counter automaton with zero-error is more powerful than its probabilistic counterpart on promise problems. Then, we obtain a similar separation result between Las Vegas one-way probabilistic one-counter automaton and one-way deterministic one-counter automaton.   We also obtain new results on classical counter automata regarding language recognition. It was conjectured that one-way probabilistic one blind-counter automata cannot recognize Kleene closure of equality language [A. Yakaryilmaz: Superiority of one-way and realtime quantum machines. RAIRO - Theor. Inf. and Applic. 46(4): 615-641 (2012)]. We show that this conjecture is false, and also show several separation results for blind/non-blind counter automata. ",new result classical quantum counter automata show one way quantum one counter automaton zero error powerful probabilistic counterpart promise problems obtain similar separation result las vegas one way probabilistic one counter automaton one way deterministic one counter automaton also obtain new result classical counter automata regard language recognition conjecture one way probabilistic one blind counter automata cannot recognize kleene closure equality language yakaryilmaz superiority one way realtime quantum machine rairo theor inf applic show conjecture false also show several separation result blind non blind counter automata,86,14,1412.6761.txt
http://arxiv.org/abs/1412.6808,Learning the nonlinear geometry of high-dimensional data: Models and   algorithms,"  Modern information processing relies on the axiom that high-dimensional data lie near low-dimensional geometric structures. This paper revisits the problem of data-driven learning of these geometric structures and puts forth two new nonlinear geometric models for data describing ""related"" objects/phenomena. The first one of these models straddles the two extremes of the subspace model and the union-of-subspaces model, and is termed the metric-constrained union-of-subspaces (MC-UoS) model. The second one of these models---suited for data drawn from a mixture of nonlinear manifolds---generalizes the kernel subspace model, and is termed the metric-constrained kernel union-of-subspaces (MC-KUoS) model. The main contributions of this paper in this regard include the following. First, it motivates and formalizes the problems of MC-UoS and MC-KUoS learning. Second, it presents algorithms that efficiently learn an MC-UoS or an MC-KUoS underlying data of interest. Third, it extends these algorithms to the case when parts of the data are missing. Last, but not least, it reports the outcomes of a series of numerical experiments involving both synthetic and real data that demonstrate the superiority of the proposed geometric models and learning algorithms over existing approaches in the literature. These experiments also help clarify the connections between this work and the literature on (subspace and kernel k-means) clustering. ",Statistics - Machine Learning ; Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Machine Learning ; ,"Wu, Tong ; Bajwa, Waheed U. ; ","Learning the nonlinear geometry of high-dimensional data: Models and   algorithms  Modern information processing relies on the axiom that high-dimensional data lie near low-dimensional geometric structures. This paper revisits the problem of data-driven learning of these geometric structures and puts forth two new nonlinear geometric models for data describing ""related"" objects/phenomena. The first one of these models straddles the two extremes of the subspace model and the union-of-subspaces model, and is termed the metric-constrained union-of-subspaces (MC-UoS) model. The second one of these models---suited for data drawn from a mixture of nonlinear manifolds---generalizes the kernel subspace model, and is termed the metric-constrained kernel union-of-subspaces (MC-KUoS) model. The main contributions of this paper in this regard include the following. First, it motivates and formalizes the problems of MC-UoS and MC-KUoS learning. Second, it presents algorithms that efficiently learn an MC-UoS or an MC-KUoS underlying data of interest. Third, it extends these algorithms to the case when parts of the data are missing. Last, but not least, it reports the outcomes of a series of numerical experiments involving both synthetic and real data that demonstrate the superiority of the proposed geometric models and learning algorithms over existing approaches in the literature. These experiments also help clarify the connections between this work and the literature on (subspace and kernel k-means) clustering. ",learn nonlinear geometry high dimensional data model algorithms modern information process rely axiom high dimensional data lie near low dimensional geometric structure paper revisit problem data drive learn geometric structure put forth two new nonlinear geometric model data describe relate object phenomena first one model straddle two extremes subspace model union subspaces model term metric constrain union subspaces mc uos model second one model suit data draw mixture nonlinear manifold generalize kernel subspace model term metric constrain kernel union subspaces mc kuos model main contributions paper regard include follow first motivate formalize problems mc uos mc kuos learn second present algorithms efficiently learn mc uos mc kuos underlie data interest third extend algorithms case part data miss last least report outcomes series numerical experiment involve synthetic real data demonstrate superiority propose geometric model learn algorithms exist approach literature experiment also help clarify connections work literature subspace kernel mean cluster,149,10,1412.6808.txt
http://arxiv.org/abs/1412.7172,Rational Groupthink,"  We study how long-lived rational agents learn from repeatedly observing each others' actions. We find that in the long run, information aggregation fails, and the fraction of private information transmitted goes to zero as the number of agents gets large. With Normal signals, in the long-run, agents learn less from observing the actions of any number of other agents than they learn from seeing three other agents' signals. We identify rational groupthink---in which agents ignore their private signals and choose the same action for long periods of time---as the cause of this failure of information aggregation. ",Computer Science - Computer Science and Game Theory ; Economics - Theoretical Economics ; Mathematics - Probability ; ,"Harel, Matan ; Mossel, Elchanan ; Strack, Philipp ; Tamuz, Omer ; ","Rational Groupthink  We study how long-lived rational agents learn from repeatedly observing each others' actions. We find that in the long run, information aggregation fails, and the fraction of private information transmitted goes to zero as the number of agents gets large. With Normal signals, in the long-run, agents learn less from observing the actions of any number of other agents than they learn from seeing three other agents' signals. We identify rational groupthink---in which agents ignore their private signals and choose the same action for long periods of time---as the cause of this failure of information aggregation. ",rational groupthink study long live rational agents learn repeatedly observe others action find long run information aggregation fail fraction private information transmit go zero number agents get large normal signal long run agents learn less observe action number agents learn see three agents signal identify rational groupthink agents ignore private signal choose action long periods time cause failure information aggregation,60,9,1412.7172.txt
http://arxiv.org/abs/1412.7646,Sub-linear Time Support Recovery for Compressed Sensing using   Sparse-Graph Codes,"  We study the support recovery problem for compressed sensing, where the goal is to reconstruct the a high-dimensional $K$-sparse signal $\mathbf{x}\in\mathbb{R}^N$, from low-dimensional linear measurements with and without noise. Our key contribution is a new compressed sensing framework through a new family of carefully designed sparse measurement matrices associated with minimal measurement costs and a low-complexity recovery algorithm. The measurement matrix in our framework is designed based on the well-crafted sparsification through capacity-approaching sparse-graph codes, where the sparse coefficients can be recovered efficiently in a few iterations by performing simple error decoding over the observations. We formally connect this general recovery problem with sparse-graph decoding in packet communication systems, and analyze our framework in terms of the measurement cost, time complexity and recovery performance. In the noiseless setting, our framework can recover any arbitrary $K$-sparse signal in $O(K)$ time using $2K$ measurements asymptotically with high probability. In the noisy setting, when the sparse coefficients take values in a finite and quantized alphabet, our framework can achieve the same goal in time $O(K\log(N/K))$ using $O(K\log(N/K))$ measurements obtained from measurement matrix with elements $\{-1,0,1\}$. When the sparsity $K$ is sub-linear in the signal dimension $K=O(N^\delta)$ for some $0<\delta<1$, our results are order-optimal in terms of measurement costs and run-time, both of which are sub-linear in the signal dimension $N$. The sub-linear measurement cost and run-time can also be achieved with continuous-valued sparse coefficients, with a slight increment in the logarithmic factors. This offers the desired scalability of our framework that can potentially enable real-time or near-real-time processing for massive datasets featuring sparsity. ",Computer Science - Information Theory ; ,"Li, Xiao ; Yin, Dong ; Pawar, Sameer ; Pedarsani, Ramtin ; Ramchandran, Kannan ; ","Sub-linear Time Support Recovery for Compressed Sensing using   Sparse-Graph Codes  We study the support recovery problem for compressed sensing, where the goal is to reconstruct the a high-dimensional $K$-sparse signal $\mathbf{x}\in\mathbb{R}^N$, from low-dimensional linear measurements with and without noise. Our key contribution is a new compressed sensing framework through a new family of carefully designed sparse measurement matrices associated with minimal measurement costs and a low-complexity recovery algorithm. The measurement matrix in our framework is designed based on the well-crafted sparsification through capacity-approaching sparse-graph codes, where the sparse coefficients can be recovered efficiently in a few iterations by performing simple error decoding over the observations. We formally connect this general recovery problem with sparse-graph decoding in packet communication systems, and analyze our framework in terms of the measurement cost, time complexity and recovery performance. In the noiseless setting, our framework can recover any arbitrary $K$-sparse signal in $O(K)$ time using $2K$ measurements asymptotically with high probability. In the noisy setting, when the sparse coefficients take values in a finite and quantized alphabet, our framework can achieve the same goal in time $O(K\log(N/K))$ using $O(K\log(N/K))$ measurements obtained from measurement matrix with elements $\{-1,0,1\}$. When the sparsity $K$ is sub-linear in the signal dimension $K=O(N^\delta)$ for some $0<\delta<1$, our results are order-optimal in terms of measurement costs and run-time, both of which are sub-linear in the signal dimension $N$. The sub-linear measurement cost and run-time can also be achieved with continuous-valued sparse coefficients, with a slight increment in the logarithmic factors. This offers the desired scalability of our framework that can potentially enable real-time or near-real-time processing for massive datasets featuring sparsity. ",sub linear time support recovery compress sense use sparse graph cod study support recovery problem compress sense goal reconstruct high dimensional sparse signal mathbf mathbb low dimensional linear measurements without noise key contribution new compress sense framework new family carefully design sparse measurement matrices associate minimal measurement cost low complexity recovery algorithm measurement matrix framework design base well craft sparsification capacity approach sparse graph cod sparse coefficients recover efficiently iterations perform simple error decode observations formally connect general recovery problem sparse graph decode packet communication systems analyze framework term measurement cost time complexity recovery performance noiseless set framework recover arbitrary sparse signal time use measurements asymptotically high probability noisy set sparse coefficients take value finite quantize alphabet framework achieve goal time log use log measurements obtain measurement matrix elements sparsity sub linear signal dimension delta delta result order optimal term measurement cost run time sub linear signal dimension sub linear measurement cost run time also achieve continuous value sparse coefficients slight increment logarithmic factor offer desire scalability framework potentially enable real time near real time process massive datasets feature sparsity,180,9,1412.7646.txt
http://arxiv.org/abs/1412.7725,Automatic Photo Adjustment Using Deep Neural Networks,"  Photo retouching enables photographers to invoke dramatic visual impressions by artistically enhancing their photos through stylistic color and tone adjustments. However, it is also a time-consuming and challenging task that requires advanced skills beyond the abilities of casual photographers. Using an automated algorithm is an appealing alternative to manual work but such an algorithm faces many hurdles. Many photographic styles rely on subtle adjustments that depend on the image content and even its semantics. Further, these adjustments are often spatially varying. Because of these characteristics, existing automatic algorithms are still limited and cover only a subset of these challenges. Recently, deep machine learning has shown unique abilities to address hard problems that resisted machine algorithms for long. This motivated us to explore the use of deep learning in the context of photo editing. In this paper, we explain how to formulate the automatic photo adjustment problem in a way suitable for this approach. We also introduce an image descriptor that accounts for the local semantics of an image. Our experiments demonstrate that our deep learning formulation applied using these descriptors successfully capture sophisticated photographic styles. In particular and unlike previous techniques, it can model local adjustments that depend on the image semantics. We show on several examples that this yields results that are qualitatively and quantitatively better than previous work. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Graphics ; Computer Science - Machine Learning ; Electrical Engineering and Systems Science - Image and Video Processing ; ,"Yan, Zhicheng ; Zhang, Hao ; Wang, Baoyuan ; Paris, Sylvain ; Yu, Yizhou ; ","Automatic Photo Adjustment Using Deep Neural Networks  Photo retouching enables photographers to invoke dramatic visual impressions by artistically enhancing their photos through stylistic color and tone adjustments. However, it is also a time-consuming and challenging task that requires advanced skills beyond the abilities of casual photographers. Using an automated algorithm is an appealing alternative to manual work but such an algorithm faces many hurdles. Many photographic styles rely on subtle adjustments that depend on the image content and even its semantics. Further, these adjustments are often spatially varying. Because of these characteristics, existing automatic algorithms are still limited and cover only a subset of these challenges. Recently, deep machine learning has shown unique abilities to address hard problems that resisted machine algorithms for long. This motivated us to explore the use of deep learning in the context of photo editing. In this paper, we explain how to formulate the automatic photo adjustment problem in a way suitable for this approach. We also introduce an image descriptor that accounts for the local semantics of an image. Our experiments demonstrate that our deep learning formulation applied using these descriptors successfully capture sophisticated photographic styles. In particular and unlike previous techniques, it can model local adjustments that depend on the image semantics. We show on several examples that this yields results that are qualitatively and quantitatively better than previous work. ",automatic photo adjustment use deep neural network photo retouch enable photographers invoke dramatic visual impressions artistically enhance photos stylistic color tone adjustments however also time consume challenge task require advance skills beyond abilities casual photographers use automate algorithm appeal alternative manual work algorithm face many hurdle many photographic style rely subtle adjustments depend image content even semantics adjustments often spatially vary characteristics exist automatic algorithms still limit cover subset challenge recently deep machine learn show unique abilities address hard problems resist machine algorithms long motivate us explore use deep learn context photo edit paper explain formulate automatic photo adjustment problem way suitable approach also introduce image descriptor account local semantics image experiment demonstrate deep learn formulation apply use descriptors successfully capture sophisticate photographic style particular unlike previous techniques model local adjustments depend image semantics show several examples yield result qualitatively quantitatively better previous work,144,11,1412.7725.txt
http://arxiv.org/abs/1412.7839,"Cloud K-SVD: A Collaborative Dictionary Learning Algorithm for Big,   Distributed Data","  This paper studies the problem of data-adaptive representations for big, distributed data. It is assumed that a number of geographically-distributed, interconnected sites have massive local data and they are interested in collaboratively learning a low-dimensional geometric structure underlying these data. In contrast to previous works on subspace-based data representations, this paper focuses on the geometric structure of a union of subspaces (UoS). In this regard, it proposes a distributed algorithm---termed cloud K-SVD---for collaborative learning of a UoS structure underlying distributed data of interest. The goal of cloud K-SVD is to learn a common overcomplete dictionary at each individual site such that every sample in the distributed data can be represented through a small number of atoms of the learned dictionary. Cloud K-SVD accomplishes this goal without requiring exchange of individual samples between sites. This makes it suitable for applications where sharing of raw data is discouraged due to either privacy concerns or large volumes of data. This paper also provides an analysis of cloud K-SVD that gives insights into its properties as well as deviations of the dictionaries learned at individual sites from a centralized solution in terms of different measures of local/global data and topology of interconnections. Finally, the paper numerically illustrates the efficacy of cloud K-SVD on real and synthetic distributed data. ",Computer Science - Machine Learning ; Computer Science - Information Theory ; Statistics - Machine Learning ; ,"Raja, Haroon ; Bajwa, Waheed U. ; ","Cloud K-SVD: A Collaborative Dictionary Learning Algorithm for Big,   Distributed Data  This paper studies the problem of data-adaptive representations for big, distributed data. It is assumed that a number of geographically-distributed, interconnected sites have massive local data and they are interested in collaboratively learning a low-dimensional geometric structure underlying these data. In contrast to previous works on subspace-based data representations, this paper focuses on the geometric structure of a union of subspaces (UoS). In this regard, it proposes a distributed algorithm---termed cloud K-SVD---for collaborative learning of a UoS structure underlying distributed data of interest. The goal of cloud K-SVD is to learn a common overcomplete dictionary at each individual site such that every sample in the distributed data can be represented through a small number of atoms of the learned dictionary. Cloud K-SVD accomplishes this goal without requiring exchange of individual samples between sites. This makes it suitable for applications where sharing of raw data is discouraged due to either privacy concerns or large volumes of data. This paper also provides an analysis of cloud K-SVD that gives insights into its properties as well as deviations of the dictionaries learned at individual sites from a centralized solution in terms of different measures of local/global data and topology of interconnections. Finally, the paper numerically illustrates the efficacy of cloud K-SVD on real and synthetic distributed data. ",cloud svd collaborative dictionary learn algorithm big distribute data paper study problem data adaptive representations big distribute data assume number geographically distribute interconnect sit massive local data interest collaboratively learn low dimensional geometric structure underlie data contrast previous work subspace base data representations paper focus geometric structure union subspaces uos regard propose distribute algorithm term cloud svd collaborative learn uos structure underlie distribute data interest goal cloud svd learn common overcomplete dictionary individual site every sample distribute data represent small number atoms learn dictionary cloud svd accomplish goal without require exchange individual sample sit make suitable applications share raw data discourage due either privacy concern large volumes data paper also provide analysis cloud svd give insights properties well deviations dictionaries learn individual sit centralize solution term different measure local global data topology interconnections finally paper numerically illustrate efficacy cloud svd real synthetic distribute data,144,10,1412.7839.txt
http://arxiv.org/abs/1412.7998,Propositional Logics of Dependence,"  In this paper, we study logics of dependence on the propositional level. We prove that several interesting propositional logics of dependence, including propositional dependence logic, propositional intuitionistic dependence logic as well as propositional inquisitive logic, are expressively complete and have disjunctive or conjunctive normal forms. We provide deduction systems and prove the completeness theorems for these logics. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03B60 ; ,"Yang, Fan ; Väänänen, Jouko ; ","Propositional Logics of Dependence  In this paper, we study logics of dependence on the propositional level. We prove that several interesting propositional logics of dependence, including propositional dependence logic, propositional intuitionistic dependence logic as well as propositional inquisitive logic, are expressively complete and have disjunctive or conjunctive normal forms. We provide deduction systems and prove the completeness theorems for these logics. ",propositional logics dependence paper study logics dependence propositional level prove several interest propositional logics dependence include propositional dependence logic propositional intuitionistic dependence logic well propositional inquisitive logic expressively complete disjunctive conjunctive normal form provide deduction systems prove completeness theorems logics,40,8,1412.7998.txt
http://arxiv.org/abs/1412.8324,A Constructive Proof on the Compositionality of Linearizability,"  Linearizability is the strongest correctness property for both shared memory and message passing systems. One of its useful features is the compositionality: a history (execution) is linearizable if and only if each object (component) subhistory is linearizable. In this paper, we propose a new hierarchical system model to address challenges in modular development of cloud systems. Object are defined by induction from the most fundamental atomic Boolean registers, and histories are represented as countable well-ordered structures of events to deal with both finite and infinite executions. Then, we present a new constructive proof on the compositionality theorem of linearizability inspired by Multiway Merge. This proof deduces a theoretically efficient algorithm which generates linearization in O(N*logP) running time with O(N) space, where P and N are process/event numbers respectively. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Lin, Haoxiang ; ","A Constructive Proof on the Compositionality of Linearizability  Linearizability is the strongest correctness property for both shared memory and message passing systems. One of its useful features is the compositionality: a history (execution) is linearizable if and only if each object (component) subhistory is linearizable. In this paper, we propose a new hierarchical system model to address challenges in modular development of cloud systems. Object are defined by induction from the most fundamental atomic Boolean registers, and histories are represented as countable well-ordered structures of events to deal with both finite and infinite executions. Then, we present a new constructive proof on the compositionality theorem of linearizability inspired by Multiway Merge. This proof deduces a theoretically efficient algorithm which generates linearization in O(N*logP) running time with O(N) space, where P and N are process/event numbers respectively. ",constructive proof compositionality linearizability linearizability strongest correctness property share memory message pass systems one useful feature compositionality history execution linearizable object component subhistory linearizable paper propose new hierarchical system model address challenge modular development cloud systems object define induction fundamental atomic boolean register histories represent countable well order structure events deal finite infinite executions present new constructive proof compositionality theorem linearizability inspire multiway merge proof deduce theoretically efficient algorithm generate linearization logp run time space process event number respectively,79,4,1412.8324.txt
http://arxiv.org/abs/1412.8358,Odd graph and its applications to the strong edge coloring,"  A strong edge coloring of a graph is a proper edge coloring in which every color class is an induced matching. The strong chromatic index $\chi_s'(G)$ of a graph $G$ is the minimum number of colors in a strong edge coloring of $G$. Let $\Delta \geq 4$ be an integer. In this note, we study the odd graphs and show the existence of some special walks. By using these results and Chang's ideas in [Discuss. Math. Graph Theory 34 (4) (2014) 723--733], we show that every planar graph with maximum degree at most $\Delta$ and girth at least $10 \Delta - 4$ has a strong edge coloring with $2\Delta - 1$ colors. In addition, we prove that if $G$ is a graph with girth at least $2\Delta - 1$ and mad$(G) < 2 + \frac{1}{3\Delta - 2}$, where $\Delta$ is the maximum degree and $\Delta \geq 4$, then $\chi_s'(G) \leq 2\Delta - 1$, if $G$ is a subcubic graph with girth at least $8$ and mad$(G) < 2 + \frac{2}{23}$, then $\chi_s'(G) \leq 5$. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C15 ; ,"Wang, Tao ; Zhao, Xiaodan ; ","Odd graph and its applications to the strong edge coloring  A strong edge coloring of a graph is a proper edge coloring in which every color class is an induced matching. The strong chromatic index $\chi_s'(G)$ of a graph $G$ is the minimum number of colors in a strong edge coloring of $G$. Let $\Delta \geq 4$ be an integer. In this note, we study the odd graphs and show the existence of some special walks. By using these results and Chang's ideas in [Discuss. Math. Graph Theory 34 (4) (2014) 723--733], we show that every planar graph with maximum degree at most $\Delta$ and girth at least $10 \Delta - 4$ has a strong edge coloring with $2\Delta - 1$ colors. In addition, we prove that if $G$ is a graph with girth at least $2\Delta - 1$ and mad$(G) < 2 + \frac{1}{3\Delta - 2}$, where $\Delta$ is the maximum degree and $\Delta \geq 4$, then $\chi_s'(G) \leq 2\Delta - 1$, if $G$ is a subcubic graph with girth at least $8$ and mad$(G) < 2 + \frac{2}{23}$, then $\chi_s'(G) \leq 5$. ",odd graph applications strong edge color strong edge color graph proper edge color every color class induce match strong chromatic index chi graph minimum number color strong edge color let delta geq integer note study odd graph show existence special walk use result chang ideas discuss math graph theory show every planar graph maximum degree delta girth least delta strong edge color delta color addition prove graph girth least delta mad frac delta delta maximum degree delta geq chi leq delta subcubic graph girth least mad frac chi leq,89,13,1412.8358.txt
http://arxiv.org/abs/1412.8591,Maze Solving Automatons for Self-Healing of Open Interconnects: Modular   Add-on for Circuit Boards,  We present the circuit board integration of a self-healing mechanism to repair open faults. The electric field driven mechanism physically restores fractured interconnects in electronic circuits and has the ability to solve mazes. The repair is performed by conductive particles dispersed in an insulating fluid. We demonstrate the integration of the healing module onto printed circuit boards and the ability of maze solving. We model and perform experiments on the influence of the geometry of the conductive particles as well as the terminal impedances of the route on the healing efficiency. The typical heal rate is 10 $\mu$m/s with healed route having resistance of 100 $\Omega$ to 20 k$\Omega$ depending on the materials and concentrations used. ,Computer Science - Emerging Technologies ; Condensed Matter - Soft Condensed Matter ; ,"Nair, Aswathi ; Raghunandan, Karthik ; Yaswanth, Vaddi ; Shridharan, Sreelal ; Sambandan, Sanjiv ; ",Maze Solving Automatons for Self-Healing of Open Interconnects: Modular   Add-on for Circuit Boards  We present the circuit board integration of a self-healing mechanism to repair open faults. The electric field driven mechanism physically restores fractured interconnects in electronic circuits and has the ability to solve mazes. The repair is performed by conductive particles dispersed in an insulating fluid. We demonstrate the integration of the healing module onto printed circuit boards and the ability of maze solving. We model and perform experiments on the influence of the geometry of the conductive particles as well as the terminal impedances of the route on the healing efficiency. The typical heal rate is 10 $\mu$m/s with healed route having resistance of 100 $\Omega$ to 20 k$\Omega$ depending on the materials and concentrations used. ,maze solve automatons self heal open interconnect modular add circuit board present circuit board integration self heal mechanism repair open fault electric field drive mechanism physically restore fracture interconnect electronic circuit ability solve mazes repair perform conductive particles disperse insulate fluid demonstrate integration heal module onto print circuit board ability maze solve model perform experiment influence geometry conductive particles well terminal impedances route heal efficiency typical heal rate mu heal route resistance omega omega depend materials concentrations use,78,8,1412.8591.txt
http://arxiv.org/abs/1501.00433,On the Uniform Computational Content of Computability Theory,"  We demonstrate that the Weihrauch lattice can be used to classify the uniform computational content of computability-theoretic properties as well as the computational content of theorems in one common setting. The properties that we study include diagonal non-computability, hyperimmunity, complete consistent extensions of Peano arithmetic, 1-genericity, Martin-L\""of randomness, and cohesiveness. The theorems that we include in our case study are the low basis theorem of Jockusch and Soare, the Kleene-Post theorem, and Friedberg's jump inversion theorem. It turns out that all the aforementioned properties and many theorems in computability theory, including all theorems that claim the existence of some Turing degree, have very little uniform computational content: they are located outside of the upper cone of binary choice (also known as LLPO); we call problems with this property indiscriminative. Since practically all theorems from classical analysis whose computational content has been classified are discriminative, our observation could yield an explanation for why theorems and results in computability theory typically have very few direct consequences in other disciplines such as analysis. A notable exception in our case study is the low basis theorem which is discriminative. This is perhaps why it is considered to be one of the most applicable theorems in computability theory. In some cases a bridge between the indiscriminative world and the discriminative world of classical mathematics can be established via a suitable residual operation and we demonstrate this in the case of the cohesiveness problem and the problem of consistent complete extensions of Peano arithmetic. Both turn out to be the quotient of two discriminative problems. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; ,"Brattka, Vasco ; Hendtlass, Matthew ; Kreuzer, Alexander P. ; ","On the Uniform Computational Content of Computability Theory  We demonstrate that the Weihrauch lattice can be used to classify the uniform computational content of computability-theoretic properties as well as the computational content of theorems in one common setting. The properties that we study include diagonal non-computability, hyperimmunity, complete consistent extensions of Peano arithmetic, 1-genericity, Martin-L\""of randomness, and cohesiveness. The theorems that we include in our case study are the low basis theorem of Jockusch and Soare, the Kleene-Post theorem, and Friedberg's jump inversion theorem. It turns out that all the aforementioned properties and many theorems in computability theory, including all theorems that claim the existence of some Turing degree, have very little uniform computational content: they are located outside of the upper cone of binary choice (also known as LLPO); we call problems with this property indiscriminative. Since practically all theorems from classical analysis whose computational content has been classified are discriminative, our observation could yield an explanation for why theorems and results in computability theory typically have very few direct consequences in other disciplines such as analysis. A notable exception in our case study is the low basis theorem which is discriminative. This is perhaps why it is considered to be one of the most applicable theorems in computability theory. In some cases a bridge between the indiscriminative world and the discriminative world of classical mathematics can be established via a suitable residual operation and we demonstrate this in the case of the cohesiveness problem and the problem of consistent complete extensions of Peano arithmetic. Both turn out to be the quotient of two discriminative problems. ",uniform computational content computability theory demonstrate weihrauch lattice use classify uniform computational content computability theoretic properties well computational content theorems one common set properties study include diagonal non computability hyperimmunity complete consistent extensions peano arithmetic genericity martin randomness cohesiveness theorems include case study low basis theorem jockusch soare kleene post theorem friedberg jump inversion theorem turn aforementioned properties many theorems computability theory include theorems claim existence turing degree little uniform computational content locate outside upper cone binary choice also know llpo call problems property indiscriminative since practically theorems classical analysis whose computational content classify discriminative observation could yield explanation theorems result computability theory typically direct consequences discipline analysis notable exception case study low basis theorem discriminative perhaps consider one applicable theorems computability theory case bridge indiscriminative world discriminative world classical mathematics establish via suitable residual operation demonstrate case cohesiveness problem problem consistent complete extensions peano arithmetic turn quotient two discriminative problems,151,8,1501.00433.txt
http://arxiv.org/abs/1501.00960,Characterizing the Google Books corpus: Strong limits to inferences of   socio-cultural and linguistic evolution,"  It is tempting to treat frequency trends from the Google Books data sets as indicators of the ""true"" popularity of various words and phrases. Doing so allows us to draw quantitatively strong conclusions about the evolution of cultural perception of a given topic, such as time or gender. However, the Google Books corpus suffers from a number of limitations which make it an obscure mask of cultural popularity. A primary issue is that the corpus is in effect a library, containing one of each book. A single, prolific author is thereby able to noticeably insert new phrases into the Google Books lexicon, whether the author is widely read or not. With this understood, the Google Books corpus remains an important data set to be considered more lexicon-like than text-like. Here, we show that a distinct problematic feature arises from the inclusion of scientific texts, which have become an increasingly substantive portion of the corpus throughout the 1900s. The result is a surge of phrases typical to academic articles but less common in general, such as references to time in the form of citations. We highlight these dynamics by examining and comparing major contributions to the statistical divergence of English data sets between decades in the period 1800--2000. We find that only the English Fiction data set from the second version of the corpus is not heavily affected by professional texts, in clear contrast to the first version of the fiction data set and both unfiltered English data sets. Our findings emphasize the need to fully characterize the dynamics of the Google Books corpus before using these data sets to draw broad conclusions about cultural and linguistic evolution. ",Physics - Physics and Society ; Condensed Matter - Statistical Mechanics ; Computer Science - Computation and Language ; Statistics - Applications ; ,"Pechenick, Eitan Adam ; Danforth, Christopher M. ; Dodds, Peter Sheridan ; ","Characterizing the Google Books corpus: Strong limits to inferences of   socio-cultural and linguistic evolution  It is tempting to treat frequency trends from the Google Books data sets as indicators of the ""true"" popularity of various words and phrases. Doing so allows us to draw quantitatively strong conclusions about the evolution of cultural perception of a given topic, such as time or gender. However, the Google Books corpus suffers from a number of limitations which make it an obscure mask of cultural popularity. A primary issue is that the corpus is in effect a library, containing one of each book. A single, prolific author is thereby able to noticeably insert new phrases into the Google Books lexicon, whether the author is widely read or not. With this understood, the Google Books corpus remains an important data set to be considered more lexicon-like than text-like. Here, we show that a distinct problematic feature arises from the inclusion of scientific texts, which have become an increasingly substantive portion of the corpus throughout the 1900s. The result is a surge of phrases typical to academic articles but less common in general, such as references to time in the form of citations. We highlight these dynamics by examining and comparing major contributions to the statistical divergence of English data sets between decades in the period 1800--2000. We find that only the English Fiction data set from the second version of the corpus is not heavily affected by professional texts, in clear contrast to the first version of the fiction data set and both unfiltered English data sets. Our findings emphasize the need to fully characterize the dynamics of the Google Books corpus before using these data sets to draw broad conclusions about cultural and linguistic evolution. ",characterize google book corpus strong limit inferences socio cultural linguistic evolution tempt treat frequency trend google book data set indicators true popularity various word phrase allow us draw quantitatively strong conclusions evolution cultural perception give topic time gender however google book corpus suffer number limitations make obscure mask cultural popularity primary issue corpus effect library contain one book single prolific author thereby able noticeably insert new phrase google book lexicon whether author widely read understand google book corpus remain important data set consider lexicon like text like show distinct problematic feature arise inclusion scientific texts become increasingly substantive portion corpus throughout result surge phrase typical academic article less common general reference time form citations highlight dynamics examine compare major contributions statistical divergence english data set decades period find english fiction data set second version corpus heavily affect professional texts clear contrast first version fiction data set unfiltered english data set find emphasize need fully characterize dynamics google book corpus use data set draw broad conclusions cultural linguistic evolution,168,10,1501.00960.txt
http://arxiv.org/abs/1501.01042,Augur: a decentralized oracle and prediction market platform,"  Augur is a trustless, decentralized oracle and platform for prediction markets. The outcomes of Augur's prediction markets are chosen by users that hold Augur's native Reputation token, who stake their tokens on the actual observed outcome and, in return, receive settlement fees from the markets. Augur's incentive structure is designed to ensure that honest, accurate reporting of outcomes is always the most profitable option for Reputation token holders. Token holders can post progressively-larger Reputation bonds to dispute proposed market outcomes. If the size of these bonds reaches a certain threshold, Reputation splits into multiple versions, one for each possible outcome of the disputed market; token holders must then exchange their Reputation tokens for one of these versions. Versions of Reputation which do not correspond to the real-world outcome will become worthless, as no one will participate in prediction markets unless they are confident that the markets will resolve correctly. Therefore, token holders will select the only version of Reputation which they know will continue to have value: the version that corresponds to reality. ",Computer Science - Cryptography and Security ; ,"Peterson, Jack ; Krug, Joseph ; Zoltu, Micah ; Williams, Austin K. ; Alexander, Stephanie ; ","Augur: a decentralized oracle and prediction market platform  Augur is a trustless, decentralized oracle and platform for prediction markets. The outcomes of Augur's prediction markets are chosen by users that hold Augur's native Reputation token, who stake their tokens on the actual observed outcome and, in return, receive settlement fees from the markets. Augur's incentive structure is designed to ensure that honest, accurate reporting of outcomes is always the most profitable option for Reputation token holders. Token holders can post progressively-larger Reputation bonds to dispute proposed market outcomes. If the size of these bonds reaches a certain threshold, Reputation splits into multiple versions, one for each possible outcome of the disputed market; token holders must then exchange their Reputation tokens for one of these versions. Versions of Reputation which do not correspond to the real-world outcome will become worthless, as no one will participate in prediction markets unless they are confident that the markets will resolve correctly. Therefore, token holders will select the only version of Reputation which they know will continue to have value: the version that corresponds to reality. ",augur decentralize oracle prediction market platform augur trustless decentralize oracle platform prediction market outcomes augur prediction market choose users hold augur native reputation token stake tokens actual observe outcome return receive settlement fee market augur incentive structure design ensure honest accurate report outcomes always profitable option reputation token holders token holders post progressively larger reputation bond dispute propose market outcomes size bond reach certain threshold reputation split multiple versions one possible outcome dispute market token holders must exchange reputation tokens one versions versions reputation correspond real world outcome become worthless one participate prediction market unless confident market resolve correctly therefore token holders select version reputation know continue value version correspond reality,111,12,1501.01042.txt
http://arxiv.org/abs/1501.02741,Salient Object Detection: A Benchmark,"  We extensively compare, qualitatively and quantitatively, 40 state-of-the-art models (28 salient object detection, 10 fixation prediction, 1 objectness, and 1 baseline) over 6 challenging datasets for the purpose of benchmarking salient object detection and segmentation methods. From the results obtained so far, our evaluation shows a consistent rapid progress over the last few years in terms of both accuracy and running time. The top contenders in this benchmark significantly outperform the models identified as the best in the previous benchmark conducted just two years ago. We find that the models designed specifically for salient object detection generally work better than models in closely related areas, which in turn provides a precise definition and suggests an appropriate treatment of this problem that distinguishes it from other problems. In particular, we analyze the influences of center bias and scene complexity in model performance, which, along with the hard cases for state-of-the-art models, provide useful hints towards constructing more challenging large scale datasets and better saliency models. Finally, we propose probable solutions for tackling several open problems such as evaluation scores and dataset bias, which also suggest future research directions in the rapidly-growing field of salient object detection. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Borji, Ali ; Cheng, Ming-Ming ; Jiang, Huaizu ; Li, Jia ; ","Salient Object Detection: A Benchmark  We extensively compare, qualitatively and quantitatively, 40 state-of-the-art models (28 salient object detection, 10 fixation prediction, 1 objectness, and 1 baseline) over 6 challenging datasets for the purpose of benchmarking salient object detection and segmentation methods. From the results obtained so far, our evaluation shows a consistent rapid progress over the last few years in terms of both accuracy and running time. The top contenders in this benchmark significantly outperform the models identified as the best in the previous benchmark conducted just two years ago. We find that the models designed specifically for salient object detection generally work better than models in closely related areas, which in turn provides a precise definition and suggests an appropriate treatment of this problem that distinguishes it from other problems. In particular, we analyze the influences of center bias and scene complexity in model performance, which, along with the hard cases for state-of-the-art models, provide useful hints towards constructing more challenging large scale datasets and better saliency models. Finally, we propose probable solutions for tackling several open problems such as evaluation scores and dataset bias, which also suggest future research directions in the rapidly-growing field of salient object detection. ",salient object detection benchmark extensively compare qualitatively quantitatively state art model salient object detection fixation prediction objectness baseline challenge datasets purpose benchmarking salient object detection segmentation methods result obtain far evaluation show consistent rapid progress last years term accuracy run time top contenders benchmark significantly outperform model identify best previous benchmark conduct two years ago find model design specifically salient object detection generally work better model closely relate areas turn provide precise definition suggest appropriate treatment problem distinguish problems particular analyze influence center bias scene complexity model performance along hard case state art model provide useful hint towards construct challenge large scale datasets better saliency model finally propose probable solutions tackle several open problems evaluation score dataset bias also suggest future research directions rapidly grow field salient object detection,129,2,1501.02741.txt
http://arxiv.org/abs/1501.03043,Functionals and hardware,"  Functionals are an important research subject in Mathematics and Computer Science as well as a challenge in Information Technologies where the current programming paradigm states that only symbolic computations are possible on higher order objects, i.e. functionals are terms, and computation is term rewriting. The idea explored in the paper is that functionals correspond to generic mechanisms for management of connections in arrays consisting of first order functional units. Functionals are higher order abstractions that are useful for the management of such large arrays. Computations on higher order objects comprise dynamic configuration of connections between first order elementary functions in the arrays. Once the functionals are considered as the generic mechanisms, they have a grounding in hardware. A conceptual framework for constructing such mechanisms is presented, and their hardware realization is discussed. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03D ; F.4.1 ; ,"Ambroszkiewicz, Stanislaw ; ","Functionals and hardware  Functionals are an important research subject in Mathematics and Computer Science as well as a challenge in Information Technologies where the current programming paradigm states that only symbolic computations are possible on higher order objects, i.e. functionals are terms, and computation is term rewriting. The idea explored in the paper is that functionals correspond to generic mechanisms for management of connections in arrays consisting of first order functional units. Functionals are higher order abstractions that are useful for the management of such large arrays. Computations on higher order objects comprise dynamic configuration of connections between first order elementary functions in the arrays. Once the functionals are considered as the generic mechanisms, they have a grounding in hardware. A conceptual framework for constructing such mechanisms is presented, and their hardware realization is discussed. ",functionals hardware functionals important research subject mathematics computer science well challenge information technologies current program paradigm state symbolic computations possible higher order object functionals term computation term rewrite idea explore paper functionals correspond generic mechanisms management connections array consist first order functional units functionals higher order abstractions useful management large array computations higher order object comprise dynamic configuration connections first order elementary function array functionals consider generic mechanisms ground hardware conceptual framework construct mechanisms present hardware realization discuss,78,10,1501.03043.txt
http://arxiv.org/abs/1501.03347,Dirichlet Process Parsimonious Mixtures for clustering,"  The parsimonious Gaussian mixture models, which exploit an eigenvalue decomposition of the group covariance matrices of the Gaussian mixture, have shown their success in particular in cluster analysis. Their estimation is in general performed by maximum likelihood estimation and has also been considered from a parametric Bayesian prospective. We propose new Dirichlet Process Parsimonious mixtures (DPPM) which represent a Bayesian nonparametric formulation of these parsimonious Gaussian mixture models. The proposed DPPM models are Bayesian nonparametric parsimonious mixture models that allow to simultaneously infer the model parameters, the optimal number of mixture components and the optimal parsimonious mixture structure from the data. We develop a Gibbs sampling technique for maximum a posteriori (MAP) estimation of the developed DPMM models and provide a Bayesian model selection framework by using Bayes factors. We apply them to cluster simulated data and real data sets, and compare them to the standard parsimonious mixture models. The obtained results highlight the effectiveness of the proposed nonparametric parsimonious mixture models as a good nonparametric alternative for the parametric parsimonious models. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; Statistics - Methodology ; ,"Chamroukhi, Faicel ; Bartcus, Marius ; Glotin, Hervé ; ","Dirichlet Process Parsimonious Mixtures for clustering  The parsimonious Gaussian mixture models, which exploit an eigenvalue decomposition of the group covariance matrices of the Gaussian mixture, have shown their success in particular in cluster analysis. Their estimation is in general performed by maximum likelihood estimation and has also been considered from a parametric Bayesian prospective. We propose new Dirichlet Process Parsimonious mixtures (DPPM) which represent a Bayesian nonparametric formulation of these parsimonious Gaussian mixture models. The proposed DPPM models are Bayesian nonparametric parsimonious mixture models that allow to simultaneously infer the model parameters, the optimal number of mixture components and the optimal parsimonious mixture structure from the data. We develop a Gibbs sampling technique for maximum a posteriori (MAP) estimation of the developed DPMM models and provide a Bayesian model selection framework by using Bayes factors. We apply them to cluster simulated data and real data sets, and compare them to the standard parsimonious mixture models. The obtained results highlight the effectiveness of the proposed nonparametric parsimonious mixture models as a good nonparametric alternative for the parametric parsimonious models. ",dirichlet process parsimonious mixtures cluster parsimonious gaussian mixture model exploit eigenvalue decomposition group covariance matrices gaussian mixture show success particular cluster analysis estimation general perform maximum likelihood estimation also consider parametric bayesian prospective propose new dirichlet process parsimonious mixtures dppm represent bayesian nonparametric formulation parsimonious gaussian mixture model propose dppm model bayesian nonparametric parsimonious mixture model allow simultaneously infer model parameters optimal number mixture components optimal parsimonious mixture structure data develop gibbs sample technique maximum posteriori map estimation develop dpmm model provide bayesian model selection framework use bay factor apply cluster simulate data real data set compare standard parsimonious mixture model obtain result highlight effectiveness propose nonparametric parsimonious mixture model good nonparametric alternative parametric parsimonious model,116,0,1501.03347.txt
http://arxiv.org/abs/1501.03872,The Dead Cryptographers Society Problem,"  This paper defines The Dead Cryptographers Society Problem - DCS (where several great cryptographers created many polynomial-time Deterministic Turing Machines (DTMs) of a specific type, ran them on their proper descriptions concatenated with some arbitrary strings, deleted them and left only the results from those running, after they died: if those DTMs only permute and sometimes invert the bits on input, is it possible to decide the language formed by such resulting strings within polynomial time?), proves some facts about its computational complexity, and discusses some possible uses on Cryptography, such as into distance keys distribution, online reverse auction and secure communication. ","Computer Science - Computational Complexity ; Computer Science - Cryptography and Security ; 94A60 (Primary), 94A62 (Secondary) ; ","Barbosa, André Luiz ; ","The Dead Cryptographers Society Problem  This paper defines The Dead Cryptographers Society Problem - DCS (where several great cryptographers created many polynomial-time Deterministic Turing Machines (DTMs) of a specific type, ran them on their proper descriptions concatenated with some arbitrary strings, deleted them and left only the results from those running, after they died: if those DTMs only permute and sometimes invert the bits on input, is it possible to decide the language formed by such resulting strings within polynomial time?), proves some facts about its computational complexity, and discusses some possible uses on Cryptography, such as into distance keys distribution, online reverse auction and secure communication. ",dead cryptographers society problem paper define dead cryptographers society problem dcs several great cryptographers create many polynomial time deterministic turing machine dtms specific type run proper descriptions concatenate arbitrary string delete leave result run die dtms permute sometimes invert bits input possible decide language form result string within polynomial time prove facts computational complexity discuss possible use cryptography distance key distribution online reverse auction secure communication,66,8,1501.03872.txt
http://arxiv.org/abs/1501.04147,Categorified Reeb Graphs,"  The Reeb graph is a construction which originated in Morse theory to study a real valued function defined on a topological space. More recently, it has been used in various applications to study noisy data which creates a desire to define a measure of similarity between these structures. Here, we exploit the fact that the category of Reeb graphs is equivalent to the category of a particular class of cosheaf. Using this equivalency, we can define an `interleaving' distance between Reeb graphs which is stable under the perturbation of a function. Along the way, we obtain a natural construction for smoothing a Reeb graph to reduce its topological complexity. The smoothed Reeb graph can be constructed in polynomial time. ",Computer Science - Computational Geometry ; ,"de Silva, Vin ; Munch, Elizabeth ; Patel, Amit ; ","Categorified Reeb Graphs  The Reeb graph is a construction which originated in Morse theory to study a real valued function defined on a topological space. More recently, it has been used in various applications to study noisy data which creates a desire to define a measure of similarity between these structures. Here, we exploit the fact that the category of Reeb graphs is equivalent to the category of a particular class of cosheaf. Using this equivalency, we can define an `interleaving' distance between Reeb graphs which is stable under the perturbation of a function. Along the way, we obtain a natural construction for smoothing a Reeb graph to reduce its topological complexity. The smoothed Reeb graph can be constructed in polynomial time. ",categorified reeb graph reeb graph construction originate morse theory study real value function define topological space recently use various applications study noisy data create desire define measure similarity structure exploit fact category reeb graph equivalent category particular class cosheaf use equivalency define interleave distance reeb graph stable perturbation function along way obtain natural construction smooth reeb graph reduce topological complexity smooth reeb graph construct polynomial time,66,3,1501.04147.txt
http://arxiv.org/abs/1501.04318,Clustering based on the In-tree Graph Structure and Affinity Propagation,"  A recently proposed clustering method, called the Nearest Descent (ND), can organize the whole dataset into a sparsely connected graph, called the In-tree. This ND-based Intree structure proves able to reveal the clustering structure underlying the dataset, except one imperfect place, that is, there are some undesired edges in this In-tree which require to be removed. Here, we propose an effective way to automatically remove the undesired edges in In-tree via an effective combination of the In-tree structure with affinity propagation (AP). The key for the combination is to add edges between the reachable nodes in In-tree before using AP to remove the undesired edges. The experiments on both synthetic and real datasets demonstrate the effectiveness of the proposed method. ",Computer Science - Machine Learning ; Computer Science - Computer Vision and Pattern Recognition ; Statistics - Machine Learning ; ,"Qiu, Teng ; Li, Yongjie ; ","Clustering based on the In-tree Graph Structure and Affinity Propagation  A recently proposed clustering method, called the Nearest Descent (ND), can organize the whole dataset into a sparsely connected graph, called the In-tree. This ND-based Intree structure proves able to reveal the clustering structure underlying the dataset, except one imperfect place, that is, there are some undesired edges in this In-tree which require to be removed. Here, we propose an effective way to automatically remove the undesired edges in In-tree via an effective combination of the In-tree structure with affinity propagation (AP). The key for the combination is to add edges between the reachable nodes in In-tree before using AP to remove the undesired edges. The experiments on both synthetic and real datasets demonstrate the effectiveness of the proposed method. ",cluster base tree graph structure affinity propagation recently propose cluster method call nearest descent nd organize whole dataset sparsely connect graph call tree nd base intree structure prove able reveal cluster structure underlie dataset except one imperfect place undesired edge tree require remove propose effective way automatically remove undesired edge tree via effective combination tree structure affinity propagation ap key combination add edge reachable nod tree use ap remove undesired edge experiment synthetic real datasets demonstrate effectiveness propose method,79,3,1501.04318.txt
http://arxiv.org/abs/1501.04343,Algorithms for Scheduling Malleable Cloud Tasks,"  Due to the ubiquity of batch data processing in cloud computing, the related problem of scheduling malleable batch tasks and its extensions have received significant attention recently. In this paper, we consider a fundamental model where a set of n tasks is to be processed on C identical machines and each task is specified by a value, a workload, a deadline and a parallelism bound. Within the parallelism bound, the number of machines assigned to a task can vary over time without affecting its workload. For this model, we obtain two core results: a sufficient and necessary condition such that a set of tasks can be finished by their deadlines on C machines, and an algorithm to produce such a schedule. These core results provide a conceptual tool and an optimal scheduling algorithm that enable proposing new algorithmic analysis and design and improving existing algorithms under various objectives. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Data Structures and Algorithms ; ","Wu, Xiaohu ; Loiseau, Patrick ; ","Algorithms for Scheduling Malleable Cloud Tasks  Due to the ubiquity of batch data processing in cloud computing, the related problem of scheduling malleable batch tasks and its extensions have received significant attention recently. In this paper, we consider a fundamental model where a set of n tasks is to be processed on C identical machines and each task is specified by a value, a workload, a deadline and a parallelism bound. Within the parallelism bound, the number of machines assigned to a task can vary over time without affecting its workload. For this model, we obtain two core results: a sufficient and necessary condition such that a set of tasks can be finished by their deadlines on C machines, and an algorithm to produce such a schedule. These core results provide a conceptual tool and an optimal scheduling algorithm that enable proposing new algorithmic analysis and design and improving existing algorithms under various objectives. ",algorithms schedule malleable cloud task due ubiquity batch data process cloud compute relate problem schedule malleable batch task extensions receive significant attention recently paper consider fundamental model set task process identical machine task specify value workload deadline parallelism bind within parallelism bind number machine assign task vary time without affect workload model obtain two core result sufficient necessary condition set task finish deadlines machine algorithm produce schedule core result provide conceptual tool optimal schedule algorithm enable propose new algorithmic analysis design improve exist algorithms various objectives,86,11,1501.04343.txt
http://arxiv.org/abs/1501.04706,A Novel Implementation of QuickHull Algorithm on the GPU,"  We present a novel GPU-accelerated implementation of the QuickHull algorihtm for calculating convex hulls of planar point sets. We also describe a practical solution to demonstrate how to efficiently implement a typical Divide-and-Conquer algorithm on the GPU. We highly utilize the parallel primitives provided by the library Thrust such as the parallel segmented scan for better efficiency and simplicity. To evaluate the performance of our implementation, we carry out four groups of experimental tests using two groups of point sets in two modes on the GPU K20c. Experimental results indicate that: our implementation can achieve the speedups of up to 10.98x over the state-of-art CPU-based convex hull implementation Qhull [16]. In addition, our implementation can find the convex hull of 20M points in about 0.2 seconds. ",Computer Science - Computational Geometry ; Computer Science - Graphics ; ,"Zhang, Jiayin ; Mei, Gang ; Xu, Nengxiong ; Zhao, Kunyang ; ","A Novel Implementation of QuickHull Algorithm on the GPU  We present a novel GPU-accelerated implementation of the QuickHull algorihtm for calculating convex hulls of planar point sets. We also describe a practical solution to demonstrate how to efficiently implement a typical Divide-and-Conquer algorithm on the GPU. We highly utilize the parallel primitives provided by the library Thrust such as the parallel segmented scan for better efficiency and simplicity. To evaluate the performance of our implementation, we carry out four groups of experimental tests using two groups of point sets in two modes on the GPU K20c. Experimental results indicate that: our implementation can achieve the speedups of up to 10.98x over the state-of-art CPU-based convex hull implementation Qhull [16]. In addition, our implementation can find the convex hull of 20M points in about 0.2 seconds. ",novel implementation quickhull algorithm gpu present novel gpu accelerate implementation quickhull algorihtm calculate convex hull planar point set also describe practical solution demonstrate efficiently implement typical divide conquer algorithm gpu highly utilize parallel primitives provide library thrust parallel segment scan better efficiency simplicity evaluate performance implementation carry four group experimental test use two group point set two modes gpu experimental result indicate implementation achieve speedups state art cpu base convex hull implementation qhull addition implementation find convex hull point second,80,4,1501.04706.txt
http://arxiv.org/abs/1501.04836,Subtropical Real Root Finding,"  We describe a new incomplete but terminating method for real root finding for large multivariate polynomials. We take an abstract view of the polynomial as the set of exponent vectors associated with sign information on the coefficients. Then we employ linear programming to heuristically find roots. There is a specialized variant for roots with exclusively positive coordinates, which is of considerable interest for applications in chemistry and systems biology. An implementation of our method combining the computer algebra system Reduce with the linear programming solver Gurobi has been successfully applied to input data originating from established mathematical models used in these areas. We have solved several hundred problems with up to more than 800000 monomials in up to 10 variables with degrees up to 12. Our method has failed due to its incompleteness in less than 8 percent of the cases. ",Computer Science - Symbolic Computation ; ,"Sturm, Thomas ; ","Subtropical Real Root Finding  We describe a new incomplete but terminating method for real root finding for large multivariate polynomials. We take an abstract view of the polynomial as the set of exponent vectors associated with sign information on the coefficients. Then we employ linear programming to heuristically find roots. There is a specialized variant for roots with exclusively positive coordinates, which is of considerable interest for applications in chemistry and systems biology. An implementation of our method combining the computer algebra system Reduce with the linear programming solver Gurobi has been successfully applied to input data originating from established mathematical models used in these areas. We have solved several hundred problems with up to more than 800000 monomials in up to 10 variables with degrees up to 12. Our method has failed due to its incompleteness in less than 8 percent of the cases. ",subtropical real root find describe new incomplete terminate method real root find large multivariate polynomials take abstract view polynomial set exponent vectors associate sign information coefficients employ linear program heuristically find root specialize variant root exclusively positive coordinate considerable interest applications chemistry systems biology implementation method combine computer algebra system reduce linear program solver gurobi successfully apply input data originate establish mathematical model use areas solve several hundred problems monomials variables degrees method fail due incompleteness less percent case,79,8,1501.04836.txt
http://arxiv.org/abs/1501.05098,Better Answers to Real Questions,"  We consider existential problems over the reals. Extended quantifier elimination generalizes the concept of regular quantifier elimination by providing in addition answers, which are descriptions of possible assignments for the quantified variables. Implementations of extended quantifier elimination via virtual substitution have been successfully applied to various problems in science and engineering. So far, the answers produced by these implementations included infinitesimal and infinite numbers, which are hard to interpret in practice. We introduce here a post-processing procedure to convert, for fixed parameters, all answers into standard real numbers. The relevance of our procedure is demonstrated by application of our implementation to various examples from the literature, where it significantly improves the quality of the results. ",Computer Science - Symbolic Computation ; Computer Science - Logic in Computer Science ; ,"Kosta, Marek ; Sturm, Thomas ; Dolzmann, Andreas ; ","Better Answers to Real Questions  We consider existential problems over the reals. Extended quantifier elimination generalizes the concept of regular quantifier elimination by providing in addition answers, which are descriptions of possible assignments for the quantified variables. Implementations of extended quantifier elimination via virtual substitution have been successfully applied to various problems in science and engineering. So far, the answers produced by these implementations included infinitesimal and infinite numbers, which are hard to interpret in practice. We introduce here a post-processing procedure to convert, for fixed parameters, all answers into standard real numbers. The relevance of our procedure is demonstrated by application of our implementation to various examples from the literature, where it significantly improves the quality of the results. ",better answer real question consider existential problems reals extend quantifier elimination generalize concept regular quantifier elimination provide addition answer descriptions possible assignments quantify variables implementations extend quantifier elimination via virtual substitution successfully apply various problems science engineer far answer produce implementations include infinitesimal infinite number hard interpret practice introduce post process procedure convert fix parameters answer standard real number relevance procedure demonstrate application implementation various examples literature significantly improve quality result,71,8,1501.05098.txt
http://arxiv.org/abs/1501.05151,Recursive Bayesian Filtering in Circular State Spaces,"  For recursive circular filtering based on circular statistics, we introduce a general framework for estimation of a circular state based on different circular distributions, specifically the wrapped normal distribution and the von Mises distribution. We propose an estimation method for circular systems with nonlinear system and measurement functions. This is achieved by relying on efficient deterministic sampling techniques. Furthermore, we show how the calculations can be simplified in a variety of important special cases, such as systems with additive noise as well as identity system or measurement functions. We introduce several novel key components, particularly a distribution-free prediction algorithm, a new and superior formula for the multiplication of wrapped normal densities, and the ability to deal with non-additive system noise. All proposed methods are thoroughly evaluated and compared to several state-of-the-art solutions. ",Computer Science - Systems and Control ; Computer Science - Robotics ; ,"Kurz, Gerhard ; Gilitschenski, Igor ; Hanebeck, Uwe D. ; ","Recursive Bayesian Filtering in Circular State Spaces  For recursive circular filtering based on circular statistics, we introduce a general framework for estimation of a circular state based on different circular distributions, specifically the wrapped normal distribution and the von Mises distribution. We propose an estimation method for circular systems with nonlinear system and measurement functions. This is achieved by relying on efficient deterministic sampling techniques. Furthermore, we show how the calculations can be simplified in a variety of important special cases, such as systems with additive noise as well as identity system or measurement functions. We introduce several novel key components, particularly a distribution-free prediction algorithm, a new and superior formula for the multiplication of wrapped normal densities, and the ability to deal with non-additive system noise. All proposed methods are thoroughly evaluated and compared to several state-of-the-art solutions. ",recursive bayesian filter circular state space recursive circular filter base circular statistics introduce general framework estimation circular state base different circular distributions specifically wrap normal distribution von mises distribution propose estimation method circular systems nonlinear system measurement function achieve rely efficient deterministic sample techniques furthermore show calculations simplify variety important special case systems additive noise well identity system measurement function introduce several novel key components particularly distribution free prediction algorithm new superior formula multiplication wrap normal densities ability deal non additive system noise propose methods thoroughly evaluate compare several state art solutions,92,11,1501.05151.txt
http://arxiv.org/abs/1501.05260,An Algebra of Reversible Quantum Computing,"  We extend the algebra of reversible computation to support quantum computing. Since the algebra is based on true concurrency, it is reversible for quantum computing and it has a sound and complete theory. ",Computer Science - Logic in Computer Science ; ,"Wang, Yong ; ","An Algebra of Reversible Quantum Computing  We extend the algebra of reversible computation to support quantum computing. Since the algebra is based on true concurrency, it is reversible for quantum computing and it has a sound and complete theory. ",algebra reversible quantum compute extend algebra reversible computation support quantum compute since algebra base true concurrency reversible quantum compute sound complete theory,22,4,1501.05260.txt
http://arxiv.org/abs/1501.05354,A speed and departure time optimization algorithm for the   Pollution-Routing Problem,"  We propose a new speed and departure time optimization algorithm for the Pollution-Routing Problem (PRP), which runs in quadratic time and returns a certified optimal schedule. This algorithm is embedded into an iterated local search-based metaheuristic to achieve a combined speed, scheduling and routing optimization. The start of the working day is set as a decision variable for individual routes, thus enabling a better assignment of human resources to required demands. Some routes that were evaluated as unprofitable can now appear as viable candidates later in the day, leading to a larger search space and further opportunities of distance optimization via better service consolidation. Extensive computational experiments on available PRP benchmark instances demonstrate the good performance of the algorithms. The flexible departure times from the depot contribute to reduce the operational costs by 8.36% on the considered instances. ",Computer Science - Data Structures and Algorithms ; ,"Kramer, Raphael ; Maculan, Nelson ; Subramanian, Anand ; Vidal, Thibaut ; ","A speed and departure time optimization algorithm for the   Pollution-Routing Problem  We propose a new speed and departure time optimization algorithm for the Pollution-Routing Problem (PRP), which runs in quadratic time and returns a certified optimal schedule. This algorithm is embedded into an iterated local search-based metaheuristic to achieve a combined speed, scheduling and routing optimization. The start of the working day is set as a decision variable for individual routes, thus enabling a better assignment of human resources to required demands. Some routes that were evaluated as unprofitable can now appear as viable candidates later in the day, leading to a larger search space and further opportunities of distance optimization via better service consolidation. Extensive computational experiments on available PRP benchmark instances demonstrate the good performance of the algorithms. The flexible departure times from the depot contribute to reduce the operational costs by 8.36% on the considered instances. ",speed departure time optimization algorithm pollution rout problem propose new speed departure time optimization algorithm pollution rout problem prp run quadratic time return certify optimal schedule algorithm embed iterate local search base metaheuristic achieve combine speed schedule rout optimization start work day set decision variable individual rout thus enable better assignment human resources require demand rout evaluate unprofitable appear viable candidates later day lead larger search space opportunities distance optimization via better service consolidation extensive computational experiment available prp benchmark instance demonstrate good performance algorithms flexible departure time depot contribute reduce operational cost consider instance,95,11,1501.05354.txt
http://arxiv.org/abs/1501.06076,Fourier Analysis of MAC Polarization,  One problem with MAC polar codes that are based on MAC polarization is that they may not achieve the entire capacity region. The reason behind this problem is that MAC polarization sometimes induces a loss in the capacity region. This paper provides a single letter necessary and sufficient condition which characterizes all the MACs that do not lose any part of their capacity region by polarization. ,Computer Science - Information Theory ; ,"Nasser, Rajai ; Telatar, Emre ; ",Fourier Analysis of MAC Polarization  One problem with MAC polar codes that are based on MAC polarization is that they may not achieve the entire capacity region. The reason behind this problem is that MAC polarization sometimes induces a loss in the capacity region. This paper provides a single letter necessary and sufficient condition which characterizes all the MACs that do not lose any part of their capacity region by polarization. ,fourier analysis mac polarization one problem mac polar cod base mac polarization may achieve entire capacity region reason behind problem mac polarization sometimes induce loss capacity region paper provide single letter necessary sufficient condition characterize macs lose part capacity region polarization,41,7,1501.06076.txt
http://arxiv.org/abs/1501.06297,Geodesic convolutional neural networks on Riemannian manifolds,"  Feature descriptors play a crucial role in a wide range of geometry analysis and processing applications, including shape correspondence, retrieval, and segmentation. In this paper, we introduce Geodesic Convolutional Neural Networks (GCNN), a generalization of the convolutional networks (CNN) paradigm to non-Euclidean manifolds. Our construction is based on a local geodesic system of polar coordinates to extract ""patches"", which are then passed through a cascade of filters and linear and non-linear operators. The coefficients of the filters and linear combination weights are optimization variables that are learned to minimize a task-specific cost function. We use GCNN to learn invariant shape features, allowing to achieve state-of-the-art performance in problems such as shape description, retrieval, and correspondence. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Masci, Jonathan ; Boscaini, Davide ; Bronstein, Michael M. ; Vandergheynst, Pierre ; ","Geodesic convolutional neural networks on Riemannian manifolds  Feature descriptors play a crucial role in a wide range of geometry analysis and processing applications, including shape correspondence, retrieval, and segmentation. In this paper, we introduce Geodesic Convolutional Neural Networks (GCNN), a generalization of the convolutional networks (CNN) paradigm to non-Euclidean manifolds. Our construction is based on a local geodesic system of polar coordinates to extract ""patches"", which are then passed through a cascade of filters and linear and non-linear operators. The coefficients of the filters and linear combination weights are optimization variables that are learned to minimize a task-specific cost function. We use GCNN to learn invariant shape features, allowing to achieve state-of-the-art performance in problems such as shape description, retrieval, and correspondence. ",geodesic convolutional neural network riemannian manifold feature descriptors play crucial role wide range geometry analysis process applications include shape correspondence retrieval segmentation paper introduce geodesic convolutional neural network gcnn generalization convolutional network cnn paradigm non euclidean manifold construction base local geodesic system polar coordinate extract patch pass cascade filter linear non linear operators coefficients filter linear combination weight optimization variables learn minimize task specific cost function use gcnn learn invariant shape feature allow achieve state art performance problems shape description retrieval correspondence,82,6,1501.06297.txt
http://arxiv.org/abs/1501.06654,Compressive Sampling of Ensembles of Correlated Signals,"  We propose several sampling architectures for the efficient acquisition of an ensemble of correlated signals. We show that without prior knowledge of the correlation structure, each of our architectures (under different sets of assumptions) can acquire the ensemble at a sub-Nyquist rate. Prior to sampling, the analog signals are diversified using simple, implementable components. The diversification is achieved by injecting types of ""structured randomness"" into the ensemble, the result of which is subsampled. For reconstruction, the ensemble is modeled as a low-rank matrix that we have observed through an (undetermined) set of linear equations. Our main results show that this matrix can be recovered using standard convex programming techniques when the total number of samples is on the order of the intrinsic degree of freedom of the ensemble --- the more heavily correlated the ensemble, the fewer samples are needed.   To motivate this study, we discuss how such ensembles arise in the context of array processing. ",Computer Science - Information Theory ; ,"Ahmed, Ali ; Romberg, Justin ; ","Compressive Sampling of Ensembles of Correlated Signals  We propose several sampling architectures for the efficient acquisition of an ensemble of correlated signals. We show that without prior knowledge of the correlation structure, each of our architectures (under different sets of assumptions) can acquire the ensemble at a sub-Nyquist rate. Prior to sampling, the analog signals are diversified using simple, implementable components. The diversification is achieved by injecting types of ""structured randomness"" into the ensemble, the result of which is subsampled. For reconstruction, the ensemble is modeled as a low-rank matrix that we have observed through an (undetermined) set of linear equations. Our main results show that this matrix can be recovered using standard convex programming techniques when the total number of samples is on the order of the intrinsic degree of freedom of the ensemble --- the more heavily correlated the ensemble, the fewer samples are needed.   To motivate this study, we discuss how such ensembles arise in the context of array processing. ",compressive sample ensembles correlate signal propose several sample architectures efficient acquisition ensemble correlate signal show without prior knowledge correlation structure architectures different set assumptions acquire ensemble sub nyquist rate prior sample analog signal diversify use simple implementable components diversification achieve inject type structure randomness ensemble result subsampled reconstruction ensemble model low rank matrix observe undetermined set linear equations main result show matrix recover use standard convex program techniques total number sample order intrinsic degree freedom ensemble heavily correlate ensemble fewer sample need motivate study discuss ensembles arise context array process,90,12,1501.06654.txt
http://arxiv.org/abs/1501.07496,Implementation of an Automatic Syllabic Division Algorithm from Speech   Files in Portuguese Language,"  A new algorithm for voice automatic syllabic splitting in the Portuguese language is proposed, which is based on the envelope of the speech signal of the input audio file. A computational implementation in MatlabTM is presented and made available at the URL http://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its straightforwardness, the proposed method is very attractive for embedded systems (e.g. i-phones). It can also be used as a screen to assist more sophisticated methods. Voice excerpts containing more than one syllable and identified by the same envelope are named as super-syllables and they are subsequently separated. The results indicate which samples corresponds to the beginning and end of each detected syllable. Preliminary tests were performed to fifty words at an identification rate circa 70% (further improvements may be incorporated to treat particular phonemes). This algorithm is also useful in voice command systems, as a tool in the teaching of Portuguese language or even for patients with speech pathology. ",Computer Science - Sound ; Computer Science - Computation and Language ; Computer Science - Data Structures and Algorithms ; Electrical Engineering and Systems Science - Audio and Speech Processing ; ,"Da Silva, E. L. F. ; de Oliveira, H. M. ; ","Implementation of an Automatic Syllabic Division Algorithm from Speech   Files in Portuguese Language  A new algorithm for voice automatic syllabic splitting in the Portuguese language is proposed, which is based on the envelope of the speech signal of the input audio file. A computational implementation in MatlabTM is presented and made available at the URL http://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its straightforwardness, the proposed method is very attractive for embedded systems (e.g. i-phones). It can also be used as a screen to assist more sophisticated methods. Voice excerpts containing more than one syllable and identified by the same envelope are named as super-syllables and they are subsequently separated. The results indicate which samples corresponds to the beginning and end of each detected syllable. Preliminary tests were performed to fifty words at an identification rate circa 70% (further improvements may be incorporated to treat particular phonemes). This algorithm is also useful in voice command systems, as a tool in the teaching of Portuguese language or even for patients with speech pathology. ",implementation automatic syllabic division algorithm speech file portuguese language new algorithm voice automatic syllabic split portuguese language propose base envelope speech signal input audio file computational implementation matlabtm present make available url http www ee ufpe br codec divisao silabica html due straightforwardness propose method attractive embed systems phone also use screen assist sophisticate methods voice excerpt contain one syllable identify envelope name super syllables subsequently separate result indicate sample correspond begin end detect syllable preliminary test perform fifty word identification rate circa improvements may incorporate treat particular phonemes algorithm also useful voice command systems tool teach portuguese language even patients speech pathology,103,4,1501.07496.txt
http://arxiv.org/abs/1501.07584,Efficient Divide-And-Conquer Classification Based on Feature-Space   Decomposition,"  This study presents a divide-and-conquer (DC) approach based on feature space decomposition for classification. When large-scale datasets are present, typical approaches usually employed truncated kernel methods on the feature space or DC approaches on the sample space. However, this did not guarantee separability between classes, owing to overfitting. To overcome such problems, this work proposes a novel DC approach on feature spaces consisting of three steps. Firstly, we divide the feature space into several subspaces using the decomposition method proposed in this paper. Subsequently, these feature subspaces are sent into individual local classifiers for training. Finally, the outcomes of local classifiers are fused together to generate the final classification results. Experiments on large-scale datasets are carried out for performance evaluation. The results show that the error rates of the proposed DC method decreased comparing with the state-of-the-art fast SVM solvers, e.g., reducing error rates by 10.53% and 7.53% on RCV1 and covtype datasets respectively. ",Computer Science - Machine Learning ; ,"Guo, Qi ; Chen, Bo-Wei ; Jiang, Feng ; Ji, Xiangyang ; Kung, Sun-Yuan ; ","Efficient Divide-And-Conquer Classification Based on Feature-Space   Decomposition  This study presents a divide-and-conquer (DC) approach based on feature space decomposition for classification. When large-scale datasets are present, typical approaches usually employed truncated kernel methods on the feature space or DC approaches on the sample space. However, this did not guarantee separability between classes, owing to overfitting. To overcome such problems, this work proposes a novel DC approach on feature spaces consisting of three steps. Firstly, we divide the feature space into several subspaces using the decomposition method proposed in this paper. Subsequently, these feature subspaces are sent into individual local classifiers for training. Finally, the outcomes of local classifiers are fused together to generate the final classification results. Experiments on large-scale datasets are carried out for performance evaluation. The results show that the error rates of the proposed DC method decreased comparing with the state-of-the-art fast SVM solvers, e.g., reducing error rates by 10.53% and 7.53% on RCV1 and covtype datasets respectively. ",efficient divide conquer classification base feature space decomposition study present divide conquer dc approach base feature space decomposition classification large scale datasets present typical approach usually employ truncate kernel methods feature space dc approach sample space however guarantee separability class owe overfitting overcome problems work propose novel dc approach feature space consist three step firstly divide feature space several subspaces use decomposition method propose paper subsequently feature subspaces send individual local classifiers train finally outcomes local classifiers fuse together generate final classification result experiment large scale datasets carry performance evaluation result show error rat propose dc method decrease compare state art fast svm solvers reduce error rat rcv covtype datasets respectively,111,11,1501.07584.txt
http://arxiv.org/abs/1501.07637,Simple Mechanisms for a Subadditive Buyer and Applications to Revenue   Monotonicity,"  We study the revenue maximization problem of a seller with n heterogeneous items for sale to a single buyer whose valuation function for sets of items is unknown and drawn from some distribution D. We show that if D is a distribution over subadditive valuations with independent items, then the better of pricing each item separately or pricing only the grand bundle achieves a constant-factor approximation to the revenue of the optimal mechanism. This includes buyers who are k-demand, additive up to a matroid constraint, or additive up to constraints of any downwards-closed set system (and whose values for the individual items are sampled independently), as well as buyers who are fractionally subadditive with item multipliers drawn independently. Our proof makes use of the core-tail decomposition framework developed in prior work showing similar results for the significantly simpler class of additive buyers [LY13, BILW14].   In the second part of the paper, we develop a connection between approximately optimal simple mechanisms and approximate revenue monotonicity with respect to buyers' valuations. Revenue non-monotonicity is the phenomenon that sometimes strictly increasing buyers' values for every set can strictly decrease the revenue of the optimal mechanism [HR12]. Using our main result, we derive a bound on how bad this degradation can be (and dub such a bound a proof of approximate revenue monotonicity); we further show that better bounds on approximate monotonicity imply a better analysis of our simple mechanisms. ",Computer Science - Computer Science and Game Theory ; ,"Rubinstein, Aviad ; Weinberg, S. Matthew ; ","Simple Mechanisms for a Subadditive Buyer and Applications to Revenue   Monotonicity  We study the revenue maximization problem of a seller with n heterogeneous items for sale to a single buyer whose valuation function for sets of items is unknown and drawn from some distribution D. We show that if D is a distribution over subadditive valuations with independent items, then the better of pricing each item separately or pricing only the grand bundle achieves a constant-factor approximation to the revenue of the optimal mechanism. This includes buyers who are k-demand, additive up to a matroid constraint, or additive up to constraints of any downwards-closed set system (and whose values for the individual items are sampled independently), as well as buyers who are fractionally subadditive with item multipliers drawn independently. Our proof makes use of the core-tail decomposition framework developed in prior work showing similar results for the significantly simpler class of additive buyers [LY13, BILW14].   In the second part of the paper, we develop a connection between approximately optimal simple mechanisms and approximate revenue monotonicity with respect to buyers' valuations. Revenue non-monotonicity is the phenomenon that sometimes strictly increasing buyers' values for every set can strictly decrease the revenue of the optimal mechanism [HR12]. Using our main result, we derive a bound on how bad this degradation can be (and dub such a bound a proof of approximate revenue monotonicity); we further show that better bounds on approximate monotonicity imply a better analysis of our simple mechanisms. ",simple mechanisms subadditive buyer applications revenue monotonicity study revenue maximization problem seller heterogeneous items sale single buyer whose valuation function set items unknown draw distribution show distribution subadditive valuations independent items better price item separately price grand bundle achieve constant factor approximation revenue optimal mechanism include buyers demand additive matroid constraint additive constraints downwards close set system whose value individual items sample independently well buyers fractionally subadditive item multipliers draw independently proof make use core tail decomposition framework develop prior work show similar result significantly simpler class additive buyers ly bilw second part paper develop connection approximately optimal simple mechanisms approximate revenue monotonicity respect buyers valuations revenue non monotonicity phenomenon sometimes strictly increase buyers value every set strictly decrease revenue optimal mechanism hr use main result derive bind bad degradation dub bind proof approximate revenue monotonicity show better bound approximate monotonicity imply better analysis simple mechanisms,146,0,1501.07637.txt
http://arxiv.org/abs/1502.00112,Bar recursion in classical realisability : dependent choice and   continuum hypothesis,"  This paper is about the bar recursion operator in the context of classical realizability. After the pioneering work of Berardi, Bezem & Coquand [1], T. Streicher has shown [10], by means of their bar recursion operator, that the realizability models of ZF, obtained from usual models of $\lambda$-calculus (Scott domains, coherent spaces, . . .), satisfy the axiom of dependent choice. We give a proof of this result, using the tools of classical realizability. Moreover, we show that these realizability models satisfy the well ordering of $\mathbb{R}$ and the continuum hypothesis These formulas are therefore realized by closed $\lambda_c$-terms. This allows to obtain programs from proofs of arithmetical formulas using all these axioms. ",Computer Science - Logic in Computer Science ; Mathematics - Logic ; 03E40 ; F.4.1 ; ,"Krivine, Jean-Louis ; ","Bar recursion in classical realisability : dependent choice and   continuum hypothesis  This paper is about the bar recursion operator in the context of classical realizability. After the pioneering work of Berardi, Bezem & Coquand [1], T. Streicher has shown [10], by means of their bar recursion operator, that the realizability models of ZF, obtained from usual models of $\lambda$-calculus (Scott domains, coherent spaces, . . .), satisfy the axiom of dependent choice. We give a proof of this result, using the tools of classical realizability. Moreover, we show that these realizability models satisfy the well ordering of $\mathbb{R}$ and the continuum hypothesis These formulas are therefore realized by closed $\lambda_c$-terms. This allows to obtain programs from proofs of arithmetical formulas using all these axioms. ",bar recursion classical realisability dependent choice continuum hypothesis paper bar recursion operator context classical realizability pioneer work berardi bezem coquand streicher show mean bar recursion operator realizability model zf obtain usual model lambda calculus scott domains coherent space satisfy axiom dependent choice give proof result use tool classical realizability moreover show realizability model satisfy well order mathbb continuum hypothesis formulas therefore realize close lambda term allow obtain program proof arithmetical formulas use axioms,73,8,1502.00112.txt
http://arxiv.org/abs/1502.01187,Reversibility of d-State Finite Cellular Automata,  This paper investigates reversibility properties of 1-dimensional 3-neighborhood d-state finite cellular automata (CAs) of length n under periodic boundary condition. A tool named reachability tree has been developed from de Bruijn graph which represents all possible reachable configurations of an n-cell CA. This tool has been used to test reversibility of CAs. We have identified a large set of reversible CAs using this tool by following some greedy strategies. ,Computer Science - Formal Languages and Automata Theory ; ,"Bhattacharjee, Kamalika ; Das, Sukanta ; ",Reversibility of d-State Finite Cellular Automata  This paper investigates reversibility properties of 1-dimensional 3-neighborhood d-state finite cellular automata (CAs) of length n under periodic boundary condition. A tool named reachability tree has been developed from de Bruijn graph which represents all possible reachable configurations of an n-cell CA. This tool has been used to test reversibility of CAs. We have identified a large set of reversible CAs using this tool by following some greedy strategies. ,reversibility state finite cellular automata paper investigate reversibility properties dimensional neighborhood state finite cellular automata cas length periodic boundary condition tool name reachability tree develop de bruijn graph represent possible reachable configurations cell ca tool use test reversibility cas identify large set reversible cas use tool follow greedy strategies,49,14,1502.01187.txt
http://arxiv.org/abs/1502.01410,On the Lexical Distinguishability of Source Code,"  Natural language is robust against noise. The meaning of many sentences survives the loss of words, sometimes many of them. Some words in a sentence, however, cannot be lost without changing the meaning of the sentence. We call these words ""wheat"" and the rest ""chaff"". The word ""not"" in the sentence ""I do not like rain"" is wheat and ""do"" is chaff. For human understanding of the purpose and behavior of source code, we hypothesize that the same holds. To quantify the extent to which we can separate code into ""wheat"" and ""chaff"", we study a large (100M LOC), diverse corpus of real-world projects in Java. Since methods represent natural, likely distinct units of code, we use the ~9M Java methods in the corpus to approximate a universe of ""sentences."" We extract their wheat by computing the function's minimal distinguishing subset (Minset). Our results confirm that functions contain work offers the first quantitative evidence for recent promising work on keyword-based programming and insight into how to develop a powerful, alternative programming model. ",Computer Science - Software Engineering ; ,"Velez, Martin ; Qiu, Dong ; Zhou, You ; Barr, Earl T. ; Su, Zhendong ; ","On the Lexical Distinguishability of Source Code  Natural language is robust against noise. The meaning of many sentences survives the loss of words, sometimes many of them. Some words in a sentence, however, cannot be lost without changing the meaning of the sentence. We call these words ""wheat"" and the rest ""chaff"". The word ""not"" in the sentence ""I do not like rain"" is wheat and ""do"" is chaff. For human understanding of the purpose and behavior of source code, we hypothesize that the same holds. To quantify the extent to which we can separate code into ""wheat"" and ""chaff"", we study a large (100M LOC), diverse corpus of real-world projects in Java. Since methods represent natural, likely distinct units of code, we use the ~9M Java methods in the corpus to approximate a universe of ""sentences."" We extract their wheat by computing the function's minimal distinguishing subset (Minset). Our results confirm that functions contain work offers the first quantitative evidence for recent promising work on keyword-based programming and insight into how to develop a powerful, alternative programming model. ",lexical distinguishability source code natural language robust noise mean many sentence survive loss word sometimes many word sentence however cannot lose without change mean sentence call word wheat rest chaff word sentence like rain wheat chaff human understand purpose behavior source code hypothesize hold quantify extent separate code wheat chaff study large loc diverse corpus real world project java since methods represent natural likely distinct units code use java methods corpus approximate universe sentence extract wheat compute function minimal distinguish subset minset result confirm function contain work offer first quantitative evidence recent promise work keyword base program insight develop powerful alternative program model,103,14,1502.01410.txt
http://arxiv.org/abs/1502.01494,Code generator matrices as RNG conditioners,  We quantify precisely the distribution of the output of a binary random number generator (RNG) after conditioning with a binary linear code generator matrix by showing the connection between the Walsh spectrum of the resulting random variable and the weight distribution of the code. Previously known bounds on the performance of linear binary codes as entropy extractors can be derived by considering generator matrices as a selector of a subset of that spectrum. We also extend this framework to the case of non-binary codes. ,"Computer Science - Information Theory ; 65C10, 60B99, 11T71, 94B99 ; E.4 ; G.3 ; ","Tomasi, Alessandro ; Meneghetti, Alessio ; Sala, Massimiliano ; ",Code generator matrices as RNG conditioners  We quantify precisely the distribution of the output of a binary random number generator (RNG) after conditioning with a binary linear code generator matrix by showing the connection between the Walsh spectrum of the resulting random variable and the weight distribution of the code. Previously known bounds on the performance of linear binary codes as entropy extractors can be derived by considering generator matrices as a selector of a subset of that spectrum. We also extend this framework to the case of non-binary codes. ,code generator matrices rng conditioners quantify precisely distribution output binary random number generator rng condition binary linear code generator matrix show connection walsh spectrum result random variable weight distribution code previously know bound performance linear binary cod entropy extractors derive consider generator matrices selector subset spectrum also extend framework case non binary cod,53,5,1502.01494.txt
http://arxiv.org/abs/1502.01566,A Matrix Laurent Series-based Fast Fourier Transform for Blocklengths   N=4 (mod 8),"  General guidelines for a new fast computation of blocklength 8m+4 DFTs are presented, which is based on a Laurent series involving matrices. Results of non-trivial real multiplicative complexity are presented for blocklengths N=64, achieving lower multiplication counts than previously published FFTs. A detailed description for the cases m=1 and m=2 is presented. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; Electrical Engineering and Systems Science - Signal Processing ; ,"de Oliveira, H. M. ; de Souza, R. M. Campello ; de Oliveira, R. C. ; ","A Matrix Laurent Series-based Fast Fourier Transform for Blocklengths   N=4 (mod 8)  General guidelines for a new fast computation of blocklength 8m+4 DFTs are presented, which is based on a Laurent series involving matrices. Results of non-trivial real multiplicative complexity are presented for blocklengths N=64, achieving lower multiplication counts than previously published FFTs. A detailed description for the cases m=1 and m=2 is presented. ",matrix laurent series base fast fourier transform blocklengths mod general guidelines new fast computation blocklength dfts present base laurent series involve matrices result non trivial real multiplicative complexity present blocklengths achieve lower multiplication count previously publish ffts detail description case present,41,7,1502.01566.txt
http://arxiv.org/abs/1502.01865,Lower Bounds for Monotone Counting Circuits,"  A {+,x}-circuit counts a given multivariate polynomial f, if its values on 0-1 inputs are the same as those of f; on other inputs the circuit may output arbitrary values. Such a circuit counts the number of monomials of f evaluated to 1 by a given 0-1 input vector (with multiplicities given by their coefficients). A circuit decides $f$ if it has the same 0-1 roots as f. We first show that some multilinear polynomials can be exponentially easier to count than to compute them, and can be exponentially easier to decide than to count them. Then we give general lower bounds on the size of counting circuits. ",Computer Science - Computational Complexity ; ,"Jukna, Stasys ; ","Lower Bounds for Monotone Counting Circuits  A {+,x}-circuit counts a given multivariate polynomial f, if its values on 0-1 inputs are the same as those of f; on other inputs the circuit may output arbitrary values. Such a circuit counts the number of monomials of f evaluated to 1 by a given 0-1 input vector (with multiplicities given by their coefficients). A circuit decides $f$ if it has the same 0-1 roots as f. We first show that some multilinear polynomials can be exponentially easier to count than to compute them, and can be exponentially easier to decide than to count them. Then we give general lower bounds on the size of counting circuits. ",lower bound monotone count circuit circuit count give multivariate polynomial value input input circuit may output arbitrary value circuit count number monomials evaluate give input vector multiplicities give coefficients circuit decide root first show multilinear polynomials exponentially easier count compute exponentially easier decide count give general lower bound size count circuit,51,8,1502.01865.txt
http://arxiv.org/abs/1502.02253,Data Bits in Karnaugh Map and Increasing Map Capability in Error   Correcting,"  To provide reliable communication in data transmission, ability of correcting errors is of prime importance. This paper intends to suggest an easy algorithm to detect and correct errors in transmission codes using the well-known Karnaugh map. Referring to past research done and proving new theorems and also using a suggested simple technique taking advantage of the easy concept of Karnaugh map, we offer an algorithm to reduce the number of occupied squares in the map and therefore, reduce substantially the execution time for placing data bits in Karnaugh map. Based on earlier papers, we first propose an algorithm for correction of two simultaneous errors in a code. Then, defining specifications for empty squares of the map, we limit the choices for selection of new squares. In addition, burst errors in sending codes is discussed, and systematically code words for correcting them will be made. ",Computer Science - Information Theory ; ,"Pezeshkpour, Pouya ; Tabandeh, Mahmoud ; ","Data Bits in Karnaugh Map and Increasing Map Capability in Error   Correcting  To provide reliable communication in data transmission, ability of correcting errors is of prime importance. This paper intends to suggest an easy algorithm to detect and correct errors in transmission codes using the well-known Karnaugh map. Referring to past research done and proving new theorems and also using a suggested simple technique taking advantage of the easy concept of Karnaugh map, we offer an algorithm to reduce the number of occupied squares in the map and therefore, reduce substantially the execution time for placing data bits in Karnaugh map. Based on earlier papers, we first propose an algorithm for correction of two simultaneous errors in a code. Then, defining specifications for empty squares of the map, we limit the choices for selection of new squares. In addition, burst errors in sending codes is discussed, and systematically code words for correcting them will be made. ",data bits karnaugh map increase map capability error correct provide reliable communication data transmission ability correct errors prime importance paper intend suggest easy algorithm detect correct errors transmission cod use well know karnaugh map refer past research do prove new theorems also use suggest simple technique take advantage easy concept karnaugh map offer algorithm reduce number occupy square map therefore reduce substantially execution time place data bits karnaugh map base earlier paper first propose algorithm correction two simultaneous errors code define specifications empty square map limit choices selection new square addition burst errors send cod discuss systematically code word correct make,101,4,1502.02253.txt
http://arxiv.org/abs/1502.02272,Rigorous Deductive Argumentation for Socially Relevant Issues,"  The most important problems for society are describable only in vague terms, dependent on subjective positions, and missing highly relevant data. This thesis is intended to revive and further develop the view that giving non-trivial, rigorous deductive arguments concerning such problems -without eliminating the complications of vagueness, subjectivity, and uncertainty- is, though very difficult, not problematic in principle, does not require the invention of new logics -classical first-order logic will do- and is something that more mathematically-inclined people should be pursuing. The framework of interpreted formal proofs is presented for formalizing and criticizing rigorous deductive arguments about vague, subjective, and uncertain issues, and its adequacy is supported largely by a number of major examples. This thesis also documents progress towards a web system for collaboratively authoring and criticizing such arguments, which is the ultimate goal of this project. ","Computer Science - Logic in Computer Science ; Mathematics - Logic ; 03Bxx, 03B52, 03B10, 03Axx ; I.2.3 ; F.4.m ; I.2.4 ; F.4.0 ; ","Wehr, Robert Dustin ; ","Rigorous Deductive Argumentation for Socially Relevant Issues  The most important problems for society are describable only in vague terms, dependent on subjective positions, and missing highly relevant data. This thesis is intended to revive and further develop the view that giving non-trivial, rigorous deductive arguments concerning such problems -without eliminating the complications of vagueness, subjectivity, and uncertainty- is, though very difficult, not problematic in principle, does not require the invention of new logics -classical first-order logic will do- and is something that more mathematically-inclined people should be pursuing. The framework of interpreted formal proofs is presented for formalizing and criticizing rigorous deductive arguments about vague, subjective, and uncertain issues, and its adequacy is supported largely by a number of major examples. This thesis also documents progress towards a web system for collaboratively authoring and criticizing such arguments, which is the ultimate goal of this project. ",rigorous deductive argumentation socially relevant issue important problems society describable vague term dependent subjective position miss highly relevant data thesis intend revive develop view give non trivial rigorous deductive arguments concern problems without eliminate complications vagueness subjectivity uncertainty though difficult problematic principle require invention new logics classical first order logic something mathematically incline people pursue framework interpret formal proof present formalize criticize rigorous deductive arguments vague subjective uncertain issue adequacy support largely number major examples thesis also document progress towards web system collaboratively author criticize arguments ultimate goal project,89,8,1502.02272.txt
http://arxiv.org/abs/1502.02348,MATLAB based language for generating randomized multiple choice   questions,"  In this work we describe a simple MATLAB based language which allows to create randomized multiple choice questions with minimal effort. This language has been successfully tested at Flinders University by the author in a number of mathematics topics including Numerical Analysis, Abstract Algebra and Partial Differential Equations. ",Computer Science - Computers and Society ; ,"Azamov, Nurulla ; ","MATLAB based language for generating randomized multiple choice   questions  In this work we describe a simple MATLAB based language which allows to create randomized multiple choice questions with minimal effort. This language has been successfully tested at Flinders University by the author in a number of mathematics topics including Numerical Analysis, Abstract Algebra and Partial Differential Equations. ",matlab base language generate randomize multiple choice question work describe simple matlab base language allow create randomize multiple choice question minimal effort language successfully test flinders university author number mathematics topics include numerical analysis abstract algebra partial differential equations,39,4,1502.02348.txt
http://arxiv.org/abs/1502.02481,Dynamic DFS Tree in Undirected Graphs: breaking the $O(m)$ barrier,"  Depth first search (DFS) tree is a fundamental data structure for solving various problems in graphs. It is well known that it takes $O(m+n)$ time to build a DFS tree for a given undirected graph $G=(V,E)$ on $n$ vertices and $m$ edges. We address the problem of maintaining a DFS tree when the graph is undergoing {\em updates} (insertion and deletion of vertices or edges). We present the following results for this problem.   (a) Fault tolerant DFS tree: There exists a data structure of size ${O}(m ~polylog~ n)$ such that given any set ${\cal F}$ of failed vertices or edges, a DFS tree of the graph $G\setminus {\cal F}$ can be reported in ${O}(n|{\cal F}| ~polylog~ n)$ time.   (b) Fully dynamic DFS tree: There exists a fully dynamic algorithm for maintaining a DFS tree that takes worst case ${O}(\sqrt{mn} ~polylog~ n)$ time per update for any arbitrary online sequence of updates.   (c) Incremental DFS tree: Given any arbitrary online sequence of edge insertions, we can maintain a DFS tree in ${O}(n ~polylog~ n)$ worst case time per edge insertion.   These are the first $o(m)$ worst case time results for maintaining a DFS tree in a dynamic environment. Moreover, our fully dynamic algorithm provides, in a seamless manner, the first deterministic algorithm with $O(1)$ query time and $o(m)$ worst case update time for the dynamic subgraph connectivity, biconnectivity, and 2-edge connectivity. ",Computer Science - Data Structures and Algorithms ; ,"Baswana, Surender ; Chaudhury, Shreejit Ray ; Choudhary, Keerti ; Khan, Shahbaz ; ","Dynamic DFS Tree in Undirected Graphs: breaking the $O(m)$ barrier  Depth first search (DFS) tree is a fundamental data structure for solving various problems in graphs. It is well known that it takes $O(m+n)$ time to build a DFS tree for a given undirected graph $G=(V,E)$ on $n$ vertices and $m$ edges. We address the problem of maintaining a DFS tree when the graph is undergoing {\em updates} (insertion and deletion of vertices or edges). We present the following results for this problem.   (a) Fault tolerant DFS tree: There exists a data structure of size ${O}(m ~polylog~ n)$ such that given any set ${\cal F}$ of failed vertices or edges, a DFS tree of the graph $G\setminus {\cal F}$ can be reported in ${O}(n|{\cal F}| ~polylog~ n)$ time.   (b) Fully dynamic DFS tree: There exists a fully dynamic algorithm for maintaining a DFS tree that takes worst case ${O}(\sqrt{mn} ~polylog~ n)$ time per update for any arbitrary online sequence of updates.   (c) Incremental DFS tree: Given any arbitrary online sequence of edge insertions, we can maintain a DFS tree in ${O}(n ~polylog~ n)$ worst case time per edge insertion.   These are the first $o(m)$ worst case time results for maintaining a DFS tree in a dynamic environment. Moreover, our fully dynamic algorithm provides, in a seamless manner, the first deterministic algorithm with $O(1)$ query time and $o(m)$ worst case update time for the dynamic subgraph connectivity, biconnectivity, and 2-edge connectivity. ",dynamic dfs tree undirected graph break barrier depth first search dfs tree fundamental data structure solve various problems graph well know take time build dfs tree give undirected graph vertices edge address problem maintain dfs tree graph undergo em update insertion deletion vertices edge present follow result problem fault tolerant dfs tree exist data structure size polylog give set cal fail vertices edge dfs tree graph setminus cal report cal polylog time fully dynamic dfs tree exist fully dynamic algorithm maintain dfs tree take worst case sqrt mn polylog time per update arbitrary online sequence update incremental dfs tree give arbitrary online sequence edge insertions maintain dfs tree polylog worst case time per edge insertion first worst case time result maintain dfs tree dynamic environment moreover fully dynamic algorithm provide seamless manner first deterministic algorithm query time worst case update time dynamic subgraph connectivity biconnectivity edge connectivity,147,1,1502.02481.txt
http://arxiv.org/abs/1502.02511,Measurement Scale Effect on Prediction of Soil Water Retention Curve and   Saturated Hydraulic Conductivity,"  Soil water retention curve (SWRC) and saturated hydraulic conductivity (SHC) are key hydraulic properties for unsaturated zone hydrology and groundwater. In particular, SWRC provides useful information on entry pore-size distribution, and SHC is required for flow and transport modeling in the hydrologic cycle. Not only the SWRC and SHC measurements are time-consuming, but also scale dependent. This means as soil column volume increases, variability of the SWRC and SHC decreases. Although prediction of the SWRC and SHC from available parameters, such as textural data, organic matter, and bulk density have been under investigation for decades, up to now no research has focused on the effect of measurement scale on the soil hydraulic properties pedotransfer functions development. In the literature, several data mining approaches have been applied, such as multiple linear regression, artificial neural networks, group method of data handling. However, in this study we develop pedotransfer functions using a novel approach called contrast pattern aided regression (CPXR) and compare it with the multiple linear regression method. For this purpose, two databases including 210 and 213 soil samples are collected to develop and evaluate pedotransfer functions for the SWRC and SHC, respectively, from the UNSODA database. The 10-fold cross-validation method is applied to evaluate the accuracy and reliability of the proposed regression-based models. Our results show that including measurement scale parameters, such as sample internal diameter and length could substantially improve the accuracy of the SWRC and SHC pedotransfer functions developed using the CPXR method, while this is not the case when MLR is used. Moreover, the CPXR method yields remarkably more accurate soil water retention curve and saturated hydraulic conductivity predictions than the MLR approach. ","Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Databases ; ","Ghanbarian, Behzad ; Taslimitehrani, Vahid ; Dong, Guozhu ; Pachepsky, Yakov A. ; ","Measurement Scale Effect on Prediction of Soil Water Retention Curve and   Saturated Hydraulic Conductivity  Soil water retention curve (SWRC) and saturated hydraulic conductivity (SHC) are key hydraulic properties for unsaturated zone hydrology and groundwater. In particular, SWRC provides useful information on entry pore-size distribution, and SHC is required for flow and transport modeling in the hydrologic cycle. Not only the SWRC and SHC measurements are time-consuming, but also scale dependent. This means as soil column volume increases, variability of the SWRC and SHC decreases. Although prediction of the SWRC and SHC from available parameters, such as textural data, organic matter, and bulk density have been under investigation for decades, up to now no research has focused on the effect of measurement scale on the soil hydraulic properties pedotransfer functions development. In the literature, several data mining approaches have been applied, such as multiple linear regression, artificial neural networks, group method of data handling. However, in this study we develop pedotransfer functions using a novel approach called contrast pattern aided regression (CPXR) and compare it with the multiple linear regression method. For this purpose, two databases including 210 and 213 soil samples are collected to develop and evaluate pedotransfer functions for the SWRC and SHC, respectively, from the UNSODA database. The 10-fold cross-validation method is applied to evaluate the accuracy and reliability of the proposed regression-based models. Our results show that including measurement scale parameters, such as sample internal diameter and length could substantially improve the accuracy of the SWRC and SHC pedotransfer functions developed using the CPXR method, while this is not the case when MLR is used. Moreover, the CPXR method yields remarkably more accurate soil water retention curve and saturated hydraulic conductivity predictions than the MLR approach. ",measurement scale effect prediction soil water retention curve saturate hydraulic conductivity soil water retention curve swrc saturate hydraulic conductivity shc key hydraulic properties unsaturated zone hydrology groundwater particular swrc provide useful information entry pore size distribution shc require flow transport model hydrologic cycle swrc shc measurements time consume also scale dependent mean soil column volume increase variability swrc shc decrease although prediction swrc shc available parameters textural data organic matter bulk density investigation decades research focus effect measurement scale soil hydraulic properties pedotransfer function development literature several data mine approach apply multiple linear regression artificial neural network group method data handle however study develop pedotransfer function use novel approach call contrast pattern aid regression cpxr compare multiple linear regression method purpose two databases include soil sample collect develop evaluate pedotransfer function swrc shc respectively unsoda database fold cross validation method apply evaluate accuracy reliability propose regression base model result show include measurement scale parameters sample internal diameter length could substantially improve accuracy swrc shc pedotransfer function develop use cpxr method case mlr use moreover cpxr method yield remarkably accurate soil water retention curve saturate hydraulic conductivity predictions mlr approach,189,9,1502.02511.txt
http://arxiv.org/abs/1502.02800,Fast integer multiplication using generalized Fermat primes,"  For almost 35 years, Sch{\""o}nhage-Strassen's algorithm has been the fastest algorithm known for multiplying integers, with a time complexity O(n $\times$ log n $\times$ log log n) for multiplying n-bit inputs. In 2007, F{\""u}rer proved that there exists K > 1 and an algorithm performing this operation in O(n $\times$ log n $\times$ K log n). Recent work by Harvey, van der Hoeven, and Lecerf showed that this complexity estimate can be improved in order to get K = 8, and conjecturally K = 4. Using an alternative algorithm, which relies on arithmetic modulo generalized Fermat primes, we obtain conjecturally the same result K = 4 via a careful complexity analysis in the deterministic multitape Turing model. ",Computer Science - Symbolic Computation ; Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; ,"Covanov, Svyatoslav ; Thomé, Emmanuel ; ","Fast integer multiplication using generalized Fermat primes  For almost 35 years, Sch{\""o}nhage-Strassen's algorithm has been the fastest algorithm known for multiplying integers, with a time complexity O(n $\times$ log n $\times$ log log n) for multiplying n-bit inputs. In 2007, F{\""u}rer proved that there exists K > 1 and an algorithm performing this operation in O(n $\times$ log n $\times$ K log n). Recent work by Harvey, van der Hoeven, and Lecerf showed that this complexity estimate can be improved in order to get K = 8, and conjecturally K = 4. Using an alternative algorithm, which relies on arithmetic modulo generalized Fermat primes, we obtain conjecturally the same result K = 4 via a careful complexity analysis in the deterministic multitape Turing model. ",fast integer multiplication use generalize fermat prim almost years sch nhage strassen algorithm fastest algorithm know multiply integers time complexity time log time log log multiply bite input rer prove exist algorithm perform operation time log time log recent work harvey van der hoeven lecerf show complexity estimate improve order get conjecturally use alternative algorithm rely arithmetic modulo generalize fermat prim obtain conjecturally result via careful complexity analysis deterministic multitape turing model,72,1,1502.02800.txt
http://arxiv.org/abs/1502.02908,Fast event-based epidemiological simulations on national scales,"  We present a computational modeling framework for data-driven simulations and analysis of infectious disease spread in large populations. For the purpose of efficient simulations, we devise a parallel solution algorithm targeting multi-socket shared memory architectures. The model integrates infectious dynamics as continuous-time Markov chains and available data such as animal movements or aging are incorporated as externally defined events. To bring out parallelism and accelerate the computations, we decompose the spatial domain and optimize cross-boundary communication using dependency-aware task scheduling. Using registered livestock data at a high spatio-temporal resolution, we demonstrate that our approach not only is resilient to varying model configurations, but also scales on all physical cores at realistic work loads. Finally, we show that these very features enable the solution of inverse problems on national scales. ","Quantitative Biology - Populations and Evolution ; Computer Science - Distributed, Parallel, and Cluster Computing ; ","Bauer, Pavol ; Engblom, Stefan ; Widgren, Stefan ; ","Fast event-based epidemiological simulations on national scales  We present a computational modeling framework for data-driven simulations and analysis of infectious disease spread in large populations. For the purpose of efficient simulations, we devise a parallel solution algorithm targeting multi-socket shared memory architectures. The model integrates infectious dynamics as continuous-time Markov chains and available data such as animal movements or aging are incorporated as externally defined events. To bring out parallelism and accelerate the computations, we decompose the spatial domain and optimize cross-boundary communication using dependency-aware task scheduling. Using registered livestock data at a high spatio-temporal resolution, we demonstrate that our approach not only is resilient to varying model configurations, but also scales on all physical cores at realistic work loads. Finally, we show that these very features enable the solution of inverse problems on national scales. ",fast event base epidemiological simulations national scale present computational model framework data drive simulations analysis infectious disease spread large populations purpose efficient simulations devise parallel solution algorithm target multi socket share memory architectures model integrate infectious dynamics continuous time markov chain available data animal movements age incorporate externally define events bring parallelism accelerate computations decompose spatial domain optimize cross boundary communication use dependency aware task schedule use register livestock data high spatio temporal resolution demonstrate approach resilient vary model configurations also scale physical core realistic work load finally show feature enable solution inverse problems national scale,96,11,1502.02908.txt
http://arxiv.org/abs/1502.03097,"Contextuality, Cohomology and Paradox","  Contextuality is a key feature of quantum mechanics that provides an important non-classical resource for quantum information and computation. Abramsky and Brandenburger used sheaf theory to give a general treatment of contextuality in quantum theory [New Journal of Physics 13 (2011) 113036]. However, contextual phenomena are found in other fields as well, for example database theory. In this paper, we shall develop this unified view of contextuality. We provide two main contributions: firstly, we expose a remarkable connection between contexuality and logical paradoxes; secondly, we show that an important class of contextuality arguments has a topological origin. More specifically, we show that ""All-vs-Nothing"" proofs of contextuality are witnessed by cohomological obstructions. ",Quantum Physics ; Computer Science - Logic in Computer Science ; Mathematics - Algebraic Topology ; ,"Abramsky, Samson ; Barbosa, Rui Soares ; Kishida, Kohei ; Lal, Raymond ; Mansfield, Shane ; ","Contextuality, Cohomology and Paradox  Contextuality is a key feature of quantum mechanics that provides an important non-classical resource for quantum information and computation. Abramsky and Brandenburger used sheaf theory to give a general treatment of contextuality in quantum theory [New Journal of Physics 13 (2011) 113036]. However, contextual phenomena are found in other fields as well, for example database theory. In this paper, we shall develop this unified view of contextuality. We provide two main contributions: firstly, we expose a remarkable connection between contexuality and logical paradoxes; secondly, we show that an important class of contextuality arguments has a topological origin. More specifically, we show that ""All-vs-Nothing"" proofs of contextuality are witnessed by cohomological obstructions. ",contextuality cohomology paradox contextuality key feature quantum mechanics provide important non classical resource quantum information computation abramsky brandenburger use sheaf theory give general treatment contextuality quantum theory new journal physics however contextual phenomena find field well example database theory paper shall develop unify view contextuality provide two main contributions firstly expose remarkable connection contexuality logical paradoxes secondly show important class contextuality arguments topological origin specifically show vs nothing proof contextuality witness cohomological obstructions,73,8,1502.03097.txt
http://arxiv.org/abs/1502.03371,The Z Transform over Finite Fields,"  Finite field transforms have many applications and, in many cases, can be implemented with a low computational complexity. In this paper, the Z Transform over a finite field is introduced and some of its properties are presented. ",Mathematics - Number Theory ; Computer Science - Numerical Analysis ; Electrical Engineering and Systems Science - Signal Processing ; ,"de Souza, R. M. Campello ; de Oliveira, H. M. ; Silva, D. ; ","The Z Transform over Finite Fields  Finite field transforms have many applications and, in many cases, can be implemented with a low computational complexity. In this paper, the Z Transform over a finite field is introduced and some of its properties are presented. ",transform finite field finite field transform many applications many case implement low computational complexity paper transform finite field introduce properties present,21,7,1502.03371.txt
http://arxiv.org/abs/1502.03387,A Full Frequency Masking Vocoder for Legal Eavesdropping Conversation   Recording,  This paper presents a new approach for a vocoder design based on full frequency masking by octaves in addition to a technique for spectral filling via beta probability distribution. Some psycho-acoustic characteristics of human hearing - inaudibility masking in frequency and phase - are used as a basis for the proposed algorithm. The results confirm that this technique may be useful to save bandwidth in applications requiring intelligibility. It is recommended for the legal eavesdropping of long voice conversations. ,Computer Science - Sound ; Electrical Engineering and Systems Science - Audio and Speech Processing ; ,"Filho, R. F. B. Sotero ; de Oliveira, H. M. ; de Souza, R. M. Campello ; ",A Full Frequency Masking Vocoder for Legal Eavesdropping Conversation   Recording  This paper presents a new approach for a vocoder design based on full frequency masking by octaves in addition to a technique for spectral filling via beta probability distribution. Some psycho-acoustic characteristics of human hearing - inaudibility masking in frequency and phase - are used as a basis for the proposed algorithm. The results confirm that this technique may be useful to save bandwidth in applications requiring intelligibility. It is recommended for the legal eavesdropping of long voice conversations. ,full frequency mask vocoder legal eavesdrop conversation record paper present new approach vocoder design base full frequency mask octaves addition technique spectral fill via beta probability distribution psycho acoustic characteristics human hear inaudibility mask frequency phase use basis propose algorithm result confirm technique may useful save bandwidth applications require intelligibility recommend legal eavesdrop long voice conversations,56,4,1502.03387.txt
http://arxiv.org/abs/1502.03951,Varieties,"  This text is devoted to the theory of varieties, which provides an important tool, based in universal algebra, for the classification of regular languages. In the introductory section, we present a number of examples that illustrate and motivate the fundamental concepts. We do this for the most part without proofs, and often without precise definitions, leaving these to the formal development of the theory that begins in Section 2. Our presentation of the theory draws heavily on the work of Gehrke, Grigorieff and Pin (2008) on the equational theory of lattices of regular languages. In the subsequent sections we consider in more detail aspects of varieties that were only briefly evoked in the introduction: Decidability, operations on languages, and characterizations in formal logic. ","Computer Science - Formal Languages and Automata Theory ; 68Q70, 20M07 ; F.4.3 ; ","Straubing, Howard ; Weil, Pascal ; ","Varieties  This text is devoted to the theory of varieties, which provides an important tool, based in universal algebra, for the classification of regular languages. In the introductory section, we present a number of examples that illustrate and motivate the fundamental concepts. We do this for the most part without proofs, and often without precise definitions, leaving these to the formal development of the theory that begins in Section 2. Our presentation of the theory draws heavily on the work of Gehrke, Grigorieff and Pin (2008) on the equational theory of lattices of regular languages. In the subsequent sections we consider in more detail aspects of varieties that were only briefly evoked in the introduction: Decidability, operations on languages, and characterizations in formal logic. ",varieties text devote theory varieties provide important tool base universal algebra classification regular languages introductory section present number examples illustrate motivate fundamental concepts part without proof often without precise definitions leave formal development theory begin section presentation theory draw heavily work gehrke grigorieff pin equational theory lattices regular languages subsequent section consider detail aspects varieties briefly evoke introduction decidability operations languages characterizations formal logic,64,8,1502.03951.txt
http://arxiv.org/abs/1502.04052,Computer-aided verification in mechanism design,"  In mechanism design, the gold standard solution concepts are dominant strategy incentive compatibility and Bayesian incentive compatibility. These solution concepts relieve the (possibly unsophisticated) bidders from the need to engage in complicated strategizing. While incentive properties are simple to state, their proofs are specific to the mechanism and can be quite complex. This raises two concerns. From a practical perspective, checking a complex proof can be a tedious process, often requiring experts knowledgeable in mechanism design. Furthermore, from a modeling perspective, if unsophisticated agents are unconvinced of incentive properties, they may strategize in unpredictable ways.   To address both concerns, we explore techniques from computer-aided verification to construct formal proofs of incentive properties. Because formal proofs can be automatically checked, agents do not need to manually check the properties, or even understand the proof. To demonstrate, we present the verification of a sophisticated mechanism: the generic reduction from Bayesian incentive compatible mechanism design to algorithm design given by Hartline, Kleinberg, and Malekian. This mechanism presents new challenges for formal verification, including essential use of randomness from both the execution of the mechanism and from the prior type distributions. As an immediate consequence, our work also formalizes Bayesian incentive compatibility for the entire family of mechanisms derived via this reduction. Finally, as an intermediate step in our formalization, we provide the first formal verification of incentive compatibility for the celebrated Vickrey-Clarke-Groves mechanism. ",Computer Science - Computer Science and Game Theory ; Computer Science - Logic in Computer Science ; ,"Barthe, Gilles ; Gaboardi, Marco ; Arias, Emilio Jesús Gallego ; Hsu, Justin ; Roth, Aaron ; Strub, Pierre-Yves ; ","Computer-aided verification in mechanism design  In mechanism design, the gold standard solution concepts are dominant strategy incentive compatibility and Bayesian incentive compatibility. These solution concepts relieve the (possibly unsophisticated) bidders from the need to engage in complicated strategizing. While incentive properties are simple to state, their proofs are specific to the mechanism and can be quite complex. This raises two concerns. From a practical perspective, checking a complex proof can be a tedious process, often requiring experts knowledgeable in mechanism design. Furthermore, from a modeling perspective, if unsophisticated agents are unconvinced of incentive properties, they may strategize in unpredictable ways.   To address both concerns, we explore techniques from computer-aided verification to construct formal proofs of incentive properties. Because formal proofs can be automatically checked, agents do not need to manually check the properties, or even understand the proof. To demonstrate, we present the verification of a sophisticated mechanism: the generic reduction from Bayesian incentive compatible mechanism design to algorithm design given by Hartline, Kleinberg, and Malekian. This mechanism presents new challenges for formal verification, including essential use of randomness from both the execution of the mechanism and from the prior type distributions. As an immediate consequence, our work also formalizes Bayesian incentive compatibility for the entire family of mechanisms derived via this reduction. Finally, as an intermediate step in our formalization, we provide the first formal verification of incentive compatibility for the celebrated Vickrey-Clarke-Groves mechanism. ",computer aid verification mechanism design mechanism design gold standard solution concepts dominant strategy incentive compatibility bayesian incentive compatibility solution concepts relieve possibly unsophisticated bidders need engage complicate strategizing incentive properties simple state proof specific mechanism quite complex raise two concern practical perspective check complex proof tedious process often require experts knowledgeable mechanism design furthermore model perspective unsophisticated agents unconvinced incentive properties may strategize unpredictable ways address concern explore techniques computer aid verification construct formal proof incentive properties formal proof automatically check agents need manually check properties even understand proof demonstrate present verification sophisticate mechanism generic reduction bayesian incentive compatible mechanism design algorithm design give hartline kleinberg malekian mechanism present new challenge formal verification include essential use randomness execution mechanism prior type distributions immediate consequence work also formalize bayesian incentive compatibility entire family mechanisms derive via reduction finally intermediate step formalization provide first formal verification incentive compatibility celebrate vickrey clarke groves mechanism,151,8,1502.04052.txt
http://arxiv.org/abs/1502.04147,Bayesian Incentive-Compatible Bandit Exploration,"  Individual decision-makers consume information revealed by the previous decision makers, and produce information that may help in future decisions. This phenomenon is common in a wide range of scenarios in the Internet economy, as well as in other domains such as medical decisions. Each decision-maker would individually prefer to ""exploit"": select an action with the highest expected reward given her current information. At the same time, each decision-maker would prefer previous decision-makers to ""explore"", producing information about the rewards of various actions. A social planner, by means of carefully designed information disclosure, can incentivize the agents to balance the exploration and exploitation so as to maximize social welfare.   We formulate this problem as a multi-armed bandit problem (and various generalizations thereof) under incentive-compatibility constraints induced by the agents' Bayesian priors. We design an incentive-compatible bandit algorithm for the social planner whose regret is asymptotically optimal among all bandit algorithms (incentive-compatible or not). Further, we provide a black-box reduction from an arbitrary multi-arm bandit algorithm to an incentive-compatible one, with only a constant multiplicative increase in regret. This reduction works for very general bandit setting that incorporate contexts and arbitrary auxiliary feedback. ",Computer Science - Computer Science and Game Theory ; ,"Mansour, Yishay ; Slivkins, Aleksandrs ; Syrgkanis, Vasilis ; ","Bayesian Incentive-Compatible Bandit Exploration  Individual decision-makers consume information revealed by the previous decision makers, and produce information that may help in future decisions. This phenomenon is common in a wide range of scenarios in the Internet economy, as well as in other domains such as medical decisions. Each decision-maker would individually prefer to ""exploit"": select an action with the highest expected reward given her current information. At the same time, each decision-maker would prefer previous decision-makers to ""explore"", producing information about the rewards of various actions. A social planner, by means of carefully designed information disclosure, can incentivize the agents to balance the exploration and exploitation so as to maximize social welfare.   We formulate this problem as a multi-armed bandit problem (and various generalizations thereof) under incentive-compatibility constraints induced by the agents' Bayesian priors. We design an incentive-compatible bandit algorithm for the social planner whose regret is asymptotically optimal among all bandit algorithms (incentive-compatible or not). Further, we provide a black-box reduction from an arbitrary multi-arm bandit algorithm to an incentive-compatible one, with only a constant multiplicative increase in regret. This reduction works for very general bandit setting that incorporate contexts and arbitrary auxiliary feedback. ",bayesian incentive compatible bandit exploration individual decision makers consume information reveal previous decision makers produce information may help future decisions phenomenon common wide range scenarios internet economy well domains medical decisions decision maker would individually prefer exploit select action highest expect reward give current information time decision maker would prefer previous decision makers explore produce information reward various action social planner mean carefully design information disclosure incentivize agents balance exploration exploitation maximize social welfare formulate problem multi arm bandit problem various generalizations thereof incentive compatibility constraints induce agents bayesian priors design incentive compatible bandit algorithm social planner whose regret asymptotically optimal among bandit algorithms incentive compatible provide black box reduction arbitrary multi arm bandit algorithm incentive compatible one constant multiplicative increase regret reduction work general bandit set incorporate contexts arbitrary auxiliary feedback,132,2,1502.04147.txt
http://arxiv.org/abs/1502.04382,Temporal Network Optimization Subject to Connectivity Constraints,"  In this work we consider \emph{temporal networks}, i.e. networks defined by a \emph{labeling} $\lambda$ assigning to each edge of an \emph{underlying graph} $G$ a set of \emph{discrete} time-labels. The labels of an edge, which are natural numbers, indicate the discrete time moments at which the edge is available. We focus on \emph{path problems} of temporal networks. In particular, we consider \emph{time-respecting} paths, i.e. paths whose edges are assigned by $\lambda$ a strictly increasing sequence of labels. We begin by giving two efficient algorithms for computing shortest time-respecting paths on a temporal network. We then prove that there is a \emph{natural analogue of Menger's theorem} holding for arbitrary temporal networks. Finally, we propose two \emph{cost minimization parameters} for temporal network design. One is the \emph{temporality} of $G$, in which the goal is to minimize the maximum number of labels of an edge, and the other is the \emph{temporal cost} of $G$, in which the goal is to minimize the total number of labels used. Optimization of these parameters is performed subject to some \emph{connectivity constraint}. We prove several lower and upper bounds for the temporality and the temporal cost of some very basic graph families such as rings, directed acyclic graphs, and trees. ","Computer Science - Discrete Mathematics ; 68R10, 68Q17, 68Q25 ; ","Mertzios, George B. ; Michail, Othon ; Spirakis, Paul G. ; ","Temporal Network Optimization Subject to Connectivity Constraints  In this work we consider \emph{temporal networks}, i.e. networks defined by a \emph{labeling} $\lambda$ assigning to each edge of an \emph{underlying graph} $G$ a set of \emph{discrete} time-labels. The labels of an edge, which are natural numbers, indicate the discrete time moments at which the edge is available. We focus on \emph{path problems} of temporal networks. In particular, we consider \emph{time-respecting} paths, i.e. paths whose edges are assigned by $\lambda$ a strictly increasing sequence of labels. We begin by giving two efficient algorithms for computing shortest time-respecting paths on a temporal network. We then prove that there is a \emph{natural analogue of Menger's theorem} holding for arbitrary temporal networks. Finally, we propose two \emph{cost minimization parameters} for temporal network design. One is the \emph{temporality} of $G$, in which the goal is to minimize the maximum number of labels of an edge, and the other is the \emph{temporal cost} of $G$, in which the goal is to minimize the total number of labels used. Optimization of these parameters is performed subject to some \emph{connectivity constraint}. We prove several lower and upper bounds for the temporality and the temporal cost of some very basic graph families such as rings, directed acyclic graphs, and trees. ",temporal network optimization subject connectivity constraints work consider emph temporal network network define emph label lambda assign edge emph underlie graph set emph discrete time label label edge natural number indicate discrete time moments edge available focus emph path problems temporal network particular consider emph time respect paths paths whose edge assign lambda strictly increase sequence label begin give two efficient algorithms compute shortest time respect paths temporal network prove emph natural analogue menger theorem hold arbitrary temporal network finally propose two emph cost minimization parameters temporal network design one emph temporality goal minimize maximum number label edge emph temporal cost goal minimize total number label use optimization parameters perform subject emph connectivity constraint prove several lower upper bound temporality temporal cost basic graph families ring direct acyclic graph tree,130,6,1502.04382.txt
http://arxiv.org/abs/1502.04634,The exp-log normal form of types,"  Lambda calculi with algebraic data types lie at the core of functional programming languages and proof assistants, but conceal at least two fundamental theoretical problems already in the presence of the simplest non-trivial data type, the sum type. First, we do not know of an explicit and implemented algorithm for deciding the beta-eta-equality of terms---and this in spite of the first decidability results proven two decades ago. Second, it is not clear how to decide when two types are essentially the same, i.e. isomorphic, in spite of the meta-theoretic results on decidability of the isomorphism.   In this paper, we present the exp-log normal form of types---derived from the representation of exponential polynomials via the unary exponential and logarithmic functions---that any type built from arrows, products, and sums, can be isomorphically mapped to. The type normal form can be used as a simple heuristic for deciding type isomorphism, thanks to the fact that it is a systematic application of the high-school identities.   We then show that the type normal form allows to reduce the standard beta-eta equational theory of the lambda calculus to a specialized version of itself, while preserving the completeness of equality on terms. We end by describing an alternative representation of normal terms of the lambda calculus with sums, together with a Coq-implemented converter into/from our new term calculus. The difference with the only other previously implemented heuristic for deciding interesting instances of eta-equality by Balat, Di Cosmo, and Fiore, is that we exploit the type information of terms substantially and this often allows us to obtain a canonical representation of terms without performing sophisticated term analyses. ",Computer Science - Logic in Computer Science ; Computer Science - Programming Languages ; Mathematics - Logic ; ,"Ilik, Danko ; ","The exp-log normal form of types  Lambda calculi with algebraic data types lie at the core of functional programming languages and proof assistants, but conceal at least two fundamental theoretical problems already in the presence of the simplest non-trivial data type, the sum type. First, we do not know of an explicit and implemented algorithm for deciding the beta-eta-equality of terms---and this in spite of the first decidability results proven two decades ago. Second, it is not clear how to decide when two types are essentially the same, i.e. isomorphic, in spite of the meta-theoretic results on decidability of the isomorphism.   In this paper, we present the exp-log normal form of types---derived from the representation of exponential polynomials via the unary exponential and logarithmic functions---that any type built from arrows, products, and sums, can be isomorphically mapped to. The type normal form can be used as a simple heuristic for deciding type isomorphism, thanks to the fact that it is a systematic application of the high-school identities.   We then show that the type normal form allows to reduce the standard beta-eta equational theory of the lambda calculus to a specialized version of itself, while preserving the completeness of equality on terms. We end by describing an alternative representation of normal terms of the lambda calculus with sums, together with a Coq-implemented converter into/from our new term calculus. The difference with the only other previously implemented heuristic for deciding interesting instances of eta-equality by Balat, Di Cosmo, and Fiore, is that we exploit the type information of terms substantially and this often allows us to obtain a canonical representation of terms without performing sophisticated term analyses. ",exp log normal form type lambda calculi algebraic data type lie core functional program languages proof assistants conceal least two fundamental theoretical problems already presence simplest non trivial data type sum type first know explicit implement algorithm decide beta eta equality term spite first decidability result prove two decades ago second clear decide two type essentially isomorphic spite meta theoretic result decidability isomorphism paper present exp log normal form type derive representation exponential polynomials via unary exponential logarithmic function type build arrows products sum isomorphically map type normal form use simple heuristic decide type isomorphism thank fact systematic application high school identities show type normal form allow reduce standard beta eta equational theory lambda calculus specialize version preserve completeness equality term end describe alternative representation normal term lambda calculus sum together coq implement converter new term calculus difference previously implement heuristic decide interest instance eta equality balat di cosmo fiore exploit type information term substantially often allow us obtain canonical representation term without perform sophisticate term analyse,167,8,1502.04634.txt
http://arxiv.org/abs/1502.05058,Tensor Spectral Clustering for Partitioning Higher-order Network   Structures,"  Spectral graph theory-based methods represent an important class of tools for studying the structure of networks. Spectral methods are based on a first-order Markov chain derived from a random walk on the graph and thus they cannot take advantage of important higher-order network substructures such as triangles, cycles, and feed-forward loops. Here we propose a Tensor Spectral Clustering (TSC) algorithm that allows for modeling higher-order network structures in a graph partitioning framework. Our TSC algorithm allows the user to specify which higher-order network structures (cycles, feed-forward loops, etc.) should be preserved by the network clustering. Higher-order network structures of interest are represented using a tensor, which we then partition by developing a multilinear spectral method. Our framework can be applied to discovering layered flows in networks as well as graph anomaly detection, which we illustrate on synthetic networks. In directed networks, a higher-order structure of particular interest is the directed 3-cycle, which captures feedback loops in networks. We demonstrate that our TSC algorithm produces large partitions that cut fewer directed 3-cycles than standard spectral clustering algorithms. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Benson, Austin R. ; Gleich, David F. ; Leskovec, Jure ; ","Tensor Spectral Clustering for Partitioning Higher-order Network   Structures  Spectral graph theory-based methods represent an important class of tools for studying the structure of networks. Spectral methods are based on a first-order Markov chain derived from a random walk on the graph and thus they cannot take advantage of important higher-order network substructures such as triangles, cycles, and feed-forward loops. Here we propose a Tensor Spectral Clustering (TSC) algorithm that allows for modeling higher-order network structures in a graph partitioning framework. Our TSC algorithm allows the user to specify which higher-order network structures (cycles, feed-forward loops, etc.) should be preserved by the network clustering. Higher-order network structures of interest are represented using a tensor, which we then partition by developing a multilinear spectral method. Our framework can be applied to discovering layered flows in networks as well as graph anomaly detection, which we illustrate on synthetic networks. In directed networks, a higher-order structure of particular interest is the directed 3-cycle, which captures feedback loops in networks. We demonstrate that our TSC algorithm produces large partitions that cut fewer directed 3-cycles than standard spectral clustering algorithms. ",tensor spectral cluster partition higher order network structure spectral graph theory base methods represent important class tool study structure network spectral methods base first order markov chain derive random walk graph thus cannot take advantage important higher order network substructures triangles cycle fee forward loop propose tensor spectral cluster tsc algorithm allow model higher order network structure graph partition framework tsc algorithm allow user specify higher order network structure cycle fee forward loop etc preserve network cluster higher order network structure interest represent use tensor partition develop multilinear spectral method framework apply discover layer flow network well graph anomaly detection illustrate synthetic network direct network higher order structure particular interest direct cycle capture feedback loop network demonstrate tsc algorithm produce large partition cut fewer direct cycle standard spectral cluster algorithms,130,6,1502.05058.txt
http://arxiv.org/abs/1502.05183,Practically-Self-Stabilizing Virtual Synchrony,"  Virtual synchrony is an important abstraction that is proven to be extremely useful when implemented over asynchronous, typically large, message-passing distributed systems. Fault tolerant design is a key criterion for the success of such implementations. This is because large distributed systems can be highly available as long as they do not depend on the full operational status of every system participant. Namely, they employ redundancy in numbers to overcome non-optimal behavior of participants and to gain global robustness and high availability.   Self-stabilizing systems can tolerate transient faults that drive the system to an arbitrary unpredicted configuration. Such systems automatically regain consistency from any such arbitrary configuration, and then produce the desired system behavior. Practically self-stabilizing systems ensure the desired system behavior for practically infinite number of successive steps e.g., $2^{64}$ steps.   We present the first practically self-stabilizing virtual synchrony algorithm. The algorithm is a combination of several new techniques that may be of independent interest. In particular, we present a new counter algorithm that establishes an efficient practically unbounded counter, that in turn can be directly used to implement a self-stabilizing Multiple-Writer Multiple-Reader (MWMR) register emulation. Other components include self-stabilizing group membership, self-stabilizing multicast, and self-stabilizing emulation of replicated state machine. As we base the replicated state machine implementation on virtual synchrony, rather than consensus, the system progresses in more extreme asynchronous executions in relation to consensus-based replicated state machine. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Dolev, Shlomi ; Georgiou, Chryssis ; Marcoullis, Ioannis ; Schiller, Elad Michael ; ","Practically-Self-Stabilizing Virtual Synchrony  Virtual synchrony is an important abstraction that is proven to be extremely useful when implemented over asynchronous, typically large, message-passing distributed systems. Fault tolerant design is a key criterion for the success of such implementations. This is because large distributed systems can be highly available as long as they do not depend on the full operational status of every system participant. Namely, they employ redundancy in numbers to overcome non-optimal behavior of participants and to gain global robustness and high availability.   Self-stabilizing systems can tolerate transient faults that drive the system to an arbitrary unpredicted configuration. Such systems automatically regain consistency from any such arbitrary configuration, and then produce the desired system behavior. Practically self-stabilizing systems ensure the desired system behavior for practically infinite number of successive steps e.g., $2^{64}$ steps.   We present the first practically self-stabilizing virtual synchrony algorithm. The algorithm is a combination of several new techniques that may be of independent interest. In particular, we present a new counter algorithm that establishes an efficient practically unbounded counter, that in turn can be directly used to implement a self-stabilizing Multiple-Writer Multiple-Reader (MWMR) register emulation. Other components include self-stabilizing group membership, self-stabilizing multicast, and self-stabilizing emulation of replicated state machine. As we base the replicated state machine implementation on virtual synchrony, rather than consensus, the system progresses in more extreme asynchronous executions in relation to consensus-based replicated state machine. ",practically self stabilize virtual synchrony virtual synchrony important abstraction prove extremely useful implement asynchronous typically large message pass distribute systems fault tolerant design key criterion success implementations large distribute systems highly available long depend full operational status every system participant namely employ redundancy number overcome non optimal behavior participants gain global robustness high availability self stabilize systems tolerate transient fault drive system arbitrary unpredicted configuration systems automatically regain consistency arbitrary configuration produce desire system behavior practically self stabilize systems ensure desire system behavior practically infinite number successive step step present first practically self stabilize virtual synchrony algorithm algorithm combination several new techniques may independent interest particular present new counter algorithm establish efficient practically unbounded counter turn directly use implement self stabilize multiple writer multiple reader mwmr register emulation components include self stabilize group membership self stabilize multicast self stabilize emulation replicate state machine base replicate state machine implementation virtual synchrony rather consensus system progress extreme asynchronous executions relation consensus base replicate state machine,163,4,1502.05183.txt
http://arxiv.org/abs/1502.05472,On the Effects of Low-Quality Training Data on Information Extraction   from Clinical Reports,"  In the last five years there has been a flurry of work on information extraction from clinical documents, i.e., on algorithms capable of extracting, from the informal and unstructured texts that are generated during everyday clinical practice, mentions of concepts relevant to such practice. Most of this literature is about methods based on supervised learning, i.e., methods for training an information extraction system from manually annotated examples. While a lot of work has been devoted to devising learning methods that generate more and more accurate information extractors, no work has been devoted to investigating the effect of the quality of training data on the learning process. Low quality in training data often derives from the fact that the person who has annotated the data is different from the one against whose judgment the automatically annotated data must be evaluated. In this paper we test the impact of such data quality issues on the accuracy of information extraction systems as applied to the clinical domain. We do this by comparing the accuracy deriving from training data annotated by the authoritative coder (i.e., the one who has also annotated the test data, and by whose judgment we must abide), with the accuracy deriving from training data annotated by a different coder. The results indicate that, although the disagreement between the two coders (as measured on the training set) is substantial, the difference is (surprisingly enough) not always statistically significant. ",Computer Science - Machine Learning ; Computer Science - Computation and Language ; Computer Science - Information Retrieval ; ,"Marcheggiani, Diego ; Sebastiani, Fabrizio ; ","On the Effects of Low-Quality Training Data on Information Extraction   from Clinical Reports  In the last five years there has been a flurry of work on information extraction from clinical documents, i.e., on algorithms capable of extracting, from the informal and unstructured texts that are generated during everyday clinical practice, mentions of concepts relevant to such practice. Most of this literature is about methods based on supervised learning, i.e., methods for training an information extraction system from manually annotated examples. While a lot of work has been devoted to devising learning methods that generate more and more accurate information extractors, no work has been devoted to investigating the effect of the quality of training data on the learning process. Low quality in training data often derives from the fact that the person who has annotated the data is different from the one against whose judgment the automatically annotated data must be evaluated. In this paper we test the impact of such data quality issues on the accuracy of information extraction systems as applied to the clinical domain. We do this by comparing the accuracy deriving from training data annotated by the authoritative coder (i.e., the one who has also annotated the test data, and by whose judgment we must abide), with the accuracy deriving from training data annotated by a different coder. The results indicate that, although the disagreement between the two coders (as measured on the training set) is substantial, the difference is (surprisingly enough) not always statistically significant. ",effect low quality train data information extraction clinical report last five years flurry work information extraction clinical document algorithms capable extract informal unstructured texts generate everyday clinical practice mention concepts relevant practice literature methods base supervise learn methods train information extraction system manually annotate examples lot work devote devise learn methods generate accurate information extractors work devote investigate effect quality train data learn process low quality train data often derive fact person annotate data different one whose judgment automatically annotate data must evaluate paper test impact data quality issue accuracy information extraction systems apply clinical domain compare accuracy derive train data annotate authoritative coder one also annotate test data whose judgment must abide accuracy derive train data annotate different coder result indicate although disagreement two coders measure train set substantial difference surprisingly enough always statistically significant,136,10,1502.05472.txt
http://arxiv.org/abs/1502.05491,Optimizing Text Quantifiers for Multivariate Loss Functions,"  We address the problem of \emph{quantification}, a supervised learning task whose goal is, given a class, to estimate the relative frequency (or \emph{prevalence}) of the class in a dataset of unlabelled items. Quantification has several applications in data and text mining, such as estimating the prevalence of positive reviews in a set of reviews of a given product, or estimating the prevalence of a given support issue in a dataset of transcripts of phone calls to tech support. So far, quantification has been addressed by learning a general-purpose classifier, counting the unlabelled items which have been assigned the class, and tuning the obtained counts according to some heuristics. In this paper we depart from the tradition of using general-purpose classifiers, and use instead a supervised learning model for \emph{structured prediction}, capable of generating classifiers directly optimized for the (multivariate and non-linear) function used for evaluating quantification accuracy. The experiments that we have run on 5500 binary high-dimensional datasets (averaging more than 14,000 documents each) show that this method is more accurate, more stable, and more efficient than existing, state-of-the-art quantification methods. ",Computer Science - Machine Learning ; Computer Science - Information Retrieval ; ,"Esuli, Andrea ; Sebastiani, Fabrizio ; ","Optimizing Text Quantifiers for Multivariate Loss Functions  We address the problem of \emph{quantification}, a supervised learning task whose goal is, given a class, to estimate the relative frequency (or \emph{prevalence}) of the class in a dataset of unlabelled items. Quantification has several applications in data and text mining, such as estimating the prevalence of positive reviews in a set of reviews of a given product, or estimating the prevalence of a given support issue in a dataset of transcripts of phone calls to tech support. So far, quantification has been addressed by learning a general-purpose classifier, counting the unlabelled items which have been assigned the class, and tuning the obtained counts according to some heuristics. In this paper we depart from the tradition of using general-purpose classifiers, and use instead a supervised learning model for \emph{structured prediction}, capable of generating classifiers directly optimized for the (multivariate and non-linear) function used for evaluating quantification accuracy. The experiments that we have run on 5500 binary high-dimensional datasets (averaging more than 14,000 documents each) show that this method is more accurate, more stable, and more efficient than existing, state-of-the-art quantification methods. ",optimize text quantifiers multivariate loss function address problem emph quantification supervise learn task whose goal give class estimate relative frequency emph prevalence class dataset unlabelled items quantification several applications data text mine estimate prevalence positive review set review give product estimate prevalence give support issue dataset transcripts phone call tech support far quantification address learn general purpose classifier count unlabelled items assign class tune obtain count accord heuristics paper depart tradition use general purpose classifiers use instead supervise learn model emph structure prediction capable generate classifiers directly optimize multivariate non linear function use evaluate quantification accuracy experiment run binary high dimensional datasets average document show method accurate stable efficient exist state art quantification methods,114,11,1502.05491.txt
http://arxiv.org/abs/1502.05507,On asymptotically good ramp secret sharing schemes,  Asymptotically good sequences of linear ramp secret sharing schemes have been intensively studied by Cramer et al. in terms of sequences of pairs of nested algebraic geometric codes. In those works the focus is on full privacy and full reconstruction. In this paper we analyze additional parameters describing the asymptotic behavior of partial information leakage and possibly also partial reconstruction giving a more complete picture of the access structure for sequences of linear ramp secret sharing schemes. Our study involves a detailed treatment of the (relative) generalized Hamming weights of the considered codes. ,"Computer Science - Information Theory ; 94A62, 94B27, 94B65 ; ","Geil, Olav ; Martin, Stefano ; Martínez-Peñas, Umberto ; Matsumoto, Ryutaroh ; Ruano, Diego ; ",On asymptotically good ramp secret sharing schemes  Asymptotically good sequences of linear ramp secret sharing schemes have been intensively studied by Cramer et al. in terms of sequences of pairs of nested algebraic geometric codes. In those works the focus is on full privacy and full reconstruction. In this paper we analyze additional parameters describing the asymptotic behavior of partial information leakage and possibly also partial reconstruction giving a more complete picture of the access structure for sequences of linear ramp secret sharing schemes. Our study involves a detailed treatment of the (relative) generalized Hamming weights of the considered codes. ,asymptotically good ramp secret share scheme asymptotically good sequence linear ramp secret share scheme intensively study cramer et al term sequence pair nest algebraic geometric cod work focus full privacy full reconstruction paper analyze additional parameters describe asymptotic behavior partial information leakage possibly also partial reconstruction give complete picture access structure sequence linear ramp secret share scheme study involve detail treatment relative generalize ham weight consider cod,67,5,1502.05507.txt
http://arxiv.org/abs/1502.05632,Capturing k-ary Existential Second Order Logic with k-ary   Inclusion-Exclusion Logic,"  In this paper we analyze k-ary inclusion-exclusion logic, INEX[k], which is obtained by extending first order logic with k-ary inclusion and exclusion atoms. We show that every formula of INEX[k] can be expressed with a formula of k-ary existential second order logic, ESO[k]. Conversely, every formula of ESO[k] with at most k-ary free relation variables can be expressed with a formula of INEX[k]. From this it follows that, on the level of sentences, INEX[k] captures the expressive power of ESO[k].   We also introduce several useful operators that can be expressed in INEX[k]. In particular, we define inclusion and exclusion quantifiers and so-called term value preserving disjunction which is essential for the proofs of the main results in this paper. Furthermore, we present a novel method of relativization for team semantics and analyze the duality of inclusion and exclusion atoms. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; F.4.1 ; ,"Rönnholm, Raine ; ","Capturing k-ary Existential Second Order Logic with k-ary   Inclusion-Exclusion Logic  In this paper we analyze k-ary inclusion-exclusion logic, INEX[k], which is obtained by extending first order logic with k-ary inclusion and exclusion atoms. We show that every formula of INEX[k] can be expressed with a formula of k-ary existential second order logic, ESO[k]. Conversely, every formula of ESO[k] with at most k-ary free relation variables can be expressed with a formula of INEX[k]. From this it follows that, on the level of sentences, INEX[k] captures the expressive power of ESO[k].   We also introduce several useful operators that can be expressed in INEX[k]. In particular, we define inclusion and exclusion quantifiers and so-called term value preserving disjunction which is essential for the proofs of the main results in this paper. Furthermore, we present a novel method of relativization for team semantics and analyze the duality of inclusion and exclusion atoms. ",capture ary existential second order logic ary inclusion exclusion logic paper analyze ary inclusion exclusion logic inex obtain extend first order logic ary inclusion exclusion atoms show every formula inex express formula ary existential second order logic eso conversely every formula eso ary free relation variables express formula inex follow level sentence inex capture expressive power eso also introduce several useful operators express inex particular define inclusion exclusion quantifiers call term value preserve disjunction essential proof main result paper furthermore present novel method relativization team semantics analyze duality inclusion exclusion atoms,91,8,1502.05632.txt
http://arxiv.org/abs/1502.05767,Automatic differentiation in machine learning: a survey,"  Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply ""autodiff"", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names ""dynamic computational graphs"" and ""differentiable programming"". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms ""autodiff"", ""automatic differentiation"", and ""symbolic differentiation"" as these are encountered more and more in machine learning settings. ","Computer Science - Symbolic Computation ; Computer Science - Machine Learning ; Statistics - Machine Learning ; 68W30, 65D25, 68T05 ; G.1.4 ; I.2.6 ; ","Baydin, Atilim Gunes ; Pearlmutter, Barak A. ; Radul, Alexey Andreyevich ; Siskind, Jeffrey Mark ; ","Automatic differentiation in machine learning: a survey  Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply ""autodiff"", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names ""dynamic computational graphs"" and ""differentiable programming"". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms ""autodiff"", ""automatic differentiation"", and ""symbolic differentiation"" as these are encountered more and more in machine learning settings. ",automatic differentiation machine learn survey derivatives mostly form gradients hessians ubiquitous machine learn automatic differentiation ad also call algorithmic differentiation simply autodiff family techniques similar general backpropagation efficiently accurately evaluate derivatives numeric function express computer program ad small establish field applications areas include computational fluid dynamics atmospheric sciences engineer design optimization recently field machine learn ad largely unaware case independently discover result despite relevance general purpose ad miss machine learn toolbox situation slowly change ongoing adoption name dynamic computational graph differentiable program survey intersection ad machine learn cover applications ad direct relevance address main implementation techniques precisely define main differentiation techniques interrelationships aim bring clarity usage term autodiff automatic differentiation symbolic differentiation encounter machine learn settings,116,10,1502.05767.txt
http://arxiv.org/abs/1502.05880,A Flexible Implementation of a Matrix Laurent Series-Based 16-Point Fast   Fourier and Hartley Transforms,"  This paper describes a flexible architecture for implementing a new fast computation of the discrete Fourier and Hartley transforms, which is based on a matrix Laurent series. The device calculates the transforms based on a single bit selection operator. The hardware structure and synthesis are presented, which handled a 16-point fast transform in 65 nsec, with a Xilinx SPARTAN 3E device. ",Computer Science - Numerical Analysis ; Computer Science - Discrete Mathematics ; Electrical Engineering and Systems Science - Signal Processing ; ,"de Oliveira, R. C. ; de Oliveira, H. M. ; de Souza, R. M. Campello ; Santos, E. J. P. ; ","A Flexible Implementation of a Matrix Laurent Series-Based 16-Point Fast   Fourier and Hartley Transforms  This paper describes a flexible architecture for implementing a new fast computation of the discrete Fourier and Hartley transforms, which is based on a matrix Laurent series. The device calculates the transforms based on a single bit selection operator. The hardware structure and synthesis are presented, which handled a 16-point fast transform in 65 nsec, with a Xilinx SPARTAN 3E device. ",flexible implementation matrix laurent series base point fast fourier hartley transform paper describe flexible architecture implement new fast computation discrete fourier hartley transform base matrix laurent series device calculate transform base single bite selection operator hardware structure synthesis present handle point fast transform nsec xilinx spartan device,47,7,1502.05880.txt
http://arxiv.org/abs/1502.06464,Rectified Factor Networks,"  We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm. On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique for deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods. ",Computer Science - Machine Learning ; Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Neural and Evolutionary Computing ; Statistics - Machine Learning ; ,"Clevert, Djork-Arné ; Mayr, Andreas ; Unterthiner, Thomas ; Hochreiter, Sepp ; ","Rectified Factor Networks  We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm. On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique for deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods. ",rectify factor network propose rectify factor network rfns efficiently construct sparse non linear high dimensional representations input rfn model identify rare small events input low interference code units small reconstruction error explain data covariance structure rfn learn generalize alternate minimization algorithm derive posterior regularization method enforce non negative normalize posterior mean proof convergence correctness rfn learn algorithm benchmarks rfns compare unsupervised methods like autoencoders rbms factor analysis ica pca contrast previous sparse cod methods rfns yield sparser cod capture data covariance structure precisely significantly smaller reconstruction error test rfns pretraining technique deep network different vision datasets rfns superior rbms autoencoders gene expression data two pharmaceutical drug discovery study rfns detect small rare gene modules reveal highly relevant new biological insights far miss unsupervised methods,124,9,1502.06464.txt
http://arxiv.org/abs/1502.06761,Minimal Distance of Propositional Models,"  We investigate the complexity of three optimization problems in Boolean propositional logic related to information theory: Given a conjunctive formula over a set of relations, find a satisfying assignment with minimal Hamming distance to a given assignment that satisfies the formula ($\mathsf{NeareastOtherSolution}$, $\mathsf{NOSol}$) or that does not need to satisfy it ($\mathsf{NearestSolution}$, $\mathsf{NSol}$). The third problem asks for two satisfying assignments with a minimal Hamming distance among all such assignments ($\mathsf{MinSolutionDistance}$, $\mathsf{MSD}$).   For all three problems we give complete classifications with respect to the relations admitted in the formula. We give polynomial time algorithms for several classes of constraint languages. For all other cases we prove hardness or completeness regarding APX, APX, NPO, or equivalence to well-known hard optimization problems. ",Computer Science - Computational Complexity ; ,"Behrisch, Mike ; Hermann, Miki ; Mengel, Stefan ; Salzer, Gernot ; ","Minimal Distance of Propositional Models  We investigate the complexity of three optimization problems in Boolean propositional logic related to information theory: Given a conjunctive formula over a set of relations, find a satisfying assignment with minimal Hamming distance to a given assignment that satisfies the formula ($\mathsf{NeareastOtherSolution}$, $\mathsf{NOSol}$) or that does not need to satisfy it ($\mathsf{NearestSolution}$, $\mathsf{NSol}$). The third problem asks for two satisfying assignments with a minimal Hamming distance among all such assignments ($\mathsf{MinSolutionDistance}$, $\mathsf{MSD}$).   For all three problems we give complete classifications with respect to the relations admitted in the formula. We give polynomial time algorithms for several classes of constraint languages. For all other cases we prove hardness or completeness regarding APX, APX, NPO, or equivalence to well-known hard optimization problems. ",minimal distance propositional model investigate complexity three optimization problems boolean propositional logic relate information theory give conjunctive formula set relations find satisfy assignment minimal ham distance give assignment satisfy formula mathsf neareastothersolution mathsf nosol need satisfy mathsf nearestsolution mathsf nsol third problem ask two satisfy assignments minimal ham distance among assignments mathsf minsolutiondistance mathsf msd three problems give complete classifications respect relations admit formula give polynomial time algorithms several class constraint languages case prove hardness completeness regard apx apx npo equivalence well know hard optimization problems,86,8,1502.06761.txt
http://arxiv.org/abs/1502.07209,Exploiting Feature and Class Relationships in Video Categorization with   Regularized Deep Neural Networks,"  In this paper, we study the challenging problem of categorizing videos according to high-level semantics such as the existence of a particular human action or a complex event. Although extensive efforts have been devoted in recent years, most existing works combined multiple video features using simple fusion strategies and neglected the utilization of inter-class semantic relationships. This paper proposes a novel unified framework that jointly exploits the feature relationships and the class relationships for improved categorization performance. Specifically, these two types of relationships are estimated and utilized by rigorously imposing regularizations in the learning process of a deep neural network (DNN). Such a regularized DNN (rDNN) can be efficiently realized using a GPU-based implementation with an affordable training cost. Through arming the DNN with better capability of harnessing both the feature and the class relationships, the proposed rDNN is more suitable for modeling video semantics. With extensive experimental evaluations, we show that rDNN produces superior performance over several state-of-the-art approaches. On the well-known Hollywood2 and Columbia Consumer Video benchmarks, we obtain very competitive results: 66.9\% and 73.5\% respectively in terms of mean average precision. In addition, to substantially evaluate our rDNN and stimulate future research on large scale video categorization, we collect and release a new benchmark dataset, called FCVID, which contains 91,223 Internet videos and 239 manually annotated categories. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Multimedia ; ,"Jiang, Yu-Gang ; Wu, Zuxuan ; Wang, Jun ; Xue, Xiangyang ; Chang, Shih-Fu ; ","Exploiting Feature and Class Relationships in Video Categorization with   Regularized Deep Neural Networks  In this paper, we study the challenging problem of categorizing videos according to high-level semantics such as the existence of a particular human action or a complex event. Although extensive efforts have been devoted in recent years, most existing works combined multiple video features using simple fusion strategies and neglected the utilization of inter-class semantic relationships. This paper proposes a novel unified framework that jointly exploits the feature relationships and the class relationships for improved categorization performance. Specifically, these two types of relationships are estimated and utilized by rigorously imposing regularizations in the learning process of a deep neural network (DNN). Such a regularized DNN (rDNN) can be efficiently realized using a GPU-based implementation with an affordable training cost. Through arming the DNN with better capability of harnessing both the feature and the class relationships, the proposed rDNN is more suitable for modeling video semantics. With extensive experimental evaluations, we show that rDNN produces superior performance over several state-of-the-art approaches. On the well-known Hollywood2 and Columbia Consumer Video benchmarks, we obtain very competitive results: 66.9\% and 73.5\% respectively in terms of mean average precision. In addition, to substantially evaluate our rDNN and stimulate future research on large scale video categorization, we collect and release a new benchmark dataset, called FCVID, which contains 91,223 Internet videos and 239 manually annotated categories. ",exploit feature class relationships video categorization regularize deep neural network paper study challenge problem categorize videos accord high level semantics existence particular human action complex event although extensive efforts devote recent years exist work combine multiple video feature use simple fusion strategies neglect utilization inter class semantic relationships paper propose novel unify framework jointly exploit feature relationships class relationships improve categorization performance specifically two type relationships estimate utilize rigorously impose regularizations learn process deep neural network dnn regularize dnn rdnn efficiently realize use gpu base implementation affordable train cost arm dnn better capability harness feature class relationships propose rdnn suitable model video semantics extensive experimental evaluations show rdnn produce superior performance several state art approach well know hollywood columbia consumer video benchmarks obtain competitive result respectively term mean average precision addition substantially evaluate rdnn stimulate future research large scale video categorization collect release new benchmark dataset call fcvid contain internet videos manually annotate categories,154,11,1502.07209.txt
http://arxiv.org/abs/1502.07331,Highly corrupted image inpainting through hypoelliptic diffusion,"  We present a new image inpainting algorithm, the Averaging and Hypoelliptic Evolution (AHE) algorithm, inspired by the one presented in [SIAM J. Imaging Sci., vol. 7, no. 2, pp. 669--695, 2014] and based upon a semi-discrete variation of the Citti-Petitot-Sarti model of the primary visual cortex V1. The AHE algorithm is based on a suitable combination of sub-Riemannian hypoelliptic diffusion and ad-hoc local averaging techniques. In particular, we focus on reconstructing highly corrupted images (i.e. where more than the 80% of the image is missing), for which we obtain reconstructions comparable with the state-of-the-art. ",Computer Science - Computer Vision and Pattern Recognition ; Mathematics - Analysis of PDEs ; ,"Boscain, Ugo ; Chertovskih, Roman ; Gauthier, Jean-Paul ; Prandi, Dario ; Remizov, Alexey ; ","Highly corrupted image inpainting through hypoelliptic diffusion  We present a new image inpainting algorithm, the Averaging and Hypoelliptic Evolution (AHE) algorithm, inspired by the one presented in [SIAM J. Imaging Sci., vol. 7, no. 2, pp. 669--695, 2014] and based upon a semi-discrete variation of the Citti-Petitot-Sarti model of the primary visual cortex V1. The AHE algorithm is based on a suitable combination of sub-Riemannian hypoelliptic diffusion and ad-hoc local averaging techniques. In particular, we focus on reconstructing highly corrupted images (i.e. where more than the 80% of the image is missing), for which we obtain reconstructions comparable with the state-of-the-art. ",highly corrupt image inpainting hypoelliptic diffusion present new image inpainting algorithm average hypoelliptic evolution ahe algorithm inspire one present siam image sci vol pp base upon semi discrete variation citti petitot sarti model primary visual cortex ahe algorithm base suitable combination sub riemannian hypoelliptic diffusion ad hoc local average techniques particular focus reconstruct highly corrupt image image miss obtain reconstructions comparable state art,63,11,1502.07331.txt
http://arxiv.org/abs/1502.07481,Cluster Synchronization of Coupled Systems with Nonidentical Linear   Dynamics,"  This paper considers the cluster synchronization problem of generic linear dynamical systems whose system models are distinct in different clusters. These nonidentical linear models render control design and coupling conditions highly correlated if static couplings are used for all individual systems. In this paper, a dynamic coupling structure, which incorporates a global weighting factor and a vanishing auxiliary control variable, is proposed for each agent and is shown to be a feasible solution. Lower bounds on the global and local weighting factors are derived under the condition that every interaction subgraph associated with each cluster admits a directed spanning tree. The spanning tree requirement is further shown to be a necessary condition when the clusters connect acyclicly with each other. Simulations for two applications, cluster heading alignment of nonidentical ships and cluster phase synchronization of nonidentical harmonic oscillators, illustrate essential parts of the derived theoretical results. ",Computer Science - Systems and Control ; ,"Liu, Zhongchang ; Wong, Wing Shing ; ","Cluster Synchronization of Coupled Systems with Nonidentical Linear   Dynamics  This paper considers the cluster synchronization problem of generic linear dynamical systems whose system models are distinct in different clusters. These nonidentical linear models render control design and coupling conditions highly correlated if static couplings are used for all individual systems. In this paper, a dynamic coupling structure, which incorporates a global weighting factor and a vanishing auxiliary control variable, is proposed for each agent and is shown to be a feasible solution. Lower bounds on the global and local weighting factors are derived under the condition that every interaction subgraph associated with each cluster admits a directed spanning tree. The spanning tree requirement is further shown to be a necessary condition when the clusters connect acyclicly with each other. Simulations for two applications, cluster heading alignment of nonidentical ships and cluster phase synchronization of nonidentical harmonic oscillators, illustrate essential parts of the derived theoretical results. ",cluster synchronization couple systems nonidentical linear dynamics paper consider cluster synchronization problem generic linear dynamical systems whose system model distinct different cluster nonidentical linear model render control design couple condition highly correlate static couple use individual systems paper dynamic couple structure incorporate global weight factor vanish auxiliary control variable propose agent show feasible solution lower bound global local weight factor derive condition every interaction subgraph associate cluster admit direct span tree span tree requirement show necessary condition cluster connect acyclicly simulations two applications cluster head alignment nonidentical ship cluster phase synchronization nonidentical harmonic oscillators illustrate essential part derive theoretical result,100,11,1502.07481.txt
http://arxiv.org/abs/1502.07884,Characterising Modal Definability of Team-Based Logics via the Universal   Modality,"  We study model and frame definability of various modal logics. Let ML(A+) denote the fragment of modal logic extended with the universal modality in which the universal modality occurs only positively. We show that a class of Kripke models is definable in ML(A+) if and only if the class is elementary and closed under disjoint unions and surjective bisimulations. We also characterise the definability of ML(A+) in the spirit of the well-known Goldblatt--Thomason theorem. We show that an elementary class F of Kripke frames is definable in ML(A+) if and only if F is closed under taking generated subframes and bounded morphic images, and reflects ultrafilter extensions and finitely generated subframes. In addition we study frame definability relative to finite transitive frames and give an analogous characterisation of ML(A+)-definability relative to finite transitive frames. Finally, we initiate the study of model and frame definability in team-based logics. We study (extended) modal dependence logic, (extended) modal inclusion logic, and modal team logic. We establish strict linear hierarchies with respect to model definability and frame definability, respectively. We show that, with respect to model and frame definability, the before mentioned team-based logics, except modal dependence logic, either coincide with ML(A+) or plain modal logic ML. Thus as a corollary we obtain model theoretic characterisation of model and frame definability for the team-based logics. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; ,"Sano, Katsuhiko ; Virtema, Jonni ; ","Characterising Modal Definability of Team-Based Logics via the Universal   Modality  We study model and frame definability of various modal logics. Let ML(A+) denote the fragment of modal logic extended with the universal modality in which the universal modality occurs only positively. We show that a class of Kripke models is definable in ML(A+) if and only if the class is elementary and closed under disjoint unions and surjective bisimulations. We also characterise the definability of ML(A+) in the spirit of the well-known Goldblatt--Thomason theorem. We show that an elementary class F of Kripke frames is definable in ML(A+) if and only if F is closed under taking generated subframes and bounded morphic images, and reflects ultrafilter extensions and finitely generated subframes. In addition we study frame definability relative to finite transitive frames and give an analogous characterisation of ML(A+)-definability relative to finite transitive frames. Finally, we initiate the study of model and frame definability in team-based logics. We study (extended) modal dependence logic, (extended) modal inclusion logic, and modal team logic. We establish strict linear hierarchies with respect to model definability and frame definability, respectively. We show that, with respect to model and frame definability, the before mentioned team-based logics, except modal dependence logic, either coincide with ML(A+) or plain modal logic ML. Thus as a corollary we obtain model theoretic characterisation of model and frame definability for the team-based logics. ",characterise modal definability team base logics via universal modality study model frame definability various modal logics let ml denote fragment modal logic extend universal modality universal modality occur positively show class kripke model definable ml class elementary close disjoint unions surjective bisimulations also characterise definability ml spirit well know goldblatt thomason theorem show elementary class kripke frame definable ml close take generate subframes bound morphic image reflect ultrafilter extensions finitely generate subframes addition study frame definability relative finite transitive frame give analogous characterisation ml definability relative finite transitive frame finally initiate study model frame definability team base logics study extend modal dependence logic extend modal inclusion logic modal team logic establish strict linear hierarchies respect model definability frame definability respectively show respect model frame definability mention team base logics except modal dependence logic either coincide ml plain modal logic ml thus corollary obtain model theoretic characterisation model frame definability team base logics,152,8,1502.07884.txt
http://arxiv.org/abs/1502.08010,Tropical differential equations,"  Tropical differential equations are introduced and an algorithm is designed which tests solvability of a system of tropical linear differential equations within the complexity polynomial in the size of the system and in its coefficients. Moreover, we show that there exists a minimal solution, and the algorithm constructs it (in case of solvability). This extends a similar complexity bound established for tropical linear systems. In case of tropical linear differential systems in one variable a polynomial complexity algorithm for testing its solvability is designed.   We prove also that the problem of solvability of a system of tropical non-linear differential equations in one variable is $NP$-hard, and this problem for arbitrary number of variables belongs to $NP$. Similar to tropical algebraic equations, a tropical differential equation expresses the (necessary) condition on the dominant term in the issue of solvability of a differential equation in power series. ",Computer Science - Symbolic Computation ; Mathematics - Algebraic Geometry ; 14T05 ; I.1.2 ; ,"Grigoriev, Dima ; ","Tropical differential equations  Tropical differential equations are introduced and an algorithm is designed which tests solvability of a system of tropical linear differential equations within the complexity polynomial in the size of the system and in its coefficients. Moreover, we show that there exists a minimal solution, and the algorithm constructs it (in case of solvability). This extends a similar complexity bound established for tropical linear systems. In case of tropical linear differential systems in one variable a polynomial complexity algorithm for testing its solvability is designed.   We prove also that the problem of solvability of a system of tropical non-linear differential equations in one variable is $NP$-hard, and this problem for arbitrary number of variables belongs to $NP$. Similar to tropical algebraic equations, a tropical differential equation expresses the (necessary) condition on the dominant term in the issue of solvability of a differential equation in power series. ",tropical differential equations tropical differential equations introduce algorithm design test solvability system tropical linear differential equations within complexity polynomial size system coefficients moreover show exist minimal solution algorithm construct case solvability extend similar complexity bind establish tropical linear systems case tropical linear differential systems one variable polynomial complexity algorithm test solvability design prove also problem solvability system tropical non linear differential equations one variable np hard problem arbitrary number variables belong np similar tropical algebraic equations tropical differential equation express necessary condition dominant term issue solvability differential equation power series,90,8,1502.08010.txt
http://arxiv.org/abs/1503.00207,Knowledge-aided Two-dimensional Autofocus for Spotlight SAR Polar Format   Imagery,"  Conventional two-dimensional (2-D) autofocus algorithms blindly estimate the phase error in the sense that they do not exploit any a priori information on the structure of the 2-D phase error. As such, they often suffer from low computational efficiency and lack of data redundancy to accurately estimate the 2-D phase error. In this paper, a knowledge-aided (KA) 2-D autofocus algorithm which is based on exploiting a priori knowledge about the 2-D phase error structure, is presented. First, as a prerequisite of the proposed KA method, the analytical structure of residual 2-D phase error in SAR imagery is investigated in the polar format algorithm (PFA) framework. Then, by incorporating this a priori information, a novel 2-D autofocus approach is proposed. The new method only requires an estimate of azimuth phase error and/or residual range cell migration, while the 2-D phase error can then be computed directly from the estimated azimuth phase error or residual range cell migration. This 2-D autofocus method can also be applied to refocus moving targets in PFA imagery. Experimental results clearly demonstrate the effectiveness and robustness of the proposed method. ",Computer Science - Information Theory ; ,"Mao, Xinhua ; ","Knowledge-aided Two-dimensional Autofocus for Spotlight SAR Polar Format   Imagery  Conventional two-dimensional (2-D) autofocus algorithms blindly estimate the phase error in the sense that they do not exploit any a priori information on the structure of the 2-D phase error. As such, they often suffer from low computational efficiency and lack of data redundancy to accurately estimate the 2-D phase error. In this paper, a knowledge-aided (KA) 2-D autofocus algorithm which is based on exploiting a priori knowledge about the 2-D phase error structure, is presented. First, as a prerequisite of the proposed KA method, the analytical structure of residual 2-D phase error in SAR imagery is investigated in the polar format algorithm (PFA) framework. Then, by incorporating this a priori information, a novel 2-D autofocus approach is proposed. The new method only requires an estimate of azimuth phase error and/or residual range cell migration, while the 2-D phase error can then be computed directly from the estimated azimuth phase error or residual range cell migration. This 2-D autofocus method can also be applied to refocus moving targets in PFA imagery. Experimental results clearly demonstrate the effectiveness and robustness of the proposed method. ",knowledge aid two dimensional autofocus spotlight sar polar format imagery conventional two dimensional autofocus algorithms blindly estimate phase error sense exploit priori information structure phase error often suffer low computational efficiency lack data redundancy accurately estimate phase error paper knowledge aid ka autofocus algorithm base exploit priori knowledge phase error structure present first prerequisite propose ka method analytical structure residual phase error sar imagery investigate polar format algorithm pfa framework incorporate priori information novel autofocus approach propose new method require estimate azimuth phase error residual range cell migration phase error compute directly estimate azimuth phase error residual range cell migration autofocus method also apply refocus move target pfa imagery experimental result clearly demonstrate effectiveness robustness propose method,117,9,1503.00207.txt
http://arxiv.org/abs/1503.00244,23-bit Metaknowledge Template Towards Big Data Knowledge Discovery and   Management,"  The global influence of Big Data is not only growing but seemingly endless. The trend is leaning towards knowledge that is attained easily and quickly from massive pools of Big Data. Today we are living in the technological world that Dr. Usama Fayyad and his distinguished research fellows discussed in the introductory explanations of Knowledge Discovery in Databases (KDD) predicted nearly two decades ago. Indeed, they were precise in their outlook on Big Data analytics. In fact, the continued improvement of the interoperability of machine learning, statistics, database building and querying fused to create this increasingly popular science- Data Mining and Knowledge Discovery. The next generation computational theories are geared towards helping to extract insightful knowledge from even larger volumes of data at higher rates of speed. As the trend increases in popularity, the need for a highly adaptive solution for knowledge discovery will be necessary. In this research paper, we are introducing the investigation and development of 23 bit-questions for a Metaknowledge template for Big Data Processing and clustering purposes. This research aims to demonstrate the construction of this methodology and proves the validity and the beneficial utilization that brings Knowledge Discovery from Big Data. ",Computer Science - Databases ; Computer Science - Artificial Intelligence ; Computer Science - Information Retrieval ; Computer Science - Machine Learning ; ,"Bari, Nima ; Vichr, Roman ; Kowsari, Kamran ; Berkovich, Simon Y. ; ","23-bit Metaknowledge Template Towards Big Data Knowledge Discovery and   Management  The global influence of Big Data is not only growing but seemingly endless. The trend is leaning towards knowledge that is attained easily and quickly from massive pools of Big Data. Today we are living in the technological world that Dr. Usama Fayyad and his distinguished research fellows discussed in the introductory explanations of Knowledge Discovery in Databases (KDD) predicted nearly two decades ago. Indeed, they were precise in their outlook on Big Data analytics. In fact, the continued improvement of the interoperability of machine learning, statistics, database building and querying fused to create this increasingly popular science- Data Mining and Knowledge Discovery. The next generation computational theories are geared towards helping to extract insightful knowledge from even larger volumes of data at higher rates of speed. As the trend increases in popularity, the need for a highly adaptive solution for knowledge discovery will be necessary. In this research paper, we are introducing the investigation and development of 23 bit-questions for a Metaknowledge template for Big Data Processing and clustering purposes. This research aims to demonstrate the construction of this methodology and proves the validity and the beneficial utilization that brings Knowledge Discovery from Big Data. ",bite metaknowledge template towards big data knowledge discovery management global influence big data grow seemingly endless trend lean towards knowledge attain easily quickly massive pool big data today live technological world dr usama fayyad distinguish research fellows discuss introductory explanations knowledge discovery databases kdd predict nearly two decades ago indeed precise outlook big data analytics fact continue improvement interoperability machine learn statistics database build query fuse create increasingly popular science data mine knowledge discovery next generation computational theories gear towards help extract insightful knowledge even larger volumes data higher rat speed trend increase popularity need highly adaptive solution knowledge discovery necessary research paper introduce investigation development bite question metaknowledge template big data process cluster purpose research aim demonstrate construction methodology prove validity beneficial utilization bring knowledge discovery big data,129,10,1503.00244.txt
http://arxiv.org/abs/1503.00245,Novel Metaknowledge-based Processing Technique for Multimedia Big Data   clustering challenges,"  Past research has challenged us with the task of showing relational patterns between text-based data and then clustering for predictive analysis using Golay Code technique. We focus on a novel approach to extract metaknowledge in multimedia datasets. Our collaboration has been an on-going task of studying the relational patterns between datapoints based on metafeatures extracted from metaknowledge in multimedia datasets. Those selected are significant to suit the mining technique we applied, Golay Code algorithm. In this research paper we summarize findings in optimization of metaknowledge representation for 23-bit representation of structured and unstructured multimedia data in order to ",Computer Science - Databases ; Computer Science - Artificial Intelligence ; Computer Science - Information Retrieval ; Computer Science - Multimedia ; ,"Bari, Nima ; Vichr, Roman ; Kowsari, Kamran ; Berkovich, Simon Y. ; ","Novel Metaknowledge-based Processing Technique for Multimedia Big Data   clustering challenges  Past research has challenged us with the task of showing relational patterns between text-based data and then clustering for predictive analysis using Golay Code technique. We focus on a novel approach to extract metaknowledge in multimedia datasets. Our collaboration has been an on-going task of studying the relational patterns between datapoints based on metafeatures extracted from metaknowledge in multimedia datasets. Those selected are significant to suit the mining technique we applied, Golay Code algorithm. In this research paper we summarize findings in optimization of metaknowledge representation for 23-bit representation of structured and unstructured multimedia data in order to ",novel metaknowledge base process technique multimedia big data cluster challenge past research challenge us task show relational pattern text base data cluster predictive analysis use golay code technique focus novel approach extract metaknowledge multimedia datasets collaboration go task study relational pattern datapoints base metafeatures extract metaknowledge multimedia datasets select significant suit mine technique apply golay code algorithm research paper summarize find optimization metaknowledge representation bite representation structure unstructured multimedia data order,71,10,1503.00245.txt
http://arxiv.org/abs/1503.00491,Utility-Theoretic Ranking for Semi-Automated Text Classification,"  \emph{Semi-Automated Text Classification} (SATC) may be defined as the task of ranking a set $\mathcal{D}$ of automatically labelled textual documents in such a way that, if a human annotator validates (i.e., inspects and corrects where appropriate) the documents in a top-ranked portion of $\mathcal{D}$ with the goal of increasing the overall labelling accuracy of $\mathcal{D}$, the expected increase is maximized. An obvious SATC strategy is to rank $\mathcal{D}$ so that the documents that the classifier has labelled with the lowest confidence are top-ranked. In this work we show that this strategy is suboptimal. We develop new utility-theoretic ranking methods based on the notion of \emph{validation gain}, defined as the improvement in classification effectiveness that would derive by validating a given automatically labelled document. We also propose a new effectiveness measure for SATC-oriented ranking methods, based on the expected reduction in classification error brought about by partially validating a list generated by a given ranking method. We report the results of experiments showing that, with respect to the baseline method above, and according to the proposed measure, our utility-theoretic ranking methods can achieve substantially higher expected reductions in classification error. ",Computer Science - Machine Learning ; ,"Berardi, Giacomo ; Esuli, Andrea ; Sebastiani, Fabrizio ; ","Utility-Theoretic Ranking for Semi-Automated Text Classification  \emph{Semi-Automated Text Classification} (SATC) may be defined as the task of ranking a set $\mathcal{D}$ of automatically labelled textual documents in such a way that, if a human annotator validates (i.e., inspects and corrects where appropriate) the documents in a top-ranked portion of $\mathcal{D}$ with the goal of increasing the overall labelling accuracy of $\mathcal{D}$, the expected increase is maximized. An obvious SATC strategy is to rank $\mathcal{D}$ so that the documents that the classifier has labelled with the lowest confidence are top-ranked. In this work we show that this strategy is suboptimal. We develop new utility-theoretic ranking methods based on the notion of \emph{validation gain}, defined as the improvement in classification effectiveness that would derive by validating a given automatically labelled document. We also propose a new effectiveness measure for SATC-oriented ranking methods, based on the expected reduction in classification error brought about by partially validating a list generated by a given ranking method. We report the results of experiments showing that, with respect to the baseline method above, and according to the proposed measure, our utility-theoretic ranking methods can achieve substantially higher expected reductions in classification error. ",utility theoretic rank semi automate text classification emph semi automate text classification satc may define task rank set mathcal automatically label textual document way human annotator validate inspect correct appropriate document top rank portion mathcal goal increase overall label accuracy mathcal expect increase maximize obvious satc strategy rank mathcal document classifier label lowest confidence top rank work show strategy suboptimal develop new utility theoretic rank methods base notion emph validation gain define improvement classification effectiveness would derive validate give automatically label document also propose new effectiveness measure satc orient rank methods base expect reduction classification error bring partially validate list generate give rank method report result experiment show respect baseline method accord propose measure utility theoretic rank methods achieve substantially higher expect reductions classification error,125,11,1503.00491.txt
http://arxiv.org/abs/1503.00941,Approximation Algorithms for Computing Maximin Share Allocations,"  We study the problem of computing maximin share guarantees, a recently introduced fairness notion. Given a set of $n$ agents and a set of goods, the maximin share of a single agent is the best that she can guarantee to herself, if she would be allowed to partition the goods in any way she prefers, into $n$ bundles, and then receive her least desirable bundle. The objective then in our problem is to find a partition, so that each agent is guaranteed her maximin share. In settings with indivisible goods, such allocations are not guaranteed to exist, so we resort to approximation algorithms. Our main result is a $2/3$-approximation, that runs in polynomial time for any number of agents. This improves upon the algorithm of Procaccia and Wang, which also produces a $2/3$-approximation but runs in polynomial time only for a constant number of agents. To achieve this, we redesign certain parts of their algorithm. Furthermore, motivated by the apparent difficulty, both theoretically and experimentally, in finding lower bounds on the existence of approximate solutions, we undertake a probabilistic analysis. We prove that in randomly generated instances, with high probability there exists a maximin share allocation. This can be seen as a justification of the experimental evidence reported in relevant works. Finally, we provide further positive results for two special cases that arise from previous works. The first one is the intriguing case of $3$ agents, for which it is already known that exact maximin share allocations do not always exist (contrary to the case of $2$ agents). We provide a $7/8$-approximation algorithm, improving the previously known result of $3/4$. The second case is when all item values belong to $\{0, 1, 2\}$, extending the $\{0, 1\}$ setting studied in Bouveret and Lema\^itre. We obtain an exact algorithm for any number of agents in this case. ",Computer Science - Computer Science and Game Theory ; F.2.2 ; G.2.1 ; ,"Amanatidis, Georgios ; Markakis, Evangelos ; Nikzad, Afshin ; Saberi, Amin ; ","Approximation Algorithms for Computing Maximin Share Allocations  We study the problem of computing maximin share guarantees, a recently introduced fairness notion. Given a set of $n$ agents and a set of goods, the maximin share of a single agent is the best that she can guarantee to herself, if she would be allowed to partition the goods in any way she prefers, into $n$ bundles, and then receive her least desirable bundle. The objective then in our problem is to find a partition, so that each agent is guaranteed her maximin share. In settings with indivisible goods, such allocations are not guaranteed to exist, so we resort to approximation algorithms. Our main result is a $2/3$-approximation, that runs in polynomial time for any number of agents. This improves upon the algorithm of Procaccia and Wang, which also produces a $2/3$-approximation but runs in polynomial time only for a constant number of agents. To achieve this, we redesign certain parts of their algorithm. Furthermore, motivated by the apparent difficulty, both theoretically and experimentally, in finding lower bounds on the existence of approximate solutions, we undertake a probabilistic analysis. We prove that in randomly generated instances, with high probability there exists a maximin share allocation. This can be seen as a justification of the experimental evidence reported in relevant works. Finally, we provide further positive results for two special cases that arise from previous works. The first one is the intriguing case of $3$ agents, for which it is already known that exact maximin share allocations do not always exist (contrary to the case of $2$ agents). We provide a $7/8$-approximation algorithm, improving the previously known result of $3/4$. The second case is when all item values belong to $\{0, 1, 2\}$, extending the $\{0, 1\}$ setting studied in Bouveret and Lema\^itre. We obtain an exact algorithm for any number of agents in this case. ",approximation algorithms compute maximin share allocations study problem compute maximin share guarantee recently introduce fairness notion give set agents set goods maximin share single agent best guarantee would allow partition goods way prefer bundle receive least desirable bundle objective problem find partition agent guarantee maximin share settings indivisible goods allocations guarantee exist resort approximation algorithms main result approximation run polynomial time number agents improve upon algorithm procaccia wang also produce approximation run polynomial time constant number agents achieve redesign certain part algorithm furthermore motivate apparent difficulty theoretically experimentally find lower bound existence approximate solutions undertake probabilistic analysis prove randomly generate instance high probability exist maximin share allocation see justification experimental evidence report relevant work finally provide positive result two special case arise previous work first one intrigue case agents already know exact maximin share allocations always exist contrary case agents provide approximation algorithm improve previously know result second case item value belong extend set study bouveret lema itre obtain exact algorithm number agents case,164,1,1503.00941.txt
http://arxiv.org/abs/1503.01239,Joint Active Learning with Feature Selection via CUR Matrix   Decomposition,"  This paper presents an unsupervised learning approach for simultaneous sample and feature selection, which is in contrast to existing works which mainly tackle these two problems separately. In fact the two tasks are often interleaved with each other: noisy and high-dimensional features will bring adverse effect on sample selection, while informative or representative samples will be beneficial to feature selection. Specifically, we propose a framework to jointly conduct active learning and feature selection based on the CUR matrix decomposition. From the data reconstruction perspective, both the selected samples and features can best approximate the original dataset respectively, such that the selected samples characterized by the features are highly representative. In particular, our method runs in one-shot without the procedure of iterative sample selection for progressive labeling. Thus, our model is especially suitable when there are few labeled samples or even in the absence of supervision, which is a particular challenge for existing methods. As the joint learning problem is NP-hard, the proposed formulation involves a convex but non-smooth optimization problem. We solve it efficiently by an iterative algorithm, and prove its global convergence. Experimental results on publicly available datasets corroborate the efficacy of our method compared with the state-of-the-art. ",Computer Science - Machine Learning ; ,"Li, Changsheng ; Wang, Xiangfeng ; Dong, Weishan ; Yan, Junchi ; Liu, Qingshan ; Zha, Hongyuan ; ","Joint Active Learning with Feature Selection via CUR Matrix   Decomposition  This paper presents an unsupervised learning approach for simultaneous sample and feature selection, which is in contrast to existing works which mainly tackle these two problems separately. In fact the two tasks are often interleaved with each other: noisy and high-dimensional features will bring adverse effect on sample selection, while informative or representative samples will be beneficial to feature selection. Specifically, we propose a framework to jointly conduct active learning and feature selection based on the CUR matrix decomposition. From the data reconstruction perspective, both the selected samples and features can best approximate the original dataset respectively, such that the selected samples characterized by the features are highly representative. In particular, our method runs in one-shot without the procedure of iterative sample selection for progressive labeling. Thus, our model is especially suitable when there are few labeled samples or even in the absence of supervision, which is a particular challenge for existing methods. As the joint learning problem is NP-hard, the proposed formulation involves a convex but non-smooth optimization problem. We solve it efficiently by an iterative algorithm, and prove its global convergence. Experimental results on publicly available datasets corroborate the efficacy of our method compared with the state-of-the-art. ",joint active learn feature selection via cur matrix decomposition paper present unsupervised learn approach simultaneous sample feature selection contrast exist work mainly tackle two problems separately fact two task often interleave noisy high dimensional feature bring adverse effect sample selection informative representative sample beneficial feature selection specifically propose framework jointly conduct active learn feature selection base cur matrix decomposition data reconstruction perspective select sample feature best approximate original dataset respectively select sample characterize feature highly representative particular method run one shoot without procedure iterative sample selection progressive label thus model especially suitable label sample even absence supervision particular challenge exist methods joint learn problem np hard propose formulation involve convex non smooth optimization problem solve efficiently iterative algorithm prove global convergence experimental result publicly available datasets corroborate efficacy method compare state art,132,12,1503.01239.txt
http://arxiv.org/abs/1503.01334,Faster quantum mixing for slowly evolving sequences of Markov chains,"  Markov chain methods are remarkably successful in computational physics, machine learning, and combinatorial optimization. The cost of such methods often reduces to the mixing time, i.e., the time required to reach the steady state of the Markov chain, which scales as $\delta^{-1}$, the inverse of the spectral gap. It has long been conjectured that quantum computers offer nearly generic quadratic improvements for mixing problems. However, except in special cases, quantum algorithms achieve a run-time of $\mathcal{O}(\sqrt{\delta^{-1}} \sqrt{N})$, which introduces a costly dependence on the Markov chain size $N,$ not present in the classical case. Here, we re-address the problem of mixing of Markov chains when these form a slowly evolving sequence. This setting is akin to the simulated annealing setting and is commonly encountered in physics, material sciences and machine learning. We provide a quantum memory-efficient algorithm with a run-time of $\mathcal{O}(\sqrt{\delta^{-1}} \sqrt[4]{N})$, neglecting logarithmic terms, which is an important improvement for large state spaces. Moreover, our algorithms output quantum encodings of distributions, which has advantages over classical outputs. Finally, we discuss the run-time bounds of mixing algorithms and show that, under certain assumptions, our algorithms are optimal. ",Quantum Physics ; Computer Science - Artificial Intelligence ; Computer Science - Data Structures and Algorithms ; ,"Orsucci, Davide ; Briegel, Hans J. ; Dunjko, Vedran ; ","Faster quantum mixing for slowly evolving sequences of Markov chains  Markov chain methods are remarkably successful in computational physics, machine learning, and combinatorial optimization. The cost of such methods often reduces to the mixing time, i.e., the time required to reach the steady state of the Markov chain, which scales as $\delta^{-1}$, the inverse of the spectral gap. It has long been conjectured that quantum computers offer nearly generic quadratic improvements for mixing problems. However, except in special cases, quantum algorithms achieve a run-time of $\mathcal{O}(\sqrt{\delta^{-1}} \sqrt{N})$, which introduces a costly dependence on the Markov chain size $N,$ not present in the classical case. Here, we re-address the problem of mixing of Markov chains when these form a slowly evolving sequence. This setting is akin to the simulated annealing setting and is commonly encountered in physics, material sciences and machine learning. We provide a quantum memory-efficient algorithm with a run-time of $\mathcal{O}(\sqrt{\delta^{-1}} \sqrt[4]{N})$, neglecting logarithmic terms, which is an important improvement for large state spaces. Moreover, our algorithms output quantum encodings of distributions, which has advantages over classical outputs. Finally, we discuss the run-time bounds of mixing algorithms and show that, under certain assumptions, our algorithms are optimal. ",faster quantum mix slowly evolve sequence markov chain markov chain methods remarkably successful computational physics machine learn combinatorial optimization cost methods often reduce mix time time require reach steady state markov chain scale delta inverse spectral gap long conjecture quantum computers offer nearly generic quadratic improvements mix problems however except special case quantum algorithms achieve run time mathcal sqrt delta sqrt introduce costly dependence markov chain size present classical case address problem mix markov chain form slowly evolve sequence set akin simulate anneal set commonly encounter physics material sciences machine learn provide quantum memory efficient algorithm run time mathcal sqrt delta sqrt neglect logarithmic term important improvement large state space moreover algorithms output quantum encode distributions advantage classical output finally discuss run time bound mix algorithms show certain assumptions algorithms optimal,131,11,1503.01334.txt
http://arxiv.org/abs/1503.01404,Complete intersection vanishing ideals on sets of clutter type over   finite fields,  In this paper we give a classification of complete intersection vanishing ideals on parameterized sets of clutter type over finite fields. ,"Mathematics - Commutative Algebra ; Computer Science - Information Theory ; Mathematics - Algebraic Geometry ; Mathematics - Combinatorics ; 14M10, 14G15, 13P25, 13P10, 11T71, 94B27, 94B05 ; ","Tochimani, Azucena ; Villarreal, Rafael H. ; ",Complete intersection vanishing ideals on sets of clutter type over   finite fields  In this paper we give a classification of complete intersection vanishing ideals on parameterized sets of clutter type over finite fields. ,complete intersection vanish ideals set clutter type finite field paper give classification complete intersection vanish ideals parameterized set clutter type finite field,22,2,1503.01404.txt
http://arxiv.org/abs/1503.01628,Minimal classes of graphs of unbounded clique-width defined by finitely   many forbidden induced subgraphs,"  We discover new hereditary classes of graphs that are minimal (with respect to set inclusion) of unbounded clique-width. The new examples include split permutation graphs and bichain graphs. Each of these classes is characterised by a finite list of minimal forbidden induced subgraphs. These, therefore, disprove a conjecture due to Daligault, Rao and Thomasse from 2010 claiming that all such minimal classes must be defined by infinitely many forbidden induced subgraphs.   In the same paper, Daligault, Rao and Thomasse make another conjecture that every hereditary class of unbounded clique-width must contain a labelled infinite antichain. We show that the two example classes we consider here satisfy this conjecture. Indeed, they each contain a canonical labelled infinite antichain, which leads us to propose a stronger conjecture: that every hereditary class of graphs that is minimal of unbounded clique-width contains a canonical labelled infinite antichain. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Atminas, A. ; Brignall, R. ; Lozin, V. ; Stacho, J. ; ","Minimal classes of graphs of unbounded clique-width defined by finitely   many forbidden induced subgraphs  We discover new hereditary classes of graphs that are minimal (with respect to set inclusion) of unbounded clique-width. The new examples include split permutation graphs and bichain graphs. Each of these classes is characterised by a finite list of minimal forbidden induced subgraphs. These, therefore, disprove a conjecture due to Daligault, Rao and Thomasse from 2010 claiming that all such minimal classes must be defined by infinitely many forbidden induced subgraphs.   In the same paper, Daligault, Rao and Thomasse make another conjecture that every hereditary class of unbounded clique-width must contain a labelled infinite antichain. We show that the two example classes we consider here satisfy this conjecture. Indeed, they each contain a canonical labelled infinite antichain, which leads us to propose a stronger conjecture: that every hereditary class of graphs that is minimal of unbounded clique-width contains a canonical labelled infinite antichain. ",minimal class graph unbounded clique width define finitely many forbid induce subgraphs discover new hereditary class graph minimal respect set inclusion unbounded clique width new examples include split permutation graph bichain graph class characterise finite list minimal forbid induce subgraphs therefore disprove conjecture due daligault rao thomasse claim minimal class must define infinitely many forbid induce subgraphs paper daligault rao thomasse make another conjecture every hereditary class unbounded clique width must contain label infinite antichain show two example class consider satisfy conjecture indeed contain canonical label infinite antichain lead us propose stronger conjecture every hereditary class graph minimal unbounded clique width contain canonical label infinite antichain,106,3,1503.01628.txt
http://arxiv.org/abs/1503.02196,Higher Weights of Affine Grassmann Codes and Their Duals,"  We consider the question of determining the higher weights or the generalized Hamming weights of affine Grassmann codes and their duals. Several initial as well as terminal higher weights of affine Grassmann codes of an arbitrary level are determined explicitly. In the case of duals of these codes, we give a formula for many initial as well as terminal higher weights. As a special case, we obtain an alternative simpler proof of the formula of Beelen et al for the minimum distance of the dual of an affine Grasmann code. ","Computer Science - Information Theory ; Mathematics - Combinatorics ; Primary 15A03, 11T06 05E99 Secondary 11T71 ; ","Datta, Mrinmoy ; Ghorpade, Sudhir R. ; ","Higher Weights of Affine Grassmann Codes and Their Duals  We consider the question of determining the higher weights or the generalized Hamming weights of affine Grassmann codes and their duals. Several initial as well as terminal higher weights of affine Grassmann codes of an arbitrary level are determined explicitly. In the case of duals of these codes, we give a formula for many initial as well as terminal higher weights. As a special case, we obtain an alternative simpler proof of the formula of Beelen et al for the minimum distance of the dual of an affine Grasmann code. ",higher weight affine grassmann cod duals consider question determine higher weight generalize ham weight affine grassmann cod duals several initial well terminal higher weight affine grassmann cod arbitrary level determine explicitly case duals cod give formula many initial well terminal higher weight special case obtain alternative simpler proof formula beelen et al minimum distance dual affine grasmann code,58,5,1503.02196.txt
http://arxiv.org/abs/1503.02577,New Algorithms for Computing a Single Component of the Discrete Fourier   Transform,"  This paper introduces the theory and hardware implementation of two new algorithms for computing a single component of the discrete Fourier transform. In terms of multiplicative complexity, both algorithms are more efficient, in general, than the well known Goertzel Algorithm. ",Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Electrical Engineering and Systems Science - Signal Processing ; Statistics - Methodology ; ,"Silva Jr., G. Jerônimo da ; de Souza, R. M. Campello ; de Oliveira, H. M. ; ","New Algorithms for Computing a Single Component of the Discrete Fourier   Transform  This paper introduces the theory and hardware implementation of two new algorithms for computing a single component of the discrete Fourier transform. In terms of multiplicative complexity, both algorithms are more efficient, in general, than the well known Goertzel Algorithm. ",new algorithms compute single component discrete fourier transform paper introduce theory hardware implementation two new algorithms compute single component discrete fourier transform term multiplicative complexity algorithms efficient general well know goertzel algorithm,32,7,1503.02577.txt
http://arxiv.org/abs/1503.02951,Mean Field Games in Nudge Systems for Societal Networks,"  We consider the general problem of resource sharing in societal networks, consisting of interconnected communication, transportation, energy and other networks important to the functioning of society. Participants in such network need to take decisions daily, both on the quantity of resources to use as well as the periods of usage. With this in mind, we discuss the problem of incentivizing users to behave in such a way that society as a whole benefits. In order to perceive societal level impact, such incentives may take the form of rewarding users with lottery tickets based on good behavior, and periodically conducting a lottery to translate these tickets into real rewards. We will pose the user decision problem as a mean field game (MFG), and the incentives question as one of trying to select a good mean field equilibrium (MFE). In such a framework, each agent (a participant in the societal network) takes a decision based on an assumed distribution of actions of his/her competitors, and the incentives provided by the social planner. The system is said to be at MFE if the agent's action is a sample drawn from the assumed distribution. We will show the existence of such an MFE under different settings, and also illustrate how to choose an attractive equilibrium using as an example demand-response in energy networks. ",Computer Science - Computer Science and Game Theory ; ,"Li, Jian ; Xia, Bainan ; Geng, Xinbo ; Ming, Hao ; Shakkottai, Srinivas ; Subramanian, Vijay ; Xie, Le ; ","Mean Field Games in Nudge Systems for Societal Networks  We consider the general problem of resource sharing in societal networks, consisting of interconnected communication, transportation, energy and other networks important to the functioning of society. Participants in such network need to take decisions daily, both on the quantity of resources to use as well as the periods of usage. With this in mind, we discuss the problem of incentivizing users to behave in such a way that society as a whole benefits. In order to perceive societal level impact, such incentives may take the form of rewarding users with lottery tickets based on good behavior, and periodically conducting a lottery to translate these tickets into real rewards. We will pose the user decision problem as a mean field game (MFG), and the incentives question as one of trying to select a good mean field equilibrium (MFE). In such a framework, each agent (a participant in the societal network) takes a decision based on an assumed distribution of actions of his/her competitors, and the incentives provided by the social planner. The system is said to be at MFE if the agent's action is a sample drawn from the assumed distribution. We will show the existence of such an MFE under different settings, and also illustrate how to choose an attractive equilibrium using as an example demand-response in energy networks. ",mean field game nudge systems societal network consider general problem resource share societal network consist interconnect communication transportation energy network important function society participants network need take decisions daily quantity resources use well periods usage mind discuss problem incentivizing users behave way society whole benefit order perceive societal level impact incentives may take form reward users lottery ticket base good behavior periodically conduct lottery translate ticket real reward pose user decision problem mean field game mfg incentives question one try select good mean field equilibrium mfe framework agent participant societal network take decision base assume distribution action competitors incentives provide social planner system say mfe agent action sample draw assume distribution show existence mfe different settings also illustrate choose attractive equilibrium use example demand response energy network,127,6,1503.02951.txt
http://arxiv.org/abs/1503.02985,SybilFrame: A Defense-in-Depth Framework for Structure-Based Sybil   Detection,"  Sybil attacks are becoming increasingly widespread, and pose a significant threat to online social systems; a single adversary can inject multiple colluding identities in the system to compromise security and privacy. Recent works have leveraged the use of social network-based trust relationships to defend against Sybil attacks. However, existing defenses are based on oversimplified assumptions, which do not hold in real world social graphs. In this work, we propose SybilFrame, a defense-in-depth framework for mitigating the problem of Sybil attacks when the oversimplified assumptions are relaxed. Our framework is able to incorporate prior information about users and edges in the social graph. We validate our framework on synthetic and real world network topologies, including a large-scale Twitter dataset with 20M nodes and 265M edges, and demonstrate that our scheme performs an order of magnitude better than previous structure-based approaches. ",Computer Science - Social and Information Networks ; Computer Science - Cryptography and Security ; ,"Gao, Peng ; Gong, Neil Zhenqiang ; Kulkarni, Sanjeev ; Thomas, Kurt ; Mittal, Prateek ; ","SybilFrame: A Defense-in-Depth Framework for Structure-Based Sybil   Detection  Sybil attacks are becoming increasingly widespread, and pose a significant threat to online social systems; a single adversary can inject multiple colluding identities in the system to compromise security and privacy. Recent works have leveraged the use of social network-based trust relationships to defend against Sybil attacks. However, existing defenses are based on oversimplified assumptions, which do not hold in real world social graphs. In this work, we propose SybilFrame, a defense-in-depth framework for mitigating the problem of Sybil attacks when the oversimplified assumptions are relaxed. Our framework is able to incorporate prior information about users and edges in the social graph. We validate our framework on synthetic and real world network topologies, including a large-scale Twitter dataset with 20M nodes and 265M edges, and demonstrate that our scheme performs an order of magnitude better than previous structure-based approaches. ",sybilframe defense depth framework structure base sybil detection sybil attack become increasingly widespread pose significant threat online social systems single adversary inject multiple collude identities system compromise security privacy recent work leverage use social network base trust relationships defend sybil attack however exist defenses base oversimplify assumptions hold real world social graph work propose sybilframe defense depth framework mitigate problem sybil attack oversimplify assumptions relax framework able incorporate prior information users edge social graph validate framework synthetic real world network topologies include large scale twitter dataset nod edge demonstrate scheme perform order magnitude better previous structure base approach,98,6,1503.02985.txt
http://arxiv.org/abs/1503.03169,"Dynamic Partitioning of Physical Memory Among Virtual Machines,   ASMI:Architectural Support for Memory Isolation","  Cloud computing relies on secure and efficient virtualization. Software level security solutions compromise the performance of virtual machines (VMs), as a large amount of computational power would be utilized for running the security modules. Moreover, software solutions are only as secure as the level that they work on. For example a security module on a hypervisor cannot provide security in the presence of an infected hypervisor. It is a challenge for virtualization technology architects to enhance the security of VMs without degrading their performance. Currently available server machines are not fully equipped to support a secure VM environment without compromising on performance. A few hardware modifications have been introduced by manufactures like Intel and AMD to provide a secure VM environment with low performance degradation. In this paper we propose a novel memory architecture model named \textit{ Architectural Support for Memory Isolation(ASMI)}, that can achieve a true isolated physical memory region to each VM without degrading performance. Along with true memory isolation, ASMI is designed to provide lower memory access times, better utilization of available memory, support for DMA isolation and support for platform independence for users of VMs. ",Computer Science - Hardware Architecture ; ,"R, Jithin ; Chandran, Priya ; ","Dynamic Partitioning of Physical Memory Among Virtual Machines,   ASMI:Architectural Support for Memory Isolation  Cloud computing relies on secure and efficient virtualization. Software level security solutions compromise the performance of virtual machines (VMs), as a large amount of computational power would be utilized for running the security modules. Moreover, software solutions are only as secure as the level that they work on. For example a security module on a hypervisor cannot provide security in the presence of an infected hypervisor. It is a challenge for virtualization technology architects to enhance the security of VMs without degrading their performance. Currently available server machines are not fully equipped to support a secure VM environment without compromising on performance. A few hardware modifications have been introduced by manufactures like Intel and AMD to provide a secure VM environment with low performance degradation. In this paper we propose a novel memory architecture model named \textit{ Architectural Support for Memory Isolation(ASMI)}, that can achieve a true isolated physical memory region to each VM without degrading performance. Along with true memory isolation, ASMI is designed to provide lower memory access times, better utilization of available memory, support for DMA isolation and support for platform independence for users of VMs. ",dynamic partition physical memory among virtual machine asmi architectural support memory isolation cloud compute rely secure efficient virtualization software level security solutions compromise performance virtual machine vms large amount computational power would utilize run security modules moreover software solutions secure level work example security module hypervisor cannot provide security presence infect hypervisor challenge virtualization technology architects enhance security vms without degrade performance currently available server machine fully equip support secure vm environment without compromise performance hardware modifications introduce manufacture like intel amd provide secure vm environment low performance degradation paper propose novel memory architecture model name textit architectural support memory isolation asmi achieve true isolate physical memory region vm without degrade performance along true memory isolation asmi design provide lower memory access time better utilization available memory support dma isolation support platform independence users vms,135,4,1503.03169.txt
http://arxiv.org/abs/1503.03185,Testing Randomness by Matching Pennies,"  In the game of Matching Pennies, Alice and Bob each hold a penny, and at every tick of the clock they simultaneously display the head or the tail sides of their coins. If they both display the same side, then Alice wins Bob's penny; if they display different sides, then Bob wins Alice's penny. To avoid giving the opponent a chance to win, both players seem to have nothing else to do but to randomly play heads and tails with equal frequencies. However, while not losing in this game is easy, not missing an opportunity to win is not. Randomizing your own moves can be made easy. Recognizing when the opponent's moves are not random can be arbitrarily hard.   The notion of randomness is central in game theory, but it is usually taken for granted. The notion of outsmarting is not central in game theory, but it is central in the practice of gaming. We pursue the idea that these two notions can be usefully viewed as two sides of the same coin. ","Computer Science - Computer Science and Game Theory ; 91A26, 68Q32 ; I.2.6 ; ","Pavlovic, Dusko ; Seidel, Peter-Michael ; Yahia, Muzamil ; ","Testing Randomness by Matching Pennies  In the game of Matching Pennies, Alice and Bob each hold a penny, and at every tick of the clock they simultaneously display the head or the tail sides of their coins. If they both display the same side, then Alice wins Bob's penny; if they display different sides, then Bob wins Alice's penny. To avoid giving the opponent a chance to win, both players seem to have nothing else to do but to randomly play heads and tails with equal frequencies. However, while not losing in this game is easy, not missing an opportunity to win is not. Randomizing your own moves can be made easy. Recognizing when the opponent's moves are not random can be arbitrarily hard.   The notion of randomness is central in game theory, but it is usually taken for granted. The notion of outsmarting is not central in game theory, but it is central in the practice of gaming. We pursue the idea that these two notions can be usefully viewed as two sides of the same coin. ",test randomness match pennies game match pennies alice bob hold penny every tick clock simultaneously display head tail side coin display side alice win bob penny display different side bob win alice penny avoid give opponent chance win players seem nothing else randomly play head tail equal frequencies however lose game easy miss opportunity win randomize move make easy recognize opponent move random arbitrarily hard notion randomness central game theory usually take grant notion outsmart central game theory central practice game pursue idea two notions usefully view two side coin,90,8,1503.03185.txt
http://arxiv.org/abs/1503.03605,An improved return-mapping scheme for nonsmooth yield surfaces: PART I -   the Haigh-Westergaard coordinates,"  The paper is devoted to the numerical solution of elastoplastic constitutive initial value problems. An improved form of the implicit return-mapping scheme for nonsmooth yield surfaces is proposed that systematically builds on a subdifferential formulation of the flow rule. The main advantage of this approach is that the treatment of singular points, such as apices or edges at which the flow direction is multivalued involves only a uniquely defined set of non-linear equations, similarly to smooth yield surfaces. This paper (PART I) is focused on isotropic models containing: $a)$ yield surfaces with one or two apices (singular points) laying on the hydrostatic axis; $b)$ plastic pseudo-potentials that are independent of the Lode angle; $c)$ nonlinear isotropic hardening (optionally). It is shown that for some models the improved integration scheme also enables to a priori decide about a type of the return and investigate existence, uniqueness and semismoothness of discretized constitutive operators in implicit form. Further, the semismooth Newton method is introduced to solve incremental boundary-value problems. The paper also contains numerical examples related to slope stability with available Matlab implementation. ","Computer Science - Computational Engineering, Finance, and Science ; ","Sysala, Stanislav ; Cermak, Martin ; Koudelka, Tomas ; Kruis, Jaroslav ; Zeman, Jan ; Blaheta, Radim ; ","An improved return-mapping scheme for nonsmooth yield surfaces: PART I -   the Haigh-Westergaard coordinates  The paper is devoted to the numerical solution of elastoplastic constitutive initial value problems. An improved form of the implicit return-mapping scheme for nonsmooth yield surfaces is proposed that systematically builds on a subdifferential formulation of the flow rule. The main advantage of this approach is that the treatment of singular points, such as apices or edges at which the flow direction is multivalued involves only a uniquely defined set of non-linear equations, similarly to smooth yield surfaces. This paper (PART I) is focused on isotropic models containing: $a)$ yield surfaces with one or two apices (singular points) laying on the hydrostatic axis; $b)$ plastic pseudo-potentials that are independent of the Lode angle; $c)$ nonlinear isotropic hardening (optionally). It is shown that for some models the improved integration scheme also enables to a priori decide about a type of the return and investigate existence, uniqueness and semismoothness of discretized constitutive operators in implicit form. Further, the semismooth Newton method is introduced to solve incremental boundary-value problems. The paper also contains numerical examples related to slope stability with available Matlab implementation. ",improve return map scheme nonsmooth yield surface part haigh westergaard coordinate paper devote numerical solution elastoplastic constitutive initial value problems improve form implicit return map scheme nonsmooth yield surface propose systematically build subdifferential formulation flow rule main advantage approach treatment singular point apices edge flow direction multivalued involve uniquely define set non linear equations similarly smooth yield surface paper part focus isotropic model contain yield surface one two apices singular point lay hydrostatic axis plastic pseudo potentials independent lode angle nonlinear isotropic harden optionally show model improve integration scheme also enable priori decide type return investigate existence uniqueness semismoothness discretized constitutive operators implicit form semismooth newton method introduce solve incremental boundary value problems paper also contain numerical examples relate slope stability available matlab implementation,124,8,1503.03605.txt
http://arxiv.org/abs/1503.04099,Algorithms and complexity for Turaev-Viro invariants,"  The Turaev-Viro invariants are a powerful family of topological invariants for distinguishing between different 3-manifolds. They are invaluable for mathematical software, but current algorithms to compute them require exponential time.   The invariants are parameterised by an integer $r \geq 3$. We resolve the question of complexity for $r=3$ and $r=4$, giving simple proofs that computing Turaev-Viro invariants for $r=3$ is polynomial time, but for $r=4$ is \#P-hard. Moreover, we give an explicit fixed-parameter tractable algorithm for arbitrary $r$, and show through concrete implementation and experimentation that this algorithm is practical---and indeed preferable---to the prior state of the art for real computation. ","Mathematics - Geometric Topology ; Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; Computer Science - Mathematical Software ; 57M27, 57Q15, 68Q17 ; F.2.2 ; G.2.1 ; G.4 ; ","Burton, Benjamin A. ; Maria, Clément ; Spreer, Jonathan ; ","Algorithms and complexity for Turaev-Viro invariants  The Turaev-Viro invariants are a powerful family of topological invariants for distinguishing between different 3-manifolds. They are invaluable for mathematical software, but current algorithms to compute them require exponential time.   The invariants are parameterised by an integer $r \geq 3$. We resolve the question of complexity for $r=3$ and $r=4$, giving simple proofs that computing Turaev-Viro invariants for $r=3$ is polynomial time, but for $r=4$ is \#P-hard. Moreover, we give an explicit fixed-parameter tractable algorithm for arbitrary $r$, and show through concrete implementation and experimentation that this algorithm is practical---and indeed preferable---to the prior state of the art for real computation. ",algorithms complexity turaev viro invariants turaev viro invariants powerful family topological invariants distinguish different manifold invaluable mathematical software current algorithms compute require exponential time invariants parameterised integer geq resolve question complexity give simple proof compute turaev viro invariants polynomial time hard moreover give explicit fix parameter tractable algorithm arbitrary show concrete implementation experimentation algorithm practical indeed preferable prior state art real computation,62,4,1503.04099.txt
http://arxiv.org/abs/1503.04424,Bridging Social Media via Distant Supervision,"  Microblog classification has received a lot of attention in recent years. Different classification tasks have been investigated, most of them focusing on classifying microblogs into a small number of classes (five or less) using a training set of manually annotated tweets. Unfortunately, labelling data is tedious and expensive, and finding tweets that cover all the classes of interest is not always straightforward, especially when some of the classes do not frequently arise in practice. In this paper we study an approach to tweet classification based on distant supervision, whereby we automatically transfer labels from one social medium to another for a single-label multi-class classification task. In particular, we apply YouTube video classes to tweets linking to these videos. This provides for free a virtually unlimited number of labelled instances that can be used as training data. The classification experiments we have run show that training a tweet classifier via these automatically labelled data achieves substantially better performance than training the same classifier with a limited amount of manually labelled data; this is advantageous, given that the automatically labelled data come at no cost. Further investigation of our approach shows its robustness when applied with different numbers of classes and across different languages. ",Computer Science - Information Retrieval ; ,"Magdy, Walid ; Sajjad, Hassan ; El-Ganainy, Tarek ; Sebastiani, Fabrizio ; ","Bridging Social Media via Distant Supervision  Microblog classification has received a lot of attention in recent years. Different classification tasks have been investigated, most of them focusing on classifying microblogs into a small number of classes (five or less) using a training set of manually annotated tweets. Unfortunately, labelling data is tedious and expensive, and finding tweets that cover all the classes of interest is not always straightforward, especially when some of the classes do not frequently arise in practice. In this paper we study an approach to tweet classification based on distant supervision, whereby we automatically transfer labels from one social medium to another for a single-label multi-class classification task. In particular, we apply YouTube video classes to tweets linking to these videos. This provides for free a virtually unlimited number of labelled instances that can be used as training data. The classification experiments we have run show that training a tweet classifier via these automatically labelled data achieves substantially better performance than training the same classifier with a limited amount of manually labelled data; this is advantageous, given that the automatically labelled data come at no cost. Further investigation of our approach shows its robustness when applied with different numbers of classes and across different languages. ",bridge social media via distant supervision microblog classification receive lot attention recent years different classification task investigate focus classify microblogs small number class five less use train set manually annotate tweet unfortunately label data tedious expensive find tweet cover class interest always straightforward especially class frequently arise practice paper study approach tweet classification base distant supervision whereby automatically transfer label one social medium another single label multi class classification task particular apply youtube video class tweet link videos provide free virtually unlimited number label instance use train data classification experiment run show train tweet classifier via automatically label data achieve substantially better performance train classifier limit amount manually label data advantageous give automatically label data come cost investigation approach show robustness apply different number class across different languages,128,10,1503.04424.txt
http://arxiv.org/abs/1503.04500,A Residual Based Sparse Approximate Inverse Preconditioning Procedure   for Large Sparse Linear Systems,"  The SPAI algorithm, a sparse approximate inverse preconditioning technique for large sparse linear systems, proposed by Grote and Huckle [SIAM J. Sci. Comput., 18 (1997), pp.~838--853.], is based on the F-norm minimization and computes a sparse approximate inverse $M$ of a large sparse matrix $A$ adaptively. However, SPAI may be costly to seek the most profitable indices at each loop and $M$ may be ineffective for preconditioning. In this paper, we propose a residual based sparse approximate inverse preconditioning procedure (RSAI), which, unlike SPAI, is based on only the {\em dominant} rather than all information on the current residual and augments sparsity patterns adaptively during the loops. RSAI is less costly to seek indices and is more effective to capture a good approximate sparsity pattern of $A^{-1}$ than SPAI. To control the sparsity of $M$ and reduce computational cost, we develop a practical RSAI($tol$) algorithm that drops small nonzero entries adaptively during the process. Numerical experiments are reported to demonstrate that RSAI($tol$) is at least competitive with SPAI and can be considerably more efficient and effective than SPAI. They also indicate that RSAI($tol$) is comparable to the PSAI($tol$) algorithm proposed by one of the authors in 2009. ",Mathematics - Numerical Analysis ; Computer Science - Numerical Analysis ; 65F10 ; ,"Jia, Zhongxiao ; Kang, Wenjie ; ","A Residual Based Sparse Approximate Inverse Preconditioning Procedure   for Large Sparse Linear Systems  The SPAI algorithm, a sparse approximate inverse preconditioning technique for large sparse linear systems, proposed by Grote and Huckle [SIAM J. Sci. Comput., 18 (1997), pp.~838--853.], is based on the F-norm minimization and computes a sparse approximate inverse $M$ of a large sparse matrix $A$ adaptively. However, SPAI may be costly to seek the most profitable indices at each loop and $M$ may be ineffective for preconditioning. In this paper, we propose a residual based sparse approximate inverse preconditioning procedure (RSAI), which, unlike SPAI, is based on only the {\em dominant} rather than all information on the current residual and augments sparsity patterns adaptively during the loops. RSAI is less costly to seek indices and is more effective to capture a good approximate sparsity pattern of $A^{-1}$ than SPAI. To control the sparsity of $M$ and reduce computational cost, we develop a practical RSAI($tol$) algorithm that drops small nonzero entries adaptively during the process. Numerical experiments are reported to demonstrate that RSAI($tol$) is at least competitive with SPAI and can be considerably more efficient and effective than SPAI. They also indicate that RSAI($tol$) is comparable to the PSAI($tol$) algorithm proposed by one of the authors in 2009. ",residual base sparse approximate inverse precondition procedure large sparse linear systems spai algorithm sparse approximate inverse precondition technique large sparse linear systems propose grote huckle siam sci comput pp base norm minimization compute sparse approximate inverse large sparse matrix adaptively however spai may costly seek profitable indices loop may ineffective precondition paper propose residual base sparse approximate inverse precondition procedure rsai unlike spai base em dominant rather information current residual augment sparsity pattern adaptively loop rsai less costly seek indices effective capture good approximate sparsity pattern spai control sparsity reduce computational cost develop practical rsai tol algorithm drop small nonzero entries adaptively process numerical experiment report demonstrate rsai tol least competitive spai considerably efficient effective spai also indicate rsai tol comparable psai tol algorithm propose one author,127,9,1503.04500.txt
http://arxiv.org/abs/1503.04522,Really Natural Linear Indexed Type Checking,"  Recent works have shown the power of linear indexed type systems for enforcing complex program properties. These systems combine linear types with a language of type-level indices, allowing more fine-grained analyses. Such systems have been fruitfully applied in diverse domains, including implicit complexity and differential privacy. A natural way to enhance the expressiveness of this approach is by allowing the indices to depend on runtime information, in the spirit of dependent types. This approach is used in DFuzz, a language for differential privacy. The DFuzz type system relies on an index language supporting real and natural number arithmetic over constants and variables. Moreover, DFuzz uses a subtyping mechanism to make types more flexible. By themselves, linearity, dependency, and subtyping each require delicate handling when performing type checking or type inference; their combination increases this challenge substantially, as the features can interact in non-trivial ways. In this paper, we study the type-checking problem for DFuzz. We show how we can reduce type checking for (a simple extension of) DFuzz to constraint solving over a first-order theory of naturals and real numbers which, although undecidable, can often be handled in practice by standard numeric solvers. ",Computer Science - Logic in Computer Science ; ,"de Amorim, Arthur Azevedo ; Arias, Emilio Jesús Gallego ; Gaboardi, Marco ; Hsu, Justin ; ","Really Natural Linear Indexed Type Checking  Recent works have shown the power of linear indexed type systems for enforcing complex program properties. These systems combine linear types with a language of type-level indices, allowing more fine-grained analyses. Such systems have been fruitfully applied in diverse domains, including implicit complexity and differential privacy. A natural way to enhance the expressiveness of this approach is by allowing the indices to depend on runtime information, in the spirit of dependent types. This approach is used in DFuzz, a language for differential privacy. The DFuzz type system relies on an index language supporting real and natural number arithmetic over constants and variables. Moreover, DFuzz uses a subtyping mechanism to make types more flexible. By themselves, linearity, dependency, and subtyping each require delicate handling when performing type checking or type inference; their combination increases this challenge substantially, as the features can interact in non-trivial ways. In this paper, we study the type-checking problem for DFuzz. We show how we can reduce type checking for (a simple extension of) DFuzz to constraint solving over a first-order theory of naturals and real numbers which, although undecidable, can often be handled in practice by standard numeric solvers. ",really natural linear index type check recent work show power linear index type systems enforce complex program properties systems combine linear type language type level indices allow fine grain analyse systems fruitfully apply diverse domains include implicit complexity differential privacy natural way enhance expressiveness approach allow indices depend runtime information spirit dependent type approach use dfuzz language differential privacy dfuzz type system rely index language support real natural number arithmetic constants variables moreover dfuzz use subtyping mechanism make type flexible linearity dependency subtyping require delicate handle perform type check type inference combination increase challenge substantially feature interact non trivial ways paper study type check problem dfuzz show reduce type check simple extension dfuzz constraint solve first order theory naturals real number although undecidable often handle practice standard numeric solvers,129,8,1503.04522.txt
http://arxiv.org/abs/1503.05496,IMP with exceptions over decorated logic,"  In this paper, we facilitate the reasoning about impure programming languages, by annotating terms with `decorations' that describe what computational (side) effect evaluation of a term may involve. In a point-free categorical language,called the `decorated logic', we formalize the mutable state and the exception effects first separately, exploiting anice duality between them, and then combined. The combined decorated logic is used as the target language forthe denotational semantics of the IMP+Exc imperative programming language, and allows us to prove equivalencesbetween programs written in IMP+Exc. The combined logic is encoded in Coq, and this encoding is used to certifysome program equivalence proofs. ",Computer Science - Logic in Computer Science ; ,"Ekici, Burak ; ","IMP with exceptions over decorated logic  In this paper, we facilitate the reasoning about impure programming languages, by annotating terms with `decorations' that describe what computational (side) effect evaluation of a term may involve. In a point-free categorical language,called the `decorated logic', we formalize the mutable state and the exception effects first separately, exploiting anice duality between them, and then combined. The combined decorated logic is used as the target language forthe denotational semantics of the IMP+Exc imperative programming language, and allows us to prove equivalencesbetween programs written in IMP+Exc. The combined logic is encoded in Coq, and this encoding is used to certifysome program equivalence proofs. ",imp exceptions decorate logic paper facilitate reason impure program languages annotate term decorations describe computational side effect evaluation term may involve point free categorical language call decorate logic formalize mutable state exception effect first separately exploit anice duality combine combine decorate logic use target language forthe denotational semantics imp exc imperative program language allow us prove equivalencesbetween program write imp exc combine logic encode coq encode use certifysome program equivalence proof,71,8,1503.05496.txt
http://arxiv.org/abs/1503.05656,Cost-Effective Conceptual Design Using Taxonomies,"  It is known that annotating named entities in unstructured and semi-structured data sets by their concepts improves the effectiveness of answering queries over these data sets. As every enterprise has a limited budget of time or computational resources, it has to annotate a subset of concepts in a given domain whose costs of annotation do not exceed the budget. We call such a subset of concepts a {\it conceptual design} for the annotated data set. We focus on finding a conceptual design that provides the most effective answers to queries over the annotated data set, i.e., a {\it cost-effective conceptual design}. Since, it is often less time-consuming and costly to annotate general concepts than specific concepts, we use information on superclass/subclass relationships between concepts in taxonomies to find a cost-effective conceptual design. We quantify the amount by which a conceptual design with concepts from a taxonomy improves the effectiveness of answering queries over an annotated data set. If the taxonomy is a tree, we prove that the problem is NP-hard and propose an efficient approximation and pseudo-polynomial time algorithms for the problem. We further prove that if the taxonomy is a directed acyclic graph, given some generally accepted hypothesis, it is not possible to find any approximation algorithm with reasonably small approximation ratio for the problem. Our empirical study using real-world data sets, taxonomies, and query workloads shows that our framework effectively quantifies the amount by which a conceptual design improves the effectiveness of answering queries. It also indicates that our algorithms are efficient for a design-time task with pseudo-polynomial algorithm being generally more effective than the approximation algorithm. ",Computer Science - Databases ; ,"Vakilian, Ali ; Chodpathumwan, Yodsawalai ; Termehchy, Arash ; Nayyeri, Amir ; ","Cost-Effective Conceptual Design Using Taxonomies  It is known that annotating named entities in unstructured and semi-structured data sets by their concepts improves the effectiveness of answering queries over these data sets. As every enterprise has a limited budget of time or computational resources, it has to annotate a subset of concepts in a given domain whose costs of annotation do not exceed the budget. We call such a subset of concepts a {\it conceptual design} for the annotated data set. We focus on finding a conceptual design that provides the most effective answers to queries over the annotated data set, i.e., a {\it cost-effective conceptual design}. Since, it is often less time-consuming and costly to annotate general concepts than specific concepts, we use information on superclass/subclass relationships between concepts in taxonomies to find a cost-effective conceptual design. We quantify the amount by which a conceptual design with concepts from a taxonomy improves the effectiveness of answering queries over an annotated data set. If the taxonomy is a tree, we prove that the problem is NP-hard and propose an efficient approximation and pseudo-polynomial time algorithms for the problem. We further prove that if the taxonomy is a directed acyclic graph, given some generally accepted hypothesis, it is not possible to find any approximation algorithm with reasonably small approximation ratio for the problem. Our empirical study using real-world data sets, taxonomies, and query workloads shows that our framework effectively quantifies the amount by which a conceptual design improves the effectiveness of answering queries. It also indicates that our algorithms are efficient for a design-time task with pseudo-polynomial algorithm being generally more effective than the approximation algorithm. ",cost effective conceptual design use taxonomies know annotate name entities unstructured semi structure data set concepts improve effectiveness answer query data set every enterprise limit budget time computational resources annotate subset concepts give domain whose cost annotation exceed budget call subset concepts conceptual design annotate data set focus find conceptual design provide effective answer query annotate data set cost effective conceptual design since often less time consume costly annotate general concepts specific concepts use information superclass subclass relationships concepts taxonomies find cost effective conceptual design quantify amount conceptual design concepts taxonomy improve effectiveness answer query annotate data set taxonomy tree prove problem np hard propose efficient approximation pseudo polynomial time algorithms problem prove taxonomy direct acyclic graph give generally accept hypothesis possible find approximation algorithm reasonably small approximation ratio problem empirical study use real world data set taxonomies query workloads show framework effectively quantify amount conceptual design improve effectiveness answer query also indicate algorithms efficient design time task pseudo polynomial algorithm generally effective approximation algorithm,165,1,1503.05656.txt
http://arxiv.org/abs/1503.06126,Polynomial complexity recognizing a tropical linear variety,  A polynomial complexity algorithm is designed which tests whether a point belongs to a given tropical linear variety. ,Computer Science - Symbolic Computation ; Mathematics - Algebraic Geometry ; 15T05 ; I.1.2 ; ,"Grigoriev, Dima ; ",Polynomial complexity recognizing a tropical linear variety  A polynomial complexity algorithm is designed which tests whether a point belongs to a given tropical linear variety. ,polynomial complexity recognize tropical linear variety polynomial complexity algorithm design test whether point belong give tropical linear variety,18,8,1503.06126.txt
http://arxiv.org/abs/1503.06483,Construction of FuzzyFind Dictionary using Golay Coding Transformation   for Searching Applications,"  Searching through a large volume of data is very critical for companies, scientists, and searching engines applications due to time complexity and memory complexity. In this paper, a new technique of generating FuzzyFind Dictionary for text mining was introduced. We simply mapped the 23 bits of the English alphabet into a FuzzyFind Dictionary or more than 23 bits by using more FuzzyFind Dictionary, and reflecting the presence or absence of particular letters. This representation preserves closeness of word distortions in terms of closeness of the created binary vectors within Hamming distance of 2 deviations. This paper talks about the Golay Coding Transformation Hash Table and how it can be used on a FuzzyFind Dictionary as a new technology for using in searching through big data. This method is introduced by linear time complexity for generating the dictionary and constant time complexity to access the data and update by new data sets, also updating for new data sets is linear time depends on new data points. This technique is based on searching only for letters of English that each segment has 23 bits, and also we have more than 23-bit and also it could work with more segments as reference table. ",Computer Science - Databases ; Computer Science - Artificial Intelligence ; Computer Science - Data Structures and Algorithms ; Computer Science - Information Retrieval ; Computer Science - Machine Learning ; ,"Kowsari, Kamran ; Yammahi, Maryam ; Bari, Nima ; Vichr, Roman ; Alsaby, Faisal ; Berkovich, Simon Y. ; ","Construction of FuzzyFind Dictionary using Golay Coding Transformation   for Searching Applications  Searching through a large volume of data is very critical for companies, scientists, and searching engines applications due to time complexity and memory complexity. In this paper, a new technique of generating FuzzyFind Dictionary for text mining was introduced. We simply mapped the 23 bits of the English alphabet into a FuzzyFind Dictionary or more than 23 bits by using more FuzzyFind Dictionary, and reflecting the presence or absence of particular letters. This representation preserves closeness of word distortions in terms of closeness of the created binary vectors within Hamming distance of 2 deviations. This paper talks about the Golay Coding Transformation Hash Table and how it can be used on a FuzzyFind Dictionary as a new technology for using in searching through big data. This method is introduced by linear time complexity for generating the dictionary and constant time complexity to access the data and update by new data sets, also updating for new data sets is linear time depends on new data points. This technique is based on searching only for letters of English that each segment has 23 bits, and also we have more than 23-bit and also it could work with more segments as reference table. ",construction fuzzyfind dictionary use golay cod transformation search applications search large volume data critical company scientists search engines applications due time complexity memory complexity paper new technique generate fuzzyfind dictionary text mine introduce simply map bits english alphabet fuzzyfind dictionary bits use fuzzyfind dictionary reflect presence absence particular letter representation preserve closeness word distortions term closeness create binary vectors within ham distance deviations paper talk golay cod transformation hash table use fuzzyfind dictionary new technology use search big data method introduce linear time complexity generate dictionary constant time complexity access data update new data set also update new data set linear time depend new data point technique base search letter english segment bits also bite also could work segment reference table,121,10,1503.06483.txt
http://arxiv.org/abs/1503.06822,Tree spanners of bounded degree graphs,"  A tree $t$-spanner of a graph $G$ is a spanning tree of $G$ such that the distance between pairs of vertices in the tree is at most $t$ times their distance in $G$. Deciding tree $t$-spanner admissible graphs has been proved to be tractable for $t<3$ and NP-complete for $t>3$, while the complexity status of this problem is unresolved when $t=3$. For every $t>2$ and $b>0$, an efficient dynamic programming algorithm to decide tree $t$-spanner admissibility of graphs with vertex degrees less than $b$ is presented. Only for $t=3$, the algorithm remains efficient, when graphs $G$ with degrees less than $b\log |V(G)|$ are examined. ",Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; ,"Papoutsakis, Ioannis ; ","Tree spanners of bounded degree graphs  A tree $t$-spanner of a graph $G$ is a spanning tree of $G$ such that the distance between pairs of vertices in the tree is at most $t$ times their distance in $G$. Deciding tree $t$-spanner admissible graphs has been proved to be tractable for $t<3$ and NP-complete for $t>3$, while the complexity status of this problem is unresolved when $t=3$. For every $t>2$ and $b>0$, an efficient dynamic programming algorithm to decide tree $t$-spanner admissibility of graphs with vertex degrees less than $b$ is presented. Only for $t=3$, the algorithm remains efficient, when graphs $G$ with degrees less than $b\log |V(G)|$ are examined. ",tree spanners bound degree graph tree spanner graph span tree distance pair vertices tree time distance decide tree spanner admissible graph prove tractable np complete complexity status problem unresolved every efficient dynamic program algorithm decide tree spanner admissibility graph vertex degrees less present algorithm remain efficient graph degrees less log examine,51,3,1503.06822.txt
http://arxiv.org/abs/1503.07759,Large-scale Biological Meta-database Management,"  Up-to-date meta-databases are vital for the analysis of biological data. However,the current exponential increase in biological data leads to exponentially increasing meta-database sizes. Large-scale meta-database management is therefore an important challenge for production platforms providing services for biological data analysis. In particular, there is often a need either to run an analysis with a particular version of a meta-database, or to rerun an analysis with an updated meta-database. We present our GeStore approach for biological meta-database management. It provides efficient storage and runtime generation of specific meta-database versions, and efficient incremental updates for biological data analysis tools. The approach is transparent to the tools, and we provide a framework that makes it easy to integrate GeStore with biological data analysis frameworks. We present the GeStore system, an evaluation of the performance characteristics of the system, and an evaluation of the benefits for a biological data analysis workflow. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Databases ; ","Pedersen, Edvard ; Bongo, Lars Ailo ; ","Large-scale Biological Meta-database Management  Up-to-date meta-databases are vital for the analysis of biological data. However,the current exponential increase in biological data leads to exponentially increasing meta-database sizes. Large-scale meta-database management is therefore an important challenge for production platforms providing services for biological data analysis. In particular, there is often a need either to run an analysis with a particular version of a meta-database, or to rerun an analysis with an updated meta-database. We present our GeStore approach for biological meta-database management. It provides efficient storage and runtime generation of specific meta-database versions, and efficient incremental updates for biological data analysis tools. The approach is transparent to the tools, and we provide a framework that makes it easy to integrate GeStore with biological data analysis frameworks. We present the GeStore system, an evaluation of the performance characteristics of the system, and an evaluation of the benefits for a biological data analysis workflow. ",large scale biological meta database management date meta databases vital analysis biological data however current exponential increase biological data lead exponentially increase meta database size large scale meta database management therefore important challenge production platforms provide service biological data analysis particular often need either run analysis particular version meta database rerun analysis update meta database present gestore approach biological meta database management provide efficient storage runtime generation specific meta database versions efficient incremental update biological data analysis tool approach transparent tool provide framework make easy integrate gestore biological data analysis frameworks present gestore system evaluation performance characteristics system evaluation benefit biological data analysis workflow,104,10,1503.07759.txt
http://arxiv.org/abs/1503.08370,Global Bandits,"  Multi-armed bandits (MAB) model sequential decision making problems, in which a learner sequentially chooses arms with unknown reward distributions in order to maximize its cumulative reward. Most of the prior work on MAB assumes that the reward distributions of each arm are independent. But in a wide variety of decision problems -- from drug dosage to dynamic pricing -- the expected rewards of different arms are correlated, so that selecting one arm provides information about the expected rewards of other arms as well. We propose and analyze a class of models of such decision problems, which we call {\em global bandits}. In the case in which rewards of all arms are deterministic functions of a single unknown parameter, we construct a greedy policy that achieves {\em bounded regret}, with a bound that depends on the single true parameter of the problem. Hence, this policy selects suboptimal arms only finitely many times with probability one. For this case we also obtain a bound on regret that is {\em independent of the true parameter}; this bound is sub-linear, with an exponent that depends on the informativeness of the arms. We also propose a variant of the greedy policy that achieves $\tilde{\mathcal{O}}(\sqrt{T})$ worst-case and $\mathcal{O}(1)$ parameter dependent regret. Finally, we perform experiments on dynamic pricing and show that the proposed algorithms achieve significant gains with respect to the well-known benchmarks. ",Computer Science - Machine Learning ; ,"Atan, Onur ; Tekin, Cem ; van der Schaar, Mihaela ; ","Global Bandits  Multi-armed bandits (MAB) model sequential decision making problems, in which a learner sequentially chooses arms with unknown reward distributions in order to maximize its cumulative reward. Most of the prior work on MAB assumes that the reward distributions of each arm are independent. But in a wide variety of decision problems -- from drug dosage to dynamic pricing -- the expected rewards of different arms are correlated, so that selecting one arm provides information about the expected rewards of other arms as well. We propose and analyze a class of models of such decision problems, which we call {\em global bandits}. In the case in which rewards of all arms are deterministic functions of a single unknown parameter, we construct a greedy policy that achieves {\em bounded regret}, with a bound that depends on the single true parameter of the problem. Hence, this policy selects suboptimal arms only finitely many times with probability one. For this case we also obtain a bound on regret that is {\em independent of the true parameter}; this bound is sub-linear, with an exponent that depends on the informativeness of the arms. We also propose a variant of the greedy policy that achieves $\tilde{\mathcal{O}}(\sqrt{T})$ worst-case and $\mathcal{O}(1)$ parameter dependent regret. Finally, we perform experiments on dynamic pricing and show that the proposed algorithms achieve significant gains with respect to the well-known benchmarks. ",global bandits multi arm bandits mab model sequential decision make problems learner sequentially choose arm unknown reward distributions order maximize cumulative reward prior work mab assume reward distributions arm independent wide variety decision problems drug dosage dynamic price expect reward different arm correlate select one arm provide information expect reward arm well propose analyze class model decision problems call em global bandits case reward arm deterministic function single unknown parameter construct greedy policy achieve em bound regret bind depend single true parameter problem hence policy select suboptimal arm finitely many time probability one case also obtain bind regret em independent true parameter bind sub linear exponent depend informativeness arm also propose variant greedy policy achieve tilde mathcal sqrt worst case mathcal parameter dependent regret finally perform experiment dynamic price show propose algorithms achieve significant gain respect well know benchmarks,139,12,1503.08370.txt
http://arxiv.org/abs/1503.08381,Towards Easier and Faster Sequence Labeling for Natural Language   Processing: A Search-based Probabilistic Online Learning Framework (SAPO),"  There are two major approaches for sequence labeling. One is the probabilistic gradient-based methods such as conditional random fields (CRF) and neural networks (e.g., RNN), which have high accuracy but drawbacks: slow training, and no support of search-based optimization (which is important in many cases). The other is the search-based learning methods such as structured perceptron and margin infused relaxed algorithm (MIRA), which have fast training but also drawbacks: low accuracy, no probabilistic information, and non-convergence in real-world tasks. We propose a novel and ""easy"" solution, a search-based probabilistic online learning method, to address most of those issues. The method is ""easy"", because the optimization algorithm at the training stage is as simple as the decoding algorithm at the test stage. This method searches the output candidates, derives probabilities, and conducts efficient online learning. We show that this method with fast training and theoretical guarantee of convergence, which is easy to implement, can support search-based optimization and obtain top accuracy. Experiments on well-known tasks show that our method has better accuracy than CRF and BiLSTM\footnote{The SAPO code is released at \url{https://github.com/lancopku/SAPO}.}. ",Computer Science - Machine Learning ; Computer Science - Artificial Intelligence ; ,"Sun, Xu ; Ma, Shuming ; Zhang, Yi ; Ren, Xuancheng ; ","Towards Easier and Faster Sequence Labeling for Natural Language   Processing: A Search-based Probabilistic Online Learning Framework (SAPO)  There are two major approaches for sequence labeling. One is the probabilistic gradient-based methods such as conditional random fields (CRF) and neural networks (e.g., RNN), which have high accuracy but drawbacks: slow training, and no support of search-based optimization (which is important in many cases). The other is the search-based learning methods such as structured perceptron and margin infused relaxed algorithm (MIRA), which have fast training but also drawbacks: low accuracy, no probabilistic information, and non-convergence in real-world tasks. We propose a novel and ""easy"" solution, a search-based probabilistic online learning method, to address most of those issues. The method is ""easy"", because the optimization algorithm at the training stage is as simple as the decoding algorithm at the test stage. This method searches the output candidates, derives probabilities, and conducts efficient online learning. We show that this method with fast training and theoretical guarantee of convergence, which is easy to implement, can support search-based optimization and obtain top accuracy. Experiments on well-known tasks show that our method has better accuracy than CRF and BiLSTM\footnote{The SAPO code is released at \url{https://github.com/lancopku/SAPO}.}. ",towards easier faster sequence label natural language process search base probabilistic online learn framework sapo two major approach sequence label one probabilistic gradient base methods conditional random field crf neural network rnn high accuracy drawbacks slow train support search base optimization important many case search base learn methods structure perceptron margin infuse relax algorithm mira fast train also drawbacks low accuracy probabilistic information non convergence real world task propose novel easy solution search base probabilistic online learn method address issue method easy optimization algorithm train stage simple decode algorithm test stage method search output candidates derive probabilities conduct efficient online learn show method fast train theoretical guarantee convergence easy implement support search base optimization obtain top accuracy experiment well know task show method better accuracy crf bilstm footnote sapo code release url https github com lancopku sapo,137,11,1503.08381.txt
http://arxiv.org/abs/1503.08925,Geometry of Interaction for MALL via Hughes-vanGlabbeek Proof-Nets,"  This paper presents, for the first time, a Geometry of Interaction (GoI) interpretation inspired from Hughes-vanGlabbeek (HvG) proof-nets for multiplicative additive linear logic (MALL). Our GoI dynamically captures HvG's geometric correctness criterion-the toggling cycle condition-in terms of algebraic operators. Our new ingredient is a scalar extension of the *-algebra in Girard's *-ring of partial isometries over a boolean polynomial ring with literals of eigenweights as indeterminates. In order to capture feedback arising from cuts, we construct a finer grained execution formula. The expansion of this execution formula is longer than that for collections of slices for multiplicative GoI, hence it is harder to prove termination. Our GoI gives a dynamical, semantical account of boolean valuations (in particular, pruning sub-proofs), conversion of weights (in particular, alpha-conversion), and additive (co)contraction, peculiar to additive proof-theory. Termination of our execution formula is shown to correspond to HvG's toggling criterion. The slice-wise restriction of our execution formula (by collapsing the boolean structure) yields the well known correspondence, explicit or implicit in previous works on multiplicative GoI, between the convergence of execution formulas and acyclicity of proof-nets. Feedback arising from the execution formula by restricting to the boolean polynomial structure yields autonomous definability of eigenweights among cuts from the rest of the eigenweights. ",Computer Science - Logic in Computer Science ; Mathematics - Logic ; ,"Hamano, Masahiro ; ","Geometry of Interaction for MALL via Hughes-vanGlabbeek Proof-Nets  This paper presents, for the first time, a Geometry of Interaction (GoI) interpretation inspired from Hughes-vanGlabbeek (HvG) proof-nets for multiplicative additive linear logic (MALL). Our GoI dynamically captures HvG's geometric correctness criterion-the toggling cycle condition-in terms of algebraic operators. Our new ingredient is a scalar extension of the *-algebra in Girard's *-ring of partial isometries over a boolean polynomial ring with literals of eigenweights as indeterminates. In order to capture feedback arising from cuts, we construct a finer grained execution formula. The expansion of this execution formula is longer than that for collections of slices for multiplicative GoI, hence it is harder to prove termination. Our GoI gives a dynamical, semantical account of boolean valuations (in particular, pruning sub-proofs), conversion of weights (in particular, alpha-conversion), and additive (co)contraction, peculiar to additive proof-theory. Termination of our execution formula is shown to correspond to HvG's toggling criterion. The slice-wise restriction of our execution formula (by collapsing the boolean structure) yields the well known correspondence, explicit or implicit in previous works on multiplicative GoI, between the convergence of execution formulas and acyclicity of proof-nets. Feedback arising from the execution formula by restricting to the boolean polynomial structure yields autonomous definability of eigenweights among cuts from the rest of the eigenweights. ",geometry interaction mall via hughes vanglabbeek proof net paper present first time geometry interaction goi interpretation inspire hughes vanglabbeek hvg proof net multiplicative additive linear logic mall goi dynamically capture hvg geometric correctness criterion toggle cycle condition term algebraic operators new ingredient scalar extension algebra girard ring partial isometries boolean polynomial ring literals eigenweights indeterminates order capture feedback arise cut construct finer grain execution formula expansion execution formula longer collections slice multiplicative goi hence harder prove termination goi give dynamical semantical account boolean valuations particular prune sub proof conversion weight particular alpha conversion additive co contraction peculiar additive proof theory termination execution formula show correspond hvg toggle criterion slice wise restriction execution formula collapse boolean structure yield well know correspondence explicit implicit previous work multiplicative goi convergence execution formulas acyclicity proof net feedback arise execution formula restrict boolean polynomial structure yield autonomous definability eigenweights among cut rest eigenweights,148,8,1503.08925.txt
http://arxiv.org/abs/1504.00169,Complete Simulation of Automata Networks,"  Consider a finite set $A$ and an integer $n \geq 1$. This paper studies the concept of complete simulation in the context of semigroups of transformations of $A^n$, also known as finite state-homogeneous automata networks. For $m \geq n$, a transformation of $A^m$ is \emph{$n$-complete of size $m$} if it may simulate every transformation of $A^n$ by updating one coordinate (or register) at a time. Using tools from memoryless computation, it is established that there is no $n$-complete transformation of size $n$, but there is such a transformation of size $n+1$. By studying the the time of simulation of various $n$-complete transformations, it is conjectured that the maximal time of simulation of any $n$-complete transformation is at least $2n$. A transformation of $A^m$ is \emph{sequentially $n$-complete of size $m$} if it may sequentially simulate every finite sequence of transformations of $A^n$; in this case, minimal examples and bounds for the size and time of simulation are determined. It is also shown that there is no $n$-complete transformation that updates all the registers in parallel, but that there exists a sequentally $n$-complete transformation that updates all but one register in parallel. This illustrates the strengths and weaknesses of parallel models of computation, such as cellular automata. ",Computer Science - Formal Languages and Automata Theory ; Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; Mathematics - Group Theory ; ,"Bridoux, Florian ; Castillo-Ramirez, Alonso ; Gadouleau, Maximilien ; ","Complete Simulation of Automata Networks  Consider a finite set $A$ and an integer $n \geq 1$. This paper studies the concept of complete simulation in the context of semigroups of transformations of $A^n$, also known as finite state-homogeneous automata networks. For $m \geq n$, a transformation of $A^m$ is \emph{$n$-complete of size $m$} if it may simulate every transformation of $A^n$ by updating one coordinate (or register) at a time. Using tools from memoryless computation, it is established that there is no $n$-complete transformation of size $n$, but there is such a transformation of size $n+1$. By studying the the time of simulation of various $n$-complete transformations, it is conjectured that the maximal time of simulation of any $n$-complete transformation is at least $2n$. A transformation of $A^m$ is \emph{sequentially $n$-complete of size $m$} if it may sequentially simulate every finite sequence of transformations of $A^n$; in this case, minimal examples and bounds for the size and time of simulation are determined. It is also shown that there is no $n$-complete transformation that updates all the registers in parallel, but that there exists a sequentally $n$-complete transformation that updates all but one register in parallel. This illustrates the strengths and weaknesses of parallel models of computation, such as cellular automata. ",complete simulation automata network consider finite set integer geq paper study concept complete simulation context semigroups transformations also know finite state homogeneous automata network geq transformation emph complete size may simulate every transformation update one coordinate register time use tool memoryless computation establish complete transformation size transformation size study time simulation various complete transformations conjecture maximal time simulation complete transformation least transformation emph sequentially complete size may sequentially simulate every finite sequence transformations case minimal examples bound size time simulation determine also show complete transformation update register parallel exist sequentally complete transformation update one register parallel illustrate strengths weaknesses parallel model computation cellular automata,104,14,1504.00169.txt
http://arxiv.org/abs/1504.00222,On the Exact and Approximate Eigenvalue Distribution for Sum of Wishart   Matrices,"  The sum of Wishart matrices has an important role in multiuser communication employing multiantenna elements, such as multiple-input multiple-output (MIMO) multiple access channel (MAC), MIMO Relay channel, and other multiuser channels where the mathematical model is best described using random matrices. In this paper, the distribution of linear combination of complex Wishart distributed matrices has been studied. We present a new closed form expression for the marginal distribution of the eigenvalues of a weighted sum of K complex central Wishart matrices having covariance matrices proportional to the identity matrix. The expression is general and allows for any set of linear coefficients. As an application example, we have used the marginal distribution expression to obtain the ergodic sum-rate capacity for the MIMO-MAC network, and the cut-set upper bound for the MIMO-Relay case, both as closed form expressions. We also present a very simple expression to approximate the sum of Wishart matrices by one equivalent Wishart matrix. All of our results are validated by means of Monte Carlo simulations. As expected, the agreement between the exact eigenvalue distribution and simulations is perfect, whereas for the approximate solution the difference is indistinguishable. ","Computer Science - Information Theory ; 94A15, 94A17, 15A18, 15B52 ; ","Kumar, S. ; Pivaro, G. F. ; Fraidenraich, G. ; Dias, C. F. ; ","On the Exact and Approximate Eigenvalue Distribution for Sum of Wishart   Matrices  The sum of Wishart matrices has an important role in multiuser communication employing multiantenna elements, such as multiple-input multiple-output (MIMO) multiple access channel (MAC), MIMO Relay channel, and other multiuser channels where the mathematical model is best described using random matrices. In this paper, the distribution of linear combination of complex Wishart distributed matrices has been studied. We present a new closed form expression for the marginal distribution of the eigenvalues of a weighted sum of K complex central Wishart matrices having covariance matrices proportional to the identity matrix. The expression is general and allows for any set of linear coefficients. As an application example, we have used the marginal distribution expression to obtain the ergodic sum-rate capacity for the MIMO-MAC network, and the cut-set upper bound for the MIMO-Relay case, both as closed form expressions. We also present a very simple expression to approximate the sum of Wishart matrices by one equivalent Wishart matrix. All of our results are validated by means of Monte Carlo simulations. As expected, the agreement between the exact eigenvalue distribution and simulations is perfect, whereas for the approximate solution the difference is indistinguishable. ",exact approximate eigenvalue distribution sum wishart matrices sum wishart matrices important role multiuser communication employ multiantenna elements multiple input multiple output mimo multiple access channel mac mimo relay channel multiuser channel mathematical model best describe use random matrices paper distribution linear combination complex wishart distribute matrices study present new close form expression marginal distribution eigenvalues weight sum complex central wishart matrices covariance matrices proportional identity matrix expression general allow set linear coefficients application example use marginal distribution expression obtain ergodic sum rate capacity mimo mac network cut set upper bind mimo relay case close form expressions also present simple expression approximate sum wishart matrices one equivalent wishart matrix result validate mean monte carlo simulations expect agreement exact eigenvalue distribution simulations perfect whereas approximate solution difference indistinguishable,126,7,1504.00222.txt
http://arxiv.org/abs/1504.00495,Exploring the complex pattern of information spreading in online blog   communities,"  Information spreading in online social communities has attracted tremendous attention due to its utmost practical values in applications. Despite that several individual-level diffusion data have been investigated, we still lack the detailed understanding of the spreading pattern of information. Here, by comparing information flows and social links in a blog community, we find that the diffusion processes are induced by three different spreading mechanisms: social spreading, self-promotion and broadcast. Although numerous previous studies have employed epidemic spreading models to simulate information diffusion, we observe that such models fail to reproduce the realistic diffusion pattern. In respect to users behaviors, strikingly, we find that most users would stick to one specific diffusion mechanism. Moreover, our observations indicate that the social spreading is not only crucial for the structure of diffusion trees, but also capable of inducing more subsequent individuals to acquire the information. Our findings suggest new directions for modeling of information diffusion in social systems and could inform design of efficient propagation strategies based on users behaviors. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Pei, Sen ; Muchnik, Lev ; Tang, Shaoting ; Zheng, Zhiming ; Makse, Hernan A. ; ","Exploring the complex pattern of information spreading in online blog   communities  Information spreading in online social communities has attracted tremendous attention due to its utmost practical values in applications. Despite that several individual-level diffusion data have been investigated, we still lack the detailed understanding of the spreading pattern of information. Here, by comparing information flows and social links in a blog community, we find that the diffusion processes are induced by three different spreading mechanisms: social spreading, self-promotion and broadcast. Although numerous previous studies have employed epidemic spreading models to simulate information diffusion, we observe that such models fail to reproduce the realistic diffusion pattern. In respect to users behaviors, strikingly, we find that most users would stick to one specific diffusion mechanism. Moreover, our observations indicate that the social spreading is not only crucial for the structure of diffusion trees, but also capable of inducing more subsequent individuals to acquire the information. Our findings suggest new directions for modeling of information diffusion in social systems and could inform design of efficient propagation strategies based on users behaviors. ",explore complex pattern information spread online blog communities information spread online social communities attract tremendous attention due utmost practical value applications despite several individual level diffusion data investigate still lack detail understand spread pattern information compare information flow social link blog community find diffusion process induce three different spread mechanisms social spread self promotion broadcast although numerous previous study employ epidemic spread model simulate information diffusion observe model fail reproduce realistic diffusion pattern respect users behaviors strikingly find users would stick one specific diffusion mechanism moreover observations indicate social spread crucial structure diffusion tree also capable induce subsequent individuals acquire information find suggest new directions model information diffusion social systems could inform design efficient propagation strategies base users behaviors,119,0,1504.00495.txt
http://arxiv.org/abs/1504.01019,On the Total-Power Capacity of Regular-LDPC Codes with Iterative   Message-Passing Decoders,"  Motivated by recently derived fundamental limits on total (transmit + decoding) power for coded communication with VLSI decoders, this paper investigates the scaling behavior of the minimum total power needed to communicate over AWGN channels as the target bit-error-probability tends to zero. We focus on regular-LDPC codes and iterative message-passing decoders. We analyze scaling behavior under two VLSI complexity models of decoding. One model abstracts power consumed in processing elements (""node model""), and another abstracts power consumed in wires which connect the processing elements (""wire model""). We prove that a coding strategy using regular-LDPC codes with Gallager-B decoding achieves order-optimal scaling of total power under the node model. However, we also prove that regular-LDPC codes and iterative message-passing decoders cannot meet existing fundamental limits on total power under the wire model. Further, if the transmit energy-per-bit is bounded, total power grows at a rate that is worse than uncoded transmission. Complementing our theoretical results, we develop detailed physical models of decoding implementations using post-layout circuit simulations. Our theoretical and numerical results show that approaching fundamental limits on total power requires increasing the complexity of both the code design and the corresponding decoding algorithm as communication distance is increased or error-probability is lowered. ",Computer Science - Information Theory ; ,"Ganesan, Karthik ; Grover, Pulkit ; Rabaey, Jan ; Goldsmith, Andrea ; ","On the Total-Power Capacity of Regular-LDPC Codes with Iterative   Message-Passing Decoders  Motivated by recently derived fundamental limits on total (transmit + decoding) power for coded communication with VLSI decoders, this paper investigates the scaling behavior of the minimum total power needed to communicate over AWGN channels as the target bit-error-probability tends to zero. We focus on regular-LDPC codes and iterative message-passing decoders. We analyze scaling behavior under two VLSI complexity models of decoding. One model abstracts power consumed in processing elements (""node model""), and another abstracts power consumed in wires which connect the processing elements (""wire model""). We prove that a coding strategy using regular-LDPC codes with Gallager-B decoding achieves order-optimal scaling of total power under the node model. However, we also prove that regular-LDPC codes and iterative message-passing decoders cannot meet existing fundamental limits on total power under the wire model. Further, if the transmit energy-per-bit is bounded, total power grows at a rate that is worse than uncoded transmission. Complementing our theoretical results, we develop detailed physical models of decoding implementations using post-layout circuit simulations. Our theoretical and numerical results show that approaching fundamental limits on total power requires increasing the complexity of both the code design and the corresponding decoding algorithm as communication distance is increased or error-probability is lowered. ",total power capacity regular ldpc cod iterative message pass decoders motivate recently derive fundamental limit total transmit decode power cod communication vlsi decoders paper investigate scale behavior minimum total power need communicate awgn channel target bite error probability tend zero focus regular ldpc cod iterative message pass decoders analyze scale behavior two vlsi complexity model decode one model abstract power consume process elements node model another abstract power consume wire connect process elements wire model prove cod strategy use regular ldpc cod gallager decode achieve order optimal scale total power node model however also prove regular ldpc cod iterative message pass decoders cannot meet exist fundamental limit total power wire model transmit energy per bite bound total power grow rate worse uncoded transmission complement theoretical result develop detail physical model decode implementations use post layout circuit simulations theoretical numerical result show approach fundamental limit total power require increase complexity code design correspond decode algorithm communication distance increase error probability lower,160,5,1504.01019.txt
http://arxiv.org/abs/1504.01442,The Effect of Recency to Human Mobility,"  In recent years, we have seen scientists attempt to model and explain human dynamics and, in particular, human movement. Many aspects of our complex life are affected by human movements such as disease spread and epidemics modeling, city planning, wireless network development, and disaster relief, to name a few. Given the myriad of applications it is clear that a complete understanding of how people move in space can lead to huge benefits to our society. In most of the recent works, scientists have focused on the idea that people movements are biased towards frequently-visited locations. According to them, human movement is based on an exploration/exploitation dichotomy in which individuals choose new locations (exploration) or return to frequently-visited locations (exploitation). In this work, we focus on the concept of recency. We propose a model in which exploitation in human movement also considers recently-visited locations and not solely frequently-visited locations. We test our hypothesis against different empirical data of human mobility and show that our proposed model is able to better explain the human trajectories in these datasets. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Barbosa, Hugo ; Neto, Fernando Buarque de Lima ; Evsukoff, Alexandre ; Menezes, Ronaldo ; ","The Effect of Recency to Human Mobility  In recent years, we have seen scientists attempt to model and explain human dynamics and, in particular, human movement. Many aspects of our complex life are affected by human movements such as disease spread and epidemics modeling, city planning, wireless network development, and disaster relief, to name a few. Given the myriad of applications it is clear that a complete understanding of how people move in space can lead to huge benefits to our society. In most of the recent works, scientists have focused on the idea that people movements are biased towards frequently-visited locations. According to them, human movement is based on an exploration/exploitation dichotomy in which individuals choose new locations (exploration) or return to frequently-visited locations (exploitation). In this work, we focus on the concept of recency. We propose a model in which exploitation in human movement also considers recently-visited locations and not solely frequently-visited locations. We test our hypothesis against different empirical data of human mobility and show that our proposed model is able to better explain the human trajectories in these datasets. ",effect recency human mobility recent years see scientists attempt model explain human dynamics particular human movement many aspects complex life affect human movements disease spread epidemics model city plan wireless network development disaster relief name give myriad applications clear complete understand people move space lead huge benefit society recent work scientists focus idea people movements bias towards frequently visit locations accord human movement base exploration exploitation dichotomy individuals choose new locations exploration return frequently visit locations exploitation work focus concept recency propose model exploitation human movement also consider recently visit locations solely frequently visit locations test hypothesis different empirical data human mobility show propose model able better explain human trajectories datasets,111,0,1504.01442.txt
http://arxiv.org/abs/1504.01708,Reactive Synthesis Without Regret,"  Two-player zero-sum games of infinite duration and their quantitative versions are used in verification to model the interaction between a controller (Eve) and its environment (Adam). The question usually addressed is that of the existence (and computability) of a strategy for Eve that can maximize her payoff against any strategy of Adam. In this work, we are interested in strategies of Eve that minimize her regret, i.e. strategies that minimize the difference between her actual payoff and the payoff she could have achieved if she had known the strategy of Adam in advance. We give algorithms to compute the strategies of Eve that ensure minimal regret against an adversary whose choice of strategy is (i) unrestricted, (ii) limited to positional strategies, or (iii) limited to word strategies. We also establish relations between the latter version and other problems studied in the literature. ",Computer Science - Computer Science and Game Theory ; Computer Science - Formal Languages and Automata Theory ; Computer Science - Logic in Computer Science ; F.1.1 ; ,"Hunter, Paul ; Pérez, Guillermo A. ; Raskin, Jean-François ; ","Reactive Synthesis Without Regret  Two-player zero-sum games of infinite duration and their quantitative versions are used in verification to model the interaction between a controller (Eve) and its environment (Adam). The question usually addressed is that of the existence (and computability) of a strategy for Eve that can maximize her payoff against any strategy of Adam. In this work, we are interested in strategies of Eve that minimize her regret, i.e. strategies that minimize the difference between her actual payoff and the payoff she could have achieved if she had known the strategy of Adam in advance. We give algorithms to compute the strategies of Eve that ensure minimal regret against an adversary whose choice of strategy is (i) unrestricted, (ii) limited to positional strategies, or (iii) limited to word strategies. We also establish relations between the latter version and other problems studied in the literature. ",reactive synthesis without regret two player zero sum game infinite duration quantitative versions use verification model interaction controller eve environment adam question usually address existence computability strategy eve maximize payoff strategy adam work interest strategies eve minimize regret strategies minimize difference actual payoff payoff could achieve know strategy adam advance give algorithms compute strategies eve ensure minimal regret adversary whose choice strategy unrestricted ii limit positional strategies iii limit word strategies also establish relations latter version problems study literature,79,8,1504.01708.txt
http://arxiv.org/abs/1504.01709,"Copyless Cost-Register Automata: Structure, Expressiveness, and Closure   Properties","  Cost register automata (CRA) and its subclass, copyless CRA, were recently proposed by Alur et al. as a new model for computing functions over strings. We study some structural properties, expressiveness, and closure properties of copyless CRA. We show that copyless CRA are strictly less expressive than weighted automata and are not closed under reverse operation. To find a better class we impose restrictions on copyless CRA, which ends successfully with a new robust computational model that is closed under reverse and other extensions. ",Computer Science - Formal Languages and Automata Theory ; ,"Mazowiecki, Filip ; Riveros, Cristian ; ","Copyless Cost-Register Automata: Structure, Expressiveness, and Closure   Properties  Cost register automata (CRA) and its subclass, copyless CRA, were recently proposed by Alur et al. as a new model for computing functions over strings. We study some structural properties, expressiveness, and closure properties of copyless CRA. We show that copyless CRA are strictly less expressive than weighted automata and are not closed under reverse operation. To find a better class we impose restrictions on copyless CRA, which ends successfully with a new robust computational model that is closed under reverse and other extensions. ",copyless cost register automata structure expressiveness closure properties cost register automata cra subclass copyless cra recently propose alur et al new model compute function string study structural properties expressiveness closure properties copyless cra show copyless cra strictly less expressive weight automata close reverse operation find better class impose restrictions copyless cra end successfully new robust computational model close reverse extensions,60,14,1504.01709.txt
http://arxiv.org/abs/1504.01782,Profit Maximization for Geographical Dispersed Green Data Centers,"  This paper aims at maximizing the profit associated with running geographically dispersed green data centers, which offer multiple classes of service. To this end, we formulate an optimization framework which relies on the accuracy of the G/D/1 queue in characterizing the workload distribution, and taps on the merits of the workload decomposition into green and brown workload served by green and brown energy resources. Moreover, we take into account of not only the Service Level Agreements (SLAs) between the data centers and clients but also different deregulated electricity markets of data centers located at different regions. We prove the convexity of our optimization problem and the performance of the proposed workload distribution strategy is evaluated via simulations. ",Computer Science - Networking and Internet Architecture ; ,"Kiani, Abbas ; Ansari, Nirwan ; ","Profit Maximization for Geographical Dispersed Green Data Centers  This paper aims at maximizing the profit associated with running geographically dispersed green data centers, which offer multiple classes of service. To this end, we formulate an optimization framework which relies on the accuracy of the G/D/1 queue in characterizing the workload distribution, and taps on the merits of the workload decomposition into green and brown workload served by green and brown energy resources. Moreover, we take into account of not only the Service Level Agreements (SLAs) between the data centers and clients but also different deregulated electricity markets of data centers located at different regions. We prove the convexity of our optimization problem and the performance of the proposed workload distribution strategy is evaluated via simulations. ",profit maximization geographical disperse green data center paper aim maximize profit associate run geographically disperse green data center offer multiple class service end formulate optimization framework rely accuracy queue characterize workload distribution tap merit workload decomposition green brown workload serve green brown energy resources moreover take account service level agreements slas data center clients also different deregulate electricity market data center locate different regions prove convexity optimization problem performance propose workload distribution strategy evaluate via simulations,76,12,1504.01782.txt
http://arxiv.org/abs/1504.02141,Detecting Falls with X-Factor Hidden Markov Models,"  Identification of falls while performing normal activities of daily living (ADL) is important to ensure personal safety and well-being. However, falling is a short term activity that occurs infrequently. This poses a challenge to traditional classification algorithms, because there may be very little training data for falls (or none at all). This paper proposes an approach for the identification of falls using a wearable device in the absence of training data for falls but with plentiful data for normal ADL. We propose three `X-Factor' Hidden Markov Model (XHMMs) approaches. The XHMMs model unseen falls using ""inflated"" output covariances (observation models). To estimate the inflated covariances, we propose a novel cross validation method to remove ""outliers"" from the normal ADL that serve as proxies for the unseen falls and allow learning the XHMMs using only normal activities. We tested the proposed XHMM approaches on two activity recognition datasets and show high detection rates for falls in the absence of fall-specific training data. We show that the traditional method of choosing a threshold based on maximum of negative of log-likelihood to identify unseen falls is ill-posed for this problem. We also show that supervised classification methods perform poorly when very limited fall data are available during the training phase. ",Computer Science - Machine Learning ; Computer Science - Artificial Intelligence ; ,"Khan, Shehroz S. ; Karg, Michelle E. ; Kulic, Dana ; Hoey, Jesse ; ","Detecting Falls with X-Factor Hidden Markov Models  Identification of falls while performing normal activities of daily living (ADL) is important to ensure personal safety and well-being. However, falling is a short term activity that occurs infrequently. This poses a challenge to traditional classification algorithms, because there may be very little training data for falls (or none at all). This paper proposes an approach for the identification of falls using a wearable device in the absence of training data for falls but with plentiful data for normal ADL. We propose three `X-Factor' Hidden Markov Model (XHMMs) approaches. The XHMMs model unseen falls using ""inflated"" output covariances (observation models). To estimate the inflated covariances, we propose a novel cross validation method to remove ""outliers"" from the normal ADL that serve as proxies for the unseen falls and allow learning the XHMMs using only normal activities. We tested the proposed XHMM approaches on two activity recognition datasets and show high detection rates for falls in the absence of fall-specific training data. We show that the traditional method of choosing a threshold based on maximum of negative of log-likelihood to identify unseen falls is ill-posed for this problem. We also show that supervised classification methods perform poorly when very limited fall data are available during the training phase. ",detect fall factor hide markov model identification fall perform normal activities daily live adl important ensure personal safety well however fall short term activity occur infrequently pose challenge traditional classification algorithms may little train data fall none paper propose approach identification fall use wearable device absence train data fall plentiful data normal adl propose three factor hide markov model xhmms approach xhmms model unseen fall use inflate output covariances observation model estimate inflate covariances propose novel cross validation method remove outliers normal adl serve proxies unseen fall allow learn xhmms use normal activities test propose xhmm approach two activity recognition datasets show high detection rat fall absence fall specific train data show traditional method choose threshold base maximum negative log likelihood identify unseen fall ill pose problem also show supervise classification methods perform poorly limit fall data available train phase,140,10,1504.02141.txt
http://arxiv.org/abs/1504.03342,A Survey on Privacy and Security in Online Social Networks,"  Online Social Networks (OSN) are a permanent presence in today's personal and professional lives of a huge segment of the population, with direct consequences to offline activities. Built on a foundation of trust-users connect to other users with common interests or overlapping personal trajectories-online social networks and the associated applications extract an unprecedented volume of personal information. Unsurprisingly, serious privacy and security risks emerged, positioning themselves along two main types of attacks: attacks that exploit the implicit trust embedded in declared social relationships; and attacks that harvest user's personal information for ill-intended use. This article provides an overview of the privacy and security issues that emerged so far in OSNs. We introduce a taxonomy of privacy and security attacks in OSNs, we overview existing solutions to mitigate those attacks, and outline challenges still to overcome. ",Computer Science - Social and Information Networks ; Computer Science - Cryptography and Security ; ,"Kayes, Imrul ; Iamnitchi, Adriana ; ","A Survey on Privacy and Security in Online Social Networks  Online Social Networks (OSN) are a permanent presence in today's personal and professional lives of a huge segment of the population, with direct consequences to offline activities. Built on a foundation of trust-users connect to other users with common interests or overlapping personal trajectories-online social networks and the associated applications extract an unprecedented volume of personal information. Unsurprisingly, serious privacy and security risks emerged, positioning themselves along two main types of attacks: attacks that exploit the implicit trust embedded in declared social relationships; and attacks that harvest user's personal information for ill-intended use. This article provides an overview of the privacy and security issues that emerged so far in OSNs. We introduce a taxonomy of privacy and security attacks in OSNs, we overview existing solutions to mitigate those attacks, and outline challenges still to overcome. ",survey privacy security online social network online social network osn permanent presence today personal professional live huge segment population direct consequences offline activities build foundation trust users connect users common interest overlap personal trajectories online social network associate applications extract unprecedented volume personal information unsurprisingly serious privacy security risk emerge position along two main type attack attack exploit implicit trust embed declare social relationships attack harvest user personal information ill intend use article provide overview privacy security issue emerge far osns introduce taxonomy privacy security attack osns overview exist solutions mitigate attack outline challenge still overcome,96,6,1504.03342.txt
http://arxiv.org/abs/1504.03856,Sparse multivariate polynomial interpolation in the basis of Schubert   polynomials,"  Schubert polynomials were discovered by A. Lascoux and M. Sch\""utzenberger in the study of cohomology rings of flag manifolds in 1980's. These polynomials generalize Schur polynomials, and form a linear basis of multivariate polynomials. In 2003, Lenart and Sottile introduced skew Schubert polynomials, which generalize skew Schur polynomials, and expand in the Schubert basis with the generalized Littlewood-Richardson coefficients.   In this paper we initiate the study of these two families of polynomials from the perspective of computational complexity theory. We first observe that skew Schubert polynomials, and therefore Schubert polynomials, are in $\CountP$ (when evaluating on non-negative integral inputs) and $\VNP$.   Our main result is a deterministic algorithm that computes the expansion of a polynomial $f$ of degree $d$ in $\Z[x_1, \dots, x_n]$ in the basis of Schubert polynomials, assuming an oracle computing Schubert polynomials. This algorithm runs in time polynomial in $n$, $d$, and the bit size of the expansion. This generalizes, and derandomizes, the sparse interpolation algorithm of symmetric polynomials in the Schur basis by Barvinok and Fomin (Advances in Applied Mathematics, 18(3):271--285). In fact, our interpolation algorithm is general enough to accommodate any linear basis satisfying certain natural properties.   Applications of the above results include a new algorithm that computes the generalized Littlewood-Richardson coefficients. ",Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; ,"Mukhopadhyay, Priyanka ; Qiao, Youming ; ","Sparse multivariate polynomial interpolation in the basis of Schubert   polynomials  Schubert polynomials were discovered by A. Lascoux and M. Sch\""utzenberger in the study of cohomology rings of flag manifolds in 1980's. These polynomials generalize Schur polynomials, and form a linear basis of multivariate polynomials. In 2003, Lenart and Sottile introduced skew Schubert polynomials, which generalize skew Schur polynomials, and expand in the Schubert basis with the generalized Littlewood-Richardson coefficients.   In this paper we initiate the study of these two families of polynomials from the perspective of computational complexity theory. We first observe that skew Schubert polynomials, and therefore Schubert polynomials, are in $\CountP$ (when evaluating on non-negative integral inputs) and $\VNP$.   Our main result is a deterministic algorithm that computes the expansion of a polynomial $f$ of degree $d$ in $\Z[x_1, \dots, x_n]$ in the basis of Schubert polynomials, assuming an oracle computing Schubert polynomials. This algorithm runs in time polynomial in $n$, $d$, and the bit size of the expansion. This generalizes, and derandomizes, the sparse interpolation algorithm of symmetric polynomials in the Schur basis by Barvinok and Fomin (Advances in Applied Mathematics, 18(3):271--285). In fact, our interpolation algorithm is general enough to accommodate any linear basis satisfying certain natural properties.   Applications of the above results include a new algorithm that computes the generalized Littlewood-Richardson coefficients. ",sparse multivariate polynomial interpolation basis schubert polynomials schubert polynomials discover lascoux sch utzenberger study cohomology ring flag manifold polynomials generalize schur polynomials form linear basis multivariate polynomials lenart sottile introduce skew schubert polynomials generalize skew schur polynomials expand schubert basis generalize littlewood richardson coefficients paper initiate study two families polynomials perspective computational complexity theory first observe skew schubert polynomials therefore schubert polynomials countp evaluate non negative integral input vnp main result deterministic algorithm compute expansion polynomial degree dot basis schubert polynomials assume oracle compute schubert polynomials algorithm run time polynomial bite size expansion generalize derandomizes sparse interpolation algorithm symmetric polynomials schur basis barvinok fomin advance apply mathematics fact interpolation algorithm general enough accommodate linear basis satisfy certain natural properties applications result include new algorithm compute generalize littlewood richardson coefficients,129,4,1504.03856.txt
http://arxiv.org/abs/1504.03872,Extended Formulations for Independence Polytopes of Regular Matroids,"  We show that the independence polytope of every regular matroid has an extended formulation of size quadratic in the size of its ground set. This generalizes a similar statement for (co-)graphic matroids, which is a simple consequence of Martin's extended formulation for the spanning-tree polytope. In our construction, we make use of Seymour's decomposition theorem for regular matroids. As a consequence, the extended formulations can be computed in polynomial time. ",Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; 52Bxx ; ,"Kaibel, Volker ; Lee, Jon ; Walter, Matthias ; Weltge, Stefan ; ","Extended Formulations for Independence Polytopes of Regular Matroids  We show that the independence polytope of every regular matroid has an extended formulation of size quadratic in the size of its ground set. This generalizes a similar statement for (co-)graphic matroids, which is a simple consequence of Martin's extended formulation for the spanning-tree polytope. In our construction, we make use of Seymour's decomposition theorem for regular matroids. As a consequence, the extended formulations can be computed in polynomial time. ",extend formulations independence polytopes regular matroids show independence polytope every regular matroid extend formulation size quadratic size grind set generalize similar statement co graphic matroids simple consequence martin extend formulation span tree polytope construction make use seymour decomposition theorem regular matroids consequence extend formulations compute polynomial time,47,8,1504.03872.txt
http://arxiv.org/abs/1504.03957,Optimal Hierarchical Radio Resource Management for HetNets with Flexible   Backhaul,"  Providing backhaul connectivity for macro and pico base stations (BSs) constitutes a significant share of infrastructure costs in future heterogeneous networks (HetNets). To address this issue, the emerging idea of flexible backhaul is proposed. Under this architecture, not all the pico BSs are connected to the backhaul, resulting in a significant reduction in the infrastructure costs. In this regard, pico BSs without backhaul connectivity need to communicate with their nearby BSs in order to have indirect accessibility to the backhaul. This makes the radio resource management (RRM) in such networks more complex and challenging. In this paper, we address the problem of cross-layer RRM in HetNets with flexible backhaul. We formulate this problem as a two-timescale non-convex stochastic optimization which jointly optimizes flow control, routing, interference mitigation and link scheduling in order to maximize a generic network utility. By exploiting a hidden convexity of this non-convex problem, we propose an iterative algorithm which converges to the global optimal solution. The proposed algorithm benefits from low complexity and low signalling, which makes it scalable. Moreover, due to the proposed two-timescale design, it is robust to the backhaul signalling latency as well. Simulation results demonstrate the significant performance gain of the proposed solution over various baselines. ",Computer Science - Information Theory ; ,"Omidvar, Naeimeh ; Liu, An ; Lau, Vincent ; Zhang, Fan ; Tsang, Danny H. K. ; Pakravan, Mohammad Reza ; ","Optimal Hierarchical Radio Resource Management for HetNets with Flexible   Backhaul  Providing backhaul connectivity for macro and pico base stations (BSs) constitutes a significant share of infrastructure costs in future heterogeneous networks (HetNets). To address this issue, the emerging idea of flexible backhaul is proposed. Under this architecture, not all the pico BSs are connected to the backhaul, resulting in a significant reduction in the infrastructure costs. In this regard, pico BSs without backhaul connectivity need to communicate with their nearby BSs in order to have indirect accessibility to the backhaul. This makes the radio resource management (RRM) in such networks more complex and challenging. In this paper, we address the problem of cross-layer RRM in HetNets with flexible backhaul. We formulate this problem as a two-timescale non-convex stochastic optimization which jointly optimizes flow control, routing, interference mitigation and link scheduling in order to maximize a generic network utility. By exploiting a hidden convexity of this non-convex problem, we propose an iterative algorithm which converges to the global optimal solution. The proposed algorithm benefits from low complexity and low signalling, which makes it scalable. Moreover, due to the proposed two-timescale design, it is robust to the backhaul signalling latency as well. Simulation results demonstrate the significant performance gain of the proposed solution over various baselines. ",optimal hierarchical radio resource management hetnets flexible backhaul provide backhaul connectivity macro pico base station bss constitute significant share infrastructure cost future heterogeneous network hetnets address issue emerge idea flexible backhaul propose architecture pico bss connect backhaul result significant reduction infrastructure cost regard pico bss without backhaul connectivity need communicate nearby bss order indirect accessibility backhaul make radio resource management rrm network complex challenge paper address problem cross layer rrm hetnets flexible backhaul formulate problem two timescale non convex stochastic optimization jointly optimize flow control rout interference mitigation link schedule order maximize generic network utility exploit hide convexity non convex problem propose iterative algorithm converge global optimal solution propose algorithm benefit low complexity low signal make scalable moreover due propose two timescale design robust backhaul signal latency well simulation result demonstrate significant performance gain propose solution various baselines,138,6,1504.03957.txt
http://arxiv.org/abs/1504.04073,The Parametric Closure Problem,"  We define the parametric closure problem, in which the input is a partially ordered set whose elements have linearly varying weights and the goal is to compute the sequence of minimum-weight lower sets of the partial order as the weights vary. We give polynomial time solutions to many important special cases of this problem including semiorders, reachability orders of bounded-treewidth graphs, partial orders of bounded width, and series-parallel partial orders. Our result for series-parallel orders provides a significant generalization of a previous result of Carlson and Eppstein on bicriterion subtree problems. ",Computer Science - Data Structures and Algorithms ; F.2.2 ; ,"Eppstein, David ; ","The Parametric Closure Problem  We define the parametric closure problem, in which the input is a partially ordered set whose elements have linearly varying weights and the goal is to compute the sequence of minimum-weight lower sets of the partial order as the weights vary. We give polynomial time solutions to many important special cases of this problem including semiorders, reachability orders of bounded-treewidth graphs, partial orders of bounded width, and series-parallel partial orders. Our result for series-parallel orders provides a significant generalization of a previous result of Carlson and Eppstein on bicriterion subtree problems. ",parametric closure problem define parametric closure problem input partially order set whose elements linearly vary weight goal compute sequence minimum weight lower set partial order weight vary give polynomial time solutions many important special case problem include semiorders reachability order bound treewidth graph partial order bound width series parallel partial order result series parallel order provide significant generalization previous result carlson eppstein bicriterion subtree problems,65,3,1504.04073.txt
http://arxiv.org/abs/1504.04217,Quantum and classical coin-flipping protocols based on bit-commitment   and their point games,"  We focus on a family of quantum coin-flipping protocols based on bit-commitment. We discuss how the semidefinite programming formulations of cheating strategies can be reduced to optimizing a linear combination of fidelity functions over a polytope. These turn out to be much simpler semidefinite programs which can be modelled using second-order cone programming problems. We then use these simplifications to construct their point games as developed by Kitaev. We also study the classical version of these protocols and use linear optimization to formulate optimal cheating strategies. We then construct the point games for the classical protocols as well using the analysis for the quantum case.   We discuss the philosophical connections between the classical and quantum protocols and their point games as viewed from optimization theory. In particular, we observe an analogy between a spectrum of physical theories (from classical to quantum) and a spectrum of convex optimization problems (from linear programming to semidefinite programming, through second-order cone programming). In this analogy, classical systems correspond to linear programming problems and the level of quantum features in the system is correlated to the level of sophistication of the semidefinite programming models on the optimization side.   Concerning security analysis, we use the classical point games to prove that every classical protocol of this type allows exactly one of the parties to entirely determine the coin-flip. Using the relationships between the quantum and classical protocols, we show that only ""classical"" protocols can saturate Kitaev's lower bound for strong coin-flipping. Moreover, if the product of Alice and Bob's optimal cheating probabilities is 1/2, then one party can cheat with probability 1. This rules out quantum protocols of this type from attaining the optimal level of security. ",Quantum Physics ; Computer Science - Cryptography and Security ; Mathematics - Optimization and Control ; ,"Nayak, Ashwin ; Sikora, Jamie ; Tunçel, Levent ; ","Quantum and classical coin-flipping protocols based on bit-commitment   and their point games  We focus on a family of quantum coin-flipping protocols based on bit-commitment. We discuss how the semidefinite programming formulations of cheating strategies can be reduced to optimizing a linear combination of fidelity functions over a polytope. These turn out to be much simpler semidefinite programs which can be modelled using second-order cone programming problems. We then use these simplifications to construct their point games as developed by Kitaev. We also study the classical version of these protocols and use linear optimization to formulate optimal cheating strategies. We then construct the point games for the classical protocols as well using the analysis for the quantum case.   We discuss the philosophical connections between the classical and quantum protocols and their point games as viewed from optimization theory. In particular, we observe an analogy between a spectrum of physical theories (from classical to quantum) and a spectrum of convex optimization problems (from linear programming to semidefinite programming, through second-order cone programming). In this analogy, classical systems correspond to linear programming problems and the level of quantum features in the system is correlated to the level of sophistication of the semidefinite programming models on the optimization side.   Concerning security analysis, we use the classical point games to prove that every classical protocol of this type allows exactly one of the parties to entirely determine the coin-flip. Using the relationships between the quantum and classical protocols, we show that only ""classical"" protocols can saturate Kitaev's lower bound for strong coin-flipping. Moreover, if the product of Alice and Bob's optimal cheating probabilities is 1/2, then one party can cheat with probability 1. This rules out quantum protocols of this type from attaining the optimal level of security. ",quantum classical coin flip protocols base bite commitment point game focus family quantum coin flip protocols base bite commitment discuss semidefinite program formulations cheat strategies reduce optimize linear combination fidelity function polytope turn much simpler semidefinite program model use second order cone program problems use simplifications construct point game develop kitaev also study classical version protocols use linear optimization formulate optimal cheat strategies construct point game classical protocols well use analysis quantum case discuss philosophical connections classical quantum protocols point game view optimization theory particular observe analogy spectrum physical theories classical quantum spectrum convex optimization problems linear program semidefinite program second order cone program analogy classical systems correspond linear program problems level quantum feature system correlate level sophistication semidefinite program model optimization side concern security analysis use classical point game prove every classical protocol type allow exactly one party entirely determine coin flip use relationships quantum classical protocols show classical protocols saturate kitaev lower bind strong coin flip moreover product alice bob optimal cheat probabilities one party cheat probability rule quantum protocols type attain optimal level security,177,8,1504.04217.txt
http://arxiv.org/abs/1504.04867,Information Hiding as a Challenge for Malware Detection,  Information hiding techniques are increasingly utilized by the current malware to hide its existence and communication attempts. In this paper we highlight this new trend by reviewing the most notable examples of malicious software that shows this capability. ,Computer Science - Cryptography and Security ; ,"Mazurczyk, Wojciech ; Caviglione, Luca ; ",Information Hiding as a Challenge for Malware Detection  Information hiding techniques are increasingly utilized by the current malware to hide its existence and communication attempts. In this paper we highlight this new trend by reviewing the most notable examples of malicious software that shows this capability. ,information hide challenge malware detection information hide techniques increasingly utilize current malware hide existence communication attempt paper highlight new trend review notable examples malicious software show capability,27,0,1504.04867.txt
http://arxiv.org/abs/1504.04869,Equitable total coloring of corona of cubic graphs,"  The minimum number of total independent partition sets of $V \cup E$ of a graph $G=(V,E)$ is called the \emph{total chromatic number} of $G$, denoted by $\chi''(G)$. If the difference between cardinalities of any two total independent sets is at most one, then the minimum number of total independent partition sets of $V \cup E$ is called the \emph{equitable total chromatic number}, and is denoted by $\chi''_=(G)$.   In this paper we consider equitable total coloring of coronas of cubic graphs, $G \circ H$. It turns out that, independly on the values of equitable total chromatic number of factors $G$ and $H$, equitable total chromatic number of corona $G \circ H$ is equal to $\Delta(G \circ H) +1$. Thereby, we confirm Total Coloring Conjecture (TCC), posed by Behzad in 1964, and Equitable Total Coloring Conjecture (ETCC), posed by Wang in 2002, for coronas of cubic graphs. As a direct consequence we get that all coronas of cubic graphs are of Type 1. ","Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; 05C15, 05C76 ; ","Furmańczyk, Hanna ; Zuazua, Rita ; ","Equitable total coloring of corona of cubic graphs  The minimum number of total independent partition sets of $V \cup E$ of a graph $G=(V,E)$ is called the \emph{total chromatic number} of $G$, denoted by $\chi''(G)$. If the difference between cardinalities of any two total independent sets is at most one, then the minimum number of total independent partition sets of $V \cup E$ is called the \emph{equitable total chromatic number}, and is denoted by $\chi''_=(G)$.   In this paper we consider equitable total coloring of coronas of cubic graphs, $G \circ H$. It turns out that, independly on the values of equitable total chromatic number of factors $G$ and $H$, equitable total chromatic number of corona $G \circ H$ is equal to $\Delta(G \circ H) +1$. Thereby, we confirm Total Coloring Conjecture (TCC), posed by Behzad in 1964, and Equitable Total Coloring Conjecture (ETCC), posed by Wang in 2002, for coronas of cubic graphs. As a direct consequence we get that all coronas of cubic graphs are of Type 1. ",equitable total color corona cubic graph minimum number total independent partition set cup graph call emph total chromatic number denote chi difference cardinalities two total independent set one minimum number total independent partition set cup call emph equitable total chromatic number denote chi paper consider equitable total color coronas cubic graph circ turn independly value equitable total chromatic number factor equitable total chromatic number corona circ equal delta circ thereby confirm total color conjecture tcc pose behzad equitable total color conjecture etcc pose wang coronas cubic graph direct consequence get coronas cubic graph type,94,13,1504.04869.txt
http://arxiv.org/abs/1504.05895,Semantic Enrichment of Mobile Phone Data Records Using Background   Knowledge,"  Every day, billions of mobile network events (i.e. CDRs) are generated by cellular phone operator companies. Latent in this data are inspiring insights about human actions and behaviors, the discovery of which is important because context-aware applications and services hold the key to user-driven, intelligent services, which can enhance our everyday lives such as social and economic development, urban planning, and health prevention. The major challenge in this area is that interpreting such a big stream of data requires a deep understanding of mobile network events' context through available background knowledge. This article addresses the issues in context awareness given heterogeneous and uncertain data of mobile network events missing reliable information on the context of this activity. The contribution of this research is a model from a combination of logical and statistical reasoning standpoints for enabling human activity inference in qualitative terms from open geographical data that aimed at improving the quality of human behaviors recognition tasks from CDRs. We use open geographical data, Openstreetmap (OSM), as a proxy for predicting the content of human activity in the area. The user study performed in Trento shows that predicted human activities (top level) match the survey data with around 93% overall accuracy. The extensive validation for predicting a more specific economic type of human activity performed in Barcelona, by employing credit card transaction data. The analysis identifies that appropriately normalized data on points of interest (POI) is a good proxy for predicting human economical activities, with 84% accuracy on average. So the model is proven to be efficient for predicting the context of human activity, when its total level could be efficiently observed from cell phone data records, missing contextual information however. ",Computer Science - Artificial Intelligence ; Computer Science - Information Theory ; 68 ; H.1.2 ; H.2.8 ; H.3.3 ; I.2.3 ; I.2.4 ; G.3 ; ,"Dashdorj, Zolzaya ; Sobolevsky, Stanislav ; Serafini, Luciano ; Antonelli, Fabrizio ; Ratti, Carlo ; ","Semantic Enrichment of Mobile Phone Data Records Using Background   Knowledge  Every day, billions of mobile network events (i.e. CDRs) are generated by cellular phone operator companies. Latent in this data are inspiring insights about human actions and behaviors, the discovery of which is important because context-aware applications and services hold the key to user-driven, intelligent services, which can enhance our everyday lives such as social and economic development, urban planning, and health prevention. The major challenge in this area is that interpreting such a big stream of data requires a deep understanding of mobile network events' context through available background knowledge. This article addresses the issues in context awareness given heterogeneous and uncertain data of mobile network events missing reliable information on the context of this activity. The contribution of this research is a model from a combination of logical and statistical reasoning standpoints for enabling human activity inference in qualitative terms from open geographical data that aimed at improving the quality of human behaviors recognition tasks from CDRs. We use open geographical data, Openstreetmap (OSM), as a proxy for predicting the content of human activity in the area. The user study performed in Trento shows that predicted human activities (top level) match the survey data with around 93% overall accuracy. The extensive validation for predicting a more specific economic type of human activity performed in Barcelona, by employing credit card transaction data. The analysis identifies that appropriately normalized data on points of interest (POI) is a good proxy for predicting human economical activities, with 84% accuracy on average. So the model is proven to be efficient for predicting the context of human activity, when its total level could be efficiently observed from cell phone data records, missing contextual information however. ",semantic enrichment mobile phone data record use background knowledge every day billions mobile network events cdrs generate cellular phone operator company latent data inspire insights human action behaviors discovery important context aware applications service hold key user drive intelligent service enhance everyday live social economic development urban plan health prevention major challenge area interpret big stream data require deep understand mobile network events context available background knowledge article address issue context awareness give heterogeneous uncertain data mobile network events miss reliable information context activity contribution research model combination logical statistical reason standpoints enable human activity inference qualitative term open geographical data aim improve quality human behaviors recognition task cdrs use open geographical data openstreetmap osm proxy predict content human activity area user study perform trento show predict human activities top level match survey data around overall accuracy extensive validation predict specific economic type human activity perform barcelona employ credit card transaction data analysis identify appropriately normalize data point interest poi good proxy predict human economical activities accuracy average model prove efficient predict context human activity total level could efficiently observe cell phone data record miss contextual information however,188,10,1504.05895.txt
http://arxiv.org/abs/1504.06043,Stability of Stochastic Approximations with `Controlled Markov' Noise   and Temporal Difference Learning,"  We are interested in understanding stability (almost sure boundedness) of stochastic approximation algorithms (SAs) driven by a `controlled Markov' process. Analyzing this class of algorithms is important, since many reinforcement learning (RL) algorithms can be cast as SAs driven by a `controlled Markov' process. In this paper, we present easily verifiable sufficient conditions for stability and convergence of SAs driven by a `controlled Markov' process. Many RL applications involve continuous state spaces. While our analysis readily ensures stability for such continuous state applications, traditional analyses do not. As compared to literature, our analysis presents a two-fold generalization (a) the Markov process may evolve in a continuous state space and (b) the process need not be ergodic under any given stationary policy. Temporal difference learning (TD) is an important policy evaluation method in reinforcement learning. The theory developed herein, is used to analyze generalized $TD(0)$, an important variant of TD. Our theory is also used to analyze a TD formulation of supervised learning for forecasting problems. ","Computer Science - Systems and Control ; Statistics - Machine Learning ; 62L20, 93E03, 93E35, 34A60 ; ","Ramaswamy, Arunselvan ; Bhatnagar, Shalabh ; ","Stability of Stochastic Approximations with `Controlled Markov' Noise   and Temporal Difference Learning  We are interested in understanding stability (almost sure boundedness) of stochastic approximation algorithms (SAs) driven by a `controlled Markov' process. Analyzing this class of algorithms is important, since many reinforcement learning (RL) algorithms can be cast as SAs driven by a `controlled Markov' process. In this paper, we present easily verifiable sufficient conditions for stability and convergence of SAs driven by a `controlled Markov' process. Many RL applications involve continuous state spaces. While our analysis readily ensures stability for such continuous state applications, traditional analyses do not. As compared to literature, our analysis presents a two-fold generalization (a) the Markov process may evolve in a continuous state space and (b) the process need not be ergodic under any given stationary policy. Temporal difference learning (TD) is an important policy evaluation method in reinforcement learning. The theory developed herein, is used to analyze generalized $TD(0)$, an important variant of TD. Our theory is also used to analyze a TD formulation of supervised learning for forecasting problems. ",stability stochastic approximations control markov noise temporal difference learn interest understand stability almost sure boundedness stochastic approximation algorithms sas drive control markov process analyze class algorithms important since many reinforcement learn rl algorithms cast sas drive control markov process paper present easily verifiable sufficient condition stability convergence sas drive control markov process many rl applications involve continuous state space analysis readily ensure stability continuous state applications traditional analyse compare literature analysis present two fold generalization markov process may evolve continuous state space process need ergodic give stationary policy temporal difference learn td important policy evaluation method reinforcement learn theory develop herein use analyze generalize td important variant td theory also use analyze td formulation supervise learn forecast problems,118,11,1504.06043.txt
http://arxiv.org/abs/1504.06234,Acyclic chromatic index of triangle-free 1-planar graphs,"  An acyclic edge coloring of a graph $G$ is a proper edge coloring such that every cycle is colored with at least three colors. The acyclic chromatic index $\chiup_{a}'(G)$ of a graph $G$ is the least number of colors in an acyclic edge coloring of $G$. It was conjectured that $\chiup'_{a}(G)\leq \Delta(G) + 2$ for any simple graph $G$ with maximum degree $\Delta(G)$. A graph is {\em $1$-planar} if it can be drawn on the plane such that every edge is crossed by at most one other edge. In this paper, we prove that every triangle-free $1$-planar graph $G$ has an acyclic edge coloring with $\Delta(G) + 16$ colors. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C15 ; ,"Chen, Jijuan ; Wang, Tao ; Zhang, Huiqin ; ","Acyclic chromatic index of triangle-free 1-planar graphs  An acyclic edge coloring of a graph $G$ is a proper edge coloring such that every cycle is colored with at least three colors. The acyclic chromatic index $\chiup_{a}'(G)$ of a graph $G$ is the least number of colors in an acyclic edge coloring of $G$. It was conjectured that $\chiup'_{a}(G)\leq \Delta(G) + 2$ for any simple graph $G$ with maximum degree $\Delta(G)$. A graph is {\em $1$-planar} if it can be drawn on the plane such that every edge is crossed by at most one other edge. In this paper, we prove that every triangle-free $1$-planar graph $G$ has an acyclic edge coloring with $\Delta(G) + 16$ colors. ",acyclic chromatic index triangle free planar graph acyclic edge color graph proper edge color every cycle color least three color acyclic chromatic index chiup graph least number color acyclic edge color conjecture chiup leq delta simple graph maximum degree delta graph em planar draw plane every edge cross one edge paper prove every triangle free planar graph acyclic edge color delta color,62,13,1504.06234.txt
http://arxiv.org/abs/1504.06320,The Fallacy of Favoring Gradual Replacement Mind Uploading Over   Scan-and-Copy,  Mind uploading speculation and debate often concludes that a procedure described as gradual in-place replacement preserves personal identity while a procedure described as destructive scan-and-copy produces some other identity in the target substrate such that personal identity is lost along with the biological brain. This paper demonstrates a chain of reasoning that establishes metaphysical equivalence between these two methods in terms of preserving personal identity. ,Computer Science - Other Computer Science ; I.2.0 ; ,"Wiley, Keith B. ; Koene, Randal A. ; ",The Fallacy of Favoring Gradual Replacement Mind Uploading Over   Scan-and-Copy  Mind uploading speculation and debate often concludes that a procedure described as gradual in-place replacement preserves personal identity while a procedure described as destructive scan-and-copy produces some other identity in the target substrate such that personal identity is lost along with the biological brain. This paper demonstrates a chain of reasoning that establishes metaphysical equivalence between these two methods in terms of preserving personal identity. ,fallacy favor gradual replacement mind upload scan copy mind upload speculation debate often conclude procedure describe gradual place replacement preserve personal identity procedure describe destructive scan copy produce identity target substrate personal identity lose along biological brain paper demonstrate chain reason establish metaphysical equivalence two methods term preserve personal identity,50,2,1504.06320.txt
http://arxiv.org/abs/1504.06544,Sampling Correctors,"  In many situations, sample data is obtained from a noisy or imperfect source. In order to address such corruptions, this paper introduces the concept of a sampling corrector. Such algorithms use structure that the distribution is purported to have, in order to allow one to make ""on-the-fly"" corrections to samples drawn from probability distributions. These algorithms then act as filters between the noisy data and the end user.   We show connections between sampling correctors, distribution learning algorithms, and distribution property testing algorithms. We show that these connections can be utilized to expand the applicability of known distribution learning and property testing algorithms as well as to achieve improved algorithms for those tasks.   As a first step, we show how to design sampling correctors using proper learning algorithms. We then focus on the question of whether algorithms for sampling correctors can be more efficient in terms of sample complexity than learning algorithms for the analogous families of distributions. When correcting monotonicity, we show that this is indeed the case when also granted query access to the cumulative distribution function. We also obtain sampling correctors for monotonicity without this stronger type of access, provided that the distribution be originally very close to monotone (namely, at a distance $O(1/\log^2 n)$). In addition to that, we consider a restricted error model that aims at capturing ""missing data"" corruptions. In this model, we show that distributions that are close to monotone have sampling correctors that are significantly more efficient than achievable by the learning approach.   We also consider the question of whether an additional source of independent random bits is required by sampling correctors to implement the correction process. ",Computer Science - Data Structures and Algorithms ; Computer Science - Machine Learning ; Mathematics - Probability ; ,"Canonne, Clément ; Gouleakis, Themis ; Rubinfeld, Ronitt ; ","Sampling Correctors  In many situations, sample data is obtained from a noisy or imperfect source. In order to address such corruptions, this paper introduces the concept of a sampling corrector. Such algorithms use structure that the distribution is purported to have, in order to allow one to make ""on-the-fly"" corrections to samples drawn from probability distributions. These algorithms then act as filters between the noisy data and the end user.   We show connections between sampling correctors, distribution learning algorithms, and distribution property testing algorithms. We show that these connections can be utilized to expand the applicability of known distribution learning and property testing algorithms as well as to achieve improved algorithms for those tasks.   As a first step, we show how to design sampling correctors using proper learning algorithms. We then focus on the question of whether algorithms for sampling correctors can be more efficient in terms of sample complexity than learning algorithms for the analogous families of distributions. When correcting monotonicity, we show that this is indeed the case when also granted query access to the cumulative distribution function. We also obtain sampling correctors for monotonicity without this stronger type of access, provided that the distribution be originally very close to monotone (namely, at a distance $O(1/\log^2 n)$). In addition to that, we consider a restricted error model that aims at capturing ""missing data"" corruptions. In this model, we show that distributions that are close to monotone have sampling correctors that are significantly more efficient than achievable by the learning approach.   We also consider the question of whether an additional source of independent random bits is required by sampling correctors to implement the correction process. ",sample correctors many situations sample data obtain noisy imperfect source order address corruptions paper introduce concept sample corrector algorithms use structure distribution purport order allow one make fly corrections sample draw probability distributions algorithms act filter noisy data end user show connections sample correctors distribution learn algorithms distribution property test algorithms show connections utilize expand applicability know distribution learn property test algorithms well achieve improve algorithms task first step show design sample correctors use proper learn algorithms focus question whether algorithms sample correctors efficient term sample complexity learn algorithms analogous families distributions correct monotonicity show indeed case also grant query access cumulative distribution function also obtain sample correctors monotonicity without stronger type access provide distribution originally close monotone namely distance log addition consider restrict error model aim capture miss data corruptions model show distributions close monotone sample correctors significantly efficient achievable learn approach also consider question whether additional source independent random bits require sample correctors implement correction process,158,12,1504.06544.txt
http://arxiv.org/abs/1504.06582,Approximate Fitting of a Circular Arc When Two Points Are Known,"  The task of approximating points with circular arcs is performed in many applications, such as polyline compression, noise filtering, and feature recognition. However, the development of algorithms that perform a significant amount of circular arcs fitting requires an efficient way of fitting circular arcs with complexity O(1). The elegant solution to this task based on an eigenvector problem for a square nonsymmetrical matrix is described in [1]. For the compression algorithm described in [2], it is necessary to solve this task when two points on the arc are known. This paper describes a different approach to efficiently fitting the arcs and solves the task when one or two points are known. ",Computer Science - Computational Geometry ; ,"Gribov, Alexander ; ","Approximate Fitting of a Circular Arc When Two Points Are Known  The task of approximating points with circular arcs is performed in many applications, such as polyline compression, noise filtering, and feature recognition. However, the development of algorithms that perform a significant amount of circular arcs fitting requires an efficient way of fitting circular arcs with complexity O(1). The elegant solution to this task based on an eigenvector problem for a square nonsymmetrical matrix is described in [1]. For the compression algorithm described in [2], it is necessary to solve this task when two points on the arc are known. This paper describes a different approach to efficiently fitting the arcs and solves the task when one or two points are known. ",approximate fit circular arc two point know task approximate point circular arc perform many applications polyline compression noise filter feature recognition however development algorithms perform significant amount circular arc fit require efficient way fit circular arc complexity elegant solution task base eigenvector problem square nonsymmetrical matrix describe compression algorithm describe necessary solve task two point arc know paper describe different approach efficiently fit arc solve task one two point know,70,4,1504.06582.txt
http://arxiv.org/abs/1504.06584,Searching for a Compressed Polyline with a Minimum Number of Vertices,"  There are many practical applications that require simplification of polylines. Some of the goals are to reduce the amount of information necessary to store, improve processing time, or simplify editing. The simplification is usually done by removing some of the vertices, making the resultant polyline go through a subset of the source polyline vertices. However, such approaches do not necessarily produce a new polyline with the minimum number of vertices. The approximate solution to find a polyline, within a specified tolerance, with the minimum number of vertices is described in this paper. ",Computer Science - Computational Geometry ; ,"Gribov, Alexander ; ","Searching for a Compressed Polyline with a Minimum Number of Vertices  There are many practical applications that require simplification of polylines. Some of the goals are to reduce the amount of information necessary to store, improve processing time, or simplify editing. The simplification is usually done by removing some of the vertices, making the resultant polyline go through a subset of the source polyline vertices. However, such approaches do not necessarily produce a new polyline with the minimum number of vertices. The approximate solution to find a polyline, within a specified tolerance, with the minimum number of vertices is described in this paper. ",search compress polyline minimum number vertices many practical applications require simplification polylines goals reduce amount information necessary store improve process time simplify edit simplification usually do remove vertices make resultant polyline go subset source polyline vertices however approach necessarily produce new polyline minimum number vertices approximate solution find polyline within specify tolerance minimum number vertices describe paper,57,4,1504.06584.txt
http://arxiv.org/abs/1504.06804,High Speed Hashing for Integers and Strings,"  These notes describe the most efficient hash functions currently known for hashing integers and strings. These modern hash functions are often an order of magnitude faster than those presented in standard text books. They are also simpler to implement, and hence a clear win in practice, but their analysis is harder. Some of the most practical hash functions have only appeared in theory papers, and some of them requires combining results from different theory papers. The goal here is to combine the information in lecture-style notes that can be used by theoreticians and practitioners alike, thus making these practical fruits of theory more widely accessible. ",Computer Science - Data Structures and Algorithms ; ,"Thorup, Mikkel ; ","High Speed Hashing for Integers and Strings  These notes describe the most efficient hash functions currently known for hashing integers and strings. These modern hash functions are often an order of magnitude faster than those presented in standard text books. They are also simpler to implement, and hence a clear win in practice, but their analysis is harder. Some of the most practical hash functions have only appeared in theory papers, and some of them requires combining results from different theory papers. The goal here is to combine the information in lecture-style notes that can be used by theoreticians and practitioners alike, thus making these practical fruits of theory more widely accessible. ",high speed hash integers string note describe efficient hash function currently know hash integers string modern hash function often order magnitude faster present standard text book also simpler implement hence clear win practice analysis harder practical hash function appear theory paper require combine result different theory paper goal combine information lecture style note use theoreticians practitioners alike thus make practical fruit theory widely accessible,64,7,1504.06804.txt
http://arxiv.org/abs/1504.06979,Obstructions for three-coloring graphs without induced paths on six   vertices,"  We prove that there are 24 4-critical $P_6$-free graphs, and give the complete list. We remark that, if $H$ is connected and not a subgraph of $P_6$, there are infinitely many 4-critical $H$-free graphs. Our result answers questions of Golovach et al. and Seymour. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Chudnovsky, Maria ; Goedgebeur, Jan ; Schaudt, Oliver ; Zhong, Mingxian ; ","Obstructions for three-coloring graphs without induced paths on six   vertices  We prove that there are 24 4-critical $P_6$-free graphs, and give the complete list. We remark that, if $H$ is connected and not a subgraph of $P_6$, there are infinitely many 4-critical $H$-free graphs. Our result answers questions of Golovach et al. and Seymour. ",obstructions three color graph without induce paths six vertices prove critical free graph give complete list remark connect subgraph infinitely many critical free graph result answer question golovach et al seymour,31,13,1504.06979.txt
http://arxiv.org/abs/1504.07056,A Deterministic Almost-Tight Distributed Algorithm for Approximating   Single-Source Shortest Paths,"  We present a deterministic $(1+o(1))$-approximation $(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for solving the single-source shortest paths problem on distributed weighted networks (the CONGEST model); here $n$ is the number of nodes in the network and $D$ is its (hop) diameter. This is the first non-trivial deterministic algorithm for this problem. It also improves (i) the running time of the randomized $(1+o(1))$-approximation $\tilde O(n^{1/2}D^{1/4}+D)$-time algorithm of Nanongkai [STOC 2014] by a factor of as large as $n^{1/8}$, and (ii) the $O(\epsilon^{-1}\log \epsilon^{-1})$-approximation factor of Lenzen and Patt-Shamir's $\tilde O(n^{1/2+\epsilon}+D)$-time algorithm [STOC 2013] within the same running time. Our running time matches the known time lower bound of $\Omega(n^{1/2}/\log n + D)$ [Elkin STOC 2004] up to subpolynomial factors, thus essentially settling the status of this problem which was raised at least a decade ago [Elkin SIGACT News 2004]. It also implies a $(2+o(1))$-approximation $(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for approximating a network's weighted diameter which almost matches the lower bound by Holzer and Pinsker [OPODIS 2015]. In achieving this result, we develop two techniques which might be of independent interest and useful in other settings: (i) a deterministic process that replaces the ""hitting set argument"" commonly used for shortest paths computation in various settings, and (ii) a simple, deterministic, construction of an $(n^{o(1)}, o(1))$-hop set of size $n^{1+o(1)}$. We combine these techniques with many distributed algorithmic techniques, some of which from problems that are not directly related to shortest paths, e.g., ruling sets [Goldberg et al. STOC 1987], source detection [Lenzen and Peleg PODC 2013], and partial distance estimation [Lenzen and Patt-Shamir PODC 2015]. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Data Structures and Algorithms ; C.2.4 ; F.2.0 ; G.2.2 ; ","Henzinger, Monika ; Krinninger, Sebastian ; Nanongkai, Danupon ; ","A Deterministic Almost-Tight Distributed Algorithm for Approximating   Single-Source Shortest Paths  We present a deterministic $(1+o(1))$-approximation $(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for solving the single-source shortest paths problem on distributed weighted networks (the CONGEST model); here $n$ is the number of nodes in the network and $D$ is its (hop) diameter. This is the first non-trivial deterministic algorithm for this problem. It also improves (i) the running time of the randomized $(1+o(1))$-approximation $\tilde O(n^{1/2}D^{1/4}+D)$-time algorithm of Nanongkai [STOC 2014] by a factor of as large as $n^{1/8}$, and (ii) the $O(\epsilon^{-1}\log \epsilon^{-1})$-approximation factor of Lenzen and Patt-Shamir's $\tilde O(n^{1/2+\epsilon}+D)$-time algorithm [STOC 2013] within the same running time. Our running time matches the known time lower bound of $\Omega(n^{1/2}/\log n + D)$ [Elkin STOC 2004] up to subpolynomial factors, thus essentially settling the status of this problem which was raised at least a decade ago [Elkin SIGACT News 2004]. It also implies a $(2+o(1))$-approximation $(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for approximating a network's weighted diameter which almost matches the lower bound by Holzer and Pinsker [OPODIS 2015]. In achieving this result, we develop two techniques which might be of independent interest and useful in other settings: (i) a deterministic process that replaces the ""hitting set argument"" commonly used for shortest paths computation in various settings, and (ii) a simple, deterministic, construction of an $(n^{o(1)}, o(1))$-hop set of size $n^{1+o(1)}$. We combine these techniques with many distributed algorithmic techniques, some of which from problems that are not directly related to shortest paths, e.g., ruling sets [Goldberg et al. STOC 1987], source detection [Lenzen and Peleg PODC 2013], and partial distance estimation [Lenzen and Patt-Shamir PODC 2015]. ",deterministic almost tight distribute algorithm approximate single source shortest paths present deterministic approximation time algorithm solve single source shortest paths problem distribute weight network congest model number nod network hop diameter first non trivial deterministic algorithm problem also improve run time randomize approximation tilde time algorithm nanongkai stoc factor large ii epsilon log epsilon approximation factor lenzen patt shamir tilde epsilon time algorithm stoc within run time run time match know time lower bind omega log elkin stoc subpolynomial factor thus essentially settle status problem raise least decade ago elkin sigact news also imply approximation time algorithm approximate network weight diameter almost match lower bind holzer pinsker opodis achieve result develop two techniques might independent interest useful settings deterministic process replace hit set argument commonly use shortest paths computation various settings ii simple deterministic construction hop set size combine techniques many distribute algorithmic techniques problems directly relate shortest paths rule set goldberg et al stoc source detection lenzen peleg podc partial distance estimation lenzen patt shamir podc,167,1,1504.07056.txt
http://arxiv.org/abs/1504.07766,A multi-class approach for ranking graph nodes: models and experiments   with incomplete data,"  After the phenomenal success of the PageRank algorithm, many researchers have extended the PageRank approach to ranking graphs with richer structures beside the simple linkage structure. In some scenarios we have to deal with multi-parameters data where each node has additional features and there are relationships between such features.   This paper stems from the need of a systematic approach when dealing with multi-parameter data. We propose models and ranking algorithms which can be used with little adjustments for a large variety of networks (bibliographic data, patent data, twitter and social data, healthcare data). In this paper we focus on several aspects which have not been addressed in the literature: (1) we propose different models for ranking multi-parameters data and a class of numerical algorithms for efficiently computing the ranking score of such models, (2) by analyzing the stability and convergence properties of the numerical schemes we tune a fast and stable technique for the ranking problem, (3) we consider the issue of the robustness of our models when data are incomplete. The comparison of the rank on the incomplete data with the rank on the full structure shows that our models compute consistent rankings whose correlation is up to 60% when just 10% of the links of the attributes are maintained suggesting the suitability of our model also when the data are incomplete. ",Mathematics - Numerical Analysis ; Computer Science - Information Retrieval ; Physics - Physics and Society ; 65F15 ; G.2.2 ; F.2.1 ; ,"Del Corso, Gianna M. ; Romani, Francesco ; ","A multi-class approach for ranking graph nodes: models and experiments   with incomplete data  After the phenomenal success of the PageRank algorithm, many researchers have extended the PageRank approach to ranking graphs with richer structures beside the simple linkage structure. In some scenarios we have to deal with multi-parameters data where each node has additional features and there are relationships between such features.   This paper stems from the need of a systematic approach when dealing with multi-parameter data. We propose models and ranking algorithms which can be used with little adjustments for a large variety of networks (bibliographic data, patent data, twitter and social data, healthcare data). In this paper we focus on several aspects which have not been addressed in the literature: (1) we propose different models for ranking multi-parameters data and a class of numerical algorithms for efficiently computing the ranking score of such models, (2) by analyzing the stability and convergence properties of the numerical schemes we tune a fast and stable technique for the ranking problem, (3) we consider the issue of the robustness of our models when data are incomplete. The comparison of the rank on the incomplete data with the rank on the full structure shows that our models compute consistent rankings whose correlation is up to 60% when just 10% of the links of the attributes are maintained suggesting the suitability of our model also when the data are incomplete. ",multi class approach rank graph nod model experiment incomplete data phenomenal success pagerank algorithm many researchers extend pagerank approach rank graph richer structure beside simple linkage structure scenarios deal multi parameters data node additional feature relationships feature paper stem need systematic approach deal multi parameter data propose model rank algorithms use little adjustments large variety network bibliographic data patent data twitter social data healthcare data paper focus several aspects address literature propose different model rank multi parameters data class numerical algorithms efficiently compute rank score model analyze stability convergence properties numerical scheme tune fast stable technique rank problem consider issue robustness model data incomplete comparison rank incomplete data rank full structure show model compute consistent rank whose correlation link attribute maintain suggest suitability model also data incomplete,127,10,1504.07766.txt
http://arxiv.org/abs/1504.07959,Sublinear-Time Decremental Algorithms for Single-Source Reachability and   Shortest Paths on Directed Graphs,"  We consider dynamic algorithms for maintaining Single-Source Reachability (SSR) and approximate Single-Source Shortest Paths (SSSP) on $n$-node $m$-edge directed graphs under edge deletions (decremental algorithms). The previous fastest algorithm for SSR and SSSP goes back three decades to Even and Shiloach [JACM 1981]; it has $ O(1) $ query time and $ O (mn) $ total update time (i.e., linear amortized update time if all edges are deleted). This algorithm serves as a building block for several other dynamic algorithms. The question whether its total update time can be improved is a major, long standing, open problem.   In this paper, we answer this question affirmatively. We obtain a randomized algorithm with an expected total update time of $ O(\min (m^{7/6} n^{2/3 + o(1)}, m^{3/4} n^{5/4 + o(1)}) ) = O (m n^{9/10 + o(1)}) $ for SSR and $(1+\epsilon)$-approximate SSSP if the edge weights are integers from $ 1 $ to $ W \leq 2^{\log^c{n}} $ and $ \epsilon \geq 1 / \log^c{n} $ for some constant $ c $. We also extend our algorithm to achieve roughly the same running time for Strongly Connected Components (SCC), improving the algorithm of Roditty and Zwick [FOCS 2002]. Our algorithm is most efficient for sparse and dense graphs. When $ m = \Theta(n) $ its running time is $ O (n^{1 + 5/6 + o(1)}) $ and when $ m = \Theta(n^2) $ its running time is $ O (n^{2 + 3/4 + o(1)}) $. For SSR we also obtain an algorithm that is faster for dense graphs and has a total update time of $ O ( m^{2/3} n^{4/3 + o(1)} + m^{3/7} n^{12/7 + o(1)}) $ which is $ O (n^{2 + 2/3}) $ when $ m = \Theta(n^2) $. All our algorithms have constant query time in the worst case and are correct with high probability against an oblivious adversary. ",Computer Science - Data Structures and Algorithms ; ,"Henzinger, Monika ; Krinninger, Sebastian ; Nanongkai, Danupon ; ","Sublinear-Time Decremental Algorithms for Single-Source Reachability and   Shortest Paths on Directed Graphs  We consider dynamic algorithms for maintaining Single-Source Reachability (SSR) and approximate Single-Source Shortest Paths (SSSP) on $n$-node $m$-edge directed graphs under edge deletions (decremental algorithms). The previous fastest algorithm for SSR and SSSP goes back three decades to Even and Shiloach [JACM 1981]; it has $ O(1) $ query time and $ O (mn) $ total update time (i.e., linear amortized update time if all edges are deleted). This algorithm serves as a building block for several other dynamic algorithms. The question whether its total update time can be improved is a major, long standing, open problem.   In this paper, we answer this question affirmatively. We obtain a randomized algorithm with an expected total update time of $ O(\min (m^{7/6} n^{2/3 + o(1)}, m^{3/4} n^{5/4 + o(1)}) ) = O (m n^{9/10 + o(1)}) $ for SSR and $(1+\epsilon)$-approximate SSSP if the edge weights are integers from $ 1 $ to $ W \leq 2^{\log^c{n}} $ and $ \epsilon \geq 1 / \log^c{n} $ for some constant $ c $. We also extend our algorithm to achieve roughly the same running time for Strongly Connected Components (SCC), improving the algorithm of Roditty and Zwick [FOCS 2002]. Our algorithm is most efficient for sparse and dense graphs. When $ m = \Theta(n) $ its running time is $ O (n^{1 + 5/6 + o(1)}) $ and when $ m = \Theta(n^2) $ its running time is $ O (n^{2 + 3/4 + o(1)}) $. For SSR we also obtain an algorithm that is faster for dense graphs and has a total update time of $ O ( m^{2/3} n^{4/3 + o(1)} + m^{3/7} n^{12/7 + o(1)}) $ which is $ O (n^{2 + 2/3}) $ when $ m = \Theta(n^2) $. All our algorithms have constant query time in the worst case and are correct with high probability against an oblivious adversary. ",sublinear time decremental algorithms single source reachability shortest paths direct graph consider dynamic algorithms maintain single source reachability ssr approximate single source shortest paths sssp node edge direct graph edge deletions decremental algorithms previous fastest algorithm ssr sssp go back three decades even shiloach jacm query time mn total update time linear amortize update time edge delete algorithm serve build block several dynamic algorithms question whether total update time improve major long stand open problem paper answer question affirmatively obtain randomize algorithm expect total update time min ssr epsilon approximate sssp edge weight integers leq log epsilon geq log constant also extend algorithm achieve roughly run time strongly connect components scc improve algorithm roditty zwick focs algorithm efficient sparse dense graph theta run time theta run time ssr also obtain algorithm faster dense graph total update time theta algorithms constant query time worst case correct high probability oblivious adversary,149,1,1504.07959.txt
http://arxiv.org/abs/1504.08117,Average Convergence Rate of Evolutionary Algorithms,"  In evolutionary optimization, it is important to understand how fast evolutionary algorithms converge to the optimum per generation, or their convergence rate. This paper proposes a new measure of the convergence rate, called average convergence rate. It is a normalised geometric mean of the reduction ratio of the fitness difference per generation. The calculation of the average convergence rate is very simple and it is applicable for most evolutionary algorithms on both continuous and discrete optimization. A theoretical study of the average convergence rate is conducted for discrete optimization. Lower bounds on the average convergence rate are derived. The limit of the average convergence rate is analysed and then the asymptotic average convergence rate is proposed. ",Computer Science - Neural and Evolutionary Computing ; ,"He, Jun ; Lin, Guangming ; ","Average Convergence Rate of Evolutionary Algorithms  In evolutionary optimization, it is important to understand how fast evolutionary algorithms converge to the optimum per generation, or their convergence rate. This paper proposes a new measure of the convergence rate, called average convergence rate. It is a normalised geometric mean of the reduction ratio of the fitness difference per generation. The calculation of the average convergence rate is very simple and it is applicable for most evolutionary algorithms on both continuous and discrete optimization. A theoretical study of the average convergence rate is conducted for discrete optimization. Lower bounds on the average convergence rate are derived. The limit of the average convergence rate is analysed and then the asymptotic average convergence rate is proposed. ",average convergence rate evolutionary algorithms evolutionary optimization important understand fast evolutionary algorithms converge optimum per generation convergence rate paper propose new measure convergence rate call average convergence rate normalise geometric mean reduction ratio fitness difference per generation calculation average convergence rate simple applicable evolutionary algorithms continuous discrete optimization theoretical study average convergence rate conduct discrete optimization lower bound average convergence rate derive limit average convergence rate analyse asymptotic average convergence rate propose,72,11,1504.08117.txt
http://arxiv.org/abs/1505.00199,Theory of Optimizing Pseudolinear Performance Measures: Application to   F-measure,"  Non-linear performance measures are widely used for the evaluation of learning algorithms. For example, $F$-measure is a commonly used performance measure for classification problems in machine learning and information retrieval community. We study the theoretical properties of a subset of non-linear performance measures called pseudo-linear performance measures which includes $F$-measure, \emph{Jaccard Index}, among many others. We establish that many notions of $F$-measures and \emph{Jaccard Index} are pseudo-linear functions of the per-class false negatives and false positives for binary, multiclass and multilabel classification. Based on this observation, we present a general reduction of such performance measure optimization problem to cost-sensitive classification problem with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the $F$-measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on pseudo-linear measures, which are asymptotic in nature. We also establish the multi-objective nature of the $F$-score maximization problem by linking the algorithm with the weighted-sum approach used in multi-objective optimization. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various $F$-measure optimization tasks. ",Computer Science - Machine Learning ; ,"Parambath, Shameem A Puthiya ; Usunier, Nicolas ; Grandvalet, Yves ; ","Theory of Optimizing Pseudolinear Performance Measures: Application to   F-measure  Non-linear performance measures are widely used for the evaluation of learning algorithms. For example, $F$-measure is a commonly used performance measure for classification problems in machine learning and information retrieval community. We study the theoretical properties of a subset of non-linear performance measures called pseudo-linear performance measures which includes $F$-measure, \emph{Jaccard Index}, among many others. We establish that many notions of $F$-measures and \emph{Jaccard Index} are pseudo-linear functions of the per-class false negatives and false positives for binary, multiclass and multilabel classification. Based on this observation, we present a general reduction of such performance measure optimization problem to cost-sensitive classification problem with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the $F$-measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on pseudo-linear measures, which are asymptotic in nature. We also establish the multi-objective nature of the $F$-score maximization problem by linking the algorithm with the weighted-sum approach used in multi-objective optimization. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various $F$-measure optimization tasks. ",theory optimize pseudolinear performance measure application measure non linear performance measure widely use evaluation learn algorithms example measure commonly use performance measure classification problems machine learn information retrieval community study theoretical properties subset non linear performance measure call pseudo linear performance measure include measure emph jaccard index among many others establish many notions measure emph jaccard index pseudo linear function per class false negative false positives binary multiclass multilabel classification base observation present general reduction performance measure optimization problem cost sensitive classification problem unknown cost propose algorithm provable guarantee obtain approximately optimal classifier measure solve series cost sensitive classification problems strength analysis valid dataset class classifiers extend exist theoretical result pseudo linear measure asymptotic nature also establish multi objective nature score maximization problem link algorithm weight sum approach use multi objective optimization present numerical experiment illustrate relative importance cost asymmetry thresholding learn linear classifiers various measure optimization task,148,11,1505.00199.txt
http://arxiv.org/abs/1505.00398,Block Basis Factorization for Scalable Kernel Matrix Evaluation,"  Kernel methods are widespread in machine learning; however, they are limited by the quadratic complexity of the construction, application, and storage of kernel matrices. Low-rank matrix approximation algorithms are widely used to address this problem and reduce the arithmetic and storage cost. However, we observed that for some datasets with wide intra-class variability, the optimal kernel parameter for smaller classes yields a matrix that is less well approximated by low-rank methods. In this paper, we propose an efficient structured low-rank approximation method---the Block Basis Factorization (BBF)---and its fast construction algorithm to approximate radial basis function (RBF) kernel matrices. Our approach has linear memory cost and floating point operations. BBF works for a wide range of kernel bandwidth parameters and extends the domain of applicability of low-rank approximation methods significantly. Our empirical results demonstrate the stability and superiority over the state-of-art kernel approximation algorithms. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; Computer Science - Numerical Analysis ; ,"Wang, Ruoxi ; Li, Yingzhou ; Mahoney, Michael W. ; Darve, Eric ; ","Block Basis Factorization for Scalable Kernel Matrix Evaluation  Kernel methods are widespread in machine learning; however, they are limited by the quadratic complexity of the construction, application, and storage of kernel matrices. Low-rank matrix approximation algorithms are widely used to address this problem and reduce the arithmetic and storage cost. However, we observed that for some datasets with wide intra-class variability, the optimal kernel parameter for smaller classes yields a matrix that is less well approximated by low-rank methods. In this paper, we propose an efficient structured low-rank approximation method---the Block Basis Factorization (BBF)---and its fast construction algorithm to approximate radial basis function (RBF) kernel matrices. Our approach has linear memory cost and floating point operations. BBF works for a wide range of kernel bandwidth parameters and extends the domain of applicability of low-rank approximation methods significantly. Our empirical results demonstrate the stability and superiority over the state-of-art kernel approximation algorithms. ",block basis factorization scalable kernel matrix evaluation kernel methods widespread machine learn however limit quadratic complexity construction application storage kernel matrices low rank matrix approximation algorithms widely use address problem reduce arithmetic storage cost however observe datasets wide intra class variability optimal kernel parameter smaller class yield matrix less well approximate low rank methods paper propose efficient structure low rank approximation method block basis factorization bbf fast construction algorithm approximate radial basis function rbf kernel matrices approach linear memory cost float point operations bbf work wide range kernel bandwidth parameters extend domain applicability low rank approximation methods significantly empirical result demonstrate stability superiority state art kernel approximation algorithms,108,11,1505.00398.txt
http://arxiv.org/abs/1505.00947,Colocated MIMO Radar Waveform Design for Transmit Beampattern Formation,"  In this paper, colocated MIMO radar waveform design is considered by minimizing the integrated side-lobe level to obtain beam patterns with lower side-lobe levels than competing methods. First, a quadratic programming problem is formulated to design beam patterns by using the criteria for a minimal integrated side-lobe level. A theorem is derived that provides a closed-form analytical optimal solution that appears to be an extension of the Rayleigh quotient minimization for a possibly singular matrix in quadratic form. Such singularities are shown to occur in the problem of interest, but proofs for the optimum solution in these singular matrix cases could not be found in the literature. Next, an additional constraint is added to obtain beam patterns with desired 3 dB beamwidths, resulting in a nonconvex quadratically constrained quadratic program which is NP-hard. A semidefinite program and a Gaussian randomized semidefinite relaxation are used to determine feasible solutions arbitrarily close to the solution to the original problem. Theoretical and numerical analyses illustrate the impacts of changing the number of transmitters and orthogonal waveforms employed in the designs. Numerical comparisons are conducted to evaluate the proposed design approaches. ",Computer Science - Information Theory ; ,"Xu, Haisheng ; Blum, Rick S. ; Wang, Jian ; Yuan, Jian ; ","Colocated MIMO Radar Waveform Design for Transmit Beampattern Formation  In this paper, colocated MIMO radar waveform design is considered by minimizing the integrated side-lobe level to obtain beam patterns with lower side-lobe levels than competing methods. First, a quadratic programming problem is formulated to design beam patterns by using the criteria for a minimal integrated side-lobe level. A theorem is derived that provides a closed-form analytical optimal solution that appears to be an extension of the Rayleigh quotient minimization for a possibly singular matrix in quadratic form. Such singularities are shown to occur in the problem of interest, but proofs for the optimum solution in these singular matrix cases could not be found in the literature. Next, an additional constraint is added to obtain beam patterns with desired 3 dB beamwidths, resulting in a nonconvex quadratically constrained quadratic program which is NP-hard. A semidefinite program and a Gaussian randomized semidefinite relaxation are used to determine feasible solutions arbitrarily close to the solution to the original problem. Theoretical and numerical analyses illustrate the impacts of changing the number of transmitters and orthogonal waveforms employed in the designs. Numerical comparisons are conducted to evaluate the proposed design approaches. ",colocated mimo radar waveform design transmit beampattern formation paper colocated mimo radar waveform design consider minimize integrate side lobe level obtain beam pattern lower side lobe level compete methods first quadratic program problem formulate design beam pattern use criteria minimal integrate side lobe level theorem derive provide close form analytical optimal solution appear extension rayleigh quotient minimization possibly singular matrix quadratic form singularities show occur problem interest proof optimum solution singular matrix case could find literature next additional constraint add obtain beam pattern desire db beamwidths result nonconvex quadratically constrain quadratic program np hard semidefinite program gaussian randomize semidefinite relaxation use determine feasible solutions arbitrarily close solution original problem theoretical numerical analyse illustrate impact change number transmitters orthogonal waveforms employ design numerical comparisons conduct evaluate propose design approach,128,7,1505.00947.txt
http://arxiv.org/abs/1505.01189,On the Rigidity of Sparse Random Graphs,"  A graph with a trivial automorphism group is said to be rigid. Wright proved that for $\frac{\log n}{n}+\omega(\frac 1n)\leq p\leq \frac 12$ a random graph $G\in G(n,p)$ is rigid whp. It is not hard to see that this lower bound is sharp and for $p<\frac{(1-\epsilon)\log n}{n}$ with positive probability $\text{aut}(G)$ is nontrivial. We show that in the sparser case $\omega(\frac 1 n)\leq p\leq \frac{\log n}{n}+\omega(\frac 1n)$, it holds whp that $G$'s $2$-core is rigid. We conclude that for all $p$, a graph in $G(n,p)$ is reconstrutible whp. In addition this yields for $\omega(\frac 1n)\leq p\leq \frac 12$ a canonical labeling algorithm that almost surely runs in polynomial time with $o(1)$ error rate. This extends the range for which such an algorithm is currently known. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; Mathematics - Probability ; ,"Linial, Nati ; Mosheiff, Jonathan ; ","On the Rigidity of Sparse Random Graphs  A graph with a trivial automorphism group is said to be rigid. Wright proved that for $\frac{\log n}{n}+\omega(\frac 1n)\leq p\leq \frac 12$ a random graph $G\in G(n,p)$ is rigid whp. It is not hard to see that this lower bound is sharp and for $p<\frac{(1-\epsilon)\log n}{n}$ with positive probability $\text{aut}(G)$ is nontrivial. We show that in the sparser case $\omega(\frac 1 n)\leq p\leq \frac{\log n}{n}+\omega(\frac 1n)$, it holds whp that $G$'s $2$-core is rigid. We conclude that for all $p$, a graph in $G(n,p)$ is reconstrutible whp. In addition this yields for $\omega(\frac 1n)\leq p\leq \frac 12$ a canonical labeling algorithm that almost surely runs in polynomial time with $o(1)$ error rate. This extends the range for which such an algorithm is currently known. ",rigidity sparse random graph graph trivial automorphism group say rigid wright prove frac log omega frac leq leq frac random graph rigid whp hard see lower bind sharp frac epsilon log positive probability text aut nontrivial show sparser case omega frac leq leq frac log omega frac hold whp core rigid conclude graph reconstrutible whp addition yield omega frac leq leq frac canonical label algorithm almost surely run polynomial time error rate extend range algorithm currently know,77,1,1505.01189.txt
http://arxiv.org/abs/1505.01668,Multi-Target Tracking in Distributed Sensor Networks using Particle PHD   Filters,"  Multi-target tracking is an important problem in civilian and military applications. This paper investigates multi-target tracking in distributed sensor networks. Data association, which arises particularly in multi-object scenarios, can be tackled by various solutions. We consider sequential Monte Carlo implementations of the Probability Hypothesis Density (PHD) filter based on random finite sets. This approach circumvents the data association issue by jointly estimating all targets in the region of interest. To this end, we develop the Diffusion Particle PHD Filter (D-PPHDF) as well as a centralized version, called the Multi-Sensor Particle PHD Filter (MS-PPHDF). Their performance is evaluated in terms of the Optimal Subpattern Assignment (OSPA) metric, benchmarked against a distributed extension of the Posterior Cram\'er-Rao Lower Bound (PCRLB), and compared to the performance of an existing distributed PHD Particle Filter. Furthermore, the robustness of the proposed tracking algorithms against outliers and their performance with respect to different amounts of clutter is investigated. ",Computer Science - Multiagent Systems ; Computer Science - Systems and Control ; Statistics - Applications ; 68 ; ,"Leonard, Mark R. ; Zoubir, Abdelhak M. ; ","Multi-Target Tracking in Distributed Sensor Networks using Particle PHD   Filters  Multi-target tracking is an important problem in civilian and military applications. This paper investigates multi-target tracking in distributed sensor networks. Data association, which arises particularly in multi-object scenarios, can be tackled by various solutions. We consider sequential Monte Carlo implementations of the Probability Hypothesis Density (PHD) filter based on random finite sets. This approach circumvents the data association issue by jointly estimating all targets in the region of interest. To this end, we develop the Diffusion Particle PHD Filter (D-PPHDF) as well as a centralized version, called the Multi-Sensor Particle PHD Filter (MS-PPHDF). Their performance is evaluated in terms of the Optimal Subpattern Assignment (OSPA) metric, benchmarked against a distributed extension of the Posterior Cram\'er-Rao Lower Bound (PCRLB), and compared to the performance of an existing distributed PHD Particle Filter. Furthermore, the robustness of the proposed tracking algorithms against outliers and their performance with respect to different amounts of clutter is investigated. ",multi target track distribute sensor network use particle phd filter multi target track important problem civilian military applications paper investigate multi target track distribute sensor network data association arise particularly multi object scenarios tackle various solutions consider sequential monte carlo implementations probability hypothesis density phd filter base random finite set approach circumvent data association issue jointly estimate target region interest end develop diffusion particle phd filter pphdf well centralize version call multi sensor particle phd filter ms pphdf performance evaluate term optimal subpattern assignment ospa metric benchmarked distribute extension posterior cram er rao lower bind pcrlb compare performance exist distribute phd particle filter furthermore robustness propose track algorithms outliers performance respect different amount clutter investigate,115,2,1505.01668.txt
http://arxiv.org/abs/1505.02091,Weihrauch-completeness for layerwise computability,"  We introduce the notion of being Weihrauch-complete for layerwise computability and provide several natural examples related to complex oscillations, the law of the iterated logarithm and Birkhoff's theorem. We also consider hitting time operators, which share the Weihrauch degree of the former examples but fail to be layerwise computable. ",Computer Science - Logic in Computer Science ; ,"Pauly, Arno ; Fouché, Willem ; Davie, George ; ","Weihrauch-completeness for layerwise computability  We introduce the notion of being Weihrauch-complete for layerwise computability and provide several natural examples related to complex oscillations, the law of the iterated logarithm and Birkhoff's theorem. We also consider hitting time operators, which share the Weihrauch degree of the former examples but fail to be layerwise computable. ",weihrauch completeness layerwise computability introduce notion weihrauch complete layerwise computability provide several natural examples relate complex oscillations law iterate logarithm birkhoff theorem also consider hit time operators share weihrauch degree former examples fail layerwise computable,35,8,1505.02091.txt
http://arxiv.org/abs/1505.02213,Measuring dependence powerfully and equitably,"  Given a high-dimensional data set we often wish to find the strongest relationships within it. A common strategy is to evaluate a measure of dependence on every variable pair and retain the highest-scoring pairs for follow-up. This strategy works well if the statistic used is equitable [Reshef et al. 2015a], i.e., if, for some measure of noise, it assigns similar scores to equally noisy relationships regardless of relationship type (e.g., linear, exponential, periodic).   In this paper, we introduce and characterize a population measure of dependence called MIC*. We show three ways that MIC* can be viewed: as the population value of MIC, a highly equitable statistic from [Reshef et al. 2011], as a canonical ""smoothing"" of mutual information, and as the supremum of an infinite sequence defined in terms of optimal one-dimensional partitions of the marginals of the joint distribution. Based on this theory, we introduce an efficient approach for computing MIC* from the density of a pair of random variables, and we define a new consistent estimator MICe for MIC* that is efficiently computable. In contrast, there is no known polynomial-time algorithm for computing the original equitable statistic MIC. We show through simulations that MICe has better bias-variance properties than MIC. We then introduce and prove the consistency of a second statistic, TICe, that is a trivial side-product of the computation of MICe and whose goal is powerful independence testing rather than equitability.   We show in simulations that MICe and TICe have good equitability and power against independence respectively. The analyses here complement a more in-depth empirical evaluation of several leading measures of dependence [Reshef et al. 2015b] that shows state-of-the-art performance for MICe and TICe. ",Statistics - Methodology ; Computer Science - Information Theory ; Computer Science - Machine Learning ; Quantitative Biology - Quantitative Methods ; Statistics - Machine Learning ; ,"Reshef, Yakir A. ; Reshef, David N. ; Finucane, Hilary K. ; Sabeti, Pardis C. ; Mitzenmacher, Michael M. ; ","Measuring dependence powerfully and equitably  Given a high-dimensional data set we often wish to find the strongest relationships within it. A common strategy is to evaluate a measure of dependence on every variable pair and retain the highest-scoring pairs for follow-up. This strategy works well if the statistic used is equitable [Reshef et al. 2015a], i.e., if, for some measure of noise, it assigns similar scores to equally noisy relationships regardless of relationship type (e.g., linear, exponential, periodic).   In this paper, we introduce and characterize a population measure of dependence called MIC*. We show three ways that MIC* can be viewed: as the population value of MIC, a highly equitable statistic from [Reshef et al. 2011], as a canonical ""smoothing"" of mutual information, and as the supremum of an infinite sequence defined in terms of optimal one-dimensional partitions of the marginals of the joint distribution. Based on this theory, we introduce an efficient approach for computing MIC* from the density of a pair of random variables, and we define a new consistent estimator MICe for MIC* that is efficiently computable. In contrast, there is no known polynomial-time algorithm for computing the original equitable statistic MIC. We show through simulations that MICe has better bias-variance properties than MIC. We then introduce and prove the consistency of a second statistic, TICe, that is a trivial side-product of the computation of MICe and whose goal is powerful independence testing rather than equitability.   We show in simulations that MICe and TICe have good equitability and power against independence respectively. The analyses here complement a more in-depth empirical evaluation of several leading measures of dependence [Reshef et al. 2015b] that shows state-of-the-art performance for MICe and TICe. ",measure dependence powerfully equitably give high dimensional data set often wish find strongest relationships within common strategy evaluate measure dependence every variable pair retain highest score pair follow strategy work well statistic use equitable reshef et al measure noise assign similar score equally noisy relationships regardless relationship type linear exponential periodic paper introduce characterize population measure dependence call mic show three ways mic view population value mic highly equitable statistic reshef et al canonical smooth mutual information supremum infinite sequence define term optimal one dimensional partition marginals joint distribution base theory introduce efficient approach compute mic density pair random variables define new consistent estimator mice mic efficiently computable contrast know polynomial time algorithm compute original equitable statistic mic show simulations mice better bias variance properties mic introduce prove consistency second statistic tice trivial side product computation mice whose goal powerful independence test rather equitability show simulations mice tice good equitability power independence respectively analyse complement depth empirical evaluation several lead measure dependence reshef et al show state art performance mice tice,171,4,1505.02213.txt
http://arxiv.org/abs/1505.02214,An Empirical Study of Leading Measures of Dependence,"  In exploratory data analysis, we are often interested in identifying promising pairwise associations for further analysis while filtering out weaker, less interesting ones. This can be accomplished by computing a measure of dependence on all variable pairs and examining the highest-scoring pairs, provided the measure of dependence used assigns similar scores to equally noisy relationships of different types. This property, called equitability, is formalized in Reshef et al. [2015b]. In addition to equitability, measures of dependence can also be assessed by the power of their corresponding independence tests as well as their runtime.   Here we present extensive empirical evaluation of the equitability, power against independence, and runtime of several leading measures of dependence. These include two statistics introduced in Reshef et al. [2015a]: MICe, which has equitability as its primary goal, and TICe, which has power against independence as its goal. Regarding equitability, our analysis finds that MICe is the most equitable method on functional relationships in most of the settings we considered, although mutual information estimation proves the most equitable at large sample sizes in some specific settings. Regarding power against independence, we find that TICe, along with Heller and Gorfine's S^DDP, is the state of the art on the relationships we tested. Our analyses also show a trade-off between power against independence and equitability consistent with the theory in Reshef et al. [2015b]. In terms of runtime, MICe and TICe are significantly faster than many other measures of dependence tested, and computing either one makes computing the other trivial. This suggests that a fast and useful strategy for achieving a combination of power against independence and equitability may be to filter relationships by TICe and then to examine the MICe of only the significant ones. ",Statistics - Methodology ; Computer Science - Information Theory ; Computer Science - Machine Learning ; Quantitative Biology - Quantitative Methods ; Statistics - Machine Learning ; ,"Reshef, David N. ; Reshef, Yakir A. ; Sabeti, Pardis C. ; Mitzenmacher, Michael M. ; ","An Empirical Study of Leading Measures of Dependence  In exploratory data analysis, we are often interested in identifying promising pairwise associations for further analysis while filtering out weaker, less interesting ones. This can be accomplished by computing a measure of dependence on all variable pairs and examining the highest-scoring pairs, provided the measure of dependence used assigns similar scores to equally noisy relationships of different types. This property, called equitability, is formalized in Reshef et al. [2015b]. In addition to equitability, measures of dependence can also be assessed by the power of their corresponding independence tests as well as their runtime.   Here we present extensive empirical evaluation of the equitability, power against independence, and runtime of several leading measures of dependence. These include two statistics introduced in Reshef et al. [2015a]: MICe, which has equitability as its primary goal, and TICe, which has power against independence as its goal. Regarding equitability, our analysis finds that MICe is the most equitable method on functional relationships in most of the settings we considered, although mutual information estimation proves the most equitable at large sample sizes in some specific settings. Regarding power against independence, we find that TICe, along with Heller and Gorfine's S^DDP, is the state of the art on the relationships we tested. Our analyses also show a trade-off between power against independence and equitability consistent with the theory in Reshef et al. [2015b]. In terms of runtime, MICe and TICe are significantly faster than many other measures of dependence tested, and computing either one makes computing the other trivial. This suggests that a fast and useful strategy for achieving a combination of power against independence and equitability may be to filter relationships by TICe and then to examine the MICe of only the significant ones. ",empirical study lead measure dependence exploratory data analysis often interest identify promise pairwise associations analysis filter weaker less interest ones accomplish compute measure dependence variable pair examine highest score pair provide measure dependence use assign similar score equally noisy relationships different type property call equitability formalize reshef et al addition equitability measure dependence also assess power correspond independence test well runtime present extensive empirical evaluation equitability power independence runtime several lead measure dependence include two statistics introduce reshef et al mice equitability primary goal tice power independence goal regard equitability analysis find mice equitable method functional relationships settings consider although mutual information estimation prove equitable large sample size specific settings regard power independence find tice along heller gorfine ddp state art relationships test analyse also show trade power independence equitability consistent theory reshef et al term runtime mice tice significantly faster many measure dependence test compute either one make compute trivial suggest fast useful strategy achieve combination power independence equitability may filter relationships tice examine mice significant ones,168,4,1505.02214.txt
http://arxiv.org/abs/1505.02348,The Topology of Biological Networks from a Complexity Perspective,"  A complexity-theoretic approach to studying biological networks is proposed. A simple graph representation is used where molecules (DNA, RNA, proteins and chemicals) are vertices and relations between them are directed and signed (promotional (+) or inhibitory (-)) edges. Based on this model, the problem of network evolution (NE) is defined formally as an optimization problem and subsequently proven to be fundamentally hard (NP-hard) by means of reduction from the Knapsack problem (KP). Second, for empirical validation, various biological networks of experimentally-validated interactions are compared against randomly generated networks with varying degree distributions. An NE instance is created using a given real or synthetic (random) network. After being reverse-reduced to a KP instance, each NE instance is fed to a KP solver and the average achieved knapsack value-to-weight ratio is recorded from multiple rounds of simulated evolutionary pressure. The results show that biological networks (and synthetic networks of similar degree distribution) achieve the highest ratios at maximal evolutionary pressure and minimal error tolerance conditions. The more distant (in degree distribution) a synthetic network is from biological networks the lower its achieved ratio. The results shed light on how computational intractability has shaped the evolution of biological networks into their current topology. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; Quantitative Biology - Molecular Networks ; ,"Atiia, Ali ; Major, François ; Waldispühl, Jérôme ; ","The Topology of Biological Networks from a Complexity Perspective  A complexity-theoretic approach to studying biological networks is proposed. A simple graph representation is used where molecules (DNA, RNA, proteins and chemicals) are vertices and relations between them are directed and signed (promotional (+) or inhibitory (-)) edges. Based on this model, the problem of network evolution (NE) is defined formally as an optimization problem and subsequently proven to be fundamentally hard (NP-hard) by means of reduction from the Knapsack problem (KP). Second, for empirical validation, various biological networks of experimentally-validated interactions are compared against randomly generated networks with varying degree distributions. An NE instance is created using a given real or synthetic (random) network. After being reverse-reduced to a KP instance, each NE instance is fed to a KP solver and the average achieved knapsack value-to-weight ratio is recorded from multiple rounds of simulated evolutionary pressure. The results show that biological networks (and synthetic networks of similar degree distribution) achieve the highest ratios at maximal evolutionary pressure and minimal error tolerance conditions. The more distant (in degree distribution) a synthetic network is from biological networks the lower its achieved ratio. The results shed light on how computational intractability has shaped the evolution of biological networks into their current topology. ",topology biological network complexity perspective complexity theoretic approach study biological network propose simple graph representation use molecules dna rna proteins chemicals vertices relations direct sign promotional inhibitory edge base model problem network evolution ne define formally optimization problem subsequently prove fundamentally hard np hard mean reduction knapsack problem kp second empirical validation various biological network experimentally validate interactions compare randomly generate network vary degree distributions ne instance create use give real synthetic random network reverse reduce kp instance ne instance feed kp solver average achieve knapsack value weight ratio record multiple round simulate evolutionary pressure result show biological network synthetic network similar degree distribution achieve highest ratios maximal evolutionary pressure minimal error tolerance condition distant degree distribution synthetic network biological network lower achieve ratio result shed light computational intractability shape evolution biological network current topology,135,6,1505.02348.txt
http://arxiv.org/abs/1505.02921,How Far Can You Get By Combining Change Detection Algorithms?,"  Given the existence of many change detection algorithms, each with its own peculiarities and strengths, we propose a combination strategy, that we termed IUTIS (In Unity There Is Strength), based on a genetic Programming framework. This combination strategy is aimed at leveraging the strengths of the algorithms and compensate for their weakness. In this paper we show our findings in applying the proposed strategy in two different scenarios. The first scenario is purely performance-based. The second scenario performance and efficiency must be balanced. Results demonstrate that starting from simple algorithms we can achieve comparable results with respect to more complex state-of-the-art change detection algorithms, while keeping the computational complexity affordable for real-time applications. ",Computer Science - Computer Vision and Pattern Recognition ; I.4.8 ; G.1.6 ; ,"Bianco, Simone ; Ciocca, Gianluigi ; Schettini, Raimondo ; ","How Far Can You Get By Combining Change Detection Algorithms?  Given the existence of many change detection algorithms, each with its own peculiarities and strengths, we propose a combination strategy, that we termed IUTIS (In Unity There Is Strength), based on a genetic Programming framework. This combination strategy is aimed at leveraging the strengths of the algorithms and compensate for their weakness. In this paper we show our findings in applying the proposed strategy in two different scenarios. The first scenario is purely performance-based. The second scenario performance and efficiency must be balanced. Results demonstrate that starting from simple algorithms we can achieve comparable results with respect to more complex state-of-the-art change detection algorithms, while keeping the computational complexity affordable for real-time applications. ",far get combine change detection algorithms give existence many change detection algorithms peculiarities strengths propose combination strategy term iutis unity strength base genetic program framework combination strategy aim leverage strengths algorithms compensate weakness paper show find apply propose strategy two different scenarios first scenario purely performance base second scenario performance efficiency must balance result demonstrate start simple algorithms achieve comparable result respect complex state art change detection algorithms keep computational complexity affordable real time applications,75,11,1505.02921.txt
http://arxiv.org/abs/1505.03001,Detecting the large entries of a sparse covariance matrix in   sub-quadratic time,"  The covariance matrix of a $p$-dimensional random variable is a fundamental quantity in data analysis. Given $n$ i.i.d. observations, it is typically estimated by the sample covariance matrix, at a computational cost of $O(np^{2})$ operations. When $n,p$ are large, this computation may be prohibitively slow. Moreover, in several contemporary applications, the population matrix is approximately sparse, and only its few large entries are of interest. This raises the following question, at the focus of our work: Assuming approximate sparsity of the covariance matrix, can its large entries be detected much faster, say in sub-quadratic time, without explicitly computing all its $p^{2}$ entries? In this paper, we present and theoretically analyze two randomized algorithms that detect the large entries of an approximately sparse sample covariance matrix using only $O(np\text{ poly log } p)$ operations. Furthermore, assuming sparsity of the population matrix, we derive sufficient conditions on the underlying random variable and on the number of samples $n$, for the sample covariance matrix to satisfy our approximate sparsity requirements. Finally, we illustrate the performance of our algorithms via several simulations. ",Statistics - Computation ; Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Shwartz, Ofer ; Nadler, Boaz ; ","Detecting the large entries of a sparse covariance matrix in   sub-quadratic time  The covariance matrix of a $p$-dimensional random variable is a fundamental quantity in data analysis. Given $n$ i.i.d. observations, it is typically estimated by the sample covariance matrix, at a computational cost of $O(np^{2})$ operations. When $n,p$ are large, this computation may be prohibitively slow. Moreover, in several contemporary applications, the population matrix is approximately sparse, and only its few large entries are of interest. This raises the following question, at the focus of our work: Assuming approximate sparsity of the covariance matrix, can its large entries be detected much faster, say in sub-quadratic time, without explicitly computing all its $p^{2}$ entries? In this paper, we present and theoretically analyze two randomized algorithms that detect the large entries of an approximately sparse sample covariance matrix using only $O(np\text{ poly log } p)$ operations. Furthermore, assuming sparsity of the population matrix, we derive sufficient conditions on the underlying random variable and on the number of samples $n$, for the sample covariance matrix to satisfy our approximate sparsity requirements. Finally, we illustrate the performance of our algorithms via several simulations. ",detect large entries sparse covariance matrix sub quadratic time covariance matrix dimensional random variable fundamental quantity data analysis give observations typically estimate sample covariance matrix computational cost np operations large computation may prohibitively slow moreover several contemporary applications population matrix approximately sparse large entries interest raise follow question focus work assume approximate sparsity covariance matrix large entries detect much faster say sub quadratic time without explicitly compute entries paper present theoretically analyze two randomize algorithms detect large entries approximately sparse sample covariance matrix use np text poly log operations furthermore assume sparsity population matrix derive sufficient condition underlie random variable number sample sample covariance matrix satisfy approximate sparsity requirements finally illustrate performance algorithms via several simulations,116,9,1505.03001.txt
http://arxiv.org/abs/1505.03653,Timed Consistent Network Updates,"  Network updates such as policy and routing changes occur frequently in Software Defined Networks (SDN). Updates should be performed consistently, preventing temporary disruptions, and should require as little overhead as possible. Scalability is increasingly becoming an essential requirement in SDN. In this paper we propose to use time-triggered network updates to achieve consistent updates. Our proposed solution requires lower overhead than existing update approaches, without compromising the consistency during the update. We demonstrate that accurate time enables far more scalable consistent updates in SDN than previously available. In addition, it provides the SDN programmer with fine-grained control over the tradeoff between consistency and scalability. ",Computer Science - Networking and Internet Architecture ; ,"Mizrahi, Tal ; Saat, Efi ; Moses, Yoram ; ","Timed Consistent Network Updates  Network updates such as policy and routing changes occur frequently in Software Defined Networks (SDN). Updates should be performed consistently, preventing temporary disruptions, and should require as little overhead as possible. Scalability is increasingly becoming an essential requirement in SDN. In this paper we propose to use time-triggered network updates to achieve consistent updates. Our proposed solution requires lower overhead than existing update approaches, without compromising the consistency during the update. We demonstrate that accurate time enables far more scalable consistent updates in SDN than previously available. In addition, it provides the SDN programmer with fine-grained control over the tradeoff between consistency and scalability. ",time consistent network update network update policy rout change occur frequently software define network sdn update perform consistently prevent temporary disruptions require little overhead possible scalability increasingly become essential requirement sdn paper propose use time trigger network update achieve consistent update propose solution require lower overhead exist update approach without compromise consistency update demonstrate accurate time enable far scalable consistent update sdn previously available addition provide sdn programmer fine grain control tradeoff consistency scalability,74,6,1505.03653.txt
http://arxiv.org/abs/1505.03898,Pinball Loss Minimization for One-bit Compressive Sensing: Convex Models   and Algorithms,"  The one-bit quantization is implemented by one single comparator that operates at low power and a high rate. Hence one-bit compressive sensing (1bit-CS) becomes attractive in signal processing. When measurements are corrupted by noise during signal acquisition and transmission, 1bit-CS is usually modeled as minimizing a loss function with a sparsity constraint. The one-sided $\ell_1$ loss and the linear loss are two popular loss functions for 1bit-CS. To improve the decoding performance on noisy data, we consider the pinball loss, which provides a bridge between the one-sided $\ell_1$ loss and the linear loss. Using the pinball loss, two convex models, an elastic-net pinball model and its modification with the $\ell_1$-norm constraint, are proposed. To efficiently solve them, the corresponding dual coordinate ascent algorithms are designed and their convergence is proved. The numerical experiments confirm the effectiveness of the proposed algorithms and the performance of the pinball loss minimization for 1bit-CS. ",Computer Science - Information Theory ; Mathematics - Numerical Analysis ; Mathematics - Optimization and Control ; Statistics - Machine Learning ; ,"Huang, Xiaolin ; Shi, Lei ; Yan, Ming ; Suykens, Johan A. K. ; ","Pinball Loss Minimization for One-bit Compressive Sensing: Convex Models   and Algorithms  The one-bit quantization is implemented by one single comparator that operates at low power and a high rate. Hence one-bit compressive sensing (1bit-CS) becomes attractive in signal processing. When measurements are corrupted by noise during signal acquisition and transmission, 1bit-CS is usually modeled as minimizing a loss function with a sparsity constraint. The one-sided $\ell_1$ loss and the linear loss are two popular loss functions for 1bit-CS. To improve the decoding performance on noisy data, we consider the pinball loss, which provides a bridge between the one-sided $\ell_1$ loss and the linear loss. Using the pinball loss, two convex models, an elastic-net pinball model and its modification with the $\ell_1$-norm constraint, are proposed. To efficiently solve them, the corresponding dual coordinate ascent algorithms are designed and their convergence is proved. The numerical experiments confirm the effectiveness of the proposed algorithms and the performance of the pinball loss minimization for 1bit-CS. ",pinball loss minimization one bite compressive sense convex model algorithms one bite quantization implement one single comparator operate low power high rate hence one bite compressive sense bite cs become attractive signal process measurements corrupt noise signal acquisition transmission bite cs usually model minimize loss function sparsity constraint one side ell loss linear loss two popular loss function bite cs improve decode performance noisy data consider pinball loss provide bridge one side ell loss linear loss use pinball loss two convex model elastic net pinball model modification ell norm constraint propose efficiently solve correspond dual coordinate ascent algorithms design convergence prove numerical experiment confirm effectiveness propose algorithms performance pinball loss minimization bite cs,113,9,1505.03898.txt
http://arxiv.org/abs/1505.03931,Robust Biomolecular Finite Automata,"  We present a uniform method for translating an arbitrary nondeterministic finite automaton (NFA) into a deterministic mass action input/output chemical reaction network (I/O CRN) that simulates it. The I/O CRN receives its input as a continuous time signal consisting of concentrations of chemical species that vary to represent the NFA's input string in a natural way. The I/O CRN exploits the inherent parallelism of chemical kinetics to simulate the NFA in real time with a number of chemical species that is linear in the size of the NFA. We prove that the simulation is correct and that it is robust with respect to perturbations of the input signal, the initial concentrations of species, the output (decision), and the rate constants of the reactions of the I/O CRN. ",Computer Science - Computational Complexity ; Computer Science - Emerging Technologies ; Computer Science - Formal Languages and Automata Theory ; ,"Klinge, Titus H. ; Lathrop, James I. ; Lutz, Jack H. ; ","Robust Biomolecular Finite Automata  We present a uniform method for translating an arbitrary nondeterministic finite automaton (NFA) into a deterministic mass action input/output chemical reaction network (I/O CRN) that simulates it. The I/O CRN receives its input as a continuous time signal consisting of concentrations of chemical species that vary to represent the NFA's input string in a natural way. The I/O CRN exploits the inherent parallelism of chemical kinetics to simulate the NFA in real time with a number of chemical species that is linear in the size of the NFA. We prove that the simulation is correct and that it is robust with respect to perturbations of the input signal, the initial concentrations of species, the output (decision), and the rate constants of the reactions of the I/O CRN. ",robust biomolecular finite automata present uniform method translate arbitrary nondeterministic finite automaton nfa deterministic mass action input output chemical reaction network crn simulate crn receive input continuous time signal consist concentrations chemical species vary represent nfa input string natural way crn exploit inherent parallelism chemical kinetics simulate nfa real time number chemical species linear size nfa prove simulation correct robust respect perturbations input signal initial concentrations species output decision rate constants reactions crn,73,9,1505.03931.txt
http://arxiv.org/abs/1505.04026,Automatic Facial Expression Recognition Using Features of Salient Facial   Patches,"  Extraction of discriminative features from salient facial patches plays a vital role in effective facial expression recognition. The accurate detection of facial landmarks improves the localization of the salient patches on face images. This paper proposes a novel framework for expression recognition by using appearance features of selected facial patches. A few prominent facial patches, depending on the position of facial landmarks, are extracted which are active during emotion elicitation. These active patches are further processed to obtain the salient patches which contain discriminative features for classification of each pair of expressions, thereby selecting different facial patches as salient for different pair of expression classes. One-against-one classification method is adopted using these features. In addition, an automated learning-free facial landmark detection technique has been proposed, which achieves similar performances as that of other state-of-art landmark detection methods, yet requires significantly less execution time. The proposed method is found to perform well consistently in different resolutions, hence, providing a solution for expression recognition in low resolution images. Experiments on CK+ and JAFFE facial expression databases show the effectiveness of the proposed system. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Happy, S L ; Routray, Aurobinda ; ","Automatic Facial Expression Recognition Using Features of Salient Facial   Patches  Extraction of discriminative features from salient facial patches plays a vital role in effective facial expression recognition. The accurate detection of facial landmarks improves the localization of the salient patches on face images. This paper proposes a novel framework for expression recognition by using appearance features of selected facial patches. A few prominent facial patches, depending on the position of facial landmarks, are extracted which are active during emotion elicitation. These active patches are further processed to obtain the salient patches which contain discriminative features for classification of each pair of expressions, thereby selecting different facial patches as salient for different pair of expression classes. One-against-one classification method is adopted using these features. In addition, an automated learning-free facial landmark detection technique has been proposed, which achieves similar performances as that of other state-of-art landmark detection methods, yet requires significantly less execution time. The proposed method is found to perform well consistently in different resolutions, hence, providing a solution for expression recognition in low resolution images. Experiments on CK+ and JAFFE facial expression databases show the effectiveness of the proposed system. ",automatic facial expression recognition use feature salient facial patch extraction discriminative feature salient facial patch play vital role effective facial expression recognition accurate detection facial landmarks improve localization salient patch face image paper propose novel framework expression recognition use appearance feature select facial patch prominent facial patch depend position facial landmarks extract active emotion elicitation active patch process obtain salient patch contain discriminative feature classification pair expressions thereby select different facial patch salient different pair expression class one one classification method adopt use feature addition automate learn free facial landmark detection technique propose achieve similar performances state art landmark detection methods yet require significantly less execution time propose method find perform well consistently different resolutions hence provide solution expression recognition low resolution image experiment ck jaffe facial expression databases show effectiveness propose system,133,0,1505.04026.txt
http://arxiv.org/abs/1505.04036,Unified way for computing dynamics of Bose-Einstein condensates and   degenerate Fermi gases,"  In this work we present a very simple and efficient numerical scheme which can be applied to study the dynamics of bosonic systems like, for instance, spinor Bose-Einstein condensates with nonlocal interactions but equally well works for Fermi gases. The method we use is a modification of well known Split Operator Method (SOM). We carefully examine this algorithm in the case of $F=1$ spinor Bose-Einstein condensate without and with dipolar interactions and for strongly interacting two-component Fermi gas. Our extension of the SOM method has many advantages: it is fast, stable, and keeps constant all the physical constraints (constants of motion) at high level. ","Computer Science - Computational Engineering, Finance, and Science ; Condensed Matter - Quantum Gases ; ","Gawryluk, Krzysztof ; Karpiuk, Tomasz ; Gajda, Mariusz ; Rzazewski, Kazimierz ; Brewczyk, Miroslaw ; ","Unified way for computing dynamics of Bose-Einstein condensates and   degenerate Fermi gases  In this work we present a very simple and efficient numerical scheme which can be applied to study the dynamics of bosonic systems like, for instance, spinor Bose-Einstein condensates with nonlocal interactions but equally well works for Fermi gases. The method we use is a modification of well known Split Operator Method (SOM). We carefully examine this algorithm in the case of $F=1$ spinor Bose-Einstein condensate without and with dipolar interactions and for strongly interacting two-component Fermi gas. Our extension of the SOM method has many advantages: it is fast, stable, and keeps constant all the physical constraints (constants of motion) at high level. ",unify way compute dynamics bose einstein condensates degenerate fermi gas work present simple efficient numerical scheme apply study dynamics bosonic systems like instance spinor bose einstein condensates nonlocal interactions equally well work fermi gas method use modification well know split operator method som carefully examine algorithm case spinor bose einstein condensate without dipolar interactions strongly interact two component fermi gas extension som method many advantage fast stable keep constant physical constraints constants motion high level,75,4,1505.04036.txt
http://arxiv.org/abs/1505.04252,Global Convergence of Unmodified 3-Block ADMM for a Class of Convex   Minimization Problems,"  The alternating direction method of multipliers (ADMM) has been successfully applied to solve structured convex optimization problems due to its superior practical performance. The convergence properties of the 2-block ADMM have been studied extensively in the literature. Specifically, it has been proven that the 2-block ADMM globally converges for any penalty parameter $\gamma>0$. In this sense, the 2-block ADMM allows the parameter to be free, i.e., there is no need to restrict the value for the parameter when implementing this algorithm in order to ensure convergence. However, for the 3-block ADMM, Chen \etal \cite{Chen-admm-failure-2013} recently constructed a counter-example showing that it can diverge if no further condition is imposed. The existing results on studying further sufficient conditions on guaranteeing the convergence of the 3-block ADMM usually require $\gamma$ to be smaller than a certain bound, which is usually either difficult to compute or too small to make it a practical algorithm. In this paper, we show that the 3-block ADMM still globally converges with any penalty parameter $\gamma>0$ if the third function $f_3$ in the objective is smooth and strongly convex, and its condition number is in $[1,1.0798)$, besides some other mild conditions. This requirement covers an important class of problems to be called regularized least squares decomposition (RLSD) in this paper. ",Mathematics - Optimization and Control ; Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Lin, Tianyi ; Ma, Shiqian ; Zhang, Shuzhong ; ","Global Convergence of Unmodified 3-Block ADMM for a Class of Convex   Minimization Problems  The alternating direction method of multipliers (ADMM) has been successfully applied to solve structured convex optimization problems due to its superior practical performance. The convergence properties of the 2-block ADMM have been studied extensively in the literature. Specifically, it has been proven that the 2-block ADMM globally converges for any penalty parameter $\gamma>0$. In this sense, the 2-block ADMM allows the parameter to be free, i.e., there is no need to restrict the value for the parameter when implementing this algorithm in order to ensure convergence. However, for the 3-block ADMM, Chen \etal \cite{Chen-admm-failure-2013} recently constructed a counter-example showing that it can diverge if no further condition is imposed. The existing results on studying further sufficient conditions on guaranteeing the convergence of the 3-block ADMM usually require $\gamma$ to be smaller than a certain bound, which is usually either difficult to compute or too small to make it a practical algorithm. In this paper, we show that the 3-block ADMM still globally converges with any penalty parameter $\gamma>0$ if the third function $f_3$ in the objective is smooth and strongly convex, and its condition number is in $[1,1.0798)$, besides some other mild conditions. This requirement covers an important class of problems to be called regularized least squares decomposition (RLSD) in this paper. ",global convergence unmodified block admm class convex minimization problems alternate direction method multipliers admm successfully apply solve structure convex optimization problems due superior practical performance convergence properties block admm study extensively literature specifically prove block admm globally converge penalty parameter gamma sense block admm allow parameter free need restrict value parameter implement algorithm order ensure convergence however block admm chen etal cite chen admm failure recently construct counter example show diverge condition impose exist result study sufficient condition guarantee convergence block admm usually require gamma smaller certain bind usually either difficult compute small make practical algorithm paper show block admm still globally converge penalty parameter gamma third function objective smooth strongly convex condition number besides mild condition requirement cover important class problems call regularize least square decomposition rlsd paper,129,7,1505.04252.txt
http://arxiv.org/abs/1505.04343,Provably Correct Algorithms for Matrix Column Subset Selection with   Selectively Sampled Data,"  We consider the problem of matrix column subset selection, which selects a subset of columns from an input matrix such that the input can be well approximated by the span of the selected columns. Column subset selection has been applied to numerous real-world data applications such as population genetics summarization, electronic circuits testing and recommendation systems. In many applications the complete data matrix is unavailable and one needs to select representative columns by inspecting only a small portion of the input matrix. In this paper we propose the first provably correct column subset selection algorithms for partially observed data matrices. Our proposed algorithms exhibit different merits and limitations in terms of statistical accuracy, computational efficiency, sample complexity and sampling schemes, which provides a nice exploration of the tradeoff between these desired properties for column subset selection. The proposed methods employ the idea of feedback driven sampling and are inspired by several sampling schemes previously introduced for low-rank matrix approximation tasks (Drineas et al., 2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and Singh, 2014). Our analysis shows that, under the assumption that the input data matrix has incoherent rows but possibly coherent columns, all algorithms provably converge to the best low-rank approximation of the original data as number of selected columns increases. Furthermore, two of the proposed algorithms enjoy a relative error bound, which is preferred for column subset selection and matrix approximation purposes. We also demonstrate through both theoretical and empirical analysis the power of feedback driven sampling compared to uniform random sampling on input matrices with highly correlated columns. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; ,"Wang, Yining ; Singh, Aarti ; ","Provably Correct Algorithms for Matrix Column Subset Selection with   Selectively Sampled Data  We consider the problem of matrix column subset selection, which selects a subset of columns from an input matrix such that the input can be well approximated by the span of the selected columns. Column subset selection has been applied to numerous real-world data applications such as population genetics summarization, electronic circuits testing and recommendation systems. In many applications the complete data matrix is unavailable and one needs to select representative columns by inspecting only a small portion of the input matrix. In this paper we propose the first provably correct column subset selection algorithms for partially observed data matrices. Our proposed algorithms exhibit different merits and limitations in terms of statistical accuracy, computational efficiency, sample complexity and sampling schemes, which provides a nice exploration of the tradeoff between these desired properties for column subset selection. The proposed methods employ the idea of feedback driven sampling and are inspired by several sampling schemes previously introduced for low-rank matrix approximation tasks (Drineas et al., 2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and Singh, 2014). Our analysis shows that, under the assumption that the input data matrix has incoherent rows but possibly coherent columns, all algorithms provably converge to the best low-rank approximation of the original data as number of selected columns increases. Furthermore, two of the proposed algorithms enjoy a relative error bound, which is preferred for column subset selection and matrix approximation purposes. We also demonstrate through both theoretical and empirical analysis the power of feedback driven sampling compared to uniform random sampling on input matrices with highly correlated columns. ",provably correct algorithms matrix column subset selection selectively sample data consider problem matrix column subset selection select subset columns input matrix input well approximate span select columns column subset selection apply numerous real world data applications population genetics summarization electronic circuit test recommendation systems many applications complete data matrix unavailable one need select representative columns inspect small portion input matrix paper propose first provably correct column subset selection algorithms partially observe data matrices propose algorithms exhibit different merit limitations term statistical accuracy computational efficiency sample complexity sample scheme provide nice exploration tradeoff desire properties column subset selection propose methods employ idea feedback drive sample inspire several sample scheme previously introduce low rank matrix approximation task drineas et al frieze et al deshpande vempala krishnamurthy singh analysis show assumption input data matrix incoherent row possibly coherent columns algorithms provably converge best low rank approximation original data number select columns increase furthermore two propose algorithms enjoy relative error bind prefer column subset selection matrix approximation purpose also demonstrate theoretical empirical analysis power feedback drive sample compare uniform random sample input matrices highly correlate columns,182,12,1505.04343.txt
http://arxiv.org/abs/1505.04911,Read Mapping on de Bruijn graph,"  Background Next Generation Sequencing (NGS) has dramatically enhanced our ability to sequence genomes, but not to assemble them. In practice, many published genome sequences remain in the state of a large set of contigs. Each contig describes the sequence found along some path of the assembly graph, however, the set of contigs does not record all the sequence information contained in that graph. Although many subsequent analyses can be performed with the set of contigs, one may ask whether mapping reads on the contigs is as informative as mapping them on the paths of the assembly graph. Currently, one lacks practical tools to perform mapping on such graphs. Results Here, we propose a formal definition of mapping on a de Bruijn graph, analyse the problem complexity which turns out to be NP-complete, and provide a practical solution.We propose a pipeline called GGMAP (Greedy Graph MAPping). Its novelty is a procedure to map reads on branching paths of the graph, for which we designed a heuristic algorithm called BGREAT (de Bruijn Graph REAd mapping Tool). For the sake of efficiency, BGREAT rewrites a read sequence as a succession of unitigs sequences. GGMAP can map millions of reads per CPU hour on a de Bruijn graph built from a large set of human genomic reads. Surprisingly, results show that up to 22% more reads can be mapped on the graph but not on the contig set. Conclusions Although mapping reads on a de Bruijn graph is complex task, our proposal offers a practical solution combining efficiency with an improved mapping capacity compared to assembly-based mapping even for complex eukaryotic data. Availability: github.com/Malfoy/BGREAT Keywords: Read mapping; De bruijn graphs; NGS; NP-completeness ",Computer Science - Data Structures and Algorithms ; Quantitative Biology - Genomics ; ,"Limasset, Antoine ; Cazaux, Bastien ; Rivals, Eric ; Peterlongo, Pierre ; ","Read Mapping on de Bruijn graph  Background Next Generation Sequencing (NGS) has dramatically enhanced our ability to sequence genomes, but not to assemble them. In practice, many published genome sequences remain in the state of a large set of contigs. Each contig describes the sequence found along some path of the assembly graph, however, the set of contigs does not record all the sequence information contained in that graph. Although many subsequent analyses can be performed with the set of contigs, one may ask whether mapping reads on the contigs is as informative as mapping them on the paths of the assembly graph. Currently, one lacks practical tools to perform mapping on such graphs. Results Here, we propose a formal definition of mapping on a de Bruijn graph, analyse the problem complexity which turns out to be NP-complete, and provide a practical solution.We propose a pipeline called GGMAP (Greedy Graph MAPping). Its novelty is a procedure to map reads on branching paths of the graph, for which we designed a heuristic algorithm called BGREAT (de Bruijn Graph REAd mapping Tool). For the sake of efficiency, BGREAT rewrites a read sequence as a succession of unitigs sequences. GGMAP can map millions of reads per CPU hour on a de Bruijn graph built from a large set of human genomic reads. Surprisingly, results show that up to 22% more reads can be mapped on the graph but not on the contig set. Conclusions Although mapping reads on a de Bruijn graph is complex task, our proposal offers a practical solution combining efficiency with an improved mapping capacity compared to assembly-based mapping even for complex eukaryotic data. Availability: github.com/Malfoy/BGREAT Keywords: Read mapping; De bruijn graphs; NGS; NP-completeness ",read map de bruijn graph background next generation sequence ngs dramatically enhance ability sequence genomes assemble practice many publish genome sequence remain state large set contigs contig describe sequence find along path assembly graph however set contigs record sequence information contain graph although many subsequent analyse perform set contigs one may ask whether map read contigs informative map paths assembly graph currently one lack practical tool perform map graph result propose formal definition map de bruijn graph analyse problem complexity turn np complete provide practical solution propose pipeline call ggmap greedy graph map novelty procedure map read branch paths graph design heuristic algorithm call bgreat de bruijn graph read map tool sake efficiency bgreat rewrite read sequence succession unitigs sequence ggmap map millions read per cpu hour de bruijn graph build large set human genomic read surprisingly result show read map graph contig set conclusions although map read de bruijn graph complex task proposal offer practical solution combine efficiency improve map capacity compare assembly base map even complex eukaryotic data availability github com malfoy bgreat keywords read map de bruijn graph ngs np completeness,184,3,1505.04911.txt
http://arxiv.org/abs/1505.04938,Convective regularization for optical flow,"  We argue that the time derivative in a fixed coordinate frame may not be the most appropriate measure of time regularity of an optical flow field. Instead, for a given velocity field $v$ we consider the convective acceleration $v_t + \nabla v v$ which describes the acceleration of objects moving according to $v$. Consequently we investigate the suitability of the nonconvex functional $\|v_t + \nabla v v\|^2_{L^2}$ as a regularization term for optical flow. We demonstrate that this term acts as both a spatial and a temporal regularizer and has an intrinsic edge-preserving property. We incorporate it into a contrast invariant and time-regularized variant of the Horn-Schunck functional, prove existence of minimizers and verify experimentally that it addresses some of the problems of basic quadratic models. For the minimization we use an iterative scheme that approximates the original nonlinear problem with a sequence of linear ones. We believe that the convective acceleration may be gainfully introduced in a variety of optical flow models. ","Mathematics - Optimization and Control ; Computer Science - Computer Vision and Pattern Recognition ; 49N45, 68T45, 68U10 ; ","Iglesias, José A. ; Kirisits, Clemens ; ","Convective regularization for optical flow  We argue that the time derivative in a fixed coordinate frame may not be the most appropriate measure of time regularity of an optical flow field. Instead, for a given velocity field $v$ we consider the convective acceleration $v_t + \nabla v v$ which describes the acceleration of objects moving according to $v$. Consequently we investigate the suitability of the nonconvex functional $\|v_t + \nabla v v\|^2_{L^2}$ as a regularization term for optical flow. We demonstrate that this term acts as both a spatial and a temporal regularizer and has an intrinsic edge-preserving property. We incorporate it into a contrast invariant and time-regularized variant of the Horn-Schunck functional, prove existence of minimizers and verify experimentally that it addresses some of the problems of basic quadratic models. For the minimization we use an iterative scheme that approximates the original nonlinear problem with a sequence of linear ones. We believe that the convective acceleration may be gainfully introduced in a variety of optical flow models. ",convective regularization optical flow argue time derivative fix coordinate frame may appropriate measure time regularity optical flow field instead give velocity field consider convective acceleration nabla describe acceleration object move accord consequently investigate suitability nonconvex functional nabla regularization term optical flow demonstrate term act spatial temporal regularizer intrinsic edge preserve property incorporate contrast invariant time regularize variant horn schunck functional prove existence minimizers verify experimentally address problems basic quadratic model minimization use iterative scheme approximate original nonlinear problem sequence linear ones believe convective acceleration may gainfully introduce variety optical flow model,91,11,1505.04938.txt
http://arxiv.org/abs/1505.05193,Synthesising Executable Gene Regulatory Networks from Single-cell Gene   Expression Data,"  Recent experimental advances in biology allow researchers to obtain gene expression profiles at single-cell resolution over hundreds, or even thousands of cells at once. These single-cell measurements provide snapshots of the states of the cells that make up a tissue, instead of the population-level averages provided by conventional high-throughput experiments. This new data therefore provides an exciting opportunity for computational modelling. In this paper we introduce the idea of viewing single-cell gene expression profiles as states of an asynchronous Boolean network, and frame model inference as the problem of reconstructing a Boolean network from its state space. We then give a scalable algorithm to solve this synthesis problem. We apply our technique to both simulated and real data. We first apply our technique to data simulated from a well established model of common myeloid progenitor differentiation. We show that our technique is able to recover the original Boolean network rules. We then apply our technique to a large dataset taken during embryonic development containing thousands of cell measurements. Our technique synthesises matching Boolean networks, and analysis of these models yields new predictions about blood development which our experimental collaborators were able to verify. ","Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Logic in Computer Science ; Quantitative Biology - Molecular Networks ; ","Fisher, Jasmin ; Köksal, Ali Sinan ; Piterman, Nir ; Woodhouse, Steven ; ","Synthesising Executable Gene Regulatory Networks from Single-cell Gene   Expression Data  Recent experimental advances in biology allow researchers to obtain gene expression profiles at single-cell resolution over hundreds, or even thousands of cells at once. These single-cell measurements provide snapshots of the states of the cells that make up a tissue, instead of the population-level averages provided by conventional high-throughput experiments. This new data therefore provides an exciting opportunity for computational modelling. In this paper we introduce the idea of viewing single-cell gene expression profiles as states of an asynchronous Boolean network, and frame model inference as the problem of reconstructing a Boolean network from its state space. We then give a scalable algorithm to solve this synthesis problem. We apply our technique to both simulated and real data. We first apply our technique to data simulated from a well established model of common myeloid progenitor differentiation. We show that our technique is able to recover the original Boolean network rules. We then apply our technique to a large dataset taken during embryonic development containing thousands of cell measurements. Our technique synthesises matching Boolean networks, and analysis of these models yields new predictions about blood development which our experimental collaborators were able to verify. ",synthesise executable gene regulatory network single cell gene expression data recent experimental advance biology allow researchers obtain gene expression profile single cell resolution hundreds even thousands cells single cell measurements provide snapshots state cells make tissue instead population level average provide conventional high throughput experiment new data therefore provide excite opportunity computational model paper introduce idea view single cell gene expression profile state asynchronous boolean network frame model inference problem reconstruct boolean network state space give scalable algorithm solve synthesis problem apply technique simulate real data first apply technique data simulate well establish model common myeloid progenitor differentiation show technique able recover original boolean network rule apply technique large dataset take embryonic development contain thousands cell measurements technique synthesise match boolean network analysis model yield new predictions blood development experimental collaborators able verify,133,6,1505.05193.txt
http://arxiv.org/abs/1505.05312,A New Oscillating-Error Technique for Classifiers,"  This paper describes a new method for reducing the error in a classifier. It uses an error correction update that includes the very simple rule of either adding or subtracting the error adjustment, based on whether the variable value is currently larger or smaller than the desired value. While a traditional neuron would sum the inputs together and then apply a function to the total, this new method can change the function decision for each input value. This gives added flexibility to the convergence procedure, where through a series of transpositions, variables that are far away can continue towards the desired value, whereas variables that are originally much closer can oscillate from one side to the other. Tests show that the method can successfully classify some benchmark datasets. It can also work in a batch mode, with reduced training times and can be used as part of a neural network architecture. Some comparisons with an earlier wave shape paper are also made. ",Computer Science - Artificial Intelligence ; ,"Greer, Kieran ; ","A New Oscillating-Error Technique for Classifiers  This paper describes a new method for reducing the error in a classifier. It uses an error correction update that includes the very simple rule of either adding or subtracting the error adjustment, based on whether the variable value is currently larger or smaller than the desired value. While a traditional neuron would sum the inputs together and then apply a function to the total, this new method can change the function decision for each input value. This gives added flexibility to the convergence procedure, where through a series of transpositions, variables that are far away can continue towards the desired value, whereas variables that are originally much closer can oscillate from one side to the other. Tests show that the method can successfully classify some benchmark datasets. It can also work in a batch mode, with reduced training times and can be used as part of a neural network architecture. Some comparisons with an earlier wave shape paper are also made. ",new oscillate error technique classifiers paper describe new method reduce error classifier use error correction update include simple rule either add subtract error adjustment base whether variable value currently larger smaller desire value traditional neuron would sum input together apply function total new method change function decision input value give add flexibility convergence procedure series transpositions variables far away continue towards desire value whereas variables originally much closer oscillate one side test show method successfully classify benchmark datasets also work batch mode reduce train time use part neural network architecture comparisons earlier wave shape paper also make,97,11,1505.05312.txt
http://arxiv.org/abs/1505.05451,Fuzzy Least Squares Twin Support Vector Machines,"  Least Squares Twin Support Vector Machine (LST-SVM) has been shown to be an efficient and fast algorithm for binary classification. It combines the operating principles of Least Squares SVM (LS-SVM) and Twin SVM (T-SVM); it constructs two non-parallel hyperplanes (as in T-SVM) by solving two systems of linear equations (as in LS-SVM). Despite its efficiency, LST-SVM is still unable to cope with two features of real-world problems. First, in many real-world applications, labels of samples are not deterministic; they come naturally with their associated membership degrees. Second, samples in real-world applications may not be equally important and their importance degrees affect the classification. In this paper, we propose Fuzzy LST-SVM (FLST-SVM) to deal with these two characteristics of real-world data. Two models are introduced for FLST-SVM: the first model builds up crisp hyperplanes using training samples and their corresponding membership degrees. The second model, on the other hand, constructs fuzzy hyperplanes using training samples and their membership degrees. Numerical evaluation of the proposed method with synthetic and real datasets demonstrate significant improvement in the classification accuracy of FLST-SVM when compared to well-known existing versions of SVM. ",Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; ,"Sartakhti, Javad Salimi ; Afrabandpey, Homayun ; Ghadiri, Nasser ; ","Fuzzy Least Squares Twin Support Vector Machines  Least Squares Twin Support Vector Machine (LST-SVM) has been shown to be an efficient and fast algorithm for binary classification. It combines the operating principles of Least Squares SVM (LS-SVM) and Twin SVM (T-SVM); it constructs two non-parallel hyperplanes (as in T-SVM) by solving two systems of linear equations (as in LS-SVM). Despite its efficiency, LST-SVM is still unable to cope with two features of real-world problems. First, in many real-world applications, labels of samples are not deterministic; they come naturally with their associated membership degrees. Second, samples in real-world applications may not be equally important and their importance degrees affect the classification. In this paper, we propose Fuzzy LST-SVM (FLST-SVM) to deal with these two characteristics of real-world data. Two models are introduced for FLST-SVM: the first model builds up crisp hyperplanes using training samples and their corresponding membership degrees. The second model, on the other hand, constructs fuzzy hyperplanes using training samples and their membership degrees. Numerical evaluation of the proposed method with synthetic and real datasets demonstrate significant improvement in the classification accuracy of FLST-SVM when compared to well-known existing versions of SVM. ",fuzzy least square twin support vector machine least square twin support vector machine lst svm show efficient fast algorithm binary classification combine operate principles least square svm ls svm twin svm svm construct two non parallel hyperplanes svm solve two systems linear equations ls svm despite efficiency lst svm still unable cope two feature real world problems first many real world applications label sample deterministic come naturally associate membership degrees second sample real world applications may equally important importance degrees affect classification paper propose fuzzy lst svm flst svm deal two characteristics real world data two model introduce flst svm first model build crisp hyperplanes use train sample correspond membership degrees second model hand construct fuzzy hyperplanes use train sample membership degrees numerical evaluation propose method synthetic real datasets demonstrate significant improvement classification accuracy flst svm compare well know exist versions svm,142,12,1505.05451.txt
http://arxiv.org/abs/1505.05917,Decentralized Sequential Composite Hypothesis Test Based on One-Bit   Communication,"  This paper considers the sequential composite hypothesis test with multiple sensors. The sensors observe random samples in parallel and communicate with a fusion center, who makes the global decision based on the sensor inputs. On one hand, in the centralized scenario, where local samples are precisely transmitted to the fusion center, the generalized sequential likelihood ratio test (GSPRT) is shown to be asymptotically optimal in terms of the expected sample size as error rates tend to zero. On the other hand, for systems with limited power and bandwidth resources, decentralized solutions that only send a summary of local samples (we particularly focus on a one-bit communication protocol) to the fusion center is of great importance. To this end, we first consider a decentralized scheme where sensors send their one-bit quantized statistics every fixed period of time to the fusion center. We show that such a uniform sampling and quantization scheme is strictly suboptimal and its suboptimality can be quantified by the KL divergence of the distributions of the quantized statistics under both hypotheses. We then propose a decentralized GSPRT based on level-triggered sampling. That is, each sensor runs its own GSPRT repeatedly and reports its local decision to the fusion center asynchronously. We show that this scheme is asymptotically optimal as the local thresholds and global thresholds grow large at different rates. Lastly, two particular models and their associated applications are studied to compare the centralized and decentralized approaches. Numerical results are provided to demonstrate that the proposed level-triggered sampling based decentralized scheme aligns closely with the centralized scheme with substantially lower communication overhead, and significantly outperforms the uniform sampling and quantization based decentralized scheme. ",Statistics - Applications ; Computer Science - Information Theory ; ,"Li, Shang ; Li, Xiaoou ; Wang, Xiaodong ; Liu, Jingchen ; ","Decentralized Sequential Composite Hypothesis Test Based on One-Bit   Communication  This paper considers the sequential composite hypothesis test with multiple sensors. The sensors observe random samples in parallel and communicate with a fusion center, who makes the global decision based on the sensor inputs. On one hand, in the centralized scenario, where local samples are precisely transmitted to the fusion center, the generalized sequential likelihood ratio test (GSPRT) is shown to be asymptotically optimal in terms of the expected sample size as error rates tend to zero. On the other hand, for systems with limited power and bandwidth resources, decentralized solutions that only send a summary of local samples (we particularly focus on a one-bit communication protocol) to the fusion center is of great importance. To this end, we first consider a decentralized scheme where sensors send their one-bit quantized statistics every fixed period of time to the fusion center. We show that such a uniform sampling and quantization scheme is strictly suboptimal and its suboptimality can be quantified by the KL divergence of the distributions of the quantized statistics under both hypotheses. We then propose a decentralized GSPRT based on level-triggered sampling. That is, each sensor runs its own GSPRT repeatedly and reports its local decision to the fusion center asynchronously. We show that this scheme is asymptotically optimal as the local thresholds and global thresholds grow large at different rates. Lastly, two particular models and their associated applications are studied to compare the centralized and decentralized approaches. Numerical results are provided to demonstrate that the proposed level-triggered sampling based decentralized scheme aligns closely with the centralized scheme with substantially lower communication overhead, and significantly outperforms the uniform sampling and quantization based decentralized scheme. ",decentralize sequential composite hypothesis test base one bite communication paper consider sequential composite hypothesis test multiple sensors sensors observe random sample parallel communicate fusion center make global decision base sensor input one hand centralize scenario local sample precisely transmit fusion center generalize sequential likelihood ratio test gsprt show asymptotically optimal term expect sample size error rat tend zero hand systems limit power bandwidth resources decentralize solutions send summary local sample particularly focus one bite communication protocol fusion center great importance end first consider decentralize scheme sensors send one bite quantize statistics every fix period time fusion center show uniform sample quantization scheme strictly suboptimal suboptimality quantify kl divergence distributions quantize statistics hypotheses propose decentralize gsprt base level trigger sample sensor run gsprt repeatedly report local decision fusion center asynchronously show scheme asymptotically optimal local thresholds global thresholds grow large different rat lastly two particular model associate applications study compare centralize decentralize approach numerical result provide demonstrate propose level trigger sample base decentralize scheme align closely centralize scheme substantially lower communication overhead significantly outperform uniform sample quantization base decentralize scheme,179,12,1505.05917.txt
http://arxiv.org/abs/1505.06036,VPG and EPG bend-numbers of Halin Graphs,"  A piecewise linear curve in the plane made up of $k+1$ line segments, each of which is either horizontal or vertical, with consecutive segments being of different orientation is called a $k$-bend path. Given a graph $G$, a collection of $k$-bend paths in which each path corresponds to a vertex in $G$ and two paths have a common point if and only if the vertices corresponding to them are adjacent in $G$ is called a $B_k$-VPG representation of $G$. Similarly, a collection of $k$-bend paths each of which corresponds to a vertex in $G$ is called an $B_k$-EPG representation of $G$ if any two paths have a line segment of non-zero length in common if and only if their corresponding vertices are adjacent in $G$. The VPG bend-number $b_v(G)$ of a graph $G$ is the minimum $k$ such that $G$ has a $B_k$-VPG representation. Similarly, the EPG bend-number $b_e(G)$ of a graph $G$ is the minimum $k$ such that $G$ has a $B_k$-EPG representation. Halin graphs are the graphs formed by taking a tree with no degree $2$ vertex and then connecting its leaves to form a cycle in such a way that the graph has a planar embedding. We prove that if $G$ is a Halin graph then $b_v(G) \leq 1$ and $b_e(G) \leq 2$. These bounds are tight. In fact, we prove the stronger result that if $G$ is a planar graph formed by connecting the leaves of any tree to form a simple cycle, then it has a VPG-representation using only one type of 1-bend paths and an EPG-representation using only one type of 2-bend paths. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C62 ; ,"Francis, Mathew C. ; Lahiri, Abhiruk ; ","VPG and EPG bend-numbers of Halin Graphs  A piecewise linear curve in the plane made up of $k+1$ line segments, each of which is either horizontal or vertical, with consecutive segments being of different orientation is called a $k$-bend path. Given a graph $G$, a collection of $k$-bend paths in which each path corresponds to a vertex in $G$ and two paths have a common point if and only if the vertices corresponding to them are adjacent in $G$ is called a $B_k$-VPG representation of $G$. Similarly, a collection of $k$-bend paths each of which corresponds to a vertex in $G$ is called an $B_k$-EPG representation of $G$ if any two paths have a line segment of non-zero length in common if and only if their corresponding vertices are adjacent in $G$. The VPG bend-number $b_v(G)$ of a graph $G$ is the minimum $k$ such that $G$ has a $B_k$-VPG representation. Similarly, the EPG bend-number $b_e(G)$ of a graph $G$ is the minimum $k$ such that $G$ has a $B_k$-EPG representation. Halin graphs are the graphs formed by taking a tree with no degree $2$ vertex and then connecting its leaves to form a cycle in such a way that the graph has a planar embedding. We prove that if $G$ is a Halin graph then $b_v(G) \leq 1$ and $b_e(G) \leq 2$. These bounds are tight. In fact, we prove the stronger result that if $G$ is a planar graph formed by connecting the leaves of any tree to form a simple cycle, then it has a VPG-representation using only one type of 1-bend paths and an EPG-representation using only one type of 2-bend paths. ",vpg epg bend number halin graph piecewise linear curve plane make line segment either horizontal vertical consecutive segment different orientation call bend path give graph collection bend paths path correspond vertex two paths common point vertices correspond adjacent call vpg representation similarly collection bend paths correspond vertex call epg representation two paths line segment non zero length common correspond vertices adjacent vpg bend number graph minimum vpg representation similarly epg bend number graph minimum epg representation halin graph graph form take tree degree vertex connect leave form cycle way graph planar embed prove halin graph leq leq bound tight fact prove stronger result planar graph form connect leave tree form simple cycle vpg representation use one type bend paths epg representation use one type bend paths,126,3,1505.06036.txt
http://arxiv.org/abs/1505.06362,Polynomially Low Error PCPs with polyloglog n Queries via Modular   Composition,"  We show that every language in NP has a PCP verifier that tosses $O(\log n)$ random coins, has perfect completeness, and a soundness error of at most $1/\text{poly}(n)$, while making at most $O(\text{poly}\log\log n)$ queries into a proof over an alphabet of size at most $n^{1/\text{poly}\log\log n}$. Previous constructions that obtain $1/\text{poly}(n)$ soundness error used either $\text{poly}\log n $ queries or an exponential sized alphabet, i.e. of size $2^{n^c}$ for some $c>0$. Our result is an exponential improvement in both parameters simultaneously.   Our result can be phrased as a polynomial-gap hardness for approximate CSPs with arity $\text{poly}\log\log n$ and alphabet size $n^{1/\text{poly}\log n}$. The ultimate goal, in this direction, would be to prove polynomial hardness for CSPs with constant arity and polynomial alphabet size (aka the sliding scale conjecture for inverse polynomial soundness error).   Our construction is based on a modular generalization of previous PCP constructions in this parameter regime, which involves a composition theorem that uses an extra `consistency' query but maintains the inverse polynomial relation between the soundness error and the alphabet size.   Our main technical/conceptual contribution is a new notion of soundness, which we refer to as {\em distributional soundness}, that replaces the previous notion of ""list decoding soundness"", and that allows us to prove a modular composition theorem with tighter parameters. This new notion of soundness allows us to invoke composition a super-constant number of times without incurring a blow-up in the soundness error. ",Computer Science - Computational Complexity ; ,"Dinur, Irit ; Harsha, Prahladh ; Kindler, Guy ; ","Polynomially Low Error PCPs with polyloglog n Queries via Modular   Composition  We show that every language in NP has a PCP verifier that tosses $O(\log n)$ random coins, has perfect completeness, and a soundness error of at most $1/\text{poly}(n)$, while making at most $O(\text{poly}\log\log n)$ queries into a proof over an alphabet of size at most $n^{1/\text{poly}\log\log n}$. Previous constructions that obtain $1/\text{poly}(n)$ soundness error used either $\text{poly}\log n $ queries or an exponential sized alphabet, i.e. of size $2^{n^c}$ for some $c>0$. Our result is an exponential improvement in both parameters simultaneously.   Our result can be phrased as a polynomial-gap hardness for approximate CSPs with arity $\text{poly}\log\log n$ and alphabet size $n^{1/\text{poly}\log n}$. The ultimate goal, in this direction, would be to prove polynomial hardness for CSPs with constant arity and polynomial alphabet size (aka the sliding scale conjecture for inverse polynomial soundness error).   Our construction is based on a modular generalization of previous PCP constructions in this parameter regime, which involves a composition theorem that uses an extra `consistency' query but maintains the inverse polynomial relation between the soundness error and the alphabet size.   Our main technical/conceptual contribution is a new notion of soundness, which we refer to as {\em distributional soundness}, that replaces the previous notion of ""list decoding soundness"", and that allows us to prove a modular composition theorem with tighter parameters. This new notion of soundness allows us to invoke composition a super-constant number of times without incurring a blow-up in the soundness error. ",polynomially low error pcps polyloglog query via modular composition show every language np pcp verifier toss log random coin perfect completeness soundness error text poly make text poly log log query proof alphabet size text poly log log previous constructions obtain text poly soundness error use either text poly log query exponential size alphabet size result exponential improvement parameters simultaneously result phrase polynomial gap hardness approximate csps arity text poly log log alphabet size text poly log ultimate goal direction would prove polynomial hardness csps constant arity polynomial alphabet size aka slide scale conjecture inverse polynomial soundness error construction base modular generalization previous pcp constructions parameter regime involve composition theorem use extra consistency query maintain inverse polynomial relation soundness error alphabet size main technical conceptual contribution new notion soundness refer em distributional soundness replace previous notion list decode soundness allow us prove modular composition theorem tighter parameters new notion soundness allow us invoke composition super constant number time without incur blow soundness error,163,1,1505.06362.txt
http://arxiv.org/abs/1505.06770,Sketching for Sequential Change-Point Detection,"  We study sequential change-point detection procedures based on linear sketches of high-dimensional signal vectors using generalized likelihood ratio (GLR) statistics. The GLR statistics allow for an unknown post-change mean that represents an anomaly or novelty. We consider both fixed and time-varying projections, derive theoretical approximations to two fundamental performance metrics: the average run length (ARL) and the expected detection delay (EDD); these approximations are shown to be highly accurate by numerical simulations. We further characterize the relative performance measure of the sketching procedure compared to that without sketching and show that there can be little performance loss when the signal strength is sufficiently large, and enough number of sketches are used. Finally, we demonstrate the good performance of sketching procedures using simulation and real-data examples on solar flare detection and failure detection in power networks. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Cao, Yang ; Thompson, Andrew ; Wang, Meng ; Xie, Yao ; ","Sketching for Sequential Change-Point Detection  We study sequential change-point detection procedures based on linear sketches of high-dimensional signal vectors using generalized likelihood ratio (GLR) statistics. The GLR statistics allow for an unknown post-change mean that represents an anomaly or novelty. We consider both fixed and time-varying projections, derive theoretical approximations to two fundamental performance metrics: the average run length (ARL) and the expected detection delay (EDD); these approximations are shown to be highly accurate by numerical simulations. We further characterize the relative performance measure of the sketching procedure compared to that without sketching and show that there can be little performance loss when the signal strength is sufficiently large, and enough number of sketches are used. Finally, we demonstrate the good performance of sketching procedures using simulation and real-data examples on solar flare detection and failure detection in power networks. ",sketch sequential change point detection study sequential change point detection procedures base linear sketch high dimensional signal vectors use generalize likelihood ratio glr statistics glr statistics allow unknown post change mean represent anomaly novelty consider fix time vary projections derive theoretical approximations two fundamental performance metrics average run length arl expect detection delay edd approximations show highly accurate numerical simulations characterize relative performance measure sketch procedure compare without sketch show little performance loss signal strength sufficiently large enough number sketch use finally demonstrate good performance sketch procedures use simulation real data examples solar flare detection failure detection power network,99,12,1505.06770.txt
http://arxiv.org/abs/1505.06819,Coalgebraic Infinite Traces and Kleisli Simulations,"  Kleisli simulation is a categorical notion introduced by Hasuo to verify finite trace inclusion. They allow us to give definitions of forward and backward simulation for various types of systems. A generic categorical theory behind Kleisli simulation has been developed and it guarantees the soundness of those simulations with respect to finite trace semantics. Moreover, those simulations can be aided by forward partial execution (FPE)---a categorical transformation of systems previously introduced by the authors.   In this paper, we give Kleisli simulation a theoretical foundation that assures its soundness also with respect to infinitary traces. There, following Jacobs' work, infinitary trace semantics is characterized as the ""largest homomorphism."" It turns out that soundness of forward simulations is rather straightforward; that of backward simulation holds too, although it requires certain additional conditions and its proof is more involved. We also show that FPE can be successfully employed in the infinitary trace setting to enhance the applicability of Kleisli simulations as witnesses of trace inclusion. Our framework is parameterized in the monad for branching as well as in the functor for linear-time behaviors; for the former we mainly use the powerset monad (for nondeterminism), the sub-Giry monad (for probability), and the lift monad (for exception). ",Computer Science - Logic in Computer Science ; ,"Urabe, Natsuki ; Hasuo, Ichiro ; ","Coalgebraic Infinite Traces and Kleisli Simulations  Kleisli simulation is a categorical notion introduced by Hasuo to verify finite trace inclusion. They allow us to give definitions of forward and backward simulation for various types of systems. A generic categorical theory behind Kleisli simulation has been developed and it guarantees the soundness of those simulations with respect to finite trace semantics. Moreover, those simulations can be aided by forward partial execution (FPE)---a categorical transformation of systems previously introduced by the authors.   In this paper, we give Kleisli simulation a theoretical foundation that assures its soundness also with respect to infinitary traces. There, following Jacobs' work, infinitary trace semantics is characterized as the ""largest homomorphism."" It turns out that soundness of forward simulations is rather straightforward; that of backward simulation holds too, although it requires certain additional conditions and its proof is more involved. We also show that FPE can be successfully employed in the infinitary trace setting to enhance the applicability of Kleisli simulations as witnesses of trace inclusion. Our framework is parameterized in the monad for branching as well as in the functor for linear-time behaviors; for the former we mainly use the powerset monad (for nondeterminism), the sub-Giry monad (for probability), and the lift monad (for exception). ",coalgebraic infinite trace kleisli simulations kleisli simulation categorical notion introduce hasuo verify finite trace inclusion allow us give definitions forward backward simulation various type systems generic categorical theory behind kleisli simulation develop guarantee soundness simulations respect finite trace semantics moreover simulations aid forward partial execution fpe categorical transformation systems previously introduce author paper give kleisli simulation theoretical foundation assure soundness also respect infinitary trace follow jacobs work infinitary trace semantics characterize largest homomorphism turn soundness forward simulations rather straightforward backward simulation hold although require certain additional condition proof involve also show fpe successfully employ infinitary trace set enhance applicability kleisli simulations witness trace inclusion framework parameterized monad branch well functor linear time behaviors former mainly use powerset monad nondeterminism sub giry monad probability lift monad exception,126,8,1505.06819.txt
http://arxiv.org/abs/1505.06897,Times series averaging from a probabilistic interpretation of   time-elastic kernel,"  At the light of regularized dynamic time warping kernels, this paper reconsider the concept of time elastic centroid (TEC) for a set of time series. From this perspective, we show first how TEC can easily be addressed as a preimage problem. Unfortunately this preimage problem is ill-posed, may suffer from over-fitting especially for long time series and getting a sub-optimal solution involves heavy computational costs. We then derive two new algorithms based on a probabilistic interpretation of kernel alignment matrices that expresses in terms of probabilistic distributions over sets of alignment paths. The first algorithm is an iterative agglomerative heuristics inspired from the state of the art DTW barycenter averaging (DBA) algorithm proposed specifically for the Dynamic Time Warping measure. The second proposed algorithm achieves a classical averaging of the aligned samples but also implements an averaging of the time of occurrences of the aligned samples. It exploits a straightforward progressive agglomerative heuristics. An experimentation that compares for 45 time series datasets classification error rates obtained by first near neighbors classifiers exploiting a single medoid or centroid estimate to represent each categories show that: i) centroids based approaches significantly outperform medoids based approaches, ii) on the considered experience, the two proposed algorithms outperform the state of the art DBA algorithm, and iii) the second proposed algorithm that implements an averaging jointly in the sample space and along the time axes emerges as the most significantly robust time elastic averaging heuristic with an interesting noise reduction capability. Index Terms-Time series averaging Time elastic kernel Dynamic Time Warping Time series clustering and classification. ",Computer Science - Machine Learning ; Computer Science - Data Structures and Algorithms ; ,"Marteau, Pierre-François ; ","Times series averaging from a probabilistic interpretation of   time-elastic kernel  At the light of regularized dynamic time warping kernels, this paper reconsider the concept of time elastic centroid (TEC) for a set of time series. From this perspective, we show first how TEC can easily be addressed as a preimage problem. Unfortunately this preimage problem is ill-posed, may suffer from over-fitting especially for long time series and getting a sub-optimal solution involves heavy computational costs. We then derive two new algorithms based on a probabilistic interpretation of kernel alignment matrices that expresses in terms of probabilistic distributions over sets of alignment paths. The first algorithm is an iterative agglomerative heuristics inspired from the state of the art DTW barycenter averaging (DBA) algorithm proposed specifically for the Dynamic Time Warping measure. The second proposed algorithm achieves a classical averaging of the aligned samples but also implements an averaging of the time of occurrences of the aligned samples. It exploits a straightforward progressive agglomerative heuristics. An experimentation that compares for 45 time series datasets classification error rates obtained by first near neighbors classifiers exploiting a single medoid or centroid estimate to represent each categories show that: i) centroids based approaches significantly outperform medoids based approaches, ii) on the considered experience, the two proposed algorithms outperform the state of the art DBA algorithm, and iii) the second proposed algorithm that implements an averaging jointly in the sample space and along the time axes emerges as the most significantly robust time elastic averaging heuristic with an interesting noise reduction capability. Index Terms-Time series averaging Time elastic kernel Dynamic Time Warping Time series clustering and classification. ",time series average probabilistic interpretation time elastic kernel light regularize dynamic time warp kernels paper reconsider concept time elastic centroid tec set time series perspective show first tec easily address preimage problem unfortunately preimage problem ill pose may suffer fit especially long time series get sub optimal solution involve heavy computational cost derive two new algorithms base probabilistic interpretation kernel alignment matrices express term probabilistic distributions set alignment paths first algorithm iterative agglomerative heuristics inspire state art dtw barycenter average dba algorithm propose specifically dynamic time warp measure second propose algorithm achieve classical average align sample also implement average time occurrences align sample exploit straightforward progressive agglomerative heuristics experimentation compare time series datasets classification error rat obtain first near neighbor classifiers exploit single medoid centroid estimate represent categories show centroids base approach significantly outperform medoids base approach ii consider experience two propose algorithms outperform state art dba algorithm iii second propose algorithm implement average jointly sample space along time ax emerge significantly robust time elastic average heuristic interest noise reduction capability index term time series average time elastic kernel dynamic time warp time series cluster classification,186,11,1505.06897.txt
http://arxiv.org/abs/1505.07368,Revisiting Actor Programming in C++,"  The actor model of computation has gained significant popularity over the last decade. Its high level of abstraction makes it appealing for concurrent applications in parallel and distributed systems. However, designing a real-world actor framework that subsumes full scalability, strong reliability, and high resource efficiency requires many conceptual and algorithmic additives to the original model.   In this paper, we report on designing and building CAF, the ""C++ Actor Framework"". CAF targets at providing a concurrent and distributed native environment for scaling up to very large, high-performance applications, and equally well down to small constrained systems. We present the key specifications and design concepts---in particular a message-transparent architecture, type-safe message interfaces, and pattern matching facilities---that make native actors a viable approach for many robust, elastic, and highly distributed developments. We demonstrate the feasibility of CAF in three scenarios: first for elastic, upscaling environments, second for including heterogeneous hardware like GPGPUs, and third for distributed runtime systems. Extensive performance evaluations indicate ideal runtime behaviour for up to 64 cores at very low memory footprint, or in the presence of GPUs. In these tests, CAF continuously outperforms the competing actor environments Erlang, Charm++, SalsaLite, Scala, ActorFoundry, and even the OpenMPI. ",Computer Science - Programming Languages ; ,"Charousset, Dominik ; Hiesgen, Raphael ; Schmidt, Thomas C. ; ","Revisiting Actor Programming in C++  The actor model of computation has gained significant popularity over the last decade. Its high level of abstraction makes it appealing for concurrent applications in parallel and distributed systems. However, designing a real-world actor framework that subsumes full scalability, strong reliability, and high resource efficiency requires many conceptual and algorithmic additives to the original model.   In this paper, we report on designing and building CAF, the ""C++ Actor Framework"". CAF targets at providing a concurrent and distributed native environment for scaling up to very large, high-performance applications, and equally well down to small constrained systems. We present the key specifications and design concepts---in particular a message-transparent architecture, type-safe message interfaces, and pattern matching facilities---that make native actors a viable approach for many robust, elastic, and highly distributed developments. We demonstrate the feasibility of CAF in three scenarios: first for elastic, upscaling environments, second for including heterogeneous hardware like GPGPUs, and third for distributed runtime systems. Extensive performance evaluations indicate ideal runtime behaviour for up to 64 cores at very low memory footprint, or in the presence of GPUs. In these tests, CAF continuously outperforms the competing actor environments Erlang, Charm++, SalsaLite, Scala, ActorFoundry, and even the OpenMPI. ",revisit actor program actor model computation gain significant popularity last decade high level abstraction make appeal concurrent applications parallel distribute systems however design real world actor framework subsume full scalability strong reliability high resource efficiency require many conceptual algorithmic additives original model paper report design build caf actor framework caf target provide concurrent distribute native environment scale large high performance applications equally well small constrain systems present key specifications design concepts particular message transparent architecture type safe message interfaces pattern match facilities make native actors viable approach many robust elastic highly distribute developments demonstrate feasibility caf three scenarios first elastic upscaling environments second include heterogeneous hardware like gpgpus third distribute runtime systems extensive performance evaluations indicate ideal runtime behaviour core low memory footprint presence gpus test caf continuously outperform compete actor environments erlang charm salsalite scala actorfoundry even openmpi,139,2,1505.07368.txt
http://arxiv.org/abs/1505.07429,Semi-algebraic colorings of complete graphs,"  We consider $m$-colorings of the edges of a complete graph, where each color class is defined semi-algebraically with bounded complexity. The case $m = 2$ was first studied by Alon et al., who applied this framework to obtain surprisingly strong Ramsey-type results for intersection graphs of geometric objects and for other graphs arising in computational geometry. Considering larger values of $m$ is relevant, e.g., to problems concerning the number of distinct distances determined by a point set.   For $p\ge 3$ and $m\ge 2$, the classical Ramsey number $R(p;m)$ is the smallest positive integer $n$ such that any $m$-coloring of the edges of $K_n$, the complete graph on $n$ vertices, contains a monochromatic $K_p$. It is a longstanding open problem that goes back to Schur (1916) to decide whether $R(p;m)=2^{O(m)}$, for a fixed $p$. We prove that this is true if each color class is defined semi-algebraically with bounded complexity. The order of magnitude of this bound is tight. Our proof is based on the Cutting Lemma of Chazelle {\em et al.}, and on a Szemer\'edi-type regularity lemma for multicolored semi-algebraic graphs, which is of independent interest. The same technique is used to address the semi-algebraic variant of a more general Ramsey-type problem of Erd\H{o}s and Shelah. ",Mathematics - Combinatorics ; Computer Science - Computational Geometry ; ,"Fox, Jacob ; Pach, Janos ; Suk, Andrew ; ","Semi-algebraic colorings of complete graphs  We consider $m$-colorings of the edges of a complete graph, where each color class is defined semi-algebraically with bounded complexity. The case $m = 2$ was first studied by Alon et al., who applied this framework to obtain surprisingly strong Ramsey-type results for intersection graphs of geometric objects and for other graphs arising in computational geometry. Considering larger values of $m$ is relevant, e.g., to problems concerning the number of distinct distances determined by a point set.   For $p\ge 3$ and $m\ge 2$, the classical Ramsey number $R(p;m)$ is the smallest positive integer $n$ such that any $m$-coloring of the edges of $K_n$, the complete graph on $n$ vertices, contains a monochromatic $K_p$. It is a longstanding open problem that goes back to Schur (1916) to decide whether $R(p;m)=2^{O(m)}$, for a fixed $p$. We prove that this is true if each color class is defined semi-algebraically with bounded complexity. The order of magnitude of this bound is tight. Our proof is based on the Cutting Lemma of Chazelle {\em et al.}, and on a Szemer\'edi-type regularity lemma for multicolored semi-algebraic graphs, which is of independent interest. The same technique is used to address the semi-algebraic variant of a more general Ramsey-type problem of Erd\H{o}s and Shelah. ",semi algebraic color complete graph consider color edge complete graph color class define semi algebraically bound complexity case first study alon et al apply framework obtain surprisingly strong ramsey type result intersection graph geometric object graph arise computational geometry consider larger value relevant problems concern number distinct distance determine point set ge ge classical ramsey number smallest positive integer color edge complete graph vertices contain monochromatic longstanding open problem go back schur decide whether fix prove true color class define semi algebraically bound complexity order magnitude bind tight proof base cut lemma chazelle em et al szemer edi type regularity lemma multicolored semi algebraic graph independent interest technique use address semi algebraic variant general ramsey type problem erd shelah,119,13,1505.07429.txt
http://arxiv.org/abs/1505.08162,Dimension and cut vertices: an application of Ramsey theory,"  Motivated by quite recent research involving the relationship between the dimension of a poset and graph-theoretic properties of its cover graph, we show that for every $d\geq 1$, if $P$ is a poset and the dimension of a subposet $B$ of $P$ is at most $d$ whenever the cover graph of $B$ is a block of the cover graph of $P$, then the dimension of $P$ is at most $d+2$. We also construct examples which show that this inequality is best possible. We consider the proof of the upper bound to be fairly elegant and relatively compact. However, we know of no simple proof for the lower bound, and our argument requires a powerful tool known as the Product Ramsey Theorem. As a consequence, our constructions involve posets of enormous size. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 06A07, 05C35 ; ","Trotter, William T. ; Walczak, Bartosz ; Wang, Ruidong ; ","Dimension and cut vertices: an application of Ramsey theory  Motivated by quite recent research involving the relationship between the dimension of a poset and graph-theoretic properties of its cover graph, we show that for every $d\geq 1$, if $P$ is a poset and the dimension of a subposet $B$ of $P$ is at most $d$ whenever the cover graph of $B$ is a block of the cover graph of $P$, then the dimension of $P$ is at most $d+2$. We also construct examples which show that this inequality is best possible. We consider the proof of the upper bound to be fairly elegant and relatively compact. However, we know of no simple proof for the lower bound, and our argument requires a powerful tool known as the Product Ramsey Theorem. As a consequence, our constructions involve posets of enormous size. ",dimension cut vertices application ramsey theory motivate quite recent research involve relationship dimension poset graph theoretic properties cover graph show every geq poset dimension subposet whenever cover graph block cover graph dimension also construct examples show inequality best possible consider proof upper bind fairly elegant relatively compact however know simple proof lower bind argument require powerful tool know product ramsey theorem consequence constructions involve posets enormous size,67,3,1505.08162.txt
http://arxiv.org/abs/1506.00147,Team Performance with Test Scores,"  Team performance is a ubiquitous area of inquiry in the social sciences, and it motivates the problem of team selection -- choosing the members of a team for maximum performance. Influential work of Hong and Page has argued that testing individuals in isolation and then assembling the highest-scoring ones into a team is not an effective method for team selection. For a broad class of performance measures, based on the expected maximum of random variables representing individual candidates, we show that tests directly measuring individual performance are indeed ineffective, but that a more subtle family of tests used in isolation can provide a constant-factor approximation for team performance. These new tests measure the ""potential"" of individuals, in a precise sense, rather than performance, to our knowledge they represent the first time that individual tests have been shown to produce near-optimal teams for a non-trivial team performance measure. We also show families of subdmodular and supermodular team performance functions for which no test applied to individuals can produce near-optimal teams, and discuss implications for submodular maximization via hill-climbing. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computer Science and Game Theory ; ,"Kleinberg, Jon ; Raghu, Maithra ; ","Team Performance with Test Scores  Team performance is a ubiquitous area of inquiry in the social sciences, and it motivates the problem of team selection -- choosing the members of a team for maximum performance. Influential work of Hong and Page has argued that testing individuals in isolation and then assembling the highest-scoring ones into a team is not an effective method for team selection. For a broad class of performance measures, based on the expected maximum of random variables representing individual candidates, we show that tests directly measuring individual performance are indeed ineffective, but that a more subtle family of tests used in isolation can provide a constant-factor approximation for team performance. These new tests measure the ""potential"" of individuals, in a precise sense, rather than performance, to our knowledge they represent the first time that individual tests have been shown to produce near-optimal teams for a non-trivial team performance measure. We also show families of subdmodular and supermodular team performance functions for which no test applied to individuals can produce near-optimal teams, and discuss implications for submodular maximization via hill-climbing. ",team performance test score team performance ubiquitous area inquiry social sciences motivate problem team selection choose members team maximum performance influential work hong page argue test individuals isolation assemble highest score ones team effective method team selection broad class performance measure base expect maximum random variables represent individual candidates show test directly measure individual performance indeed ineffective subtle family test use isolation provide constant factor approximation team performance new test measure potential individuals precise sense rather performance knowledge represent first time individual test show produce near optimal team non trivial team performance measure also show families subdmodular supermodular team performance function test apply individuals produce near optimal team discuss implications submodular maximization via hill climb,115,7,1506.00147.txt
http://arxiv.org/abs/1506.00272,Synapse: Synthetic Application Profiler and Emulator,"  We introduce Synapse motivated by the needs to estimate and emulate workload execution characteristics on high-performance and distributed heterogeneous resources. Synapse has a platform independent application profiler, and the ability to emulate profiled workloads on a variety of heterogeneous resources. Synapse is used as a proxy application (or ""representative application"") for real workloads, with the added advantage that it can be tuned at arbitrary levels of granularity in ways that are simply not possible using real applications. Experiments show that automated profiling using Synapse represents application characteristics with high fidelity. Emulation using Synapse can reproduce the application behavior in the original runtime environment, as well as reproducing properties when used in a different run-time environments. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Merzky, Andre ; Jha, Shantenu ; ","Synapse: Synthetic Application Profiler and Emulator  We introduce Synapse motivated by the needs to estimate and emulate workload execution characteristics on high-performance and distributed heterogeneous resources. Synapse has a platform independent application profiler, and the ability to emulate profiled workloads on a variety of heterogeneous resources. Synapse is used as a proxy application (or ""representative application"") for real workloads, with the added advantage that it can be tuned at arbitrary levels of granularity in ways that are simply not possible using real applications. Experiments show that automated profiling using Synapse represents application characteristics with high fidelity. Emulation using Synapse can reproduce the application behavior in the original runtime environment, as well as reproducing properties when used in a different run-time environments. ",synapse synthetic application profiler emulator introduce synapse motivate need estimate emulate workload execution characteristics high performance distribute heterogeneous resources synapse platform independent application profiler ability emulate profile workloads variety heterogeneous resources synapse use proxy application representative application real workloads add advantage tune arbitrary level granularity ways simply possible use real applications experiment show automate profile use synapse represent application characteristics high fidelity emulation use synapse reproduce application behavior original runtime environment well reproduce properties use different run time environments,79,7,1506.00272.txt
http://arxiv.org/abs/1506.00290,Compressing Communication in Distributed Protocols,"  We show how to compress communication in selection protocols, where the goal is to agree on a sequence of random bits using only a broadcast channel. More specifically, we present a generic method for converting any selection protocol, into another selection protocol where each message is ``short'' while preserving the same number of rounds, the same output distribution, and the same resilience to error. Assuming that the output of the protocol lies in some universe of size $M$, in our resulting protocol each message consists of only $\mathsf{polylog}(M,n,d)$ many bits, where $n$ is the number of parties and $d$ is the number of rounds. Our transformation works in the presence of either static or adaptive Byzantine faults.   As a corollary, we conclude that for any $\mathsf{poly}(n)$-round collective coin-flipping protocol, leader election protocol, or general selection protocols, messages of length $\mathsf{polylog}(n)$ suffice (in the presence of either static or adaptive Byzantine faults). ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Kalai, Yael Tauman ; Komargodski, Ilan ; ","Compressing Communication in Distributed Protocols  We show how to compress communication in selection protocols, where the goal is to agree on a sequence of random bits using only a broadcast channel. More specifically, we present a generic method for converting any selection protocol, into another selection protocol where each message is ``short'' while preserving the same number of rounds, the same output distribution, and the same resilience to error. Assuming that the output of the protocol lies in some universe of size $M$, in our resulting protocol each message consists of only $\mathsf{polylog}(M,n,d)$ many bits, where $n$ is the number of parties and $d$ is the number of rounds. Our transformation works in the presence of either static or adaptive Byzantine faults.   As a corollary, we conclude that for any $\mathsf{poly}(n)$-round collective coin-flipping protocol, leader election protocol, or general selection protocols, messages of length $\mathsf{polylog}(n)$ suffice (in the presence of either static or adaptive Byzantine faults). ",compress communication distribute protocols show compress communication selection protocols goal agree sequence random bits use broadcast channel specifically present generic method convert selection protocol another selection protocol message short preserve number round output distribution resilience error assume output protocol lie universe size result protocol message consist mathsf polylog many bits number party number round transformation work presence either static adaptive byzantine fault corollary conclude mathsf poly round collective coin flip protocol leader election protocol general selection protocols message length mathsf polylog suffice presence either static adaptive byzantine fault,88,12,1506.00290.txt
http://arxiv.org/abs/1506.00366,Formal Concept Analysis for Knowledge Discovery from Biological Data,"  Due to rapid advancement in high-throughput techniques, such as microarrays and next generation sequencing technologies, biological data are increasing exponentially. The current challenge in computational biology and bioinformatics research is how to analyze these huge raw biological data to extract biologically meaningful knowledge. This review paper presents the applications of formal concept analysis for the analysis and knowledge discovery from biological data, including gene expression discretization, gene co-expression mining, gene expression clustering, finding genes in gene regulatory networks, enzyme/protein classifications, binding site classifications, and so on. It also presents a list of FCA-based software tools applied in biological domain and covers the challenges faced so far. ","Computer Science - Artificial Intelligence ; Computer Science - Computational Engineering, Finance, and Science ; Quantitative Biology - Genomics ; ","Raza, Khalid ; ","Formal Concept Analysis for Knowledge Discovery from Biological Data  Due to rapid advancement in high-throughput techniques, such as microarrays and next generation sequencing technologies, biological data are increasing exponentially. The current challenge in computational biology and bioinformatics research is how to analyze these huge raw biological data to extract biologically meaningful knowledge. This review paper presents the applications of formal concept analysis for the analysis and knowledge discovery from biological data, including gene expression discretization, gene co-expression mining, gene expression clustering, finding genes in gene regulatory networks, enzyme/protein classifications, binding site classifications, and so on. It also presents a list of FCA-based software tools applied in biological domain and covers the challenges faced so far. ",formal concept analysis knowledge discovery biological data due rapid advancement high throughput techniques microarrays next generation sequence technologies biological data increase exponentially current challenge computational biology bioinformatics research analyze huge raw biological data extract biologically meaningful knowledge review paper present applications formal concept analysis analysis knowledge discovery biological data include gene expression discretization gene co expression mine gene expression cluster find genes gene regulatory network enzyme protein classifications bind site classifications also present list fca base software tool apply biological domain cover challenge face far,85,10,1506.00366.txt
http://arxiv.org/abs/1506.00529,Desirability and the birth of incomplete preferences,"  We establish an equivalence between two seemingly different theories: one is the traditional axiomatisation of incomplete preferences on horse lotteries based on the mixture independence axiom; the other is the theory of desirable gambles developed in the context of imprecise probability. The equivalence allows us to revisit incomplete preferences from the viewpoint of desirability and through the derived notion of coherent lower previsions. On this basis, we obtain new results and insights: in particular, we show that the theory of incomplete preferences can be developed assuming only the existence of a worst act---no best act is needed---, and that a weakened Archimedean axiom suffices too; this axiom allows us also to address some controversy about the regularity assumption (that probabilities should be positive---they need not), which enables us also to deal with uncountable possibility spaces; we show that it is always possible to extend in a minimal way a preference relation to one with a worst act, and yet the resulting relation is never Archimedean, except in a trivial case; we show that the traditional notion of state independence coincides with the notion called strong independence in imprecise probability---this leads us to give much a weaker definition of state independence than the traditional one; we rework and uniform the notions of complete preferences, beliefs, values; we argue that Archimedeanity does not capture all the problems that can be modelled with sets of expected utilities and we provide a new notion that does precisely that. Perhaps most importantly, we argue throughout that desirability is a powerful and natural setting to model, and work with, incomplete preferences, even in case of non-Archimedean problems. This leads us to suggest that desirability, rather than preference, should be the primitive notion at the basis of decision-theoretic axiomatisations. ",Computer Science - Artificial Intelligence ; ,"Zaffalon, Marco ; Miranda, Enrique ; ","Desirability and the birth of incomplete preferences  We establish an equivalence between two seemingly different theories: one is the traditional axiomatisation of incomplete preferences on horse lotteries based on the mixture independence axiom; the other is the theory of desirable gambles developed in the context of imprecise probability. The equivalence allows us to revisit incomplete preferences from the viewpoint of desirability and through the derived notion of coherent lower previsions. On this basis, we obtain new results and insights: in particular, we show that the theory of incomplete preferences can be developed assuming only the existence of a worst act---no best act is needed---, and that a weakened Archimedean axiom suffices too; this axiom allows us also to address some controversy about the regularity assumption (that probabilities should be positive---they need not), which enables us also to deal with uncountable possibility spaces; we show that it is always possible to extend in a minimal way a preference relation to one with a worst act, and yet the resulting relation is never Archimedean, except in a trivial case; we show that the traditional notion of state independence coincides with the notion called strong independence in imprecise probability---this leads us to give much a weaker definition of state independence than the traditional one; we rework and uniform the notions of complete preferences, beliefs, values; we argue that Archimedeanity does not capture all the problems that can be modelled with sets of expected utilities and we provide a new notion that does precisely that. Perhaps most importantly, we argue throughout that desirability is a powerful and natural setting to model, and work with, incomplete preferences, even in case of non-Archimedean problems. This leads us to suggest that desirability, rather than preference, should be the primitive notion at the basis of decision-theoretic axiomatisations. ",desirability birth incomplete preferences establish equivalence two seemingly different theories one traditional axiomatisation incomplete preferences horse lotteries base mixture independence axiom theory desirable gamble develop context imprecise probability equivalence allow us revisit incomplete preferences viewpoint desirability derive notion coherent lower previsions basis obtain new result insights particular show theory incomplete preferences develop assume existence worst act best act need weaken archimedean axiom suffice axiom allow us also address controversy regularity assumption probabilities positive need enable us also deal uncountable possibility space show always possible extend minimal way preference relation one worst act yet result relation never archimedean except trivial case show traditional notion state independence coincide notion call strong independence imprecise probability lead us give much weaker definition state independence traditional one rework uniform notions complete preferences beliefs value argue archimedeanity capture problems model set expect utilities provide new notion precisely perhaps importantly argue throughout desirability powerful natural set model work incomplete preferences even case non archimedean problems lead us suggest desirability rather preference primitive notion basis decision theoretic axiomatisations,170,8,1506.00529.txt
http://arxiv.org/abs/1506.00552,Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than   Random Selection,"  There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of Nesterov [SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection rule achieves the same convergence rate as the Gauss-Southwell selection rule. This result suggests that we should never use the Gauss-Southwell rule, as it is typically much more expensive than random selection. However, the empirical behaviours of these algorithms contradict this theoretical result: in applications where the computational costs of the selection rules are comparable, the Gauss-Southwell selection rule tends to perform substantially better than random coordinate selection. We give a simple analysis of the Gauss-Southwell rule showing that---except in extreme cases---its convergence rate is faster than choosing random coordinates. Further, in this work we (i) show that exact coordinate optimization improves the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that gives an even faster convergence rate given knowledge of the Lipschitz constants of the partial derivatives, (iii) analyze the effect of approximate Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the Gauss-Southwell rule. ",Mathematics - Optimization and Control ; Computer Science - Machine Learning ; Statistics - Computation ; Statistics - Machine Learning ; ,"Nutini, Julie ; Schmidt, Mark ; Laradji, Issam H. ; Friedlander, Michael ; Koepke, Hoyt ; ","Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than   Random Selection  There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of Nesterov [SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection rule achieves the same convergence rate as the Gauss-Southwell selection rule. This result suggests that we should never use the Gauss-Southwell rule, as it is typically much more expensive than random selection. However, the empirical behaviours of these algorithms contradict this theoretical result: in applications where the computational costs of the selection rules are comparable, the Gauss-Southwell selection rule tends to perform substantially better than random coordinate selection. We give a simple analysis of the Gauss-Southwell rule showing that---except in extreme cases---its convergence rate is faster than choosing random coordinates. Further, in this work we (i) show that exact coordinate optimization improves the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that gives an even faster convergence rate given knowledge of the Lipschitz constants of the partial derivatives, (iii) analyze the effect of approximate Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the Gauss-Southwell rule. ",coordinate descent converge faster gauss southwell rule random selection significant recent work theory application randomize coordinate descent algorithms begin work nesterov siam optim show random coordinate selection rule achieve convergence rate gauss southwell selection rule result suggest never use gauss southwell rule typically much expensive random selection however empirical behaviours algorithms contradict theoretical result applications computational cost selection rule comparable gauss southwell selection rule tend perform substantially better random coordinate selection give simple analysis gauss southwell rule show except extreme case convergence rate faster choose random coordinate work show exact coordinate optimization improve convergence rate certain sparse problems ii propose gauss southwell lipschitz rule give even faster convergence rate give knowledge lipschitz constants partial derivatives iii analyze effect approximate gauss southwell rule iv analyze proximal gradient variants gauss southwell rule,130,7,1506.00552.txt
http://arxiv.org/abs/1506.00571,Calculation of the confidence bounds for the fraction nonconforming of   normal populations of measurements in clinical laboratory medicine,"  The fraction nonconforming is a key quality measure used in statistical quality control design in clinical laboratory medicine. The confidence bounds of normal populations of measurements for the fraction nonconforming each of the lower and upper quality specification limits when both the random and the systematic error are unknown can be calculated using the noncentral t-distribution, as it is described in detail and illustrated with examples. ","Computer Science - Computational Engineering, Finance, and Science ; 6804 ; J.2 ; ","Hatjimihail, Aristides T. ; ","Calculation of the confidence bounds for the fraction nonconforming of   normal populations of measurements in clinical laboratory medicine  The fraction nonconforming is a key quality measure used in statistical quality control design in clinical laboratory medicine. The confidence bounds of normal populations of measurements for the fraction nonconforming each of the lower and upper quality specification limits when both the random and the systematic error are unknown can be calculated using the noncentral t-distribution, as it is described in detail and illustrated with examples. ",calculation confidence bound fraction nonconforming normal populations measurements clinical laboratory medicine fraction nonconforming key quality measure use statistical quality control design clinical laboratory medicine confidence bound normal populations measurements fraction nonconforming lower upper quality specification limit random systematic error unknown calculate use noncentral distribution describe detail illustrate examples,48,9,1506.00571.txt
http://arxiv.org/abs/1506.00573,Two-dimensional Decoding Algorithms and Recording Techniques for Bit   Patterned Media Feasibility Demonstrations,"  Recording experiments and decoding algorithms are presented for evaluating the bit-error-rate of state-of-the-art magnetic bitpatterned media. The recording experiments are performed with a static tester and conventional hard-disk-drive heads. As the reader dimensions are larger than the bit dimensions in both the down-track and the cross-track directions, a two-dimensional bit decoding algorithm is required. Two such algorithms are presented in details together with the methodology implemented to accurately retrieve island positions during recording. Using these techniques, a 1.6 Td/in$^2$ magnetic bit pattern media is demonstrated to support 2D bit error rates below 1e-2 under shingled magnetic recording conditions. ","Computer Science - Information Theory ; 94A40 (Primary), 94B10, 82D40, 68P30 (Secondary) ; B.4.2 ; B.3.2 ; ","Obukhov, Yuri ; Jubert, Pierre-Olivier ; Bedau, Daniel ; Grobis, Michael ; ","Two-dimensional Decoding Algorithms and Recording Techniques for Bit   Patterned Media Feasibility Demonstrations  Recording experiments and decoding algorithms are presented for evaluating the bit-error-rate of state-of-the-art magnetic bitpatterned media. The recording experiments are performed with a static tester and conventional hard-disk-drive heads. As the reader dimensions are larger than the bit dimensions in both the down-track and the cross-track directions, a two-dimensional bit decoding algorithm is required. Two such algorithms are presented in details together with the methodology implemented to accurately retrieve island positions during recording. Using these techniques, a 1.6 Td/in$^2$ magnetic bit pattern media is demonstrated to support 2D bit error rates below 1e-2 under shingled magnetic recording conditions. ",two dimensional decode algorithms record techniques bite pattern media feasibility demonstrations record experiment decode algorithms present evaluate bite error rate state art magnetic bitpatterned media record experiment perform static tester conventional hard disk drive head reader dimension larger bite dimension track cross track directions two dimensional bite decode algorithm require two algorithms present detail together methodology implement accurately retrieve island position record use techniques td magnetic bite pattern media demonstrate support bite error rat shingle magnetic record condition,78,10,1506.00573.txt
http://arxiv.org/abs/1506.00768,Soft Computing Techniques for Change Detection in remotely sensed images   : A Review,"  With the advent of remote sensing satellites, a huge repository of remotely sensed images is available. Change detection in remotely sensed images has been an active research area as it helps us understand the transitions that are taking place on the Earths surface. This paper discusses the methods and their classifications proposed by various researchers for change detection. Since use of soft computing based techniques are now very popular among research community, this paper also presents a classification based on learning techniques used in soft-computing methods for change detection. ",Computer Science - Neural and Evolutionary Computing ; Computer Science - Computer Vision and Pattern Recognition ; ,"Khurana, Madhu ; Saxena, Vikas ; ","Soft Computing Techniques for Change Detection in remotely sensed images   : A Review  With the advent of remote sensing satellites, a huge repository of remotely sensed images is available. Change detection in remotely sensed images has been an active research area as it helps us understand the transitions that are taking place on the Earths surface. This paper discusses the methods and their classifications proposed by various researchers for change detection. Since use of soft computing based techniques are now very popular among research community, this paper also presents a classification based on learning techniques used in soft-computing methods for change detection. ",soft compute techniques change detection remotely sense image review advent remote sense satellite huge repository remotely sense image available change detection remotely sense image active research area help us understand transition take place earth surface paper discuss methods classifications propose various researchers change detection since use soft compute base techniques popular among research community paper also present classification base learn techniques use soft compute methods change detection,67,7,1506.00768.txt
http://arxiv.org/abs/1506.01110,Multi-View Factorization Machines,"  For a learning task, data can usually be collected from different sources or be represented from multiple views. For example, laboratory results from different medical examinations are available for disease diagnosis, and each of them can only reflect the health state of a person from a particular aspect/view. Therefore, different views provide complementary information for learning tasks. An effective integration of the multi-view information is expected to facilitate the learning performance. In this paper, we propose a general predictor, named multi-view machines (MVMs), that can effectively include all the possible interactions between features from multiple views. A joint factorization is embedded for the full-order interaction parameters which allows parameter estimation under sparsity. Moreover, MVMs can work in conjunction with different loss functions for a variety of machine learning tasks. A stochastic gradient descent method is presented to learn the MVM model. We further illustrate the advantages of MVMs through comparison with other methods for multi-view classification, including support vector machines (SVMs), support tensor machines (STMs) and factorization machines (FMs). ",Computer Science - Machine Learning ; Statistics - Machine Learning ; H.2.8 ; ,"Cao, Bokai ; Zhou, Hucheng ; Li, Guoqiang ; Yu, Philip S. ; ","Multi-View Factorization Machines  For a learning task, data can usually be collected from different sources or be represented from multiple views. For example, laboratory results from different medical examinations are available for disease diagnosis, and each of them can only reflect the health state of a person from a particular aspect/view. Therefore, different views provide complementary information for learning tasks. An effective integration of the multi-view information is expected to facilitate the learning performance. In this paper, we propose a general predictor, named multi-view machines (MVMs), that can effectively include all the possible interactions between features from multiple views. A joint factorization is embedded for the full-order interaction parameters which allows parameter estimation under sparsity. Moreover, MVMs can work in conjunction with different loss functions for a variety of machine learning tasks. A stochastic gradient descent method is presented to learn the MVM model. We further illustrate the advantages of MVMs through comparison with other methods for multi-view classification, including support vector machines (SVMs), support tensor machines (STMs) and factorization machines (FMs). ",multi view factorization machine learn task data usually collect different source represent multiple view example laboratory result different medical examinations available disease diagnosis reflect health state person particular aspect view therefore different view provide complementary information learn task effective integration multi view information expect facilitate learn performance paper propose general predictor name multi view machine mvms effectively include possible interactions feature multiple view joint factorization embed full order interaction parameters allow parameter estimation sparsity moreover mvms work conjunction different loss function variety machine learn task stochastic gradient descent method present learn mvm model illustrate advantage mvms comparison methods multi view classification include support vector machine svms support tensor machine stms factorization machine fms,113,2,1506.01110.txt
http://arxiv.org/abs/1506.01634,Signs of universality in the structure of culture,"  Understanding the dynamics of opinions, preferences and of culture as whole requires more use of empirical data than has been done so far. It is clear that an important role in driving this dynamics is played by social influence, which is the essential ingredient of many quantitative models. Such models require that all traits are fixed when specifying the ""initial cultural state"". Typically, this initial state is randomly generated, from a uniform distribution over the set of possible combinations of traits. However, recent work has shown that the outcome of social influence dynamics strongly depends on the nature of the initial state. If the latter is sampled from empirical data instead of being generated in a uniformly random way, a higher level of cultural diversity is found after long-term dynamics, for the same level of propensity towards collective behavior in the short-term. Moreover, if the initial state is randomized by shuffling the empirical traits among people, the level of long-term cultural diversity is in-between those obtained for the empirical and uniformly random counterparts. The current study repeats the analysis for multiple empirical data sets, showing that the results are remarkably similar, although the matrix of correlations between cultural variables clearly differs across data sets. This points towards robust structural properties inherent in empirical cultural states, possibly due to universal laws governing the dynamics of culture in the real world. The results also suggest that this dynamics might be characterized by criticality and involve mechanisms beyond social influence. ","Physics - Physics and Society ; Computer Science - Computers and Society ; Physics - Data Analysis, Statistics and Probability ; ","Băbeanu, Alexandru-Ionuţ ; Talman, Leandros ; Garlaschelli, Diego ; ","Signs of universality in the structure of culture  Understanding the dynamics of opinions, preferences and of culture as whole requires more use of empirical data than has been done so far. It is clear that an important role in driving this dynamics is played by social influence, which is the essential ingredient of many quantitative models. Such models require that all traits are fixed when specifying the ""initial cultural state"". Typically, this initial state is randomly generated, from a uniform distribution over the set of possible combinations of traits. However, recent work has shown that the outcome of social influence dynamics strongly depends on the nature of the initial state. If the latter is sampled from empirical data instead of being generated in a uniformly random way, a higher level of cultural diversity is found after long-term dynamics, for the same level of propensity towards collective behavior in the short-term. Moreover, if the initial state is randomized by shuffling the empirical traits among people, the level of long-term cultural diversity is in-between those obtained for the empirical and uniformly random counterparts. The current study repeats the analysis for multiple empirical data sets, showing that the results are remarkably similar, although the matrix of correlations between cultural variables clearly differs across data sets. This points towards robust structural properties inherent in empirical cultural states, possibly due to universal laws governing the dynamics of culture in the real world. The results also suggest that this dynamics might be characterized by criticality and involve mechanisms beyond social influence. ",sign universality structure culture understand dynamics opinions preferences culture whole require use empirical data do far clear important role drive dynamics play social influence essential ingredient many quantitative model model require traits fix specify initial cultural state typically initial state randomly generate uniform distribution set possible combinations traits however recent work show outcome social influence dynamics strongly depend nature initial state latter sample empirical data instead generate uniformly random way higher level cultural diversity find long term dynamics level propensity towards collective behavior short term moreover initial state randomize shuffle empirical traits among people level long term cultural diversity obtain empirical uniformly random counterparts current study repeat analysis multiple empirical data set show result remarkably similar although matrix correlations cultural variables clearly differ across data set point towards robust structural properties inherent empirical cultural state possibly due universal laws govern dynamics culture real world result also suggest dynamics might characterize criticality involve mechanisms beyond social influence,156,10,1506.01634.txt
http://arxiv.org/abs/1506.01978,Information measures and cognitive limits in multilayer navigation,"  Cities and their transportation systems become increasingly complex and multimodal as they grow, and it is natural to wonder if it is possible to quantitatively characterize our difficulty to navigate in them and whether such navigation exceeds our cognitive limits. A transition between different searching strategies for navigating in metropolitan maps has been observed for large, complex metropolitan networks. This evidence suggests the existence of another limit associated to the cognitive overload and caused by large amounts of information to process. In this light, we analyzed the world's 15 largest metropolitan networks and estimated the information limit for determining a trip in a transportation system to be on the order of 8 bits. Similar to the ""Dunbar number,"" which represents a limit to the size of an individual's friendship circle, our cognitive limit suggests that maps should not consist of more than about $250$ connections points to be easily readable. We also show that including connections with other transportation modes dramatically increases the information needed to navigate in multilayer transportation networks: in large cities such as New York, Paris, and Tokyo, more than $80\%$ of trips are above the 8-bit limit. Multimodal transportation systems in large cities have thus already exceeded human cognitive limits and consequently the traditional view of navigation in cities has to be revised substantially. ",Physics - Physics and Society ; Condensed Matter - Disordered Systems and Neural Networks ; Computer Science - Social and Information Networks ; ,"Gallotti, Riccardo ; Porter, Mason A. ; Barthelemy, Marc ; ","Information measures and cognitive limits in multilayer navigation  Cities and their transportation systems become increasingly complex and multimodal as they grow, and it is natural to wonder if it is possible to quantitatively characterize our difficulty to navigate in them and whether such navigation exceeds our cognitive limits. A transition between different searching strategies for navigating in metropolitan maps has been observed for large, complex metropolitan networks. This evidence suggests the existence of another limit associated to the cognitive overload and caused by large amounts of information to process. In this light, we analyzed the world's 15 largest metropolitan networks and estimated the information limit for determining a trip in a transportation system to be on the order of 8 bits. Similar to the ""Dunbar number,"" which represents a limit to the size of an individual's friendship circle, our cognitive limit suggests that maps should not consist of more than about $250$ connections points to be easily readable. We also show that including connections with other transportation modes dramatically increases the information needed to navigate in multilayer transportation networks: in large cities such as New York, Paris, and Tokyo, more than $80\%$ of trips are above the 8-bit limit. Multimodal transportation systems in large cities have thus already exceeded human cognitive limits and consequently the traditional view of navigation in cities has to be revised substantially. ",information measure cognitive limit multilayer navigation cities transportation systems become increasingly complex multimodal grow natural wonder possible quantitatively characterize difficulty navigate whether navigation exceed cognitive limit transition different search strategies navigate metropolitan map observe large complex metropolitan network evidence suggest existence another limit associate cognitive overload cause large amount information process light analyze world largest metropolitan network estimate information limit determine trip transportation system order bits similar dunbar number represent limit size individual friendship circle cognitive limit suggest map consist connections point easily readable also show include connections transportation modes dramatically increase information need navigate multilayer transportation network large cities new york paris tokyo trip bite limit multimodal transportation systems large cities thus already exceed human cognitive limit consequently traditional view navigation cities revise substantially,125,6,1506.01978.txt
http://arxiv.org/abs/1506.02228,Strong converse exponents for the feedback-assisted classical capacity   of entanglement-breaking channels,"  Quantum entanglement can be used in a communication scheme to establish a correlation between successive channel inputs that is impossible by classical means. It is known that the classical capacity of quantum channels can be enhanced by such entangled encoding schemes, but this is not always the case. In this paper, we prove that a strong converse theorem holds for the classical capacity of an entanglement-breaking channel even when it is assisted by a classical feedback link from the receiver to the transmitter. In doing so, we identify a bound on the strong converse exponent, which determines the exponentially decaying rate at which the success probability tends to zero, for a sequence of codes with communication rate exceeding capacity. Proving a strong converse, along with an achievability theorem, shows that the classical capacity is a sharp boundary between reliable and unreliable communication regimes. One of the main tools in our proof is the sandwiched Renyi relative entropy. The same method of proof is used to derive an exponential bound on the success probability when communicating over an arbitrary quantum channel assisted by classical feedback, provided that the transmitter does not use entangled encoding schemes. ",Quantum Physics ; Computer Science - Information Theory ; ,"Ding, Dawei ; Wilde, Mark M. ; ","Strong converse exponents for the feedback-assisted classical capacity   of entanglement-breaking channels  Quantum entanglement can be used in a communication scheme to establish a correlation between successive channel inputs that is impossible by classical means. It is known that the classical capacity of quantum channels can be enhanced by such entangled encoding schemes, but this is not always the case. In this paper, we prove that a strong converse theorem holds for the classical capacity of an entanglement-breaking channel even when it is assisted by a classical feedback link from the receiver to the transmitter. In doing so, we identify a bound on the strong converse exponent, which determines the exponentially decaying rate at which the success probability tends to zero, for a sequence of codes with communication rate exceeding capacity. Proving a strong converse, along with an achievability theorem, shows that the classical capacity is a sharp boundary between reliable and unreliable communication regimes. One of the main tools in our proof is the sandwiched Renyi relative entropy. The same method of proof is used to derive an exponential bound on the success probability when communicating over an arbitrary quantum channel assisted by classical feedback, provided that the transmitter does not use entangled encoding schemes. ",strong converse exponents feedback assist classical capacity entanglement break channel quantum entanglement use communication scheme establish correlation successive channel input impossible classical mean know classical capacity quantum channel enhance entangle encode scheme always case paper prove strong converse theorem hold classical capacity entanglement break channel even assist classical feedback link receiver transmitter identify bind strong converse exponent determine exponentially decay rate success probability tend zero sequence cod communication rate exceed capacity prove strong converse along achievability theorem show classical capacity sharp boundary reliable unreliable communication regimes one main tool proof sandwich renyi relative entropy method proof use derive exponential bind success probability communicate arbitrary quantum channel assist classical feedback provide transmitter use entangle encode scheme,115,12,1506.02228.txt
http://arxiv.org/abs/1506.02438,High-Dimensional Continuous Control Using Generalized Advantage   Estimation,"  Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.   Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time. ",Computer Science - Machine Learning ; Computer Science - Robotics ; Computer Science - Systems and Control ; ,"Schulman, John ; Moritz, Philipp ; Levine, Sergey ; Jordan, Michael ; Abbeel, Pieter ; ","High-Dimensional Continuous Control Using Generalized Advantage   Estimation  Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.   Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time. ",high dimensional continuous control use generalize advantage estimation policy gradient methods appeal approach reinforcement learn directly optimize cumulative reward straightforwardly use nonlinear function approximators neural network two main challenge large number sample typically require difficulty obtain stable steady improvement despite nonstationarity incoming data address first challenge use value function substantially reduce variance policy gradient estimate cost bias exponentially weight estimator advantage function analogous td lambda address second challenge use trust region optimization procedure policy value function represent neural network approach yield strong empirical result highly challenge locomotion task learn run gaits bipedal quadrupedal simulate robots learn policy get biped stand start lie grind contrast body prior work use hand craft policy representations neural network policies map directly raw kinematics joint torques algorithm fully model free amount simulate experience require learn task bipeds correspond weeks real time,136,6,1506.02438.txt
http://arxiv.org/abs/1506.02930,Arguments for the Effectiveness of Human Problem Solving,"  The question of how humans solve problem has been addressed extensively. However, the direct study of the effectiveness of this process seems to be overlooked. In this paper, we address the issue of the effectiveness of human problem solving: we analyze where this effectiveness comes from and what cognitive mechanisms or heuristics are involved. Our results are based on the optimal probabilistic problem solving strategy that appeared in Solomonoff paper on general problem solving system. We provide arguments that a certain set of cognitive mechanisms or heuristics drive human problem solving in the similar manner as the optimal Solomonoff strategy. The results presented in this paper can serve both cognitive psychology in better understanding of human problem solving processes as well as artificial intelligence in designing more human-like agents. ",Computer Science - Artificial Intelligence ; 68T20 ; I.2.0 ; I.2.8 ; ,"Duris, Frantisek ; ","Arguments for the Effectiveness of Human Problem Solving  The question of how humans solve problem has been addressed extensively. However, the direct study of the effectiveness of this process seems to be overlooked. In this paper, we address the issue of the effectiveness of human problem solving: we analyze where this effectiveness comes from and what cognitive mechanisms or heuristics are involved. Our results are based on the optimal probabilistic problem solving strategy that appeared in Solomonoff paper on general problem solving system. We provide arguments that a certain set of cognitive mechanisms or heuristics drive human problem solving in the similar manner as the optimal Solomonoff strategy. The results presented in this paper can serve both cognitive psychology in better understanding of human problem solving processes as well as artificial intelligence in designing more human-like agents. ",arguments effectiveness human problem solve question humans solve problem address extensively however direct study effectiveness process seem overlook paper address issue effectiveness human problem solve analyze effectiveness come cognitive mechanisms heuristics involve result base optimal probabilistic problem solve strategy appear solomonoff paper general problem solve system provide arguments certain set cognitive mechanisms heuristics drive human problem solve similar manner optimal solomonoff strategy result present paper serve cognitive psychology better understand human problem solve process well artificial intelligence design human like agents,81,11,1506.02930.txt
http://arxiv.org/abs/1506.03171,Error Correction by Structural Simplicity: Correcting Samplable Additive   Errors,"  This paper explores the possibilities and limitations of error correction by the structural simplicity of error mechanisms. Specifically, we consider channel models, called \emph{samplable additive channels}, in which (a) errors are efficiently sampled without the knowledge of the coding scheme or the transmitted codeword; (b) the entropy of the error distribution is bounded; and (c) the number of errors introduced by the channel is unbounded. For the channels, several negative and positive results are provided. Assuming the existence of one-way functions, there are samplable additive errors of entropy $n^{\epsilon}$ for $\epsilon \in (0,1)$ that are pseudorandom, and thus not correctable by efficient coding schemes. It is shown that there is an oracle algorithm that induces a samplable distribution over $\{0,1\}^n$ of entropy $m = \omega( \log n)$ that is not pseudorandom, but is uncorrectable by efficient schemes of rate less than $1 - m/n - o(1)$. The results indicate that restricting error mechanisms to be efficiently samplable and not pseudorandom is insufficient for error correction. As positive results, some conditions are provided under which efficient error correction is possible. ",Computer Science - Information Theory ; ,"Yasunaga, Kenji ; ","Error Correction by Structural Simplicity: Correcting Samplable Additive   Errors  This paper explores the possibilities and limitations of error correction by the structural simplicity of error mechanisms. Specifically, we consider channel models, called \emph{samplable additive channels}, in which (a) errors are efficiently sampled without the knowledge of the coding scheme or the transmitted codeword; (b) the entropy of the error distribution is bounded; and (c) the number of errors introduced by the channel is unbounded. For the channels, several negative and positive results are provided. Assuming the existence of one-way functions, there are samplable additive errors of entropy $n^{\epsilon}$ for $\epsilon \in (0,1)$ that are pseudorandom, and thus not correctable by efficient coding schemes. It is shown that there is an oracle algorithm that induces a samplable distribution over $\{0,1\}^n$ of entropy $m = \omega( \log n)$ that is not pseudorandom, but is uncorrectable by efficient schemes of rate less than $1 - m/n - o(1)$. The results indicate that restricting error mechanisms to be efficiently samplable and not pseudorandom is insufficient for error correction. As positive results, some conditions are provided under which efficient error correction is possible. ",error correction structural simplicity correct samplable additive errors paper explore possibilities limitations error correction structural simplicity error mechanisms specifically consider channel model call emph samplable additive channel errors efficiently sample without knowledge cod scheme transmit codeword entropy error distribution bound number errors introduce channel unbounded channel several negative positive result provide assume existence one way function samplable additive errors entropy epsilon epsilon pseudorandom thus correctable efficient cod scheme show oracle algorithm induce samplable distribution entropy omega log pseudorandom uncorrectable efficient scheme rate less result indicate restrict error mechanisms efficiently samplable pseudorandom insufficient error correction positive result condition provide efficient error correction possible,102,5,1506.03171.txt
http://arxiv.org/abs/1506.03410,Random Projection Forests,"  Ensemble methods---particularly those based on decision trees---have recently demonstrated superior performance in a variety of machine learning settings. We introduce a generalization of many existing decision tree methods called ""Random Projection Forests"" (RPF), which is any decision forest that uses (possibly data dependent and random) linear projections. Using this framework, we introduce a special case, called ""Lumberjack"", using very sparse random projections, that is, linear combinations of a small subset of features. Lumberjack obtains statistically significantly improved accuracy over Random Forests, Gradient Boosted Trees, and other approaches on a standard benchmark suites for classification with varying dimension, sample size, and number of classes. To illustrate how, why, and when Lumberjack outperforms other methods, we conduct extensive simulated experiments, in vectors, images, and nonlinear manifolds. Lumberjack typically yields improved performance over existing decision trees ensembles, while mitigating computational efficiency and scalability, and maintaining interpretability. Lumberjack can easily be incorporated into other ensemble methods such as boosting to obtain potentially similar gains. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; 68T10 ; I.5.2 ; ,"Tomita, Tyler M. ; Browne, James ; Shen, Cencheng ; Patsolic, Jesse L. ; Yim, Jason ; Priebe, Carey E. ; Burns, Randal ; Maggioni, Mauro ; Vogelstein, Joshua T. ; ","Random Projection Forests  Ensemble methods---particularly those based on decision trees---have recently demonstrated superior performance in a variety of machine learning settings. We introduce a generalization of many existing decision tree methods called ""Random Projection Forests"" (RPF), which is any decision forest that uses (possibly data dependent and random) linear projections. Using this framework, we introduce a special case, called ""Lumberjack"", using very sparse random projections, that is, linear combinations of a small subset of features. Lumberjack obtains statistically significantly improved accuracy over Random Forests, Gradient Boosted Trees, and other approaches on a standard benchmark suites for classification with varying dimension, sample size, and number of classes. To illustrate how, why, and when Lumberjack outperforms other methods, we conduct extensive simulated experiments, in vectors, images, and nonlinear manifolds. Lumberjack typically yields improved performance over existing decision trees ensembles, while mitigating computational efficiency and scalability, and maintaining interpretability. Lumberjack can easily be incorporated into other ensemble methods such as boosting to obtain potentially similar gains. ",random projection forest ensemble methods particularly base decision tree recently demonstrate superior performance variety machine learn settings introduce generalization many exist decision tree methods call random projection forest rpf decision forest use possibly data dependent random linear projections use framework introduce special case call lumberjack use sparse random projections linear combinations small subset feature lumberjack obtain statistically significantly improve accuracy random forest gradient boost tree approach standard benchmark suit classification vary dimension sample size number class illustrate lumberjack outperform methods conduct extensive simulate experiment vectors image nonlinear manifold lumberjack typically yield improve performance exist decision tree ensembles mitigate computational efficiency scalability maintain interpretability lumberjack easily incorporate ensemble methods boost obtain potentially similar gain,113,12,1506.03410.txt
http://arxiv.org/abs/1506.03437,Topology design for stochastically-forced consensus networks,"  We study an optimal control problem aimed at achieving a desired tradeoff between the network coherence and communication requirements in the distributed controller. Our objective is to add a certain number of edges to an undirected network, with a known graph Laplacian, in order to optimally enhance closed-loop performance. To promote controller sparsity, we introduce $\ell_1$-regularization into the optimal ${\cal H}_2$ formulation and cast the design problem as a semidefinite program. We derive a Lagrange dual, provide interpretation of dual variables, and exploit structure of the optimality conditions for undirected networks to develop customized proximal gradient and Newton algorithms that are well-suited for large problems. We illustrate that our algorithms can solve the problems with more than million edges in the controller graph in a few minutes, on a PC. We also exploit structure of connected resistive networks to demonstrate how additional edges can be systematically added in order to minimize the ${\cal H}_2$ norm of the closed-loop system. ",Mathematics - Optimization and Control ; Computer Science - Systems and Control ; ,"Hassan-Moghaddam, Sepideh ; Jovanović, Mihailo R. ; ","Topology design for stochastically-forced consensus networks  We study an optimal control problem aimed at achieving a desired tradeoff between the network coherence and communication requirements in the distributed controller. Our objective is to add a certain number of edges to an undirected network, with a known graph Laplacian, in order to optimally enhance closed-loop performance. To promote controller sparsity, we introduce $\ell_1$-regularization into the optimal ${\cal H}_2$ formulation and cast the design problem as a semidefinite program. We derive a Lagrange dual, provide interpretation of dual variables, and exploit structure of the optimality conditions for undirected networks to develop customized proximal gradient and Newton algorithms that are well-suited for large problems. We illustrate that our algorithms can solve the problems with more than million edges in the controller graph in a few minutes, on a PC. We also exploit structure of connected resistive networks to demonstrate how additional edges can be systematically added in order to minimize the ${\cal H}_2$ norm of the closed-loop system. ",topology design stochastically force consensus network study optimal control problem aim achieve desire tradeoff network coherence communication requirements distribute controller objective add certain number edge undirected network know graph laplacian order optimally enhance close loop performance promote controller sparsity introduce ell regularization optimal cal formulation cast design problem semidefinite program derive lagrange dual provide interpretation dual variables exploit structure optimality condition undirected network develop customize proximal gradient newton algorithms well suit large problems illustrate algorithms solve problems million edge controller graph minutes pc also exploit structure connect resistive network demonstrate additional edge systematically add order minimize cal norm close loop system,101,6,1506.03437.txt
http://arxiv.org/abs/1506.03523,Sparsification of Matrices and Compressed Sensing,"  Compressed sensing is a signal processing technique whereby the limits imposed by the Shannon--Nyquist theorem can be exceeded provided certain conditions are imposed on the signal. Such conditions occur in many real-world scenarios, and compressed sensing has emerging applications in medical imaging, big data, and statistics. Finding practical matrix constructions and computationally efficient recovery algorithms for compressed sensing is an area of intense research interest. Many probabilistic matrix constructions have been proposed, and it is now well known that matrices with entries drawn from a suitable probability distribution are essentially optimal for compressed sensing.   Potential applications have motivated the search for constructions of sparse compressed sensing matrices (i.e., matrices containing few non-zero entries). Various constructions have been proposed, and simulations suggest that their performance is comparable to that of dense matrices. In this paper, extensive simulations are presented which suggest that sparsification leads to a marked improvement in compressed sensing performance for a large class of matrix constructions and for many different recovery algorithms. ","Computer Science - Information Theory ; 94A12, 94A15 ; ","Hegarty, Fintan ; Catháin, Padraig Ó ; Zhao, Yunbin ; ","Sparsification of Matrices and Compressed Sensing  Compressed sensing is a signal processing technique whereby the limits imposed by the Shannon--Nyquist theorem can be exceeded provided certain conditions are imposed on the signal. Such conditions occur in many real-world scenarios, and compressed sensing has emerging applications in medical imaging, big data, and statistics. Finding practical matrix constructions and computationally efficient recovery algorithms for compressed sensing is an area of intense research interest. Many probabilistic matrix constructions have been proposed, and it is now well known that matrices with entries drawn from a suitable probability distribution are essentially optimal for compressed sensing.   Potential applications have motivated the search for constructions of sparse compressed sensing matrices (i.e., matrices containing few non-zero entries). Various constructions have been proposed, and simulations suggest that their performance is comparable to that of dense matrices. In this paper, extensive simulations are presented which suggest that sparsification leads to a marked improvement in compressed sensing performance for a large class of matrix constructions and for many different recovery algorithms. ",sparsification matrices compress sense compress sense signal process technique whereby limit impose shannon nyquist theorem exceed provide certain condition impose signal condition occur many real world scenarios compress sense emerge applications medical image big data statistics find practical matrix constructions computationally efficient recovery algorithms compress sense area intense research interest many probabilistic matrix constructions propose well know matrices entries draw suitable probability distribution essentially optimal compress sense potential applications motivate search constructions sparse compress sense matrices matrices contain non zero entries various constructions propose simulations suggest performance comparable dense matrices paper extensive simulations present suggest sparsification lead mark improvement compress sense performance large class matrix constructions many different recovery algorithms,110,7,1506.03523.txt
http://arxiv.org/abs/1506.03872,Diamond Sampling for Approximate Maximum All-pairs Dot-product (MAD)   Search,"  Given two sets of vectors, $A = \{{a_1}, \dots, {a_m}\}$ and $B=\{{b_1},\dots,{b_n}\}$, our problem is to find the top-$t$ dot products, i.e., the largest $|{a_i}\cdot{b_j}|$ among all possible pairs. This is a fundamental mathematical problem that appears in numerous data applications involving similarity search, link prediction, and collaborative filtering. We propose a sampling-based approach that avoids direct computation of all $mn$ dot products. We select diamonds (i.e., four-cycles) from the weighted tripartite representation of $A$ and $B$. The probability of selecting a diamond corresponding to pair $(i,j)$ is proportional to $({a_i}\cdot{b_j})^2$, amplifying the focus on the largest-magnitude entries. Experimental results indicate that diamond sampling is orders of magnitude faster than direct computation and requires far fewer samples than any competing approach. We also apply diamond sampling to the special case of maximum inner product search, and get significantly better results than the state-of-the-art hashing methods. ",Computer Science - Social and Information Networks ; Computer Science - Data Structures and Algorithms ; ,"Ballard, Grey ; Pinar, Ali ; Kolda, Tamara G. ; Seshadhri, C. ; ","Diamond Sampling for Approximate Maximum All-pairs Dot-product (MAD)   Search  Given two sets of vectors, $A = \{{a_1}, \dots, {a_m}\}$ and $B=\{{b_1},\dots,{b_n}\}$, our problem is to find the top-$t$ dot products, i.e., the largest $|{a_i}\cdot{b_j}|$ among all possible pairs. This is a fundamental mathematical problem that appears in numerous data applications involving similarity search, link prediction, and collaborative filtering. We propose a sampling-based approach that avoids direct computation of all $mn$ dot products. We select diamonds (i.e., four-cycles) from the weighted tripartite representation of $A$ and $B$. The probability of selecting a diamond corresponding to pair $(i,j)$ is proportional to $({a_i}\cdot{b_j})^2$, amplifying the focus on the largest-magnitude entries. Experimental results indicate that diamond sampling is orders of magnitude faster than direct computation and requires far fewer samples than any competing approach. We also apply diamond sampling to the special case of maximum inner product search, and get significantly better results than the state-of-the-art hashing methods. ",diamond sample approximate maximum pair dot product mad search give two set vectors dot dot problem find top dot products largest cdot among possible pair fundamental mathematical problem appear numerous data applications involve similarity search link prediction collaborative filter propose sample base approach avoid direct computation mn dot products select diamonds four cycle weight tripartite representation probability select diamond correspond pair proportional cdot amplify focus largest magnitude entries experimental result indicate diamond sample order magnitude faster direct computation require far fewer sample compete approach also apply diamond sample special case maximum inner product search get significantly better result state art hash methods,102,12,1506.03872.txt
http://arxiv.org/abs/1506.04391,CamFlow: Managed Data-sharing for Cloud Services,"  A model of cloud services is emerging whereby a few trusted providers manage the underlying hardware and communications whereas many companies build on this infrastructure to offer higher level, cloud-hosted PaaS services and/or SaaS applications. From the start, strong isolation between cloud tenants was seen to be of paramount importance, provided first by virtual machines (VM) and later by containers, which share the operating system (OS) kernel. Increasingly it is the case that applications also require facilities to effect isolation and protection of data managed by those applications. They also require flexible data sharing with other applications, often across the traditional cloud-isolation boundaries; for example, when government provides many related services for its citizens on a common platform. Similar considerations apply to the end-users of applications. But in particular, the incorporation of cloud services within `Internet of Things' architectures is driving the requirements for both protection and cross-application data sharing.   These concerns relate to the management of data. Traditional access control is application and principal/role specific, applied at policy enforcement points, after which there is no subsequent control over where data flows; a crucial issue once data has left its owner's control by cloud-hosted applications and within cloud-services. Information Flow Control (IFC), in addition, offers system-wide, end-to-end, flow control based on the properties of the data. We discuss the potential of cloud-deployed IFC for enforcing owners' dataflow policy with regard to protection and sharing, as well as safeguarding against malicious or buggy software. In addition, the audit log associated with IFC provides transparency, giving configurable system-wide visibility over data flows. [...] ","Computer Science - Cryptography and Security ; Computer Science - Distributed, Parallel, and Cluster Computing ; D.4.6 ; ","Pasquier, Thomas F. J. -M. ; Singh, Jatinder ; Eyers, David ; Bacon, Jean ; ","CamFlow: Managed Data-sharing for Cloud Services  A model of cloud services is emerging whereby a few trusted providers manage the underlying hardware and communications whereas many companies build on this infrastructure to offer higher level, cloud-hosted PaaS services and/or SaaS applications. From the start, strong isolation between cloud tenants was seen to be of paramount importance, provided first by virtual machines (VM) and later by containers, which share the operating system (OS) kernel. Increasingly it is the case that applications also require facilities to effect isolation and protection of data managed by those applications. They also require flexible data sharing with other applications, often across the traditional cloud-isolation boundaries; for example, when government provides many related services for its citizens on a common platform. Similar considerations apply to the end-users of applications. But in particular, the incorporation of cloud services within `Internet of Things' architectures is driving the requirements for both protection and cross-application data sharing.   These concerns relate to the management of data. Traditional access control is application and principal/role specific, applied at policy enforcement points, after which there is no subsequent control over where data flows; a crucial issue once data has left its owner's control by cloud-hosted applications and within cloud-services. Information Flow Control (IFC), in addition, offers system-wide, end-to-end, flow control based on the properties of the data. We discuss the potential of cloud-deployed IFC for enforcing owners' dataflow policy with regard to protection and sharing, as well as safeguarding against malicious or buggy software. In addition, the audit log associated with IFC provides transparency, giving configurable system-wide visibility over data flows. [...] ",camflow manage data share cloud service model cloud service emerge whereby trust providers manage underlie hardware communications whereas many company build infrastructure offer higher level cloud host paas service saas applications start strong isolation cloud tenant see paramount importance provide first virtual machine vm later containers share operate system os kernel increasingly case applications also require facilities effect isolation protection data manage applications also require flexible data share applications often across traditional cloud isolation boundaries example government provide many relate service citizens common platform similar considerations apply end users applications particular incorporation cloud service within internet things architectures drive requirements protection cross application data share concern relate management data traditional access control application principal role specific apply policy enforcement point subsequent control data flow crucial issue data leave owner control cloud host applications within cloud service information flow control ifc addition offer system wide end end flow control base properties data discuss potential cloud deploy ifc enforce owners dataflow policy regard protection share well safeguard malicious buggy software addition audit log associate ifc provide transparency give configurable system wide visibility data flow,182,10,1506.04391.txt
http://arxiv.org/abs/1506.04440,Traces of Hecke Operators and Refined Weight Enumerators of Reed-Solomon   Codes,"  We study the quadratic residue weight enumerators of the dual projective Reed-Solomon codes of dimensions $5$ and $q-4$ over the finite field $\mathbb{F}_q$. Our main results are formulas for the coefficients of the the quadratic residue weight enumerators for such codes. If $q=p^v$ and we fix $v$ and vary $p$ then our formulas for the coefficients of the dimension $q-4$ code involve only polynomials in $p$ and the trace of the $q$th and $(q/p^2)$th Hecke operators acting on spaces of cusp forms for the congruence groups $\operatorname{SL}_2 (\mathbb{Z}), \Gamma_0(2)$, and $\Gamma_0(4)$. The main tool we use is the Eichler-Selberg trace formula, which gives along the way a variation of a theorem of Birch on the distribution of rational point counts for elliptic curves with prescribed $2$-torsion over a fixed finite field. ","Mathematics - Number Theory ; Computer Science - Information Theory ; Primary 11T71, Secondary 11F25, 11G20, 94B27 ; ","Kaplan, Nathan ; Petrow, Ian ; ","Traces of Hecke Operators and Refined Weight Enumerators of Reed-Solomon   Codes  We study the quadratic residue weight enumerators of the dual projective Reed-Solomon codes of dimensions $5$ and $q-4$ over the finite field $\mathbb{F}_q$. Our main results are formulas for the coefficients of the the quadratic residue weight enumerators for such codes. If $q=p^v$ and we fix $v$ and vary $p$ then our formulas for the coefficients of the dimension $q-4$ code involve only polynomials in $p$ and the trace of the $q$th and $(q/p^2)$th Hecke operators acting on spaces of cusp forms for the congruence groups $\operatorname{SL}_2 (\mathbb{Z}), \Gamma_0(2)$, and $\Gamma_0(4)$. The main tool we use is the Eichler-Selberg trace formula, which gives along the way a variation of a theorem of Birch on the distribution of rational point counts for elliptic curves with prescribed $2$-torsion over a fixed finite field. ",trace hecke operators refine weight enumerators reed solomon cod study quadratic residue weight enumerators dual projective reed solomon cod dimension finite field mathbb main result formulas coefficients quadratic residue weight enumerators cod fix vary formulas coefficients dimension code involve polynomials trace th th hecke operators act space cusp form congruence group operatorname sl mathbb gamma gamma main tool use eichler selberg trace formula give along way variation theorem birch distribution rational point count elliptic curve prescribe torsion fix finite field,80,5,1506.04440.txt
http://arxiv.org/abs/1506.04496,"The Peano software - parallel, automaton-based, dynamically adaptive   grid traversals","  We discuss the design decisions, design alternatives and rationale behind the third generation of Peano, a framework for dynamically adaptive Cartesian meshes derived from spacetrees. Peano ties the mesh traversal to the mesh storage and supports only one element-wise traversal order resulting from space-filling curves. The user is not free to choose a traversal order herself. The traversal can exploit regular grid subregions and shared memory as well as distributed memory systems with almost no modifications to a serial application code. We formalize the software design by means of two interacting automata---one automaton for the multiscale grid traversal and one for the application-specific algorithmic steps. This yields a callback-based programming paradigm. We further sketch the supported application types and the two data storage schemes realized, before we detail high-performance computing aspects and lessons learned. Special emphasis is put on observations regarding the used programming idioms and algorithmic concepts. This transforms our report from a ""one way to implement things"" code description into a generic discussion and summary of some alternatives, rationale and design decisions to be made for any tree-based adaptive mesh refinement software. ",Computer Science - Mathematical Software ; ,"Weinzierl, Tobias ; ","The Peano software - parallel, automaton-based, dynamically adaptive   grid traversals  We discuss the design decisions, design alternatives and rationale behind the third generation of Peano, a framework for dynamically adaptive Cartesian meshes derived from spacetrees. Peano ties the mesh traversal to the mesh storage and supports only one element-wise traversal order resulting from space-filling curves. The user is not free to choose a traversal order herself. The traversal can exploit regular grid subregions and shared memory as well as distributed memory systems with almost no modifications to a serial application code. We formalize the software design by means of two interacting automata---one automaton for the multiscale grid traversal and one for the application-specific algorithmic steps. This yields a callback-based programming paradigm. We further sketch the supported application types and the two data storage schemes realized, before we detail high-performance computing aspects and lessons learned. Special emphasis is put on observations regarding the used programming idioms and algorithmic concepts. This transforms our report from a ""one way to implement things"" code description into a generic discussion and summary of some alternatives, rationale and design decisions to be made for any tree-based adaptive mesh refinement software. ",peano software parallel automaton base dynamically adaptive grid traversals discuss design decisions design alternatives rationale behind third generation peano framework dynamically adaptive cartesian mesh derive spacetrees peano tie mesh traversal mesh storage support one element wise traversal order result space fill curve user free choose traversal order traversal exploit regular grid subregions share memory well distribute memory systems almost modifications serial application code formalize software design mean two interact automata one automaton multiscale grid traversal one application specific algorithmic step yield callback base program paradigm sketch support application type two data storage scheme realize detail high performance compute aspects lessons learn special emphasis put observations regard use program idioms algorithmic concepts transform report one way implement things code description generic discussion summary alternatives rationale design decisions make tree base adaptive mesh refinement software,133,14,1506.04496.txt
http://arxiv.org/abs/1506.04497,Lower bounds for the dynamically defined measures,"  The dynamically defined measure (DDM) $\Phi$ arising from a finite measure $\phi_0$ on an initial $\sigma$-algebra on a set and an invertible map acting on the latter is considered. Several lower bounds for it are obtained and sufficient conditions for its positivity are deduced under the general assumption that there exists an invariant measure $\Lambda$ such that $\Lambda\ll\phi_0$.   In particular, DDMs arising from the Hellinger integral $\mathcal{J}_\alpha(\Lambda,\phi_0)\geq\mathcal{H}^{\alpha,0}(\Lambda,\phi_0)\geq\mathcal{H}_\alpha(\Lambda,\phi_0)$ are constructed with $\mathcal{H}_{0}\left(\Lambda,\phi_0\right)(Q) = \Phi(Q)$, $\mathcal{H}_{1}\left(\Lambda,\phi_0\right)(Q) = \Lambda(Q)$, and \[\Phi(Q)^{1-\alpha}\Lambda(Q)^{\alpha}\geq\mathcal{J}_{\alpha}\left(\Lambda,\phi_0\right)(Q)\] for all measurable $Q$ and $\alpha\in[0,1]$, and further computable lower bounds for them are obtained and analyzed. It is shown, in particular, that $(0,1)\owns\alpha\longmapsto\mathcal{H}_{\alpha}(\Lambda,\phi_0)(Q)$ is completely determined by the $\Lambda$-essential supremum of $d\Lambda/d\phi_0$ for all $0<\alpha<1$ if $\Lambda$ is ergodic, and if also a condition for the continuity at $0$ is satisfied, the above inequalities become equalities. In general, for every measurable $Q$, it is shown that $[0,1]\owns\alpha\longmapsto\mathcal{J}_{\alpha}(\Lambda,\phi_0)(Q)$ is log-convex, all one-sided derivatives of $(0,1)\owns\alpha\longmapsto\mathcal{H}^{\alpha,0}(\Lambda,\phi_0)(Q)$ and $(0,1)\owns\alpha\longmapsto\mathcal{J}_{\alpha}(\Lambda,\phi_0)(Q)$ are obtained, and some lower bounds for the functions by means of the derivatives are given. Some sufficient conditions for the continuity and a one-sided differentiability of $(0,1)\owns\alpha\longmapsto\mathcal{H}_{\alpha}(\Lambda,\phi_0)(Q)$ are provided. ","Mathematics - Dynamical Systems ; Computer Science - Information Theory ; Mathematical Physics ; 28A99, 37A60, 37A05, 82C05 ; ","Werner, Ivan ; ","Lower bounds for the dynamically defined measures  The dynamically defined measure (DDM) $\Phi$ arising from a finite measure $\phi_0$ on an initial $\sigma$-algebra on a set and an invertible map acting on the latter is considered. Several lower bounds for it are obtained and sufficient conditions for its positivity are deduced under the general assumption that there exists an invariant measure $\Lambda$ such that $\Lambda\ll\phi_0$.   In particular, DDMs arising from the Hellinger integral $\mathcal{J}_\alpha(\Lambda,\phi_0)\geq\mathcal{H}^{\alpha,0}(\Lambda,\phi_0)\geq\mathcal{H}_\alpha(\Lambda,\phi_0)$ are constructed with $\mathcal{H}_{0}\left(\Lambda,\phi_0\right)(Q) = \Phi(Q)$, $\mathcal{H}_{1}\left(\Lambda,\phi_0\right)(Q) = \Lambda(Q)$, and \[\Phi(Q)^{1-\alpha}\Lambda(Q)^{\alpha}\geq\mathcal{J}_{\alpha}\left(\Lambda,\phi_0\right)(Q)\] for all measurable $Q$ and $\alpha\in[0,1]$, and further computable lower bounds for them are obtained and analyzed. It is shown, in particular, that $(0,1)\owns\alpha\longmapsto\mathcal{H}_{\alpha}(\Lambda,\phi_0)(Q)$ is completely determined by the $\Lambda$-essential supremum of $d\Lambda/d\phi_0$ for all $0<\alpha<1$ if $\Lambda$ is ergodic, and if also a condition for the continuity at $0$ is satisfied, the above inequalities become equalities. In general, for every measurable $Q$, it is shown that $[0,1]\owns\alpha\longmapsto\mathcal{J}_{\alpha}(\Lambda,\phi_0)(Q)$ is log-convex, all one-sided derivatives of $(0,1)\owns\alpha\longmapsto\mathcal{H}^{\alpha,0}(\Lambda,\phi_0)(Q)$ and $(0,1)\owns\alpha\longmapsto\mathcal{J}_{\alpha}(\Lambda,\phi_0)(Q)$ are obtained, and some lower bounds for the functions by means of the derivatives are given. Some sufficient conditions for the continuity and a one-sided differentiability of $(0,1)\owns\alpha\longmapsto\mathcal{H}_{\alpha}(\Lambda,\phi_0)(Q)$ are provided. ",lower bound dynamically define measure dynamically define measure ddm phi arise finite measure phi initial sigma algebra set invertible map act latter consider several lower bound obtain sufficient condition positivity deduce general assumption exist invariant measure lambda lambda phi particular ddms arise hellinger integral mathcal alpha lambda phi geq mathcal alpha lambda phi geq mathcal alpha lambda phi construct mathcal leave lambda phi right phi mathcal leave lambda phi right lambda phi alpha lambda alpha geq mathcal alpha leave lambda phi right measurable alpha computable lower bound obtain analyze show particular own alpha longmapsto mathcal alpha lambda phi completely determine lambda essential supremum lambda phi alpha lambda ergodic also condition continuity satisfy inequalities become equalities general every measurable show own alpha longmapsto mathcal alpha lambda phi log convex one side derivatives own alpha longmapsto mathcal alpha lambda phi own alpha longmapsto mathcal alpha lambda phi obtain lower bound function mean derivatives give sufficient condition continuity one side differentiability own alpha longmapsto mathcal alpha lambda phi provide,166,7,1506.04497.txt
http://arxiv.org/abs/1506.04651,Task-based adaptive multiresolution for time-space multi-scale   reaction-diffusion systems on multi-core architectures,"  A new solver featuring time-space adaptation and error control has been recently introduced to tackle the numerical solution of stiff reaction-diffusion systems. Based on operator splitting, finite volume adaptive multiresolution and high order time integrators with specific stability properties for each operator, this strategy yields high computational efficiency for large multidimensional computations on standard architectures such as powerful workstations. However, the data structure of the original implementation, based on trees of pointers, provides limited opportunities for efficiency enhancements, while posing serious challenges in terms of parallel programming and load balancing. The present contribution proposes a new implementation of the whole set of numerical methods including Radau5 and ROCK4, relying on a fully different data structure together with the use of a specific library, TBB, for shared-memory, task-based parallelism with work-stealing. The performance of our implementation is assessed in a series of test-cases of increasing difficulty in two and three dimensions on multi-core and many-core architectures, demonstrating high scalability. ","Computer Science - Numerical Analysis ; Computer Science - Distributed, Parallel, and Cluster Computing ; Mathematics - Analysis of PDEs ; Mathematics - Numerical Analysis ; ","Descombes, Stéphane ; Duarte, Max ; Dumont, Thierry ; Guillet, Thomas ; Louvet, Violaine ; Massot, Marc ; ","Task-based adaptive multiresolution for time-space multi-scale   reaction-diffusion systems on multi-core architectures  A new solver featuring time-space adaptation and error control has been recently introduced to tackle the numerical solution of stiff reaction-diffusion systems. Based on operator splitting, finite volume adaptive multiresolution and high order time integrators with specific stability properties for each operator, this strategy yields high computational efficiency for large multidimensional computations on standard architectures such as powerful workstations. However, the data structure of the original implementation, based on trees of pointers, provides limited opportunities for efficiency enhancements, while posing serious challenges in terms of parallel programming and load balancing. The present contribution proposes a new implementation of the whole set of numerical methods including Radau5 and ROCK4, relying on a fully different data structure together with the use of a specific library, TBB, for shared-memory, task-based parallelism with work-stealing. The performance of our implementation is assessed in a series of test-cases of increasing difficulty in two and three dimensions on multi-core and many-core architectures, demonstrating high scalability. ",task base adaptive multiresolution time space multi scale reaction diffusion systems multi core architectures new solver feature time space adaptation error control recently introduce tackle numerical solution stiff reaction diffusion systems base operator split finite volume adaptive multiresolution high order time integrators specific stability properties operator strategy yield high computational efficiency large multidimensional computations standard architectures powerful workstations however data structure original implementation base tree pointers provide limit opportunities efficiency enhancements pose serious challenge term parallel program load balance present contribution propose new implementation whole set numerical methods include radau rock rely fully different data structure together use specific library tbb share memory task base parallelism work steal performance implementation assess series test case increase difficulty two three dimension multi core many core architectures demonstrate high scalability,127,11,1506.04651.txt
http://arxiv.org/abs/1506.04773,DistFlow Extensions for AC Transmission Systems,"  Convex relaxations of the power flow equations and, in particular, the Semi-Definite Programming (SDP), Second-Order Cone (SOC), and Convex DistFlow (CDF) relaxations, have attracted significant interest in recent years. Thus far, studies of the CDF model and its connection to the other relaxations have been limited to power distribution systems, which omit several parameters necessary for modeling transmission systems. To increase the applicability of the CDF relaxation, this paper develops an extended CDF model that is suitable for transmission systems by incorporating bus shunts, line charging, and transformers. Additionally, a theoretical result shows that the established equivalence of the SOC and CDF models for distribution systems also holds in this transmission system extension. ",Mathematics - Optimization and Control ; Computer Science - Systems and Control ; ,"Coffrin, Carleton ; Hijazi, Hassan L. ; Van Hentenryck, Pascal ; ","DistFlow Extensions for AC Transmission Systems  Convex relaxations of the power flow equations and, in particular, the Semi-Definite Programming (SDP), Second-Order Cone (SOC), and Convex DistFlow (CDF) relaxations, have attracted significant interest in recent years. Thus far, studies of the CDF model and its connection to the other relaxations have been limited to power distribution systems, which omit several parameters necessary for modeling transmission systems. To increase the applicability of the CDF relaxation, this paper develops an extended CDF model that is suitable for transmission systems by incorporating bus shunts, line charging, and transformers. Additionally, a theoretical result shows that the established equivalence of the SOC and CDF models for distribution systems also holds in this transmission system extension. ",distflow extensions ac transmission systems convex relaxations power flow equations particular semi definite program sdp second order cone soc convex distflow cdf relaxations attract significant interest recent years thus far study cdf model connection relaxations limit power distribution systems omit several parameters necessary model transmission systems increase applicability cdf relaxation paper develop extend cdf model suitable transmission systems incorporate bus shunt line charge transformers additionally theoretical result show establish equivalence soc cdf model distribution systems also hold transmission system extension,80,7,1506.04773.txt
http://arxiv.org/abs/1506.04972,A Unified Successive Pseudo-Convex Approximation Framework,"  In this paper, we propose a successive pseudo-convex approximation algorithm to efficiently compute stationary points for a large class of possibly nonconvex optimization problems. The stationary points are obtained by solving a sequence of successively refined approximate problems, each of which is much easier to solve than the original problem. To achieve convergence, the approximate problem only needs to exhibit a weak form of convexity, namely, pseudo-convexity. We show that the proposed framework not only includes as special cases a number of existing methods, for example, the gradient method and the Jacobi algorithm, but also leads to new algorithms which enjoy easier implementation and faster convergence speed. We also propose a novel line search method for nondifferentiable optimization problems, which is carried out over a properly constructed differentiable function with the benefit of a simplified implementation as compared to state-of-the-art line search techniques that directly operate on the original nondifferentiable objective function. The advantages of the proposed algorithm are shown, both theoretically and numerically, by several example applications, namely, MIMO broadcast channel capacity computation, energy efficiency maximization in massive MIMO systems and LASSO in sparse signal recovery. ",Mathematics - Optimization and Control ; Computer Science - Numerical Analysis ; ,"Yang, Yang ; Pesavento, Marius ; ","A Unified Successive Pseudo-Convex Approximation Framework  In this paper, we propose a successive pseudo-convex approximation algorithm to efficiently compute stationary points for a large class of possibly nonconvex optimization problems. The stationary points are obtained by solving a sequence of successively refined approximate problems, each of which is much easier to solve than the original problem. To achieve convergence, the approximate problem only needs to exhibit a weak form of convexity, namely, pseudo-convexity. We show that the proposed framework not only includes as special cases a number of existing methods, for example, the gradient method and the Jacobi algorithm, but also leads to new algorithms which enjoy easier implementation and faster convergence speed. We also propose a novel line search method for nondifferentiable optimization problems, which is carried out over a properly constructed differentiable function with the benefit of a simplified implementation as compared to state-of-the-art line search techniques that directly operate on the original nondifferentiable objective function. The advantages of the proposed algorithm are shown, both theoretically and numerically, by several example applications, namely, MIMO broadcast channel capacity computation, energy efficiency maximization in massive MIMO systems and LASSO in sparse signal recovery. ",unify successive pseudo convex approximation framework paper propose successive pseudo convex approximation algorithm efficiently compute stationary point large class possibly nonconvex optimization problems stationary point obtain solve sequence successively refine approximate problems much easier solve original problem achieve convergence approximate problem need exhibit weak form convexity namely pseudo convexity show propose framework include special case number exist methods example gradient method jacobi algorithm also lead new algorithms enjoy easier implementation faster convergence speed also propose novel line search method nondifferentiable optimization problems carry properly construct differentiable function benefit simplify implementation compare state art line search techniques directly operate original nondifferentiable objective function advantage propose algorithm show theoretically numerically several example applications namely mimo broadcast channel capacity computation energy efficiency maximization massive mimo systems lasso sparse signal recovery,127,9,1506.04972.txt
http://arxiv.org/abs/1506.05193,"Anxiety, Alcohol, and Academics: A Textual Analysis of Student Facebook   Confessions Pages","  What do college students reveal to their peers on social media under complete anonymity? Do their campus environments relate to the topics of their disclosure? To answer these questions, I analyze Facebook confessions pages. Popular on hundreds of college campuses, these pages allow students to anonymously post personal confessions on a public community forum. In this preliminary research note, I analyze several explanatory factors of online student confessional behavior. Aggregating nearly 200,000 confessions posts spanning a period of 3 years, I combine Latent Dirichlet Allocation (LDA) with human verification through Mechanical Turk to scalably identify topics in these online confessions. Where possible, I also link posts to real-world news events parsed from Twitter. I find that confessions mentioning socioeconomics as well as mental and physical health occur more often at top-ranking, expensive private colleges. While event-related confessions most often mention timely school-related events, many mention global and domestic events outside of the local campus sphere. Results suggest that undergraduates from different campuses disclose about topics such as race, socioeonomics, and politics differently, but in aggregate, post in similar patterns over time. Additionally, results confirm that anonymous Facebook confessors receive support for confessions on important, but taboo topics such as health and socioeconomic status. ",Computer Science - Social and Information Networks ; Computer Science - Computers and Society ; ,"Barari, Soubhik ; ","Anxiety, Alcohol, and Academics: A Textual Analysis of Student Facebook   Confessions Pages  What do college students reveal to their peers on social media under complete anonymity? Do their campus environments relate to the topics of their disclosure? To answer these questions, I analyze Facebook confessions pages. Popular on hundreds of college campuses, these pages allow students to anonymously post personal confessions on a public community forum. In this preliminary research note, I analyze several explanatory factors of online student confessional behavior. Aggregating nearly 200,000 confessions posts spanning a period of 3 years, I combine Latent Dirichlet Allocation (LDA) with human verification through Mechanical Turk to scalably identify topics in these online confessions. Where possible, I also link posts to real-world news events parsed from Twitter. I find that confessions mentioning socioeconomics as well as mental and physical health occur more often at top-ranking, expensive private colleges. While event-related confessions most often mention timely school-related events, many mention global and domestic events outside of the local campus sphere. Results suggest that undergraduates from different campuses disclose about topics such as race, socioeonomics, and politics differently, but in aggregate, post in similar patterns over time. Additionally, results confirm that anonymous Facebook confessors receive support for confessions on important, but taboo topics such as health and socioeconomic status. ",anxiety alcohol academics textual analysis student facebook confessions page college students reveal peer social media complete anonymity campus environments relate topics disclosure answer question analyze facebook confessions page popular hundreds college campuses page allow students anonymously post personal confessions public community forum preliminary research note analyze several explanatory factor online student confessional behavior aggregate nearly confessions post span period years combine latent dirichlet allocation lda human verification mechanical turk scalably identify topics online confessions possible also link post real world news events parse twitter find confessions mention socioeconomics well mental physical health occur often top rank expensive private colleges event relate confessions often mention timely school relate events many mention global domestic events outside local campus sphere result suggest undergraduates different campuses disclose topics race socioeonomics politics differently aggregate post similar pattern time additionally result confirm anonymous facebook confessors receive support confessions important taboo topics health socioeconomic status,148,11,1506.05193.txt
http://arxiv.org/abs/1506.05231,The Fractality of Polar and Reed-Muller Codes,"  The generator matrices of polar codes and Reed-Muller codes are obtained by selecting rows from the Kronecker product of a lower-triangular binary square matrix. For polar codes, the selection is based on the Bhattacharyya parameter of the row, which is closely related to the error probability of the corresponding input bit under sequential decoding. For Reed-Muller codes, the selection is based on the Hamming weight of the row. This work investigates the properties of the index sets pointing to those rows in the infinite blocklength limit. In particular, the Lebesgue measure, the Hausdorff dimension, and the self-similarity of these sets will be discussed. It is shown that these index sets have several properties that are common to fractals. ",Computer Science - Information Theory ; ,"Geiger, Bernhard C. ; ","The Fractality of Polar and Reed-Muller Codes  The generator matrices of polar codes and Reed-Muller codes are obtained by selecting rows from the Kronecker product of a lower-triangular binary square matrix. For polar codes, the selection is based on the Bhattacharyya parameter of the row, which is closely related to the error probability of the corresponding input bit under sequential decoding. For Reed-Muller codes, the selection is based on the Hamming weight of the row. This work investigates the properties of the index sets pointing to those rows in the infinite blocklength limit. In particular, the Lebesgue measure, the Hausdorff dimension, and the self-similarity of these sets will be discussed. It is shown that these index sets have several properties that are common to fractals. ",fractality polar reed muller cod generator matrices polar cod reed muller cod obtain select row kronecker product lower triangular binary square matrix polar cod selection base bhattacharyya parameter row closely relate error probability correspond input bite sequential decode reed muller cod selection base ham weight row work investigate properties index set point row infinite blocklength limit particular lebesgue measure hausdorff dimension self similarity set discuss show index set several properties common fractals,72,5,1506.05231.txt
http://arxiv.org/abs/1506.05855,Information-based inference for singular models and finite sample sizes:   A frequentist information criterion,"  In the information-based paradigm of inference, model selection is performed by selecting the candidate model with the best estimated predictive performance. The success of this approach depends on the accuracy of the estimate of the predictive complexity. In the large-sample-size limit of a regular model, the predictive performance is well estimated by the Akaike Information Criterion (AIC). However, this approximation can either significantly under or over-estimating the complexity in a wide range of important applications where models are either non-regular or finite-sample-size corrections are significant. We introduce an improved approximation for the complexity that is used to define a new information criterion: the Frequentist Information Criterion (QIC). QIC extends the applicability of information-based inference to the finite-sample-size regime of regular models and to singular models. We demonstrate the power and the comparative advantage of QIC in a number of example analyses. ","Statistics - Machine Learning ; Computer Science - Machine Learning ; Physics - Data Analysis, Statistics and Probability ; ","LaMont, Colin H. ; Wiggins, Paul A. ; ","Information-based inference for singular models and finite sample sizes:   A frequentist information criterion  In the information-based paradigm of inference, model selection is performed by selecting the candidate model with the best estimated predictive performance. The success of this approach depends on the accuracy of the estimate of the predictive complexity. In the large-sample-size limit of a regular model, the predictive performance is well estimated by the Akaike Information Criterion (AIC). However, this approximation can either significantly under or over-estimating the complexity in a wide range of important applications where models are either non-regular or finite-sample-size corrections are significant. We introduce an improved approximation for the complexity that is used to define a new information criterion: the Frequentist Information Criterion (QIC). QIC extends the applicability of information-based inference to the finite-sample-size regime of regular models and to singular models. We demonstrate the power and the comparative advantage of QIC in a number of example analyses. ",information base inference singular model finite sample size frequentist information criterion information base paradigm inference model selection perform select candidate model best estimate predictive performance success approach depend accuracy estimate predictive complexity large sample size limit regular model predictive performance well estimate akaike information criterion aic however approximation either significantly estimate complexity wide range important applications model either non regular finite sample size corrections significant introduce improve approximation complexity use define new information criterion frequentist information criterion qic qic extend applicability information base inference finite sample size regime regular model singular model demonstrate power comparative advantage qic number example analyse,100,12,1506.05855.txt
http://arxiv.org/abs/1506.06011,A Markovian Analysis of IEEE 802.11 Broadcast Transmission Networks with   Buffering,"  The purpose of this paper is to analyze the so-called back-off technique of the IEEE 802.11 protocol in broadcast mode with waiting queues. In contrast to existing models, packets arriving when a station (or node) is in back-off state are not discarded, but are stored in a buffer of infinite capacity. As in previous studies, the key point of our analysis hinges on the assumption that the time on the channel is viewed as a random succession of transmission slots (whose duration corresponds to the length of a packet) and mini-slots during which the back-o? of the station is decremented. These events occur independently, with given probabilities. The state of a node is represented by a two-dimensional Markov chain in discrete-time, formed by the back-off counter and the number of packets at the station. Two models are proposed both of which are shown to cope reasonably well with the physical principles of the protocol. The stabillity (ergodicity) conditions are obtained and interpreted in terms of maximum throughput. Several approximations related to these models are also discussed. ","Computer Science - Performance ; Mathematics - Probability ; Primary 60J10, secondary 30D05, 30E99 ; ","Fayolle, Guy ; Muhlethaler, Paul ; ","A Markovian Analysis of IEEE 802.11 Broadcast Transmission Networks with   Buffering  The purpose of this paper is to analyze the so-called back-off technique of the IEEE 802.11 protocol in broadcast mode with waiting queues. In contrast to existing models, packets arriving when a station (or node) is in back-off state are not discarded, but are stored in a buffer of infinite capacity. As in previous studies, the key point of our analysis hinges on the assumption that the time on the channel is viewed as a random succession of transmission slots (whose duration corresponds to the length of a packet) and mini-slots during which the back-o? of the station is decremented. These events occur independently, with given probabilities. The state of a node is represented by a two-dimensional Markov chain in discrete-time, formed by the back-off counter and the number of packets at the station. Two models are proposed both of which are shown to cope reasonably well with the physical principles of the protocol. The stabillity (ergodicity) conditions are obtained and interpreted in terms of maximum throughput. Several approximations related to these models are also discussed. ",markovian analysis ieee broadcast transmission network buffer purpose paper analyze call back technique ieee protocol broadcast mode wait queue contrast exist model packets arrive station node back state discard store buffer infinite capacity previous study key point analysis hinge assumption time channel view random succession transmission slot whose duration correspond length packet mini slot back station decremented events occur independently give probabilities state node represent two dimensional markov chain discrete time form back counter number packets station two model propose show cope reasonably well physical principles protocol stabillity ergodicity condition obtain interpret term maximum throughput several approximations relate model also discuss,101,11,1506.06011.txt
http://arxiv.org/abs/1506.06055,Low PMEPR OFDM radar waveform design using the iterative least squares   algorithm,"  This letter considers waveform design of orthogonal frequency division multiplexing (OFDM) signal for radar applications, and aims at mitigating the envelope fluctuation in OFDM. A novel method is proposed to reduce the peak-to-mean envelope power ratio (PMEPR), which is commonly used to evaluate the fluctuation. The proposed method is based on the tone reservation approach, in which some bits or subcarriers of OFDM are allocated for decreasing PMEPR. We introduce the coefficient of variation of envelopes (CVE) as the cost function for waveform optimization, and develop an iterative least squares algorithm. Minimizing CVE leads to distinct PMEPR reduction, and it is guaranteed that the cost function monotonically decreases by applying the iterative algorithm. Simulations demonstrate that the envelope is significantly smoothed by the proposed method. ",Electrical Engineering and Systems Science - Signal Processing ; Computer Science - Information Theory ; ,"Huang, Tianyao ; Zhao, Tong ; ","Low PMEPR OFDM radar waveform design using the iterative least squares   algorithm  This letter considers waveform design of orthogonal frequency division multiplexing (OFDM) signal for radar applications, and aims at mitigating the envelope fluctuation in OFDM. A novel method is proposed to reduce the peak-to-mean envelope power ratio (PMEPR), which is commonly used to evaluate the fluctuation. The proposed method is based on the tone reservation approach, in which some bits or subcarriers of OFDM are allocated for decreasing PMEPR. We introduce the coefficient of variation of envelopes (CVE) as the cost function for waveform optimization, and develop an iterative least squares algorithm. Minimizing CVE leads to distinct PMEPR reduction, and it is guaranteed that the cost function monotonically decreases by applying the iterative algorithm. Simulations demonstrate that the envelope is significantly smoothed by the proposed method. ",low pmepr ofdm radar waveform design use iterative least square algorithm letter consider waveform design orthogonal frequency division multiplexing ofdm signal radar applications aim mitigate envelope fluctuation ofdm novel method propose reduce peak mean envelope power ratio pmepr commonly use evaluate fluctuation propose method base tone reservation approach bits subcarriers ofdm allocate decrease pmepr introduce coefficient variation envelop cve cost function waveform optimization develop iterative least square algorithm minimize cve lead distinct pmepr reduction guarantee cost function monotonically decrease apply iterative algorithm simulations demonstrate envelope significantly smooth propose method,89,4,1506.06055.txt
http://arxiv.org/abs/1506.06138,The evolution of lossy compression,"  In complex environments, there are costs to both ignorance and perception. An organism needs to track fitness-relevant information about its world, but the more information it tracks, the more resources it must devote to memory and processing. Rate-distortion theory shows that, when errors are allowed, remarkably efficient internal representations can be found by biologically-plausible hill-climbing mechanisms. We identify two regimes: a high-fidelity regime where perceptual costs scale logarithmically with environmental complexity, and a low-fidelity regime where perceptual costs are, remarkably, independent of the environment. When environmental complexity is rising, Darwinian evolution should drive organisms to the threshold between the high- and low-fidelity regimes. Organisms that code efficiently will find themselves able to make, just barely, the most subtle distinctions in their environment. ",Quantitative Biology - Neurons and Cognition ; Computer Science - Information Theory ; Nonlinear Sciences - Adaptation and Self-Organizing Systems ; Physics - Physics and Society ; Quantitative Biology - Populations and Evolution ; ,"Marzen, Sarah E. ; DeDeo, Simon ; ","The evolution of lossy compression  In complex environments, there are costs to both ignorance and perception. An organism needs to track fitness-relevant information about its world, but the more information it tracks, the more resources it must devote to memory and processing. Rate-distortion theory shows that, when errors are allowed, remarkably efficient internal representations can be found by biologically-plausible hill-climbing mechanisms. We identify two regimes: a high-fidelity regime where perceptual costs scale logarithmically with environmental complexity, and a low-fidelity regime where perceptual costs are, remarkably, independent of the environment. When environmental complexity is rising, Darwinian evolution should drive organisms to the threshold between the high- and low-fidelity regimes. Organisms that code efficiently will find themselves able to make, just barely, the most subtle distinctions in their environment. ",evolution lossy compression complex environments cost ignorance perception organism need track fitness relevant information world information track resources must devote memory process rate distortion theory show errors allow remarkably efficient internal representations find biologically plausible hill climb mechanisms identify two regimes high fidelity regime perceptual cost scale logarithmically environmental complexity low fidelity regime perceptual cost remarkably independent environment environmental complexity rise darwinian evolution drive organisms threshold high low fidelity regimes organisms code efficiently find able make barely subtle distinctions environment,80,9,1506.06138.txt
http://arxiv.org/abs/1506.06305,"Social media affects the timing, location, and severity of school   shootings","  Over the past two decades, school shootings within the United States have repeatedly devastated communities and shaken public opinion. Many of these attacks appear to be `lone wolf' ones driven by specific individual motivations, and the identification of precursor signals and hence actionable policy measures would thus seem highly unlikely. Here, we take a system-wide view and investigate the timing of school attacks and the dynamical feedback with social media. We identify a trend divergence in which college attacks have continued to accelerate over the last 25 years while those carried out on K-12 schools have slowed down. We establish the copycat effect in school shootings and uncover a statistical association between social media chatter and the probability of an attack in the following days. While hinting at causality, this relationship may also help mitigate the frequency and intensity of future attacks. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Garcia-Bernardo, J. ; Qi, H. ; Shultz, J. M. ; Cohen, A. M. ; Johnson, N. F. ; Dodds, P. S. ; ","Social media affects the timing, location, and severity of school   shootings  Over the past two decades, school shootings within the United States have repeatedly devastated communities and shaken public opinion. Many of these attacks appear to be `lone wolf' ones driven by specific individual motivations, and the identification of precursor signals and hence actionable policy measures would thus seem highly unlikely. Here, we take a system-wide view and investigate the timing of school attacks and the dynamical feedback with social media. We identify a trend divergence in which college attacks have continued to accelerate over the last 25 years while those carried out on K-12 schools have slowed down. We establish the copycat effect in school shootings and uncover a statistical association between social media chatter and the probability of an attack in the following days. While hinting at causality, this relationship may also help mitigate the frequency and intensity of future attacks. ",social media affect time location severity school shoot past two decades school shoot within unite state repeatedly devastate communities shake public opinion many attack appear lone wolf ones drive specific individual motivations identification precursor signal hence actionable policy measure would thus seem highly unlikely take system wide view investigate time school attack dynamical feedback social media identify trend divergence college attack continue accelerate last years carry school slow establish copycat effect school shoot uncover statistical association social media chatter probability attack follow days hint causality relationship may also help mitigate frequency intensity future attack,94,10,1506.06305.txt
http://arxiv.org/abs/1506.07094,pyMOR - Generic Algorithms and Interfaces for Model Order Reduction,"  Reduced basis methods are projection-based model order reduction techniques for reducing the computational complexity of solving parametrized partial differential equation problems. In this work we discuss the design of pyMOR, a freely available software library of model order reduction algorithms, in particular reduced basis methods, implemented with the Python programming language. As its main design feature, all reduction algorithms in pyMOR are implemented generically via operations on well-defined vector array, operator and discretization interface classes. This allows for an easy integration with existing open-source high-performance partial differential equation solvers without adding any model reduction specific code to these solvers. Besides an in-depth discussion of pyMOR's design philosophy and architecture, we present several benchmark results and numerical examples showing the feasibility of our approach. ","Computer Science - Mathematical Software ; Mathematics - Numerical Analysis ; 35-04, 35J20, 35L03, 65-04, 65N30, 65Y05, 68N01 ; ","Milk, René ; Rave, Stephan ; Schindler, Felix ; ","pyMOR - Generic Algorithms and Interfaces for Model Order Reduction  Reduced basis methods are projection-based model order reduction techniques for reducing the computational complexity of solving parametrized partial differential equation problems. In this work we discuss the design of pyMOR, a freely available software library of model order reduction algorithms, in particular reduced basis methods, implemented with the Python programming language. As its main design feature, all reduction algorithms in pyMOR are implemented generically via operations on well-defined vector array, operator and discretization interface classes. This allows for an easy integration with existing open-source high-performance partial differential equation solvers without adding any model reduction specific code to these solvers. Besides an in-depth discussion of pyMOR's design philosophy and architecture, we present several benchmark results and numerical examples showing the feasibility of our approach. ",pymor generic algorithms interfaces model order reduction reduce basis methods projection base model order reduction techniques reduce computational complexity solve parametrized partial differential equation problems work discuss design pymor freely available software library model order reduction algorithms particular reduce basis methods implement python program language main design feature reduction algorithms pymor implement generically via operations well define vector array operator discretization interface class allow easy integration exist open source high performance partial differential equation solvers without add model reduction specific code solvers besides depth discussion pymor design philosophy architecture present several benchmark result numerical examples show feasibility approach,98,11,1506.07094.txt
http://arxiv.org/abs/1506.07212,Elicitation Complexity of Statistical Properties,"  A property, or statistical functional, is said to be elicitable if it minimizes expected loss for some loss function. The study of which properties are elicitable sheds light on the capabilities and limits of empirical risk minimization. While several recent papers have asked which properties are elicitable, we instead advocate for a more nuanced question: how many dimensions are required to indirectly elicit a given property? This number is called the elicitation complexity of the property. We lay the foundation for a general theory of elicitation complexity, including several basic results about how elicitation complexity behaves, and the complexity of standard properties of interest. Building on this foundation, we establish several upper and lower bounds for the broad class of Bayes risks. We apply these results by proving tight complexity bounds, with respect to identifiable properties, for variance, financial risk measures, entropy, norms, and new properties of interest. We then show how some of these bounds can extend to other practical classes of properties, and conclude with a discussion of open directions. ",Computer Science - Machine Learning ; Mathematics - Optimization and Control ; Mathematics - Statistics Theory ; Quantitative Finance - Mathematical Finance ; ,"Frongillo, Rafael ; Kash, Ian A. ; ","Elicitation Complexity of Statistical Properties  A property, or statistical functional, is said to be elicitable if it minimizes expected loss for some loss function. The study of which properties are elicitable sheds light on the capabilities and limits of empirical risk minimization. While several recent papers have asked which properties are elicitable, we instead advocate for a more nuanced question: how many dimensions are required to indirectly elicit a given property? This number is called the elicitation complexity of the property. We lay the foundation for a general theory of elicitation complexity, including several basic results about how elicitation complexity behaves, and the complexity of standard properties of interest. Building on this foundation, we establish several upper and lower bounds for the broad class of Bayes risks. We apply these results by proving tight complexity bounds, with respect to identifiable properties, for variance, financial risk measures, entropy, norms, and new properties of interest. We then show how some of these bounds can extend to other practical classes of properties, and conclude with a discussion of open directions. ",elicitation complexity statistical properties property statistical functional say elicitable minimize expect loss loss function study properties elicitable shed light capabilities limit empirical risk minimization several recent paper ask properties elicitable instead advocate nuanced question many dimension require indirectly elicit give property number call elicitation complexity property lay foundation general theory elicitation complexity include several basic result elicitation complexity behave complexity standard properties interest build foundation establish several upper lower bound broad class bay risk apply result prove tight complexity bound respect identifiable properties variance financial risk measure entropy norms new properties interest show bound extend practical class properties conclude discussion open directions,102,8,1506.07212.txt
http://arxiv.org/abs/1506.07437,The Truncated & Supplemented Pascal Matrix and Applications,"  In this paper, we introduce the $k\times n$ (with $k\leq n$) truncated, supplemented Pascal matrix which has the property that any $k$ columns form a linearly independent set. This property is also present in Reed-Solomon codes; however, Reed-Solomon codes are completely dense, whereas the truncated, supplemented Pascal matrix has multiple zeros. If the maximal-distance separable code conjecture is correct, then our matrix has the maximal number of columns (with the aformentioned property) that the conjecture allows. This matrix has applications in coding, network coding, and matroid theory. ","Mathematics - Combinatorics ; Computer Science - Information Theory ; 05B20, 05B35 ; ","Hua, M. ; Damelin, S. B. ; Sun, J. ; Yu, M. ; ","The Truncated & Supplemented Pascal Matrix and Applications  In this paper, we introduce the $k\times n$ (with $k\leq n$) truncated, supplemented Pascal matrix which has the property that any $k$ columns form a linearly independent set. This property is also present in Reed-Solomon codes; however, Reed-Solomon codes are completely dense, whereas the truncated, supplemented Pascal matrix has multiple zeros. If the maximal-distance separable code conjecture is correct, then our matrix has the maximal number of columns (with the aformentioned property) that the conjecture allows. This matrix has applications in coding, network coding, and matroid theory. ",truncate supplement pascal matrix applications paper introduce time leq truncate supplement pascal matrix property columns form linearly independent set property also present reed solomon cod however reed solomon cod completely dense whereas truncate supplement pascal matrix multiple zero maximal distance separable code conjecture correct matrix maximal number columns aformentioned property conjecture allow matrix applications cod network cod matroid theory,59,5,1506.07437.txt
http://arxiv.org/abs/1506.07990,"Bisimulation and expressivity for conditional belief, degrees of belief,   and safe belief","  Plausibility models are Kripke models that agents use to reason about knowledge and belief, both of themselves and of each other. Such models are used to interpret the notions of conditional belief, degrees of belief, and safe belief. The logic of conditional belief contains that modality and also the knowledge modality, and similarly for the logic of degrees of belief and the logic of safe belief. With respect to these logics, plausibility models may contain too much information. A proper notion of bisimulation is required that characterises them. We define that notion of bisimulation and prove the required characterisations: on the class of image-finite and preimage-finite models (with respect to the plausibility relation), two pointed Kripke models are modally equivalent in either of the three logics, if and only if they are bisimilar. As a result, the information content of such a model can be similarly expressed in the logic of conditional belief, or the logic of degrees of belief, or that of safe belief. This, we found a surprising result. Still, that does not mean that the logics are equally expressive: the logics of conditional and degrees of belief are incomparable, the logics of degrees of belief and safe belief are incomparable, while the logic of safe belief is more expressive than the logic of conditional belief. In view of the result on bisimulation characterisation, this is an equally surprising result. We hope our insights may contribute to the growing community of formal epistemology and on the relation between qualitative and quantitative modelling. ",Computer Science - Artificial Intelligence ; Computer Science - Logic in Computer Science ; ,"Andersen, Mikkel Birkegaard ; Bolander, Thomas ; van Ditmarsch, Hans ; Jensen, Martin Holm ; ","Bisimulation and expressivity for conditional belief, degrees of belief,   and safe belief  Plausibility models are Kripke models that agents use to reason about knowledge and belief, both of themselves and of each other. Such models are used to interpret the notions of conditional belief, degrees of belief, and safe belief. The logic of conditional belief contains that modality and also the knowledge modality, and similarly for the logic of degrees of belief and the logic of safe belief. With respect to these logics, plausibility models may contain too much information. A proper notion of bisimulation is required that characterises them. We define that notion of bisimulation and prove the required characterisations: on the class of image-finite and preimage-finite models (with respect to the plausibility relation), two pointed Kripke models are modally equivalent in either of the three logics, if and only if they are bisimilar. As a result, the information content of such a model can be similarly expressed in the logic of conditional belief, or the logic of degrees of belief, or that of safe belief. This, we found a surprising result. Still, that does not mean that the logics are equally expressive: the logics of conditional and degrees of belief are incomparable, the logics of degrees of belief and safe belief are incomparable, while the logic of safe belief is more expressive than the logic of conditional belief. In view of the result on bisimulation characterisation, this is an equally surprising result. We hope our insights may contribute to the growing community of formal epistemology and on the relation between qualitative and quantitative modelling. ",bisimulation expressivity conditional belief degrees belief safe belief plausibility model kripke model agents use reason knowledge belief model use interpret notions conditional belief degrees belief safe belief logic conditional belief contain modality also knowledge modality similarly logic degrees belief logic safe belief respect logics plausibility model may contain much information proper notion bisimulation require characterise define notion bisimulation prove require characterisations class image finite preimage finite model respect plausibility relation two point kripke model modally equivalent either three logics bisimilar result information content model similarly express logic conditional belief logic degrees belief safe belief find surprise result still mean logics equally expressive logics conditional degrees belief incomparable logics degrees belief safe belief incomparable logic safe belief expressive logic conditional belief view result bisimulation characterisation equally surprise result hope insights may contribute grow community formal epistemology relation qualitative quantitative model,139,8,1506.07990.txt
http://arxiv.org/abs/1506.08009,Skopus: Mining top-k sequential patterns under leverage,"  This paper presents a framework for exact discovery of the top-k sequential patterns under Leverage. It combines (1) a novel definition of the expected support for a sequential pattern - a concept on which most interestingness measures directly rely - with (2) SkOPUS: a new branch-and-bound algorithm for the exact discovery of top-k sequential patterns under a given measure of interest. Our interestingness measure employs the partition approach. A pattern is interesting to the extent that it is more frequent than can be explained by assuming independence between any of the pairs of patterns from which it can be composed. The larger the support compared to the expectation under independence, the more interesting is the pattern. We build on these two elements to exactly extract the k sequential patterns with highest leverage, consistent with our definition of expected support. We conduct experiments on both synthetic data with known patterns and real-world datasets; both experiments confirm the consistency and relevance of our approach with regard to the state of the art. This article was published in Data Mining and Knowledge Discovery and is accessible at http://dx.doi.org/10.1007/s10618-016-0467-9. ",Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Petitjean, Francois ; Li, Tao ; Tatti, Nikolaj ; Webb, Geoffrey I. ; ","Skopus: Mining top-k sequential patterns under leverage  This paper presents a framework for exact discovery of the top-k sequential patterns under Leverage. It combines (1) a novel definition of the expected support for a sequential pattern - a concept on which most interestingness measures directly rely - with (2) SkOPUS: a new branch-and-bound algorithm for the exact discovery of top-k sequential patterns under a given measure of interest. Our interestingness measure employs the partition approach. A pattern is interesting to the extent that it is more frequent than can be explained by assuming independence between any of the pairs of patterns from which it can be composed. The larger the support compared to the expectation under independence, the more interesting is the pattern. We build on these two elements to exactly extract the k sequential patterns with highest leverage, consistent with our definition of expected support. We conduct experiments on both synthetic data with known patterns and real-world datasets; both experiments confirm the consistency and relevance of our approach with regard to the state of the art. This article was published in Data Mining and Knowledge Discovery and is accessible at http://dx.doi.org/10.1007/s10618-016-0467-9. ",skopus mine top sequential pattern leverage paper present framework exact discovery top sequential pattern leverage combine novel definition expect support sequential pattern concept interestingness measure directly rely skopus new branch bind algorithm exact discovery top sequential pattern give measure interest interestingness measure employ partition approach pattern interest extent frequent explain assume independence pair pattern compose larger support compare expectation independence interest pattern build two elements exactly extract sequential pattern highest leverage consistent definition expect support conduct experiment synthetic data know pattern real world datasets experiment confirm consistency relevance approach regard state art article publish data mine knowledge discovery accessible http dx doi org,103,10,1506.08009.txt
http://arxiv.org/abs/1506.08231,"A zero-sum monetary system, interest rates, and implications","  To the knowledge of the author, this is the first time it has been shown that interest rates that are extremely high by modern standards (100% and higher) are necessary within a zero-sum monetary system, and not just driven by greed. Extreme interest rates that appeared in various places and times reinforce the idea that hard money may have contributed to high rates of interest. Here a model is presented that examines the interest rate required to succeed as an investor in a zero-sum fixed quantity hard-money system. Even when the playing field is significantly tilted toward the investor, interest rates need to be much higher than expected. In a completely fair zero-sum system, an investor cannot break even without charging 100% interest. Even with a 5% advantage, an investor won't break even at 15% interest. From this it is concluded that what we consider usurious rates today are, within a hard-money system, driven by necessity.   Cryptocurrency is a novel form of hard-currency. The inability to virtualize the money creates a system close to zero-sum because of the limited supply design. Therefore, within the bounds of a cryptocurrency system that limits money creation, interest rates must rise to levels that the modern world considers usury. It is impossible, therefore, that a cryptocurrency that is not expandable could take over a modern economy and replace modern fiat currency. ","Computer Science - Computational Engineering, Finance, and Science ; J.4.1 ; ","Hanley, Brian P. ; ","A zero-sum monetary system, interest rates, and implications  To the knowledge of the author, this is the first time it has been shown that interest rates that are extremely high by modern standards (100% and higher) are necessary within a zero-sum monetary system, and not just driven by greed. Extreme interest rates that appeared in various places and times reinforce the idea that hard money may have contributed to high rates of interest. Here a model is presented that examines the interest rate required to succeed as an investor in a zero-sum fixed quantity hard-money system. Even when the playing field is significantly tilted toward the investor, interest rates need to be much higher than expected. In a completely fair zero-sum system, an investor cannot break even without charging 100% interest. Even with a 5% advantage, an investor won't break even at 15% interest. From this it is concluded that what we consider usurious rates today are, within a hard-money system, driven by necessity.   Cryptocurrency is a novel form of hard-currency. The inability to virtualize the money creates a system close to zero-sum because of the limited supply design. Therefore, within the bounds of a cryptocurrency system that limits money creation, interest rates must rise to levels that the modern world considers usury. It is impossible, therefore, that a cryptocurrency that is not expandable could take over a modern economy and replace modern fiat currency. ",zero sum monetary system interest rat implications knowledge author first time show interest rat extremely high modern standards higher necessary within zero sum monetary system drive greed extreme interest rat appear various place time reinforce idea hard money may contribute high rat interest model present examine interest rate require succeed investor zero sum fix quantity hard money system even play field significantly tilt toward investor interest rat need much higher expect completely fair zero sum system investor cannot break even without charge interest even advantage investor break even interest conclude consider usurious rat today within hard money system drive necessity cryptocurrency novel form hard currency inability virtualize money create system close zero sum limit supply design therefore within bound cryptocurrency system limit money creation interest rat must rise level modern world consider usury impossible therefore cryptocurrency expandable could take modern economy replace modern fiat currency,145,12,1506.08231.txt
http://arxiv.org/abs/1506.08235,Optimal Seed Solver: Optimizing Seed Selection in Read Mapping,"  Motivation: Optimizing seed selection is an important problem in read mapping. The number of non-overlapping seeds a mapper selects determines the sensitivity of the mapper while the total frequency of all selected seeds determines the speed of the mapper. Modern seed-and-extend mappers usually select seeds with either an equal and fixed-length scheme or with an inflexible placement scheme, both of which limit the potential of the mapper to select less frequent seeds to speed up the mapping process. Therefore, it is crucial to develop a new algorithm that can adjust both the individual seed length and the seed placement, as well as derive less frequent seeds.   Results: We present the Optimal Seed Solver (OSS), a dynamic programming algorithm that discovers the least frequently-occurring set of x seeds in an L-bp read in $O(x \times L)$ operations on average and in $O(x \times L^{2})$ operations in the worst case. We compared OSS against four state-of-the-art seed selection schemes and observed that OSS provides a 3-fold reduction of average seed frequency over the best previous seed selection optimizations. ","Computer Science - Computational Engineering, Finance, and Science ; Quantitative Biology - Genomics ; ","Xin, Hongyi ; Zhu, Richard ; Nahar, Sunny ; Emmons, John ; Pekhimenko, Gennady ; Kingsford, Carl ; Alkan, Can ; Mutlu, Onur ; ","Optimal Seed Solver: Optimizing Seed Selection in Read Mapping  Motivation: Optimizing seed selection is an important problem in read mapping. The number of non-overlapping seeds a mapper selects determines the sensitivity of the mapper while the total frequency of all selected seeds determines the speed of the mapper. Modern seed-and-extend mappers usually select seeds with either an equal and fixed-length scheme or with an inflexible placement scheme, both of which limit the potential of the mapper to select less frequent seeds to speed up the mapping process. Therefore, it is crucial to develop a new algorithm that can adjust both the individual seed length and the seed placement, as well as derive less frequent seeds.   Results: We present the Optimal Seed Solver (OSS), a dynamic programming algorithm that discovers the least frequently-occurring set of x seeds in an L-bp read in $O(x \times L)$ operations on average and in $O(x \times L^{2})$ operations in the worst case. We compared OSS against four state-of-the-art seed selection schemes and observed that OSS provides a 3-fold reduction of average seed frequency over the best previous seed selection optimizations. ",optimal seed solver optimize seed selection read map motivation optimize seed selection important problem read map number non overlap seed mapper select determine sensitivity mapper total frequency select seed determine speed mapper modern seed extend mappers usually select seed either equal fix length scheme inflexible placement scheme limit potential mapper select less frequent seed speed map process therefore crucial develop new algorithm adjust individual seed length seed placement well derive less frequent seed result present optimal seed solver oss dynamic program algorithm discover least frequently occur set seed bp read time operations average time operations worst case compare oss four state art seed selection scheme observe oss provide fold reduction average seed frequency best previous seed selection optimizations,118,11,1506.08235.txt
http://arxiv.org/abs/1506.08238,Deciding Univariate Polynomial Problems Using Untrusted Certificates in   Isabelle/HOL,"  We present a proof procedure for univariate real polynomial problems in Isabelle/HOL. The core mathematics of our procedure is based on univariate cylindrical algebraic decomposition. We follow the approach of untrusted certificates, separating solving from verifying: efficient external tools perform expensive real algebraic computations, producing evidence that is formally checked within Isabelle's logic. This allows us to exploit highly-tuned computer algebra systems like Mathematica to guide our procedure without impacting the correctness of its results. We present experiments demonstrating the efficacy of this approach, in many cases yielding orders of magnitude improvements over previous methods. ",Computer Science - Logic in Computer Science ; ,"Li, Wenda ; Passmore, Grant Olney ; Paulson, Lawrence C. ; ","Deciding Univariate Polynomial Problems Using Untrusted Certificates in   Isabelle/HOL  We present a proof procedure for univariate real polynomial problems in Isabelle/HOL. The core mathematics of our procedure is based on univariate cylindrical algebraic decomposition. We follow the approach of untrusted certificates, separating solving from verifying: efficient external tools perform expensive real algebraic computations, producing evidence that is formally checked within Isabelle's logic. This allows us to exploit highly-tuned computer algebra systems like Mathematica to guide our procedure without impacting the correctness of its results. We present experiments demonstrating the efficacy of this approach, in many cases yielding orders of magnitude improvements over previous methods. ",decide univariate polynomial problems use untrusted certificate isabelle hol present proof procedure univariate real polynomial problems isabelle hol core mathematics procedure base univariate cylindrical algebraic decomposition follow approach untrusted certificate separate solve verify efficient external tool perform expensive real algebraic computations produce evidence formally check within isabelle logic allow us exploit highly tune computer algebra systems like mathematica guide procedure without impact correctness result present experiment demonstrate efficacy approach many case yield order magnitude improvements previous methods,77,8,1506.08238.txt
http://arxiv.org/abs/1506.08353,A note on patch-based low-rank minimization for fast image denoising,"  Patch-based low-rank minimization for image processing attracts much attention in recent years. The minimization of the matrix rank coupled with the Frobenius norm data fidelity can be solved by the hard thresholding filter with principle component analysis (PCA) or singular value decomposition (SVD). Based on this idea, we propose a patch-based low-rank minimization method for image denoising. The main denoising process is stated in three equivalent way: PCA, SVD and low-rank minimization. Compared to recent patch-based sparse representation methods, experiments demonstrate that the proposed method is rather rapid, and it is effective for a variety of natural grayscale images and color images, especially for texture parts in images. Further improvements of this method are also given. In addition, due to the simplicity of this method, we could provide an explanation of the choice of the threshold parameter, estimation of PSNR values, and give other insights into this method. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Hu, Haijuan ; Froment, Jacques ; Liu, Quansheng ; ","A note on patch-based low-rank minimization for fast image denoising  Patch-based low-rank minimization for image processing attracts much attention in recent years. The minimization of the matrix rank coupled with the Frobenius norm data fidelity can be solved by the hard thresholding filter with principle component analysis (PCA) or singular value decomposition (SVD). Based on this idea, we propose a patch-based low-rank minimization method for image denoising. The main denoising process is stated in three equivalent way: PCA, SVD and low-rank minimization. Compared to recent patch-based sparse representation methods, experiments demonstrate that the proposed method is rather rapid, and it is effective for a variety of natural grayscale images and color images, especially for texture parts in images. Further improvements of this method are also given. In addition, due to the simplicity of this method, we could provide an explanation of the choice of the threshold parameter, estimation of PSNR values, and give other insights into this method. ",note patch base low rank minimization fast image denoising patch base low rank minimization image process attract much attention recent years minimization matrix rank couple frobenius norm data fidelity solve hard thresholding filter principle component analysis pca singular value decomposition svd base idea propose patch base low rank minimization method image denoising main denoising process state three equivalent way pca svd low rank minimization compare recent patch base sparse representation methods experiment demonstrate propose method rather rapid effective variety natural grayscale image color image especially texture part image improvements method also give addition due simplicity method could provide explanation choice threshold parameter estimation psnr value give insights method,108,11,1506.08353.txt
http://arxiv.org/abs/1506.08435,Large-scale Optimization-based Non-negative Computational Framework for   Diffusion Equations: Parallel Implementation and Performance Studies,"  It is well-known that the standard Galerkin formulation, which is often the formulation of choice under the finite element method for solving self-adjoint diffusion equations, does not meet maximum principles and the non-negative constraint for anisotropic diffusion equations. Recently, optimization-based methodologies that satisfy maximum principles and the non-negative constraint for steady-state and transient diffusion-type equations have been proposed. To date, these methodologies have been tested only on small-scale academic problems. The purpose of this paper is to systematically study the performance of the non-negative methodology in the context of high performance computing (HPC). PETSc and TAO libraries are, respectively, used for the parallel environment and optimization solvers. For large-scale problems, it is important for computational scientists to understand the computational performance of current algorithms available in these scientific libraries. The numerical experiments are conducted on the state-of-the-art HPC systems, and a single-core performance model is used to better characterize the efficiency of the solvers. Our studies indicate that the proposed non-negative computational framework for diffusion-type equations exhibits excellent strong scaling for real-world large-scale problems. ","Computer Science - Numerical Analysis ; Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Performance ; ","Chang, J. ; Karra, S. ; Nakshatrala, K. B. ; ","Large-scale Optimization-based Non-negative Computational Framework for   Diffusion Equations: Parallel Implementation and Performance Studies  It is well-known that the standard Galerkin formulation, which is often the formulation of choice under the finite element method for solving self-adjoint diffusion equations, does not meet maximum principles and the non-negative constraint for anisotropic diffusion equations. Recently, optimization-based methodologies that satisfy maximum principles and the non-negative constraint for steady-state and transient diffusion-type equations have been proposed. To date, these methodologies have been tested only on small-scale academic problems. The purpose of this paper is to systematically study the performance of the non-negative methodology in the context of high performance computing (HPC). PETSc and TAO libraries are, respectively, used for the parallel environment and optimization solvers. For large-scale problems, it is important for computational scientists to understand the computational performance of current algorithms available in these scientific libraries. The numerical experiments are conducted on the state-of-the-art HPC systems, and a single-core performance model is used to better characterize the efficiency of the solvers. Our studies indicate that the proposed non-negative computational framework for diffusion-type equations exhibits excellent strong scaling for real-world large-scale problems. ",large scale optimization base non negative computational framework diffusion equations parallel implementation performance study well know standard galerkin formulation often formulation choice finite element method solve self adjoint diffusion equations meet maximum principles non negative constraint anisotropic diffusion equations recently optimization base methodologies satisfy maximum principles non negative constraint steady state transient diffusion type equations propose date methodologies test small scale academic problems purpose paper systematically study performance non negative methodology context high performance compute hpc petsc tao libraries respectively use parallel environment optimization solvers large scale problems important computational scientists understand computational performance current algorithms available scientific libraries numerical experiment conduct state art hpc systems single core performance model use better characterize efficiency solvers study indicate propose non negative computational framework diffusion type equations exhibit excellent strong scale real world large scale problems,134,9,1506.08435.txt
http://arxiv.org/abs/1506.08544,Exact and approximate inference in graphical models: variable   elimination and beyond,"  Probabilistic graphical models offer a powerful framework to account for the dependence structure between variables, which is represented as a graph. However, the dependence between variables may render inference tasks intractable. In this paper we review techniques exploiting the graph structure for exact inference, borrowed from optimisation and computer science. They are built on the principle of variable elimination whose complexity is dictated in an intricate way by the order in which variables are eliminated. The so-called treewidth of the graph characterises this algorithmic complexity: low-treewidth graphs can be processed efficiently. The first message that we illustrate is therefore the idea that for inference in graphical model, the number of variables is not the limiting factor, and it is worth checking for the treewidth before turning to approximate methods. We show how algorithms providing an upper bound of the treewidth can be exploited to derive a 'good' elimination order enabling to perform exact inference. The second message is that when the treewidth is too large, algorithms for approximate inference linked to the principle of variable elimination, such as loopy belief propagation and variational approaches, can lead to accurate results while being much less time consuming than Monte-Carlo approaches. We illustrate the techniques reviewed in this article on benchmarks of inference problems in genetic linkage analysis and computer vision, as well as on hidden variables restoration in coupled Hidden Markov Models. ",Statistics - Machine Learning ; Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; ,"Peyrard, Nathalie ; Cros, Marie-Josée ; de Givry, Simon ; Franc, Alain ; Robin, Stéphane ; Sabbadin, Régis ; Schiex, Thomas ; Vignes, Matthieu ; ","Exact and approximate inference in graphical models: variable   elimination and beyond  Probabilistic graphical models offer a powerful framework to account for the dependence structure between variables, which is represented as a graph. However, the dependence between variables may render inference tasks intractable. In this paper we review techniques exploiting the graph structure for exact inference, borrowed from optimisation and computer science. They are built on the principle of variable elimination whose complexity is dictated in an intricate way by the order in which variables are eliminated. The so-called treewidth of the graph characterises this algorithmic complexity: low-treewidth graphs can be processed efficiently. The first message that we illustrate is therefore the idea that for inference in graphical model, the number of variables is not the limiting factor, and it is worth checking for the treewidth before turning to approximate methods. We show how algorithms providing an upper bound of the treewidth can be exploited to derive a 'good' elimination order enabling to perform exact inference. The second message is that when the treewidth is too large, algorithms for approximate inference linked to the principle of variable elimination, such as loopy belief propagation and variational approaches, can lead to accurate results while being much less time consuming than Monte-Carlo approaches. We illustrate the techniques reviewed in this article on benchmarks of inference problems in genetic linkage analysis and computer vision, as well as on hidden variables restoration in coupled Hidden Markov Models. ",exact approximate inference graphical model variable elimination beyond probabilistic graphical model offer powerful framework account dependence structure variables represent graph however dependence variables may render inference task intractable paper review techniques exploit graph structure exact inference borrow optimisation computer science build principle variable elimination whose complexity dictate intricate way order variables eliminate call treewidth graph characterise algorithmic complexity low treewidth graph process efficiently first message illustrate therefore idea inference graphical model number variables limit factor worth check treewidth turn approximate methods show algorithms provide upper bind treewidth exploit derive good elimination order enable perform exact inference second message treewidth large algorithms approximate inference link principle variable elimination loopy belief propagation variational approach lead accurate result much less time consume monte carlo approach illustrate techniques review article benchmarks inference problems genetic linkage analysis computer vision well hide variables restoration couple hide markov model,142,3,1506.08544.txt
http://arxiv.org/abs/1506.08547,Commutativity in the Algorithmic Lovasz Local Lemma,"  We consider the recent formulation of the Algorithmic Lov\'asz Local Lemma [10,2,3] for finding objects that avoid `bad features', or `flaws'. It extends the Moser-Tardos resampling algorithm [17] to more general discrete spaces. At each step the method picks a flaw present in the current state and goes to a new state according to some prespecified probability distribution (which depends on the current state and the selected flaw). However, it is less flexible than the Moser-Tardos method since [10,2,3] require a specific flaw selection rule, whereas [17] allows an arbitrary rule (and thus can potentially be implemented more efficiently).   We formulate a new ""commutativity"" condition, and prove that it is sufficient for an arbitrary rule to work. It also enables an efficient parallelization under an additional assumption. We then show that existing resampling oracles for perfect matchings and permutations do satisfy this condition. ",Computer Science - Data Structures and Algorithms ; ,"Kolmogorov, Vladimir ; ","Commutativity in the Algorithmic Lovasz Local Lemma  We consider the recent formulation of the Algorithmic Lov\'asz Local Lemma [10,2,3] for finding objects that avoid `bad features', or `flaws'. It extends the Moser-Tardos resampling algorithm [17] to more general discrete spaces. At each step the method picks a flaw present in the current state and goes to a new state according to some prespecified probability distribution (which depends on the current state and the selected flaw). However, it is less flexible than the Moser-Tardos method since [10,2,3] require a specific flaw selection rule, whereas [17] allows an arbitrary rule (and thus can potentially be implemented more efficiently).   We formulate a new ""commutativity"" condition, and prove that it is sufficient for an arbitrary rule to work. It also enables an efficient parallelization under an additional assumption. We then show that existing resampling oracles for perfect matchings and permutations do satisfy this condition. ",commutativity algorithmic lovasz local lemma consider recent formulation algorithmic lov asz local lemma find object avoid bad feature flaw extend moser tardos resampling algorithm general discrete space step method pick flaw present current state go new state accord prespecified probability distribution depend current state select flaw however less flexible moser tardos method since require specific flaw selection rule whereas allow arbitrary rule thus potentially implement efficiently formulate new commutativity condition prove sufficient arbitrary rule work also enable efficient parallelization additional assumption show exist resampling oracles perfect match permutations satisfy condition,90,8,1506.08547.txt
http://arxiv.org/abs/1506.08752,On Tightly Bounding the Dubins Traveling Salesman's Optimum,"  The Dubins Traveling Salesman Problem (DTSP) has generated significant interest over the last decade due to its occurrence in several civil and military surveillance applications. Currently, there is no algorithm that can find an optimal solution to the problem. In addition, relaxing the motion constraints and solving the resulting Euclidean TSP (ETSP) provides the only lower bound available for the problem. However, in many problem instances, the lower bound computed by solving the ETSP is far below the cost of the feasible solutions obtained by some well-known algorithms for the DTSP. This article addresses this fundamental issue and presents the first systematic procedure for developing tight lower bounds for the DTSP. ",Mathematics - Optimization and Control ; Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Computer Science - Robotics ; ,"Manyam, Satyanarayana ; Rathinam, Sivakumar ; ","On Tightly Bounding the Dubins Traveling Salesman's Optimum  The Dubins Traveling Salesman Problem (DTSP) has generated significant interest over the last decade due to its occurrence in several civil and military surveillance applications. Currently, there is no algorithm that can find an optimal solution to the problem. In addition, relaxing the motion constraints and solving the resulting Euclidean TSP (ETSP) provides the only lower bound available for the problem. However, in many problem instances, the lower bound computed by solving the ETSP is far below the cost of the feasible solutions obtained by some well-known algorithms for the DTSP. This article addresses this fundamental issue and presents the first systematic procedure for developing tight lower bounds for the DTSP. ",tightly bound dubins travel salesman optimum dubins travel salesman problem dtsp generate significant interest last decade due occurrence several civil military surveillance applications currently algorithm find optimal solution problem addition relax motion constraints solve result euclidean tsp etsp provide lower bind available problem however many problem instance lower bind compute solve etsp far cost feasible solutions obtain well know algorithms dtsp article address fundamental issue present first systematic procedure develop tight lower bound dtsp,74,4,1506.08752.txt
http://arxiv.org/abs/1506.08977,A comparative study of divisive hierarchical clustering algorithms,"  A general scheme for divisive hierarchical clustering algorithms is proposed. It is made of three main steps : first a splitting procedure for the subdivision of clusters into two subclusters, second a local evaluation of the bipartitions resulting from the tentative splits and, third, a formula for determining the nodes levels of the resulting dendrogram. A number of such algorithms is given. These algorithms are compared using the Goodman-Kruskal correlation coefficient. As a global criterion it is an internal goodness-of-fit measure based on the set order induced by the hierarchy compared to the order associated to the given dissimilarities. Applied to a hundred of random data tables, these comparisons are in favor of two methods based on unusual ratio-type formulas for the splitting procedures, namely the Silhouette criterion and Dunn's criterion. These two criteria take into account both the within cluster and the between cluster mean dissimilarity. In general the results of these two algorithms are better than the classical Agglomerative Average Link method. ",Computer Science - Data Structures and Algorithms ; Quantitative Biology - Quantitative Methods ; 62-07 ; ,"Roux, Maurice ; ","A comparative study of divisive hierarchical clustering algorithms  A general scheme for divisive hierarchical clustering algorithms is proposed. It is made of three main steps : first a splitting procedure for the subdivision of clusters into two subclusters, second a local evaluation of the bipartitions resulting from the tentative splits and, third, a formula for determining the nodes levels of the resulting dendrogram. A number of such algorithms is given. These algorithms are compared using the Goodman-Kruskal correlation coefficient. As a global criterion it is an internal goodness-of-fit measure based on the set order induced by the hierarchy compared to the order associated to the given dissimilarities. Applied to a hundred of random data tables, these comparisons are in favor of two methods based on unusual ratio-type formulas for the splitting procedures, namely the Silhouette criterion and Dunn's criterion. These two criteria take into account both the within cluster and the between cluster mean dissimilarity. In general the results of these two algorithms are better than the classical Agglomerative Average Link method. ",comparative study divisive hierarchical cluster algorithms general scheme divisive hierarchical cluster algorithms propose make three main step first split procedure subdivision cluster two subclusters second local evaluation bipartitions result tentative split third formula determine nod level result dendrogram number algorithms give algorithms compare use goodman kruskal correlation coefficient global criterion internal goodness fit measure base set order induce hierarchy compare order associate give dissimilarities apply hundred random data table comparisons favor two methods base unusual ratio type formulas split procedures namely silhouette criterion dunn criterion two criteria take account within cluster cluster mean dissimilarity general result two algorithms better classical agglomerative average link method,104,11,1506.08977.txt
http://arxiv.org/abs/1506.09140,Pure Strategies in Imperfect Information Stochastic Games,"  We consider imperfect information stochastic games where we require the players to use pure (i.e. non randomised) strategies. We consider reachability, safety, B\""uchi and co-B\""uchi objectives, and investigate the existence of almost-sure/positively winning strategies for the first player when the second player is perfectly informed or more informed than the first player. We obtain decidability results for positive reachability and almost-sure B\""uchi with optimal algorithms to decide existence of a pure winning strategy and to compute one if exists. We complete the picture by showing that positive safety is undecidable when restricting to pure strategies even if the second player is perfectly informed. ",Computer Science - Formal Languages and Automata Theory ; Computer Science - Computer Science and Game Theory ; ,"Carayol, Arnaud ; Löding, Christof ; Serre, Olivier ; ","Pure Strategies in Imperfect Information Stochastic Games  We consider imperfect information stochastic games where we require the players to use pure (i.e. non randomised) strategies. We consider reachability, safety, B\""uchi and co-B\""uchi objectives, and investigate the existence of almost-sure/positively winning strategies for the first player when the second player is perfectly informed or more informed than the first player. We obtain decidability results for positive reachability and almost-sure B\""uchi with optimal algorithms to decide existence of a pure winning strategy and to compute one if exists. We complete the picture by showing that positive safety is undecidable when restricting to pure strategies even if the second player is perfectly informed. ",pure strategies imperfect information stochastic game consider imperfect information stochastic game require players use pure non randomise strategies consider reachability safety uchi co uchi objectives investigate existence almost sure positively win strategies first player second player perfectly inform inform first player obtain decidability result positive reachability almost sure uchi optimal algorithms decide existence pure win strategy compute one exist complete picture show positive safety undecidable restrict pure strategies even second player perfectly inform,73,8,1506.09140.txt
http://arxiv.org/abs/1506.09145,"Track Layouts, Layered Path Decompositions, and Leveled Planarity","  We investigate two types of graph layouts, track layouts and layered path decompositions, and the relations between their associated parameters track-number and layered pathwidth. We use these two types of layouts to characterize leveled planar graphs, which are the graphs with planar leveled drawings with no dummy vertices. It follows from the known NP-completeness of leveled planarity that track-number and layered pathwidth are also NP-complete, even for the smallest constant parameter values that make these parameters nontrivial. We prove that the graphs with bounded layered pathwidth include outerplanar graphs, Halin graphs, and squaregraphs, but that (despite having bounded track-number) series-parallel graphs do not have bounded layered pathwidth. Finally, we investigate the parameterized complexity of these layouts, showing that past methods used for book layouts do not work to parameterize the problem by treewidth or almost-tree number but that the problem is (non-uniformly) fixed-parameter tractable for tree-depth. ",Mathematics - Combinatorics ; Computer Science - Data Structures and Algorithms ; 05C10 ; ,"Bannister, Michael J. ; Devanny, William E. ; Dujmović, Vida ; Eppstein, David ; Wood, David R. ; ","Track Layouts, Layered Path Decompositions, and Leveled Planarity  We investigate two types of graph layouts, track layouts and layered path decompositions, and the relations between their associated parameters track-number and layered pathwidth. We use these two types of layouts to characterize leveled planar graphs, which are the graphs with planar leveled drawings with no dummy vertices. It follows from the known NP-completeness of leveled planarity that track-number and layered pathwidth are also NP-complete, even for the smallest constant parameter values that make these parameters nontrivial. We prove that the graphs with bounded layered pathwidth include outerplanar graphs, Halin graphs, and squaregraphs, but that (despite having bounded track-number) series-parallel graphs do not have bounded layered pathwidth. Finally, we investigate the parameterized complexity of these layouts, showing that past methods used for book layouts do not work to parameterize the problem by treewidth or almost-tree number but that the problem is (non-uniformly) fixed-parameter tractable for tree-depth. ",track layouts layer path decompositions level planarity investigate two type graph layouts track layouts layer path decompositions relations associate parameters track number layer pathwidth use two type layouts characterize level planar graph graph planar level draw dummy vertices follow know np completeness level planarity track number layer pathwidth also np complete even smallest constant parameter value make parameters nontrivial prove graph bound layer pathwidth include outerplanar graph halin graph squaregraphs despite bound track number series parallel graph bound layer pathwidth finally investigate parameterized complexity layouts show past methods use book layouts work parameterize problem treewidth almost tree number problem non uniformly fix parameter tractable tree depth,106,3,1506.09145.txt
http://arxiv.org/abs/1507.01088,Generic properties of subgroups of free groups and finite presentations,"  Asymptotic properties of finitely generated subgroups of free groups, and of finite group presentations, can be considered in several fashions, depending on the way these objects are represented and on the distribution assumed on these representations: here we assume that they are represented by tuples of reduced words (generators of a subgroup) or of cyclically reduced words (relators). Classical models consider fixed size tuples of words (e.g. the few-generator model) or exponential size tuples (e.g. Gromov's density model), and they usually consider that equal length words are equally likely. We generalize both the few-generator and the density models with probabilistic schemes that also allow variability in the size of tuples and non-uniform distributions on words of a given length.Our first results rely on a relatively mild prefix-heaviness hypothesis on the distributions, which states essentially that the probability of a word decreases exponentially fast as its length grows. Under this hypothesis, we generalize several classical results: exponentially generically a randomly chosen tuple is a basis of the subgroup it generates, this subgroup is malnormal and the tuple satisfies a small cancellation property, even for exponential size tuples. In the special case of the uniform distribution on words of a given length, we give a phase transition theorem for the central tree property, a combinatorial property closely linked to the fact that a tuple freely generates a subgroup. We then further refine our results when the distribution is specified by a Markovian scheme, and in particular we give a phase transition theorem which generalizes the classical results on the densities up to which a tuple of cyclically reduced words chosen uniformly at random exponentially generically satisfies a small cancellation property, and beyond which it presents a trivial group. ",Mathematics - Group Theory ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Bassino, Frédérique ; Nicaud, Cyril ; Weil, Pascal ; ","Generic properties of subgroups of free groups and finite presentations  Asymptotic properties of finitely generated subgroups of free groups, and of finite group presentations, can be considered in several fashions, depending on the way these objects are represented and on the distribution assumed on these representations: here we assume that they are represented by tuples of reduced words (generators of a subgroup) or of cyclically reduced words (relators). Classical models consider fixed size tuples of words (e.g. the few-generator model) or exponential size tuples (e.g. Gromov's density model), and they usually consider that equal length words are equally likely. We generalize both the few-generator and the density models with probabilistic schemes that also allow variability in the size of tuples and non-uniform distributions on words of a given length.Our first results rely on a relatively mild prefix-heaviness hypothesis on the distributions, which states essentially that the probability of a word decreases exponentially fast as its length grows. Under this hypothesis, we generalize several classical results: exponentially generically a randomly chosen tuple is a basis of the subgroup it generates, this subgroup is malnormal and the tuple satisfies a small cancellation property, even for exponential size tuples. In the special case of the uniform distribution on words of a given length, we give a phase transition theorem for the central tree property, a combinatorial property closely linked to the fact that a tuple freely generates a subgroup. We then further refine our results when the distribution is specified by a Markovian scheme, and in particular we give a phase transition theorem which generalizes the classical results on the densities up to which a tuple of cyclically reduced words chosen uniformly at random exponentially generically satisfies a small cancellation property, and beyond which it presents a trivial group. ",generic properties subgroups free group finite presentations asymptotic properties finitely generate subgroups free group finite group presentations consider several fashion depend way object represent distribution assume representations assume represent tuples reduce word generators subgroup cyclically reduce word relators classical model consider fix size tuples word generator model exponential size tuples gromov density model usually consider equal length word equally likely generalize generator density model probabilistic scheme also allow variability size tuples non uniform distributions word give length first result rely relatively mild prefix heaviness hypothesis distributions state essentially probability word decrease exponentially fast length grow hypothesis generalize several classical result exponentially generically randomly choose tuple basis subgroup generate subgroup malnormal tuple satisfy small cancellation property even exponential size tuples special case uniform distribution word give length give phase transition theorem central tree property combinatorial property closely link fact tuple freely generate subgroup refine result distribution specify markovian scheme particular give phase transition theorem generalize classical result densities tuple cyclically reduce word choose uniformly random exponentially generically satisfy small cancellation property beyond present trivial group,174,14,1507.01088.txt
http://arxiv.org/abs/1507.01089,(Pure) transcendence bases in $\phi$-deformed shuffle bialgebras,"  Computations with integro-differential operators are often carried out in an associative algebra with unit, and they are essentially non-commutative computations. By adjoining a cocommutative co-product, one can have those operators perform act on a bialgebra isomorphic to an enveloping algebra. That gives an adequate framework for a computer-algebra implementation via monoidal factorization, (pure) transcendence bases and Poincar\'e--Birkhoff--Witt bases. In this paper, we systematically study these deformations, obtaining necessary and sufficient conditions for the operators to exist, and we give the most general cocommutative deformations of the shuffle co-product and an effective construction of pairs of bases in duality. The paper ends by the combinatorial setting of local systems of coordinates on the group of group-like series. ",Computer Science - Symbolic Computation ; Mathematical Physics ; Mathematics - Combinatorics ; Mathematics - Group Theory ; ,"Bui, Van Chiên ; Duchamp, Gérard H. E. ; Ngô, Quoc Hoan ; Minh, Vincel Hoang Ngoc ; Tollu, Christophe ; ","(Pure) transcendence bases in $\phi$-deformed shuffle bialgebras  Computations with integro-differential operators are often carried out in an associative algebra with unit, and they are essentially non-commutative computations. By adjoining a cocommutative co-product, one can have those operators perform act on a bialgebra isomorphic to an enveloping algebra. That gives an adequate framework for a computer-algebra implementation via monoidal factorization, (pure) transcendence bases and Poincar\'e--Birkhoff--Witt bases. In this paper, we systematically study these deformations, obtaining necessary and sufficient conditions for the operators to exist, and we give the most general cocommutative deformations of the shuffle co-product and an effective construction of pairs of bases in duality. The paper ends by the combinatorial setting of local systems of coordinates on the group of group-like series. ",pure transcendence base phi deform shuffle bialgebras computations integro differential operators often carry associative algebra unit essentially non commutative computations adjoin cocommutative co product one operators perform act bialgebra isomorphic envelop algebra give adequate framework computer algebra implementation via monoidal factorization pure transcendence base poincar birkhoff witt base paper systematically study deformations obtain necessary sufficient condition operators exist give general cocommutative deformations shuffle co product effective construction pair base duality paper end combinatorial set local systems coordinate group group like series,81,4,1507.01089.txt
http://arxiv.org/abs/1507.01239,Experiments on Parallel Training of Deep Neural Network using Model   Averaging,"  In this work we apply model averaging to parallel training of deep neural network (DNN). Parallelization is done in a model averaging manner. Data is partitioned and distributed to different nodes for local model updates, and model averaging across nodes is done every few minibatches. We use multiple GPUs for data parallelization, and Message Passing Interface (MPI) for communication between nodes, which allows us to perform model averaging frequently without losing much time on communication. We investigate the effectiveness of Natural Gradient Stochastic Gradient Descent (NG-SGD) and Restricted Boltzmann Machine (RBM) pretraining for parallel training in model-averaging framework, and explore the best setups in term of different learning rate schedules, averaging frequencies and minibatch sizes. It is shown that NG-SGD and RBM pretraining benefits parameter-averaging based model training. On the 300h Switchboard dataset, a 9.3 times speedup is achieved using 16 GPUs and 17 times speedup using 32 GPUs with limited decoding accuracy loss. ",Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; ,"Su, Hang ; Chen, Haoyu ; ","Experiments on Parallel Training of Deep Neural Network using Model   Averaging  In this work we apply model averaging to parallel training of deep neural network (DNN). Parallelization is done in a model averaging manner. Data is partitioned and distributed to different nodes for local model updates, and model averaging across nodes is done every few minibatches. We use multiple GPUs for data parallelization, and Message Passing Interface (MPI) for communication between nodes, which allows us to perform model averaging frequently without losing much time on communication. We investigate the effectiveness of Natural Gradient Stochastic Gradient Descent (NG-SGD) and Restricted Boltzmann Machine (RBM) pretraining for parallel training in model-averaging framework, and explore the best setups in term of different learning rate schedules, averaging frequencies and minibatch sizes. It is shown that NG-SGD and RBM pretraining benefits parameter-averaging based model training. On the 300h Switchboard dataset, a 9.3 times speedup is achieved using 16 GPUs and 17 times speedup using 32 GPUs with limited decoding accuracy loss. ",experiment parallel train deep neural network use model average work apply model average parallel train deep neural network dnn parallelization do model average manner data partition distribute different nod local model update model average across nod do every minibatches use multiple gpus data parallelization message pass interface mpi communication nod allow us perform model average frequently without lose much time communication investigate effectiveness natural gradient stochastic gradient descent ng sgd restrict boltzmann machine rbm pretraining parallel train model average framework explore best setups term different learn rate schedule average frequencies minibatch size show ng sgd rbm pretraining benefit parameter average base model train switchboard dataset time speedup achieve use gpus time speedup use gpus limit decode accuracy loss,118,11,1507.01239.txt
http://arxiv.org/abs/1507.01279,Scan $B$-Statistic for Kernel Change-Point Detection,"  Detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning. Kernel-based nonparametric statistics have been used for this task which enjoy fewer assumptions on the distributions than the parametric approach and can handle high-dimensional data. In this paper we focus on the scenario when the amount of background data is large, and propose two related computationally efficient kernel-based statistics for change-point detection, which are inspired by the recently developed $B$-statistics. A novel theoretical result of the paper is the characterization of the tail probability of these statistics using the change-of-measure technique, which focuses on characterizing the tail of the detection statistics rather than obtaining its asymptotic distribution under the null distribution. Such approximations are crucial to control the false alarm rate, which corresponds to the significance level in offline change-point detection and the average-run-length in online change-point detection. Our approximations are shown to be highly accurate. Thus, they provide a convenient way to find detection thresholds for both offline and online cases without the need to resort to the more expensive simulations or bootstrapping. We show that our methods perform well on both synthetic data and real data. ",Computer Science - Machine Learning ; Mathematics - Statistics Theory ; Statistics - Machine Learning ; ,"Li, Shuang ; Xie, Yao ; Dai, Hanjun ; Song, Le ; ","Scan $B$-Statistic for Kernel Change-Point Detection  Detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning. Kernel-based nonparametric statistics have been used for this task which enjoy fewer assumptions on the distributions than the parametric approach and can handle high-dimensional data. In this paper we focus on the scenario when the amount of background data is large, and propose two related computationally efficient kernel-based statistics for change-point detection, which are inspired by the recently developed $B$-statistics. A novel theoretical result of the paper is the characterization of the tail probability of these statistics using the change-of-measure technique, which focuses on characterizing the tail of the detection statistics rather than obtaining its asymptotic distribution under the null distribution. Such approximations are crucial to control the false alarm rate, which corresponds to the significance level in offline change-point detection and the average-run-length in online change-point detection. Our approximations are shown to be highly accurate. Thus, they provide a convenient way to find detection thresholds for both offline and online cases without the need to resort to the more expensive simulations or bootstrapping. We show that our methods perform well on both synthetic data and real data. ",scan statistic kernel change point detection detect emergence abrupt change point classic problem statistics machine learn kernel base nonparametric statistics use task enjoy fewer assumptions distributions parametric approach handle high dimensional data paper focus scenario amount background data large propose two relate computationally efficient kernel base statistics change point detection inspire recently develop statistics novel theoretical result paper characterization tail probability statistics use change measure technique focus characterize tail detection statistics rather obtain asymptotic distribution null distribution approximations crucial control false alarm rate correspond significance level offline change point detection average run length online change point detection approximations show highly accurate thus provide convenient way find detection thresholds offline online case without need resort expensive simulations bootstrapping show methods perform well synthetic data real data,125,11,1507.01279.txt
http://arxiv.org/abs/1507.01345,DiffNodesets: An Efficient Structure for Fast Mining Frequent Itemsets,"  Mining frequent itemsets is an essential problem in data mining and plays an important role in many data mining applications. In recent years, some itemset representations based on node sets have been proposed, which have shown to be very efficient for mining frequent itemsets. In this paper, we propose DiffNodeset, a novel and more efficient itemset representation, for mining frequent itemsets. Based on the DiffNodeset structure, we present an efficient algorithm, named dFIN, to mining frequent itemsets. To achieve high efficiency, dFIN finds frequent itemsets using a set-enumeration tree with a hybrid search strategy and directly enumerates frequent itemsets without candidate generation under some case. For evaluating the performance of dFIN, we have conduct extensive experiments to compare it against with existing leading algorithms on a variety of real and synthetic datasets. The experimental results show that dFIN is significantly faster than these leading algorithms. ",Computer Science - Data Structures and Algorithms ; Computer Science - Databases ; ,"Deng, Zhi-Hong ; ","DiffNodesets: An Efficient Structure for Fast Mining Frequent Itemsets  Mining frequent itemsets is an essential problem in data mining and plays an important role in many data mining applications. In recent years, some itemset representations based on node sets have been proposed, which have shown to be very efficient for mining frequent itemsets. In this paper, we propose DiffNodeset, a novel and more efficient itemset representation, for mining frequent itemsets. Based on the DiffNodeset structure, we present an efficient algorithm, named dFIN, to mining frequent itemsets. To achieve high efficiency, dFIN finds frequent itemsets using a set-enumeration tree with a hybrid search strategy and directly enumerates frequent itemsets without candidate generation under some case. For evaluating the performance of dFIN, we have conduct extensive experiments to compare it against with existing leading algorithms on a variety of real and synthetic datasets. The experimental results show that dFIN is significantly faster than these leading algorithms. ",diffnodesets efficient structure fast mine frequent itemsets mine frequent itemsets essential problem data mine play important role many data mine applications recent years itemset representations base node set propose show efficient mine frequent itemsets paper propose diffnodeset novel efficient itemset representation mine frequent itemsets base diffnodeset structure present efficient algorithm name dfin mine frequent itemsets achieve high efficiency dfin find frequent itemsets use set enumeration tree hybrid search strategy directly enumerate frequent itemsets without candidate generation case evaluate performance dfin conduct extensive experiment compare exist lead algorithms variety real synthetic datasets experimental result show dfin significantly faster lead algorithms,99,11,1507.01345.txt
http://arxiv.org/abs/1507.01581,Joint Calibration for Semantic Segmentation,"  Semantic segmentation is the task of assigning a class-label to each pixel in an image. We propose a region-based semantic segmentation framework which handles both full and weak supervision, and addresses three common problems: (1) Objects occur at multiple scales and therefore we should use regions at multiple scales. However, these regions are overlapping which creates conflicting class predictions at the pixel-level. (2) Class frequencies are highly imbalanced in realistic datasets. (3) Each pixel can only be assigned to a single class, which creates competition between classes. We address all three problems with a joint calibration method which optimizes a multi-class loss defined over the final pixel-level output labeling, as opposed to simply region classification. Our method outperforms the state-of-the-art on the popular SIFT Flow [18] dataset in both the fully and weakly supervised setting by a considerably margin (+6% and +10%, respectively). ",Computer Science - Computer Vision and Pattern Recognition ; 68T45 ; ,"Caesar, Holger ; Uijlings, Jasper ; Ferrari, Vittorio ; ","Joint Calibration for Semantic Segmentation  Semantic segmentation is the task of assigning a class-label to each pixel in an image. We propose a region-based semantic segmentation framework which handles both full and weak supervision, and addresses three common problems: (1) Objects occur at multiple scales and therefore we should use regions at multiple scales. However, these regions are overlapping which creates conflicting class predictions at the pixel-level. (2) Class frequencies are highly imbalanced in realistic datasets. (3) Each pixel can only be assigned to a single class, which creates competition between classes. We address all three problems with a joint calibration method which optimizes a multi-class loss defined over the final pixel-level output labeling, as opposed to simply region classification. Our method outperforms the state-of-the-art on the popular SIFT Flow [18] dataset in both the fully and weakly supervised setting by a considerably margin (+6% and +10%, respectively). ",joint calibration semantic segmentation semantic segmentation task assign class label pixel image propose region base semantic segmentation framework handle full weak supervision address three common problems object occur multiple scale therefore use regions multiple scale however regions overlap create conflict class predictions pixel level class frequencies highly imbalanced realistic datasets pixel assign single class create competition class address three problems joint calibration method optimize multi class loss define final pixel level output label oppose simply region classification method outperform state art popular sift flow dataset fully weakly supervise set considerably margin respectively,92,11,1507.01581.txt
http://arxiv.org/abs/1507.01988,Automata and Quantum Computing,"  Quantum computing is a new model of computation, based on quantum physics. Quantum computers can be exponentially faster than conventional computers for problems such as factoring. Besides full-scale quantum computers, more restricted models such as quantum versions of finite automata have been studied. In this paper, we survey various models of quantum finite automata and their properties. We also provide some open questions and new directions for researchers.   Keywords: quantum finite automata, probabilistic finite automata, nondeterminism, bounded error, unbounded error, state complexity, decidability and undecidability, computational complexity ","Computer Science - Formal Languages and Automata Theory ; Computer Science - Computational Complexity ; Quantum Physics ; 68Q10, 68Q12, 68Q15, 68Q19, 68Q45 ; ","Ambainis, Andris ; Yakaryılmaz, Abuzer ; ","Automata and Quantum Computing  Quantum computing is a new model of computation, based on quantum physics. Quantum computers can be exponentially faster than conventional computers for problems such as factoring. Besides full-scale quantum computers, more restricted models such as quantum versions of finite automata have been studied. In this paper, we survey various models of quantum finite automata and their properties. We also provide some open questions and new directions for researchers.   Keywords: quantum finite automata, probabilistic finite automata, nondeterminism, bounded error, unbounded error, state complexity, decidability and undecidability, computational complexity ",automata quantum compute quantum compute new model computation base quantum physics quantum computers exponentially faster conventional computers problems factor besides full scale quantum computers restrict model quantum versions finite automata study paper survey various model quantum finite automata properties also provide open question new directions researchers keywords quantum finite automata probabilistic finite automata nondeterminism bound error unbounded error state complexity decidability undecidability computational complexity,64,14,1507.01988.txt
http://arxiv.org/abs/1507.02103,Measuring centrality by a generalization of degree,"  Network analysis has emerged as a key technique in communication studies, economics, geography, history and sociology, among others. A fundamental issue is how to identify key nodes, for which purpose a number of centrality measures have been developed. This paper proposes a new parametric family of centrality measures called generalized degree. It is based on the idea that a relationship to a more interconnected node contributes to centrality in a greater extent than a connection to a less central one. Generalized degree improves on degree by redistributing its sum over the network with the consideration of the global structure. Application of the measure is supported by a set of basic properties. A sufficient condition is given for generalized degree to be rank monotonic, excluding counter-intuitive changes in the centrality ranking after certain modifications of the network. The measure has a graph interpretation and can be calculated iteratively. Generalized degree is recommended to apply besides degree since it preserves most favourable attributes of degree, but better reflects the role of the nodes in the network and has an increased ability to distinguish among their importance. ","Computer Science - Social and Information Networks ; Physics - Physics and Society ; 15A06, 91D30 ; ","Csató, László ; ","Measuring centrality by a generalization of degree  Network analysis has emerged as a key technique in communication studies, economics, geography, history and sociology, among others. A fundamental issue is how to identify key nodes, for which purpose a number of centrality measures have been developed. This paper proposes a new parametric family of centrality measures called generalized degree. It is based on the idea that a relationship to a more interconnected node contributes to centrality in a greater extent than a connection to a less central one. Generalized degree improves on degree by redistributing its sum over the network with the consideration of the global structure. Application of the measure is supported by a set of basic properties. A sufficient condition is given for generalized degree to be rank monotonic, excluding counter-intuitive changes in the centrality ranking after certain modifications of the network. The measure has a graph interpretation and can be calculated iteratively. Generalized degree is recommended to apply besides degree since it preserves most favourable attributes of degree, but better reflects the role of the nodes in the network and has an increased ability to distinguish among their importance. ",measure centrality generalization degree network analysis emerge key technique communication study economics geography history sociology among others fundamental issue identify key nod purpose number centrality measure develop paper propose new parametric family centrality measure call generalize degree base idea relationship interconnect node contribute centrality greater extent connection less central one generalize degree improve degree redistribute sum network consideration global structure application measure support set basic properties sufficient condition give generalize degree rank monotonic exclude counter intuitive change centrality rank certain modifications network measure graph interpretation calculate iteratively generalize degree recommend apply besides degree since preserve favourable attribute degree better reflect role nod network increase ability distinguish among importance,108,6,1507.02103.txt
http://arxiv.org/abs/1507.02178,"Directed multicut is W[1]-hard, even for four terminal pairs","  We prove that Multicut in directed graphs, parameterized by the size of the cutset, is W[1]-hard and hence unlikely to be fixed-parameter tractable even if restricted to instances with only four terminal pairs. This negative result almost completely resolves one of the central open problems in the area of parameterized complexity of graph separation problems, posted originally by Marx and Razgon [SIAM J. Comput. 43(2):355-388 (2014)], leaving only the case of three terminal pairs open.   Our gadget methodology allows us also to prove W[1]-hardness of the Steiner Orientation problem parameterized by the number of terminal pairs, resolving an open problem of Cygan, Kortsarz, and Nutov [SIAM J. Discrete Math. 27(3):1503-1513 (2013)]. ",Computer Science - Data Structures and Algorithms ; ,"Pilipczuk, Marcin ; Wahlström, Magnus ; ","Directed multicut is W[1]-hard, even for four terminal pairs  We prove that Multicut in directed graphs, parameterized by the size of the cutset, is W[1]-hard and hence unlikely to be fixed-parameter tractable even if restricted to instances with only four terminal pairs. This negative result almost completely resolves one of the central open problems in the area of parameterized complexity of graph separation problems, posted originally by Marx and Razgon [SIAM J. Comput. 43(2):355-388 (2014)], leaving only the case of three terminal pairs open.   Our gadget methodology allows us also to prove W[1]-hardness of the Steiner Orientation problem parameterized by the number of terminal pairs, resolving an open problem of Cygan, Kortsarz, and Nutov [SIAM J. Discrete Math. 27(3):1503-1513 (2013)]. ",direct multicut hard even four terminal pair prove multicut direct graph parameterized size cutset hard hence unlikely fix parameter tractable even restrict instance four terminal pair negative result almost completely resolve one central open problems area parameterized complexity graph separation problems post originally marx razgon siam comput leave case three terminal pair open gadget methodology allow us also prove hardness steiner orientation problem parameterized number terminal pair resolve open problem cygan kortsarz nutov siam discrete math,76,3,1507.02178.txt
http://arxiv.org/abs/1507.02180,A note on the definition of sliding block codes and the   Curtis-Hedlund-Lyndon Theorem,"  In this note we propose an alternative definition for sliding block codes between shift spaces. This definition coincides with the usual definition in the case that the shift space is defined on a finite alphabet, but it encompass a larger class of maps when the alphabet is infinite. In any case, the proposed definition keeps the idea that a sliding block code is a map with a local rule. Using this new definition we prove that the Curtis-Hedlund-Lyndon Theorem always holds for shift spaces over countable alphabets. ","Mathematics - Dynamical Systems ; Computer Science - Information Theory ; Mathematics - History and Overview ; 37B10, 37B15 ; ","Sobottka, Marcelo ; Gonçalves, Daniel ; ","A note on the definition of sliding block codes and the   Curtis-Hedlund-Lyndon Theorem  In this note we propose an alternative definition for sliding block codes between shift spaces. This definition coincides with the usual definition in the case that the shift space is defined on a finite alphabet, but it encompass a larger class of maps when the alphabet is infinite. In any case, the proposed definition keeps the idea that a sliding block code is a map with a local rule. Using this new definition we prove that the Curtis-Hedlund-Lyndon Theorem always holds for shift spaces over countable alphabets. ",note definition slide block cod curtis hedlund lyndon theorem note propose alternative definition slide block cod shift space definition coincide usual definition case shift space define finite alphabet encompass larger class map alphabet infinite case propose definition keep idea slide block code map local rule use new definition prove curtis hedlund lyndon theorem always hold shift space countable alphabets,59,5,1507.02180.txt
http://arxiv.org/abs/1507.02184,"The ""art of trellis decoding"" is fixed-parameter tractable","  Given n subspaces of a finite-dimensional vector space over a fixed finite field $\mathbb F$, we wish to find a linear layout $V_1,V_2,\ldots,V_n$ of the subspaces such that $\dim((V_1+V_2+\cdots+V_i) \cap (V_{i+1}+\cdots+V_n))\le k$ for all i, such a linear layout is said to have width at most k. When restricted to 1-dimensional subspaces, this problem is equivalent to computing the trellis-width (or minimum trellis state-complexity) of a linear code in coding theory and computing the path-width of an $\mathbb F$-represented matroid in matroid theory.   We present a fixed-parameter tractable algorithm to construct a linear layout of width at most k, if it exists, for input subspaces of a finite-dimensional vector space over $\mathbb F$. As corollaries, we obtain a fixed-parameter tractable algorithm to produce a path-decomposition of width at most k for an input $\mathbb F$-represented matroid of path-width at most k, and a fixed-parameter tractable algorithm to find a linear rank-decomposition of width at most k for an input graph of linear rank-width at most k. In both corollaries, no such algorithms were known previously.   It was previously known that a fixed-parameter tractable algorithm exists for the decision version of the problem for matroid path-width, a theorem by Geelen, Gerards, and Whittle~(2002) implies that for each fixed finite field $\mathbb F$, there are finitely many forbidden $\mathbb F$-representable minors for the class of matroids of path-width at most k. An algorithm by Hlin\v{e}n\'y (2006) can detect a minor in an input $\mathbb F$-represented matroid of bounded branch-width. However, this indirect approach would not produce an actual path-decomposition. Our algorithm is the first one to construct such a path-decomposition and does not depend on the finiteness of forbidden minors. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Jeong, Jisu ; Kim, Eun Jung ; Oum, Sang-il ; ","The ""art of trellis decoding"" is fixed-parameter tractable  Given n subspaces of a finite-dimensional vector space over a fixed finite field $\mathbb F$, we wish to find a linear layout $V_1,V_2,\ldots,V_n$ of the subspaces such that $\dim((V_1+V_2+\cdots+V_i) \cap (V_{i+1}+\cdots+V_n))\le k$ for all i, such a linear layout is said to have width at most k. When restricted to 1-dimensional subspaces, this problem is equivalent to computing the trellis-width (or minimum trellis state-complexity) of a linear code in coding theory and computing the path-width of an $\mathbb F$-represented matroid in matroid theory.   We present a fixed-parameter tractable algorithm to construct a linear layout of width at most k, if it exists, for input subspaces of a finite-dimensional vector space over $\mathbb F$. As corollaries, we obtain a fixed-parameter tractable algorithm to produce a path-decomposition of width at most k for an input $\mathbb F$-represented matroid of path-width at most k, and a fixed-parameter tractable algorithm to find a linear rank-decomposition of width at most k for an input graph of linear rank-width at most k. In both corollaries, no such algorithms were known previously.   It was previously known that a fixed-parameter tractable algorithm exists for the decision version of the problem for matroid path-width, a theorem by Geelen, Gerards, and Whittle~(2002) implies that for each fixed finite field $\mathbb F$, there are finitely many forbidden $\mathbb F$-representable minors for the class of matroids of path-width at most k. An algorithm by Hlin\v{e}n\'y (2006) can detect a minor in an input $\mathbb F$-represented matroid of bounded branch-width. However, this indirect approach would not produce an actual path-decomposition. Our algorithm is the first one to construct such a path-decomposition and does not depend on the finiteness of forbidden minors. ",art trellis decode fix parameter tractable give subspaces finite dimensional vector space fix finite field mathbb wish find linear layout ldots subspaces dim cdots cap cdots le linear layout say width restrict dimensional subspaces problem equivalent compute trellis width minimum trellis state complexity linear code cod theory compute path width mathbb represent matroid matroid theory present fix parameter tractable algorithm construct linear layout width exist input subspaces finite dimensional vector space mathbb corollaries obtain fix parameter tractable algorithm produce path decomposition width input mathbb represent matroid path width fix parameter tractable algorithm find linear rank decomposition width input graph linear rank width corollaries algorithms know previously previously know fix parameter tractable algorithm exist decision version problem matroid path width theorem geelen gerards whittle imply fix finite field mathbb finitely many forbid mathbb representable minors class matroids path width algorithm hlin detect minor input mathbb represent matroid bound branch width however indirect approach would produce actual path decomposition algorithm first one construct path decomposition depend finiteness forbid minors,167,4,1507.02184.txt
http://arxiv.org/abs/1507.02250,Privacy-Preserving Nonlinear Observer Design Using Contraction Analysis,"  Real-time information processing applications such as those enabling a more intelligent infrastructure are increasingly focused on analyzing privacy-sensitive data obtained from individuals. To produce accurate statistics about the habits of a population of users of a system, this data might need to be processed through model-based estimators. Moreover, models of population dynamics, originating for example from epidemiology or the social sciences, are often necessarily nonlinear. Motivated by these trends, this paper presents an approach to design nonlinear privacy-preserving model-based observers, relying on additive input or output noise to give differential privacy guarantees to the individuals providing the input data. For the case of output perturbation, contraction analysis allows us to design convergent observers as well as set the level of privacy-preserving noise appropriately. Two examples illustrate the approach: estimating the edge formation probabilities in a dynamic social network, and syndromic surveillance relying on an epidemiological model. ",Computer Science - Systems and Control ; Computer Science - Information Theory ; Computer Science - Social and Information Networks ; ,"Ny, Jerome Le ; ","Privacy-Preserving Nonlinear Observer Design Using Contraction Analysis  Real-time information processing applications such as those enabling a more intelligent infrastructure are increasingly focused on analyzing privacy-sensitive data obtained from individuals. To produce accurate statistics about the habits of a population of users of a system, this data might need to be processed through model-based estimators. Moreover, models of population dynamics, originating for example from epidemiology or the social sciences, are often necessarily nonlinear. Motivated by these trends, this paper presents an approach to design nonlinear privacy-preserving model-based observers, relying on additive input or output noise to give differential privacy guarantees to the individuals providing the input data. For the case of output perturbation, contraction analysis allows us to design convergent observers as well as set the level of privacy-preserving noise appropriately. Two examples illustrate the approach: estimating the edge formation probabilities in a dynamic social network, and syndromic surveillance relying on an epidemiological model. ",privacy preserve nonlinear observer design use contraction analysis real time information process applications enable intelligent infrastructure increasingly focus analyze privacy sensitive data obtain individuals produce accurate statistics habit population users system data might need process model base estimators moreover model population dynamics originate example epidemiology social sciences often necessarily nonlinear motivate trend paper present approach design nonlinear privacy preserve model base observers rely additive input output noise give differential privacy guarantee individuals provide input data case output perturbation contraction analysis allow us design convergent observers well set level privacy preserve noise appropriately two examples illustrate approach estimate edge formation probabilities dynamic social network syndromic surveillance rely epidemiological model,108,10,1507.02250.txt
http://arxiv.org/abs/1507.02385,Towards Effective Codebookless Model for Image Classification,"  The bag-of-features (BoF) model for image classification has been thoroughly studied over the last decade. Different from the widely used BoF methods which modeled images with a pre-trained codebook, the alternative codebook free image modeling method, which we call Codebookless Model (CLM), attracted little attention. In this paper, we present an effective CLM that represents an image with a single Gaussian for classification. By embedding Gaussian manifold into a vector space, we show that the simple incorporation of our CLM into a linear classifier achieves very competitive accuracy compared with state-of-the-art BoF methods (e.g., Fisher Vector). Since our CLM lies in a high dimensional Riemannian manifold, we further propose a joint learning method of low-rank transformation with support vector machine (SVM) classifier on the Gaussian manifold, in order to reduce computational and storage cost. To study and alleviate the side effect of background clutter on our CLM, we also present a simple yet effective partial background removal method based on saliency detection. Experiments are extensively conducted on eight widely used databases to demonstrate the effectiveness and efficiency of our CLM method. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Wang, Qilong ; Li, Peihua ; Zhang, Lei ; Zuo, Wangmeng ; ","Towards Effective Codebookless Model for Image Classification  The bag-of-features (BoF) model for image classification has been thoroughly studied over the last decade. Different from the widely used BoF methods which modeled images with a pre-trained codebook, the alternative codebook free image modeling method, which we call Codebookless Model (CLM), attracted little attention. In this paper, we present an effective CLM that represents an image with a single Gaussian for classification. By embedding Gaussian manifold into a vector space, we show that the simple incorporation of our CLM into a linear classifier achieves very competitive accuracy compared with state-of-the-art BoF methods (e.g., Fisher Vector). Since our CLM lies in a high dimensional Riemannian manifold, we further propose a joint learning method of low-rank transformation with support vector machine (SVM) classifier on the Gaussian manifold, in order to reduce computational and storage cost. To study and alleviate the side effect of background clutter on our CLM, we also present a simple yet effective partial background removal method based on saliency detection. Experiments are extensively conducted on eight widely used databases to demonstrate the effectiveness and efficiency of our CLM method. ",towards effective codebookless model image classification bag feature bof model image classification thoroughly study last decade different widely use bof methods model image pre train codebook alternative codebook free image model method call codebookless model clm attract little attention paper present effective clm represent image single gaussian classification embed gaussian manifold vector space show simple incorporation clm linear classifier achieve competitive accuracy compare state art bof methods fisher vector since clm lie high dimensional riemannian manifold propose joint learn method low rank transformation support vector machine svm classifier gaussian manifold order reduce computational storage cost study alleviate side effect background clutter clm also present simple yet effective partial background removal method base saliency detection experiment extensively conduct eight widely use databases demonstrate effectiveness efficiency clm method,126,11,1507.02385.txt
http://arxiv.org/abs/1507.02908,On Existence and Properties of Approximate Pure Nash Equilibria in   Bandwidth Allocation Games,"  In \emph{bandwidth allocation games} (BAGs), the strategy of a player consists of various demands on different resources. The player's utility is at most the sum of these demands, provided they are fully satisfied. Every resource has a limited capacity and if it is exceeded by the total demand, it has to be split between the players. Since these games generally do not have pure Nash equilibria, we consider approximate pure Nash equilibria, in which no player can improve her utility by more than some fixed factor $\alpha$ through unilateral strategy changes. There is a threshold $\alpha_\delta$ (where $\delta$ is a parameter that limits the demand of each player on a specific resource) such that $\alpha$-approximate pure Nash equilibria always exist for $\alpha \geq \alpha_\delta$, but not for $\alpha < \alpha_\delta$. We give both upper and lower bounds on this threshold $\alpha_\delta$ and show that the corresponding decision problem is ${\sf NP}$-hard. We also show that the $\alpha$-approximate price of anarchy for BAGs is $\alpha+1$. For a restricted version of the game, where demands of players only differ slightly from each other (e.g. symmetric games), we show that approximate Nash equilibria can be reached (and thus also be computed) in polynomial time using the best-response dynamic. Finally, we show that a broader class of utility-maximization games (which includes BAGs) converges quickly towards states whose social welfare is close to the optimum. ",Computer Science - Computer Science and Game Theory ; ,"Drees, Maximilian ; Feldotto, Matthias ; Riechers, Sören ; Skopalik, Alexander ; ","On Existence and Properties of Approximate Pure Nash Equilibria in   Bandwidth Allocation Games  In \emph{bandwidth allocation games} (BAGs), the strategy of a player consists of various demands on different resources. The player's utility is at most the sum of these demands, provided they are fully satisfied. Every resource has a limited capacity and if it is exceeded by the total demand, it has to be split between the players. Since these games generally do not have pure Nash equilibria, we consider approximate pure Nash equilibria, in which no player can improve her utility by more than some fixed factor $\alpha$ through unilateral strategy changes. There is a threshold $\alpha_\delta$ (where $\delta$ is a parameter that limits the demand of each player on a specific resource) such that $\alpha$-approximate pure Nash equilibria always exist for $\alpha \geq \alpha_\delta$, but not for $\alpha < \alpha_\delta$. We give both upper and lower bounds on this threshold $\alpha_\delta$ and show that the corresponding decision problem is ${\sf NP}$-hard. We also show that the $\alpha$-approximate price of anarchy for BAGs is $\alpha+1$. For a restricted version of the game, where demands of players only differ slightly from each other (e.g. symmetric games), we show that approximate Nash equilibria can be reached (and thus also be computed) in polynomial time using the best-response dynamic. Finally, we show that a broader class of utility-maximization games (which includes BAGs) converges quickly towards states whose social welfare is close to the optimum. ",existence properties approximate pure nash equilibria bandwidth allocation game emph bandwidth allocation game bag strategy player consist various demand different resources player utility sum demand provide fully satisfy every resource limit capacity exceed total demand split players since game generally pure nash equilibria consider approximate pure nash equilibria player improve utility fix factor alpha unilateral strategy change threshold alpha delta delta parameter limit demand player specific resource alpha approximate pure nash equilibria always exist alpha geq alpha delta alpha alpha delta give upper lower bound threshold alpha delta show correspond decision problem sf np hard also show alpha approximate price anarchy bag alpha restrict version game demand players differ slightly symmetric game show approximate nash equilibria reach thus also compute polynomial time use best response dynamic finally show broader class utility maximization game include bag converge quickly towards state whose social welfare close optimum,144,8,1507.02908.txt
http://arxiv.org/abs/1507.02954,An Iterative Receiver for OFDM With Sparsity-Based Parametric Channel   Estimation,"  In this work we design a receiver that iteratively passes soft information between the channel estimation and data decoding stages. The receiver incorporates sparsity-based parametric channel estimation. State-of-the-art sparsity-based iterative receivers simplify the channel estimation problem by restricting the multipath delays to a grid. Our receiver does not impose such a restriction. As a result it does not suffer from the leakage effect, which destroys sparsity. Communication at near capacity rates in high SNR requires a large modulation order. Due to the close proximity of modulation symbols in such systems, the grid-based approximation is of insufficient accuracy. We show numerically that a state-of-the-art iterative receiver with grid-based sparse channel estimation exhibits a bit-error-rate floor in the high SNR regime. On the contrary, our receiver performs very close to the perfect channel state information bound for all SNR values. We also demonstrate both theoretically and numerically that parametric channel estimation works well in dense channels, i.e., when the number of multipath components is large and each individual component cannot be resolved. ",Computer Science - Information Theory ; Electrical Engineering and Systems Science - Signal Processing ; Statistics - Applications ; ,"Hansen, Thomas L. ; Jørgensen, Peter B. ; Badiu, Mihai-Alin ; Fleury, Bernard H. ; ","An Iterative Receiver for OFDM With Sparsity-Based Parametric Channel   Estimation  In this work we design a receiver that iteratively passes soft information between the channel estimation and data decoding stages. The receiver incorporates sparsity-based parametric channel estimation. State-of-the-art sparsity-based iterative receivers simplify the channel estimation problem by restricting the multipath delays to a grid. Our receiver does not impose such a restriction. As a result it does not suffer from the leakage effect, which destroys sparsity. Communication at near capacity rates in high SNR requires a large modulation order. Due to the close proximity of modulation symbols in such systems, the grid-based approximation is of insufficient accuracy. We show numerically that a state-of-the-art iterative receiver with grid-based sparse channel estimation exhibits a bit-error-rate floor in the high SNR regime. On the contrary, our receiver performs very close to the perfect channel state information bound for all SNR values. We also demonstrate both theoretically and numerically that parametric channel estimation works well in dense channels, i.e., when the number of multipath components is large and each individual component cannot be resolved. ",iterative receiver ofdm sparsity base parametric channel estimation work design receiver iteratively pass soft information channel estimation data decode stag receiver incorporate sparsity base parametric channel estimation state art sparsity base iterative receivers simplify channel estimation problem restrict multipath delay grid receiver impose restriction result suffer leakage effect destroy sparsity communication near capacity rat high snr require large modulation order due close proximity modulation symbols systems grid base approximation insufficient accuracy show numerically state art iterative receiver grid base sparse channel estimation exhibit bite error rate floor high snr regime contrary receiver perform close perfect channel state information bind snr value also demonstrate theoretically numerically parametric channel estimation work well dense channel number multipath components large individual component cannot resolve,120,9,1507.02954.txt
http://arxiv.org/abs/1507.02980,A hybrid mathematical model of collective motion under alignment and   chemotaxis,"  In this paper we propose and study a hybrid discrete in continuous mathematical model of collective motion under alignment and chemotaxis effect. Starting from the paper by Di Costanzo et al (2015a), in which the Cucker-Smale model (Cucker and Smale, 2007) was coupled with other cell mechanisms, to describe the cell migration and self-organization in the zebrafish lateral line primordium, we introduce a simplified model in which the coupling between an alignment and chemotaxis mechanism acts on a system of interacting particles. In particular we rely on a hybrid description in which the agents are discrete entities, while the chemoattractant is considered as a continuous signal. The proposed model is then studied both from an analytical and a numerical point of view. From the analytic point of view we prove, globally in time, existence and uniqueness of the solution. Then, the asymptotic behaviour of a linearised version of the system is investigated. Through a suitable Lyapunov functional we show that for $t\rightarrow +\infty$, the migrating aggregate exponentially converges to a state in which all the particles have a same position with zero velocity. Finally, we present a comparison between the analytical findings and some numerical results, concerning the behaviour of the full nonlinear system. ","Mathematics - Classical Analysis and ODEs ; Computer Science - Systems and Control ; Mathematics - Dynamical Systems ; Mathematics - Optimization and Control ; Quantitative Biology - Cell Behavior ; 82C22, 34D05, 92C17 ; ","Di Costanzo, Ezio ; Menci, Marta ; Messina, Eleonora ; Natalini, Roberto ; Vecchio, Antonia ; ","A hybrid mathematical model of collective motion under alignment and   chemotaxis  In this paper we propose and study a hybrid discrete in continuous mathematical model of collective motion under alignment and chemotaxis effect. Starting from the paper by Di Costanzo et al (2015a), in which the Cucker-Smale model (Cucker and Smale, 2007) was coupled with other cell mechanisms, to describe the cell migration and self-organization in the zebrafish lateral line primordium, we introduce a simplified model in which the coupling between an alignment and chemotaxis mechanism acts on a system of interacting particles. In particular we rely on a hybrid description in which the agents are discrete entities, while the chemoattractant is considered as a continuous signal. The proposed model is then studied both from an analytical and a numerical point of view. From the analytic point of view we prove, globally in time, existence and uniqueness of the solution. Then, the asymptotic behaviour of a linearised version of the system is investigated. Through a suitable Lyapunov functional we show that for $t\rightarrow +\infty$, the migrating aggregate exponentially converges to a state in which all the particles have a same position with zero velocity. Finally, we present a comparison between the analytical findings and some numerical results, concerning the behaviour of the full nonlinear system. ",hybrid mathematical model collective motion alignment chemotaxis paper propose study hybrid discrete continuous mathematical model collective motion alignment chemotaxis effect start paper di costanzo et al cucker smale model cucker smale couple cell mechanisms describe cell migration self organization zebrafish lateral line primordium introduce simplify model couple alignment chemotaxis mechanism act system interact particles particular rely hybrid description agents discrete entities chemoattractant consider continuous signal propose model study analytical numerical point view analytic point view prove globally time existence uniqueness solution asymptotic behaviour linearise version system investigate suitable lyapunov functional show rightarrow infty migrate aggregate exponentially converge state particles position zero velocity finally present comparison analytical find numerical result concern behaviour full nonlinear system,114,11,1507.02980.txt
http://arxiv.org/abs/1507.03344,Entanglement in Reversible Quantum Computing,"  Similarly to the modelling of entanglement in the algebra of quantum computing, we also model entanglement as a synchronization among an event and its shadows in reversible quantum computing. We give the semantics and axioms of shadow constant for reversible quantum computing. ",Computer Science - Logic in Computer Science ; ,"Wang, Yong ; ","Entanglement in Reversible Quantum Computing  Similarly to the modelling of entanglement in the algebra of quantum computing, we also model entanglement as a synchronization among an event and its shadows in reversible quantum computing. We give the semantics and axioms of shadow constant for reversible quantum computing. ",entanglement reversible quantum compute similarly model entanglement algebra quantum compute also model entanglement synchronization among event shadow reversible quantum compute give semantics axioms shadow constant reversible quantum compute,28,4,1507.03344.txt
http://arxiv.org/abs/1507.03348,Deciding Circular-Arc Graph Isomorphism in Parameterized Logspace,"  We compute a canonical circular-arc representation for a given circular-arc (CA) graph which implies solving the isomorphism and recognition problem for this class. To accomplish this we split the class of CA graphs into uniform and non-uniform ones and employ a generalized version of the argument given by K\""obler et al (2013) that has been used to show that the subclass of Helly CA graphs can be canonized in logspace. For uniform CA graphs our approach works in logspace and in addition to that Helly CA graphs are a strict subset of uniform CA graphs. Thus our result is a generalization of the canonization result for Helly CA graphs. In the non-uniform case a specific set of ambiguous vertices arises. By choosing the parameter to be the cardinality of this set the obstacle can be solved by brute force. This leads to an O(k + log n) space algorithm to compute a canonical representation for non-uniform and therefore all CA graphs. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; G.2.2 ; ,"Chandoo, Maurice ; ","Deciding Circular-Arc Graph Isomorphism in Parameterized Logspace  We compute a canonical circular-arc representation for a given circular-arc (CA) graph which implies solving the isomorphism and recognition problem for this class. To accomplish this we split the class of CA graphs into uniform and non-uniform ones and employ a generalized version of the argument given by K\""obler et al (2013) that has been used to show that the subclass of Helly CA graphs can be canonized in logspace. For uniform CA graphs our approach works in logspace and in addition to that Helly CA graphs are a strict subset of uniform CA graphs. Thus our result is a generalization of the canonization result for Helly CA graphs. In the non-uniform case a specific set of ambiguous vertices arises. By choosing the parameter to be the cardinality of this set the obstacle can be solved by brute force. This leads to an O(k + log n) space algorithm to compute a canonical representation for non-uniform and therefore all CA graphs. ",decide circular arc graph isomorphism parameterized logspace compute canonical circular arc representation give circular arc ca graph imply solve isomorphism recognition problem class accomplish split class ca graph uniform non uniform ones employ generalize version argument give obler et al use show subclass helly ca graph canonize logspace uniform ca graph approach work logspace addition helly ca graph strict subset uniform ca graph thus result generalization canonization result helly ca graph non uniform case specific set ambiguous vertices arise choose parameter cardinality set obstacle solve brute force lead log space algorithm compute canonical representation non uniform therefore ca graph,99,3,1507.03348.txt
http://arxiv.org/abs/1507.03403,Time-Space Trade-offs for Triangulations and Voronoi Diagrams,"  Let $S$ be a planar $n$-point set. A triangulation for $S$ is a maximal plane straight-line graph with vertex set $S$. The Voronoi diagram for $S$ is the subdivision of the plane into cells such that all points in a cell have the same nearest neighbor in $S$. Classically, both structures can be computed in $O(n \log n)$ time and $O(n)$ space. We study the situation when the available workspace is limited: given a parameter $s \in \{1, \dots, n\}$, an $s$-workspace algorithm has read-only access to an input array with the points from $S$ in arbitrary order, and it may use only $O(s)$ additional words of $\Theta(\log n)$ bits for reading and writing intermediate data. The output should then be written to a write-only structure. We describe a deterministic $s$-workspace algorithm for computing an arbitrary triangulation of $S$ in time $O(n^2/s + n \log n \log s )$ and a randomized $s$-workspace algorithm for finding the Voronoi diagram of $S$ in expected time $O((n^2/s) \log s + n \log s \log^*s)$. ",Computer Science - Computational Geometry ; Computer Science - Data Structures and Algorithms ; ,"Korman, Matias ; Mulzer, Wolfgang ; van Renssen, Andre ; Roeloffzen, Marcel ; Seiferth, Paul ; Stein, Yannik ; ","Time-Space Trade-offs for Triangulations and Voronoi Diagrams  Let $S$ be a planar $n$-point set. A triangulation for $S$ is a maximal plane straight-line graph with vertex set $S$. The Voronoi diagram for $S$ is the subdivision of the plane into cells such that all points in a cell have the same nearest neighbor in $S$. Classically, both structures can be computed in $O(n \log n)$ time and $O(n)$ space. We study the situation when the available workspace is limited: given a parameter $s \in \{1, \dots, n\}$, an $s$-workspace algorithm has read-only access to an input array with the points from $S$ in arbitrary order, and it may use only $O(s)$ additional words of $\Theta(\log n)$ bits for reading and writing intermediate data. The output should then be written to a write-only structure. We describe a deterministic $s$-workspace algorithm for computing an arbitrary triangulation of $S$ in time $O(n^2/s + n \log n \log s )$ and a randomized $s$-workspace algorithm for finding the Voronoi diagram of $S$ in expected time $O((n^2/s) \log s + n \log s \log^*s)$. ",time space trade off triangulations voronoi diagram let planar point set triangulation maximal plane straight line graph vertex set voronoi diagram subdivision plane cells point cell nearest neighbor classically structure compute log time space study situation available workspace limit give parameter dot workspace algorithm read access input array point arbitrary order may use additional word theta log bits read write intermediate data output write write structure describe deterministic workspace algorithm compute arbitrary triangulation time log log randomize workspace algorithm find voronoi diagram expect time log log log,87,1,1507.03403.txt
http://arxiv.org/abs/1507.03528,Visibility-Aware Optimal Contagion of Malware Epidemics,"  Recent innovations in the design of computer viruses have led to new trade-offs for the attacker. Multiple variants of a malware may spread at different rates and have different levels of visibility to the network. In this work we examine the optimal strategies for the attacker so as to trade off the extent of spread of the malware against the need for stealth. We show that in the mean-field deterministic regime, this spread-stealth trade-off is optimized by computationally simple single-threshold policies. Specifically, we show that only one variant of the malware is spread by the attacker at each time, as there exists a time up to which the attacker prioritizes maximizing the spread of the malware, and after which she prioritizes stealth. ",Computer Science - Cryptography and Security ; Computer Science - Systems and Control ; Mathematics - Optimization and Control ; ,"Eshghi, Soheil ; Sarkar, Saswati ; Venkatesh, Santosh S. ; ","Visibility-Aware Optimal Contagion of Malware Epidemics  Recent innovations in the design of computer viruses have led to new trade-offs for the attacker. Multiple variants of a malware may spread at different rates and have different levels of visibility to the network. In this work we examine the optimal strategies for the attacker so as to trade off the extent of spread of the malware against the need for stealth. We show that in the mean-field deterministic regime, this spread-stealth trade-off is optimized by computationally simple single-threshold policies. Specifically, we show that only one variant of the malware is spread by the attacker at each time, as there exists a time up to which the attacker prioritizes maximizing the spread of the malware, and after which she prioritizes stealth. ",visibility aware optimal contagion malware epidemics recent innovations design computer viruses lead new trade off attacker multiple variants malware may spread different rat different level visibility network work examine optimal strategies attacker trade extent spread malware need stealth show mean field deterministic regime spread stealth trade optimize computationally simple single threshold policies specifically show one variant malware spread attacker time exist time attacker prioritize maximize spread malware prioritize stealth,69,0,1507.03528.txt
http://arxiv.org/abs/1507.03838,Towards Green and Infinite Capacity in Wireless Communication Networks:   Beyond The Shannon Theorem,"  New and novel way for resources allocation in wireless communication has been proposed in this paper. Under this new method, it has been shown that the required power budget becomes independent of the number of served terminals in the downlink. However, the required power depends only of the coverage area, i.e. the channel losses at the cell boarder. Therefore, huge number (theoretically any number) of terminals could be supported concurrently at finite and small downlink power budget. This could be very useful to support the downlink signalling channels in HSPA+, LTE, and 5G. It can be very useful also to support huge D2D communication downlinks. Moreover, and based on the same concept, a new system configuration for a single link point-to-point communication has been presented. With this new configuration, the achieved data rate becomes independent of the required transmit power. This means that any data rate can be achieved at the target BER and with small and finite transmit power. This seems violating with some major results of the Shannon theorem. This issue will be discussed in details in this article. ",Computer Science - Information Theory ; ,"Elmusrati, Mohammed ; ","Towards Green and Infinite Capacity in Wireless Communication Networks:   Beyond The Shannon Theorem  New and novel way for resources allocation in wireless communication has been proposed in this paper. Under this new method, it has been shown that the required power budget becomes independent of the number of served terminals in the downlink. However, the required power depends only of the coverage area, i.e. the channel losses at the cell boarder. Therefore, huge number (theoretically any number) of terminals could be supported concurrently at finite and small downlink power budget. This could be very useful to support the downlink signalling channels in HSPA+, LTE, and 5G. It can be very useful also to support huge D2D communication downlinks. Moreover, and based on the same concept, a new system configuration for a single link point-to-point communication has been presented. With this new configuration, the achieved data rate becomes independent of the required transmit power. This means that any data rate can be achieved at the target BER and with small and finite transmit power. This seems violating with some major results of the Shannon theorem. This issue will be discussed in details in this article. ",towards green infinite capacity wireless communication network beyond shannon theorem new novel way resources allocation wireless communication propose paper new method show require power budget become independent number serve terminals downlink however require power depend coverage area channel losses cell boarder therefore huge number theoretically number terminals could support concurrently finite small downlink power budget could useful support downlink signal channel hspa lte useful also support huge communication downlinks moreover base concept new system configuration single link point point communication present new configuration achieve data rate become independent require transmit power mean data rate achieve target ber small finite transmit power seem violate major result shannon theorem issue discuss detail article,111,12,1507.03838.txt
http://arxiv.org/abs/1507.04027,Fuzzy Overlapping Community Quality Metrics,"  Modularity is widely used to effectively measure the strength of the disjoint community structure found by community detection algorithms. Several overlapping extensions of modularity were proposed to measure the quality of overlapping community structure. However, all these extensions differ just in the way they define the belonging coefficient and belonging function. Yet, there is lack of systematic comparison of different extensions. To fill this gap, we overview overlapping extensions of modularity and generalize them with a uniform definition enabling application of different belonging coefficients and belonging functions to select the best. In addition, we extend localized modularity, modularity density, and eight local community quality metrics to enable their usages for overlapping communities. The experimental results on a large number of real networks and synthetic networks using overlapping extensions of modularity, overlapping modularity density, and local metrics show that the best results are obtained when the product of the belonging coefficients of two nodes is used as the belonging function. Moreover, the results may be used to guide researchers on which metrics to adopt when measuring the quality of overlapping community structure. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Chen, Mingming ; Szymanski, Boleslaw K. ; ","Fuzzy Overlapping Community Quality Metrics  Modularity is widely used to effectively measure the strength of the disjoint community structure found by community detection algorithms. Several overlapping extensions of modularity were proposed to measure the quality of overlapping community structure. However, all these extensions differ just in the way they define the belonging coefficient and belonging function. Yet, there is lack of systematic comparison of different extensions. To fill this gap, we overview overlapping extensions of modularity and generalize them with a uniform definition enabling application of different belonging coefficients and belonging functions to select the best. In addition, we extend localized modularity, modularity density, and eight local community quality metrics to enable their usages for overlapping communities. The experimental results on a large number of real networks and synthetic networks using overlapping extensions of modularity, overlapping modularity density, and local metrics show that the best results are obtained when the product of the belonging coefficients of two nodes is used as the belonging function. Moreover, the results may be used to guide researchers on which metrics to adopt when measuring the quality of overlapping community structure. ",fuzzy overlap community quality metrics modularity widely use effectively measure strength disjoint community structure find community detection algorithms several overlap extensions modularity propose measure quality overlap community structure however extensions differ way define belong coefficient belong function yet lack systematic comparison different extensions fill gap overview overlap extensions modularity generalize uniform definition enable application different belong coefficients belong function select best addition extend localize modularity modularity density eight local community quality metrics enable usages overlap communities experimental result large number real network synthetic network use overlap extensions modularity overlap modularity density local metrics show best result obtain product belong coefficients two nod use belong function moreover result may use guide researchers metrics adopt measure quality overlap community structure,118,6,1507.04027.txt
http://arxiv.org/abs/1507.04394,Time-triggered smart transducer networks,"  The time-triggered approach is a well-suited approach for building distributed hard real-time systems. Since many applications of transducer networks have real-time requirements, a time-triggered communication interface for smart transducers is desirable, however such a time-triggered interface must still support features for monitoring, maintenance, plug-and-play, etc. The approach of the OMG Smart Transducer Interface consists of clusters of time-triggered smart transducer nodes that contain special interfaces supporting configuration, diagnostics, and maintenance without affecting the deterministic real-time communication. This paper discusses the applicability of the time-triggered approach for smart transducer networks and presents a case study application of a time-triggered smart transducer network. ",Computer Science - Networking and Internet Architecture ; ,"Elmenreich, Wilfried ; ","Time-triggered smart transducer networks  The time-triggered approach is a well-suited approach for building distributed hard real-time systems. Since many applications of transducer networks have real-time requirements, a time-triggered communication interface for smart transducers is desirable, however such a time-triggered interface must still support features for monitoring, maintenance, plug-and-play, etc. The approach of the OMG Smart Transducer Interface consists of clusters of time-triggered smart transducer nodes that contain special interfaces supporting configuration, diagnostics, and maintenance without affecting the deterministic real-time communication. This paper discusses the applicability of the time-triggered approach for smart transducer networks and presents a case study application of a time-triggered smart transducer network. ",time trigger smart transducer network time trigger approach well suit approach build distribute hard real time systems since many applications transducer network real time requirements time trigger communication interface smart transducers desirable however time trigger interface must still support feature monitor maintenance plug play etc approach omg smart transducer interface consist cluster time trigger smart transducer nod contain special interfaces support configuration diagnostics maintenance without affect deterministic real time communication paper discuss applicability time trigger approach smart transducer network present case study application time trigger smart transducer network,88,11,1507.04394.txt
http://arxiv.org/abs/1507.04500,The Complexity of All-switches Strategy Improvement,"  Strategy improvement is a widely-used and well-studied class of algorithms for solving graph-based infinite games. These algorithms are parameterized by a switching rule, and one of the most natural rules is ""all switches"" which switches as many edges as possible in each iteration. Continuing a recent line of work, we study all-switches strategy improvement from the perspective of computational complexity. We consider two natural decision problems, both of which have as input a game $G$, a starting strategy $s$, and an edge $e$. The problems are: 1.) The edge switch problem, namely, is the edge $e$ ever switched by all-switches strategy improvement when it is started from $s$ on game $G$? 2.) The optimal strategy problem, namely, is the edge $e$ used in the final strategy that is found by strategy improvement when it is started from $s$ on game $G$? We show $\mathtt{PSPACE}$-completeness of the edge switch problem and optimal strategy problem for the following settings: Parity games with the discrete strategy improvement algorithm of V\""oge and Jurdzi\'nski; mean-payoff games with the gain-bias algorithm [14,37]; and discounted-payoff games and simple stochastic games with their standard strategy improvement algorithms. We also show $\mathtt{PSPACE}$-completeness of an analogous problem to edge switch for the bottom-antipodal algorithm for finding the sink of an Acyclic Unique Sink Orientation on a cube. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; Computer Science - Computer Science and Game Theory ; Computer Science - Logic in Computer Science ; ,"Fearnley, John ; Savani, Rahul ; ","The Complexity of All-switches Strategy Improvement  Strategy improvement is a widely-used and well-studied class of algorithms for solving graph-based infinite games. These algorithms are parameterized by a switching rule, and one of the most natural rules is ""all switches"" which switches as many edges as possible in each iteration. Continuing a recent line of work, we study all-switches strategy improvement from the perspective of computational complexity. We consider two natural decision problems, both of which have as input a game $G$, a starting strategy $s$, and an edge $e$. The problems are: 1.) The edge switch problem, namely, is the edge $e$ ever switched by all-switches strategy improvement when it is started from $s$ on game $G$? 2.) The optimal strategy problem, namely, is the edge $e$ used in the final strategy that is found by strategy improvement when it is started from $s$ on game $G$? We show $\mathtt{PSPACE}$-completeness of the edge switch problem and optimal strategy problem for the following settings: Parity games with the discrete strategy improvement algorithm of V\""oge and Jurdzi\'nski; mean-payoff games with the gain-bias algorithm [14,37]; and discounted-payoff games and simple stochastic games with their standard strategy improvement algorithms. We also show $\mathtt{PSPACE}$-completeness of an analogous problem to edge switch for the bottom-antipodal algorithm for finding the sink of an Acyclic Unique Sink Orientation on a cube. ",complexity switch strategy improvement strategy improvement widely use well study class algorithms solve graph base infinite game algorithms parameterized switch rule one natural rule switch switch many edge possible iteration continue recent line work study switch strategy improvement perspective computational complexity consider two natural decision problems input game start strategy edge problems edge switch problem namely edge ever switch switch strategy improvement start game optimal strategy problem namely edge use final strategy find strategy improvement start game show mathtt pspace completeness edge switch problem optimal strategy problem follow settings parity game discrete strategy improvement algorithm oge jurdzi nski mean payoff game gain bias algorithm discount payoff game simple stochastic game standard strategy improvement algorithms also show mathtt pspace completeness analogous problem edge switch bottom antipodal algorithm find sink acyclic unique sink orientation cube,133,8,1507.04500.txt
http://arxiv.org/abs/1507.04606,Extension of Modularity Density for Overlapping Community Structure,"  Modularity is widely used to effectively measure the strength of the disjoint community structure found by community detection algorithms. Although several overlapping extensions of modularity were proposed to measure the quality of overlapping community structure, there is lack of systematic comparison of different extensions. To fill this gap, we overview overlapping extensions of modularity to select the best. In addition, we extend the Modularity Density metric to enable its usage for overlapping communities. The experimental results on four real networks using overlapping extensions of modularity, overlapping modularity density, and six other community quality metrics show that the best results are obtained when the product of the belonging coefficients of two nodes is used as the belonging function. Moreover, our experiments indicate that overlapping modularity density is a better measure of the quality of overlapping community structure than other metrics considered. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Chen, Mingming ; Kuzmin, Konstantin ; Szymanski, Boleslaw K. ; ","Extension of Modularity Density for Overlapping Community Structure  Modularity is widely used to effectively measure the strength of the disjoint community structure found by community detection algorithms. Although several overlapping extensions of modularity were proposed to measure the quality of overlapping community structure, there is lack of systematic comparison of different extensions. To fill this gap, we overview overlapping extensions of modularity to select the best. In addition, we extend the Modularity Density metric to enable its usage for overlapping communities. The experimental results on four real networks using overlapping extensions of modularity, overlapping modularity density, and six other community quality metrics show that the best results are obtained when the product of the belonging coefficients of two nodes is used as the belonging function. Moreover, our experiments indicate that overlapping modularity density is a better measure of the quality of overlapping community structure than other metrics considered. ",extension modularity density overlap community structure modularity widely use effectively measure strength disjoint community structure find community detection algorithms although several overlap extensions modularity propose measure quality overlap community structure lack systematic comparison different extensions fill gap overview overlap extensions modularity select best addition extend modularity density metric enable usage overlap communities experimental result four real network use overlap extensions modularity overlap modularity density six community quality metrics show best result obtain product belong coefficients two nod use belong function moreover experiment indicate overlap modularity density better measure quality overlap community structure metrics consider,94,6,1507.04606.txt
http://arxiv.org/abs/1507.05014,Compositional Construction of Approximate Abstractions of Interconnected   Control Systems,"  We consider a compositional construction of approximate abstractions of interconnected control systems. In our framework, an abstraction acts as a substitute in the controller design process and is itself a continuous control system. The abstraction is related to the concrete control system via a so-called simulation function: a Lyapunov-like function, which is used to establish a quantitative bound between the behavior of the approximate abstraction and the concrete system. In the first part of the paper, we provide a small gain type condition that facilitates the compositional construction of an abstraction of an interconnected control system together with a simulation function from the abstractions and simulation functions of the individual subsystems. In the second part of the paper, we restrict our attention to linear control system and characterize simulation functions in terms of controlled invariant, externally stabilizable subspaces. Based on those characterizations, we propose a particular scheme to construct abstractions for linear control systems. We illustrate the compositional construction of an abstraction on an interconnected system consisting of four linear subsystems. We use the abstraction as a substitute to synthesize a controller to enforce a certain linear temporal logic specification. ",Mathematics - Optimization and Control ; Computer Science - Systems and Control ; 93C10 ; I.2.8 ; ,"Rungger, Matthias ; Zamani, Majid ; ","Compositional Construction of Approximate Abstractions of Interconnected   Control Systems  We consider a compositional construction of approximate abstractions of interconnected control systems. In our framework, an abstraction acts as a substitute in the controller design process and is itself a continuous control system. The abstraction is related to the concrete control system via a so-called simulation function: a Lyapunov-like function, which is used to establish a quantitative bound between the behavior of the approximate abstraction and the concrete system. In the first part of the paper, we provide a small gain type condition that facilitates the compositional construction of an abstraction of an interconnected control system together with a simulation function from the abstractions and simulation functions of the individual subsystems. In the second part of the paper, we restrict our attention to linear control system and characterize simulation functions in terms of controlled invariant, externally stabilizable subspaces. Based on those characterizations, we propose a particular scheme to construct abstractions for linear control systems. We illustrate the compositional construction of an abstraction on an interconnected system consisting of four linear subsystems. We use the abstraction as a substitute to synthesize a controller to enforce a certain linear temporal logic specification. ",compositional construction approximate abstractions interconnect control systems consider compositional construction approximate abstractions interconnect control systems framework abstraction act substitute controller design process continuous control system abstraction relate concrete control system via call simulation function lyapunov like function use establish quantitative bind behavior approximate abstraction concrete system first part paper provide small gain type condition facilitate compositional construction abstraction interconnect control system together simulation function abstractions simulation function individual subsystems second part paper restrict attention linear control system characterize simulation function term control invariant externally stabilizable subspaces base characterizations propose particular scheme construct abstractions linear control systems illustrate compositional construction abstraction interconnect system consist four linear subsystems use abstraction substitute synthesize controller enforce certain linear temporal logic specification,117,8,1507.05014.txt
http://arxiv.org/abs/1507.05305,Computability and Complexity of Categorical Structures,"  We examine various categorical structures that can and cannot be constructed. We show that total computable functions can be mimicked by constructible functors. More generally, whatever can be done by a Turing machine can be constructed by categories. Since there are infinitary constructions in category theory, it is shown that category theory is strictly more powerful than Turing machines. In particular, categories can solve the Halting Problem for Turing machines. We also show that categories can solve any problem in the arithmetic hierarchy. ","Computer Science - Computational Complexity ; Computer Science - Logic in Computer Science ; Mathematics - Category Theory ; 18-XX, 03-XX, 03D15, 68Q30, 03D10 ; ","Yanofsky, Noson S. ; ","Computability and Complexity of Categorical Structures  We examine various categorical structures that can and cannot be constructed. We show that total computable functions can be mimicked by constructible functors. More generally, whatever can be done by a Turing machine can be constructed by categories. Since there are infinitary constructions in category theory, it is shown that category theory is strictly more powerful than Turing machines. In particular, categories can solve the Halting Problem for Turing machines. We also show that categories can solve any problem in the arithmetic hierarchy. ",computability complexity categorical structure examine various categorical structure cannot construct show total computable function mimic constructible functors generally whatever do turing machine construct categories since infinitary constructions category theory show category theory strictly powerful turing machine particular categories solve halt problem turing machine also show categories solve problem arithmetic hierarchy,50,8,1507.05305.txt
http://arxiv.org/abs/1507.05492,Parallel Toolkit for Measuring the Quality of Network Community   Structure,"  Many networks display community structure which identifies groups of nodes within which connections are denser than between them. Detecting and characterizing such community structure, which is known as community detection, is one of the fundamental issues in the study of network systems. It has received a considerable attention in the last years. Numerous techniques have been developed for both efficient and effective community detection. Among them, the most efficient algorithm is the label propagation algorithm whose computational complexity is O(|E|). Although it is linear in the number of edges, the running time is still too long for very large networks, creating the need for parallel community detection. Also, computing community quality metrics for community structure is computationally expensive both with and without ground truth. However, to date we are not aware of any effort to introduce parallelism for this problem. In this paper, we provide a parallel toolkit to calculate the values of such metrics. We evaluate the parallel algorithms on both distributed memory machine and shared memory machine. The experimental results show that they yield a significant performance gain over sequential execution in terms of total running time, speedup, and efficiency. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Chen, Mingming ; Liu, Sisi ; Szymanski, Boleslaw K. ; ","Parallel Toolkit for Measuring the Quality of Network Community   Structure  Many networks display community structure which identifies groups of nodes within which connections are denser than between them. Detecting and characterizing such community structure, which is known as community detection, is one of the fundamental issues in the study of network systems. It has received a considerable attention in the last years. Numerous techniques have been developed for both efficient and effective community detection. Among them, the most efficient algorithm is the label propagation algorithm whose computational complexity is O(|E|). Although it is linear in the number of edges, the running time is still too long for very large networks, creating the need for parallel community detection. Also, computing community quality metrics for community structure is computationally expensive both with and without ground truth. However, to date we are not aware of any effort to introduce parallelism for this problem. In this paper, we provide a parallel toolkit to calculate the values of such metrics. We evaluate the parallel algorithms on both distributed memory machine and shared memory machine. The experimental results show that they yield a significant performance gain over sequential execution in terms of total running time, speedup, and efficiency. ",parallel toolkit measure quality network community structure many network display community structure identify group nod within connections denser detect characterize community structure know community detection one fundamental issue study network systems receive considerable attention last years numerous techniques develop efficient effective community detection among efficient algorithm label propagation algorithm whose computational complexity although linear number edge run time still long large network create need parallel community detection also compute community quality metrics community structure computationally expensive without grind truth however date aware effort introduce parallelism problem paper provide parallel toolkit calculate value metrics evaluate parallel algorithms distribute memory machine share memory machine experimental result show yield significant performance gain sequential execution term total run time speedup efficiency,117,6,1507.05492.txt
http://arxiv.org/abs/1507.05500,The Complexity of Non-Iterated Probabilistic Justification Logic,"  The logic PJ is a probabilistic logic defined by adding (non-iterated) probability operators to the basic justification logic J. In this paper we establish upper and lower bounds for the complexity of the derivability problem in the logic PJ. The main result of the paper is that the complex- ity of the derivability problem in PJ remains the same as the complexity of the derivability problem in the underlying logic J, which is {\Pi}p2-complete. This implies hat the probability operators do not increase the complex- ity of the logic, although they arguably enrich the expressiveness of the language. ",Computer Science - Logic in Computer Science ; ,"Kokkinis, Ioannis ; ","The Complexity of Non-Iterated Probabilistic Justification Logic  The logic PJ is a probabilistic logic defined by adding (non-iterated) probability operators to the basic justification logic J. In this paper we establish upper and lower bounds for the complexity of the derivability problem in the logic PJ. The main result of the paper is that the complex- ity of the derivability problem in PJ remains the same as the complexity of the derivability problem in the underlying logic J, which is {\Pi}p2-complete. This implies hat the probability operators do not increase the complex- ity of the logic, although they arguably enrich the expressiveness of the language. ",complexity non iterate probabilistic justification logic logic pj probabilistic logic define add non iterate probability operators basic justification logic paper establish upper lower bound complexity derivability problem logic pj main result paper complex ity derivability problem pj remain complexity derivability problem underlie logic pi complete imply hat probability operators increase complex ity logic although arguably enrich expressiveness language,58,8,1507.05500.txt
http://arxiv.org/abs/1507.05819,On Identifying Anomalies in Tor Usage with Applications in Detecting   Internet Censorship,"  We develop a means to detect ongoing per-country anomalies in the daily usage metrics of the Tor anonymous communication network, and demonstrate the applicability of this technique to identifying likely periods of internet censorship and related events. The presented approach identifies contiguous anomalous periods, rather than daily spikes or drops, and allows anomalies to be ranked according to deviation from expected behaviour.   The developed method is implemented as a running tool, with outputs published daily by mailing list. This list highlights per-country anomalous Tor usage, and produces a daily ranking of countries according to the level of detected anomalous behaviour. This list has been active since August 2016, and is in use by a number of individuals, academics, and NGOs as an early warning system for potential censorship events.   We focus on Tor, however the presented approach is more generally applicable to usage data of other services, both individually and in combination. We demonstrate that combining multiple data sources allows more specific identification of likely Tor blocking events. We demonstrate the our approach in comparison to existing anomaly detection tools, and against both known historical internet censorship events and synthetic datasets. Finally, we detail a number of significant recent anomalous events and behaviours identified by our tool. ",Computer Science - Computers and Society ; Computer Science - Machine Learning ; Computer Science - Networking and Internet Architecture ; ,"Wright, Joss ; Darer, Alexander ; Farnan, Oliver ; ","On Identifying Anomalies in Tor Usage with Applications in Detecting   Internet Censorship  We develop a means to detect ongoing per-country anomalies in the daily usage metrics of the Tor anonymous communication network, and demonstrate the applicability of this technique to identifying likely periods of internet censorship and related events. The presented approach identifies contiguous anomalous periods, rather than daily spikes or drops, and allows anomalies to be ranked according to deviation from expected behaviour.   The developed method is implemented as a running tool, with outputs published daily by mailing list. This list highlights per-country anomalous Tor usage, and produces a daily ranking of countries according to the level of detected anomalous behaviour. This list has been active since August 2016, and is in use by a number of individuals, academics, and NGOs as an early warning system for potential censorship events.   We focus on Tor, however the presented approach is more generally applicable to usage data of other services, both individually and in combination. We demonstrate that combining multiple data sources allows more specific identification of likely Tor blocking events. We demonstrate the our approach in comparison to existing anomaly detection tools, and against both known historical internet censorship events and synthetic datasets. Finally, we detail a number of significant recent anomalous events and behaviours identified by our tool. ",identify anomalies tor usage applications detect internet censorship develop mean detect ongoing per country anomalies daily usage metrics tor anonymous communication network demonstrate applicability technique identify likely periods internet censorship relate events present approach identify contiguous anomalous periods rather daily spike drop allow anomalies rank accord deviation expect behaviour develop method implement run tool output publish daily mail list list highlight per country anomalous tor usage produce daily rank countries accord level detect anomalous behaviour list active since august use number individuals academics ngos early warn system potential censorship events focus tor however present approach generally applicable usage data service individually combination demonstrate combine multiple data source allow specific identification likely tor block events demonstrate approach comparison exist anomaly detection tool know historical internet censorship events synthetic datasets finally detail number significant recent anomalous events behaviours identify tool,138,10,1507.05819.txt
http://arxiv.org/abs/1507.05880,A study of the classification of low-dimensional data with supervised   manifold learning,"  Supervised manifold learning methods learn data representations by preserving the geometric structure of data while enhancing the separation between data samples from different classes. In this work, we propose a theoretical study of supervised manifold learning for classification. We consider nonlinear dimensionality reduction algorithms that yield linearly separable embeddings of training data and present generalization bounds for this type of algorithms. A necessary condition for satisfactory generalization performance is that the embedding allow the construction of a sufficiently regular interpolation function in relation with the separation margin of the embedding. We show that for supervised embeddings satisfying this condition, the classification error decays at an exponential rate with the number of training samples. Finally, we examine the separability of supervised nonlinear embeddings that aim to preserve the low-dimensional geometric structure of data based on graph representations. The proposed analysis is supported by experiments on several real data sets. ",Computer Science - Machine Learning ; ,"Vural, Elif ; Guillemot, Christine ; ","A study of the classification of low-dimensional data with supervised   manifold learning  Supervised manifold learning methods learn data representations by preserving the geometric structure of data while enhancing the separation between data samples from different classes. In this work, we propose a theoretical study of supervised manifold learning for classification. We consider nonlinear dimensionality reduction algorithms that yield linearly separable embeddings of training data and present generalization bounds for this type of algorithms. A necessary condition for satisfactory generalization performance is that the embedding allow the construction of a sufficiently regular interpolation function in relation with the separation margin of the embedding. We show that for supervised embeddings satisfying this condition, the classification error decays at an exponential rate with the number of training samples. Finally, we examine the separability of supervised nonlinear embeddings that aim to preserve the low-dimensional geometric structure of data based on graph representations. The proposed analysis is supported by experiments on several real data sets. ",study classification low dimensional data supervise manifold learn supervise manifold learn methods learn data representations preserve geometric structure data enhance separation data sample different class work propose theoretical study supervise manifold learn classification consider nonlinear dimensionality reduction algorithms yield linearly separable embeddings train data present generalization bound type algorithms necessary condition satisfactory generalization performance embed allow construction sufficiently regular interpolation function relation separation margin embed show supervise embeddings satisfy condition classification error decay exponential rate number train sample finally examine separability supervise nonlinear embeddings aim preserve low dimensional geometric structure data base graph representations propose analysis support experiment several real data set,102,10,1507.05880.txt
http://arxiv.org/abs/1507.05995,A Warm Restart Strategy for Solving Sudoku by Sparse Optimization   Methods,"  This paper is concerned with the popular Sudoku problem. We proposed a warm restart strategy for solving Sudoku puzzles, based on the sparse optimization technique. Furthermore, we defined a new difficulty level for Sudoku puzzles. The efficiency of the proposed method is tested using a dataset of Sudoku puzzles, and the numerical results show that the accurate recovery rate can be enhanced from 84%+ to 99%+ using the L1 sparse optimization method. ","Mathematics - Optimization and Control ; Computer Science - Distributed, Parallel, and Cluster Computing ; 90C05, 90C25 ; ","Tang, Yuchao ; Wu, Zhenggang ; Zhu, Chuanxi ; ","A Warm Restart Strategy for Solving Sudoku by Sparse Optimization   Methods  This paper is concerned with the popular Sudoku problem. We proposed a warm restart strategy for solving Sudoku puzzles, based on the sparse optimization technique. Furthermore, we defined a new difficulty level for Sudoku puzzles. The efficiency of the proposed method is tested using a dataset of Sudoku puzzles, and the numerical results show that the accurate recovery rate can be enhanced from 84%+ to 99%+ using the L1 sparse optimization method. ",warm restart strategy solve sudoku sparse optimization methods paper concern popular sudoku problem propose warm restart strategy solve sudoku puzzle base sparse optimization technique furthermore define new difficulty level sudoku puzzle efficiency propose method test use dataset sudoku puzzle numerical result show accurate recovery rate enhance use sparse optimization method,50,9,1507.05995.txt
http://arxiv.org/abs/1507.06011,Using coarse GPS data to quantify city-scale transportation system   resilience to extreme events,"  This article proposes a method to quantitatively measure the resilience of transportation systems using GPS data from taxis. The granularity of the GPS data necessary for this analysis is relatively coarse; it only requires coordinates for the beginning and end of trips, the metered distance, and the total travel time. The method works by computing the historical distribution of pace (normalized travel times) between various regions of a city and measuring the pace deviations during an unusual event. This method is applied to a dataset of nearly 700 million taxi trips in New York City, which is used to analyze the transportation infrastructure resilience to Hurricane Sandy. The analysis indicates that Hurricane Sandy impacted traffic conditions for more than five days, and caused a peak delay of two minutes per mile. Practically, it identifies that the evacuation caused only minor disruptions, but significant delays were encountered during the post-disaster reentry process. Since the implementation of this method is very efficient, it could potentially be used as an online monitoring tool, representing a first step toward quantifying city scale resilience with coarse GPS data. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Donovan, Brian ; Work, Daniel B. ; ","Using coarse GPS data to quantify city-scale transportation system   resilience to extreme events  This article proposes a method to quantitatively measure the resilience of transportation systems using GPS data from taxis. The granularity of the GPS data necessary for this analysis is relatively coarse; it only requires coordinates for the beginning and end of trips, the metered distance, and the total travel time. The method works by computing the historical distribution of pace (normalized travel times) between various regions of a city and measuring the pace deviations during an unusual event. This method is applied to a dataset of nearly 700 million taxi trips in New York City, which is used to analyze the transportation infrastructure resilience to Hurricane Sandy. The analysis indicates that Hurricane Sandy impacted traffic conditions for more than five days, and caused a peak delay of two minutes per mile. Practically, it identifies that the evacuation caused only minor disruptions, but significant delays were encountered during the post-disaster reentry process. Since the implementation of this method is very efficient, it could potentially be used as an online monitoring tool, representing a first step toward quantifying city scale resilience with coarse GPS data. ",use coarse gps data quantify city scale transportation system resilience extreme events article propose method quantitatively measure resilience transportation systems use gps data taxis granularity gps data necessary analysis relatively coarse require coordinate begin end trip meter distance total travel time method work compute historical distribution pace normalize travel time various regions city measure pace deviations unusual event method apply dataset nearly million taxi trip new york city use analyze transportation infrastructure resilience hurricane sandy analysis indicate hurricane sandy impact traffic condition five days cause peak delay two minutes per mile practically identify evacuation cause minor disruptions significant delay encounter post disaster reentry process since implementation method efficient could potentially use online monitor tool represent first step toward quantify city scale resilience coarse gps data,125,4,1507.06011.txt
http://arxiv.org/abs/1507.06111,COMs: Complexes of Oriented Matroids,"  In his seminal 1983 paper, Jim Lawrence introduced lopsided sets and featured them as asymmetric counterparts of oriented matroids, both sharing the key property of strong elimination. Moreover, symmetry of faces holds in both structures as well as in the so-called affine oriented matroids. These two fundamental properties (formulated for covectors) together lead to the natural notion of ""conditional oriented matroid"" (abbreviated COM). These novel structures can be characterized in terms of three cocircuits axioms, generalizing the familiar characterization for oriented matroids. We describe a binary composition scheme by which every COM can successively be erected as a certain complex of oriented matroids, in essentially the same way as a lopsided set can be glued together from its maximal hypercube faces. A realizable COM is represented by a hyperplane arrangement restricted to an open convex set. Among these are the examples formed by linear extensions of ordered sets, generalizing the oriented matroids corresponding to the permutohedra. Relaxing realizability to local realizability, we capture a wider class of combinatorial objects: we show that non-positively curved Coxeter zonotopal complexes give rise to locally realizable COMs. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Bandelt, Hans-Juergen ; Chepoi, Victor ; Knauer, Kolja ; ","COMs: Complexes of Oriented Matroids  In his seminal 1983 paper, Jim Lawrence introduced lopsided sets and featured them as asymmetric counterparts of oriented matroids, both sharing the key property of strong elimination. Moreover, symmetry of faces holds in both structures as well as in the so-called affine oriented matroids. These two fundamental properties (formulated for covectors) together lead to the natural notion of ""conditional oriented matroid"" (abbreviated COM). These novel structures can be characterized in terms of three cocircuits axioms, generalizing the familiar characterization for oriented matroids. We describe a binary composition scheme by which every COM can successively be erected as a certain complex of oriented matroids, in essentially the same way as a lopsided set can be glued together from its maximal hypercube faces. A realizable COM is represented by a hyperplane arrangement restricted to an open convex set. Among these are the examples formed by linear extensions of ordered sets, generalizing the oriented matroids corresponding to the permutohedra. Relaxing realizability to local realizability, we capture a wider class of combinatorial objects: we show that non-positively curved Coxeter zonotopal complexes give rise to locally realizable COMs. ",coms complexes orient matroids seminal paper jim lawrence introduce lopsided set feature asymmetric counterparts orient matroids share key property strong elimination moreover symmetry face hold structure well call affine orient matroids two fundamental properties formulate covectors together lead natural notion conditional orient matroid abbreviate com novel structure characterize term three cocircuits axioms generalize familiar characterization orient matroids describe binary composition scheme every com successively erect certain complex orient matroids essentially way lopsided set glue together maximal hypercube face realizable com represent hyperplane arrangement restrict open convex set among examples form linear extensions order set generalize orient matroids correspond permutohedra relax realizability local realizability capture wider class combinatorial object show non positively curve coxeter zonotopal complexes give rise locally realizable coms,120,8,1507.06111.txt
http://arxiv.org/abs/1507.06576,Abstract Gringo,"  This paper defines the syntax and semantics of the input language of the ASP grounder GRINGO. The definition covers several constructs that were not discussed in earlier work on the semantics of that language, including intervals, pools, division of integers, aggregates with non-numeric values, and lparse-style aggregate expressions. The definition is abstract in the sense that it disregards some details related to representing programs by strings of ASCII characters. It serves as a specification for GRINGO from Version 4.5 on. ",Computer Science - Programming Languages ; ,"Gebser, Martin ; Harrison, Amelia ; Kaminski, Roland ; Lifschitz, Vladimir ; Schaub, Torsten ; ","Abstract Gringo  This paper defines the syntax and semantics of the input language of the ASP grounder GRINGO. The definition covers several constructs that were not discussed in earlier work on the semantics of that language, including intervals, pools, division of integers, aggregates with non-numeric values, and lparse-style aggregate expressions. The definition is abstract in the sense that it disregards some details related to representing programs by strings of ASCII characters. It serves as a specification for GRINGO from Version 4.5 on. ",abstract gringo paper define syntax semantics input language asp grounder gringo definition cover several construct discuss earlier work semantics language include intervals pool division integers aggregate non numeric value lparse style aggregate expressions definition abstract sense disregard detail relate represent program string ascii character serve specification gringo version,48,8,1507.06576.txt
http://arxiv.org/abs/1507.06616,Robust Monotone Submodular Function Maximization,"  We consider a robust formulation, introduced by Krause et al. (2008), of the classical cardinality constrained monotone submodular function maximization problem, and give the first constant factor approximation results. The robustness considered is w.r.t. adversarial removal of up to $\tau$ elements from the chosen set. For the fundamental case of $\tau=1$, we give a deterministic $(1-1/e)-1/\Theta(m)$ approximation algorithm, where $m$ is an input parameter and number of queries scale as $O(n^{m+1})$. In the process, we develop a deterministic $(1-1/e)-1/\Theta(m)$ approximate greedy algorithm for bi-objective maximization of (two) monotone submodular functions. Generalizing the ideas and using a result from Chekuri et al. (2010), we show a randomized $(1-1/e)-\epsilon$ approximation for constant $\tau$ and $\epsilon\leq \frac{1}{\tilde{\Omega}(\tau)}$, making $O(n^{1/\epsilon^3})$ queries. Further, for $\tau\ll \sqrt{k}$, we give a fast and practical 0.387 algorithm. Finally, we also give a black box result result for the much more general setting of robust maximization subject to an Independence System. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; Mathematics - Optimization and Control ; ,"Orlin, James B. ; Schulz, Andreas S. ; Udwani, Rajan ; ","Robust Monotone Submodular Function Maximization  We consider a robust formulation, introduced by Krause et al. (2008), of the classical cardinality constrained monotone submodular function maximization problem, and give the first constant factor approximation results. The robustness considered is w.r.t. adversarial removal of up to $\tau$ elements from the chosen set. For the fundamental case of $\tau=1$, we give a deterministic $(1-1/e)-1/\Theta(m)$ approximation algorithm, where $m$ is an input parameter and number of queries scale as $O(n^{m+1})$. In the process, we develop a deterministic $(1-1/e)-1/\Theta(m)$ approximate greedy algorithm for bi-objective maximization of (two) monotone submodular functions. Generalizing the ideas and using a result from Chekuri et al. (2010), we show a randomized $(1-1/e)-\epsilon$ approximation for constant $\tau$ and $\epsilon\leq \frac{1}{\tilde{\Omega}(\tau)}$, making $O(n^{1/\epsilon^3})$ queries. Further, for $\tau\ll \sqrt{k}$, we give a fast and practical 0.387 algorithm. Finally, we also give a black box result result for the much more general setting of robust maximization subject to an Independence System. ",robust monotone submodular function maximization consider robust formulation introduce krause et al classical cardinality constrain monotone submodular function maximization problem give first constant factor approximation result robustness consider adversarial removal tau elements choose set fundamental case tau give deterministic theta approximation algorithm input parameter number query scale process develop deterministic theta approximate greedy algorithm bi objective maximization two monotone submodular function generalize ideas use result chekuri et al show randomize epsilon approximation constant tau epsilon leq frac tilde omega tau make epsilon query tau sqrt give fast practical algorithm finally also give black box result result much general set robust maximization subject independence system,104,1,1507.06616.txt
http://arxiv.org/abs/1507.07045,A Truth Serum for Large-Scale Evaluations,"  A major challenge in obtaining large-scale evaluations, e.g., product or service reviews on online platforms, labeling images, grading in online courses, etc., is that of eliciting honest responses from agents in the absence of verifiability. We propose a new reward mechanism with strong incentive properties applicable in a wide variety of such settings. This mechanism has a simple and intuitive output agreement structure: an agent gets a reward only if her response for an evaluation matches that of her peer. But instead of the reward being the same across different answers, it is inversely proportional to a popularity index of each answer. This index is a second order population statistic that captures how frequently two agents performing the same evaluation agree on the particular answer. Rare agreements thus earn a higher reward than agreements that are relatively more common.   In the regime where there are a large number of evaluation tasks, we show that truthful behavior is a strict Bayes-Nash equilibrium of the game induced by the mechanism. Further, we show that the truthful equilibrium is approximately optimal in terms of expected payoffs to the agents across all symmetric equilibria, where the approximation error vanishes in the number of evaluation tasks. Moreover, under a mild condition on strategy space, we show that any symmetric equilibrium that gives a higher expected payoff than the truthful equilibrium must be close to being fully informative if the number of evaluations is large. These last two results are driven by a new notion of an agreement measure that is shown to be monotonic in information loss. This notion and its properties are of independent interest. ",Computer Science - Computer Science and Game Theory ; Computer Science - Artificial Intelligence ; Computer Science - Multiagent Systems ; ,"Kamble, Vijay ; Marn, David ; Shah, Nihar ; Parekh, Abhay ; Ramachandran, Kannan ; ","A Truth Serum for Large-Scale Evaluations  A major challenge in obtaining large-scale evaluations, e.g., product or service reviews on online platforms, labeling images, grading in online courses, etc., is that of eliciting honest responses from agents in the absence of verifiability. We propose a new reward mechanism with strong incentive properties applicable in a wide variety of such settings. This mechanism has a simple and intuitive output agreement structure: an agent gets a reward only if her response for an evaluation matches that of her peer. But instead of the reward being the same across different answers, it is inversely proportional to a popularity index of each answer. This index is a second order population statistic that captures how frequently two agents performing the same evaluation agree on the particular answer. Rare agreements thus earn a higher reward than agreements that are relatively more common.   In the regime where there are a large number of evaluation tasks, we show that truthful behavior is a strict Bayes-Nash equilibrium of the game induced by the mechanism. Further, we show that the truthful equilibrium is approximately optimal in terms of expected payoffs to the agents across all symmetric equilibria, where the approximation error vanishes in the number of evaluation tasks. Moreover, under a mild condition on strategy space, we show that any symmetric equilibrium that gives a higher expected payoff than the truthful equilibrium must be close to being fully informative if the number of evaluations is large. These last two results are driven by a new notion of an agreement measure that is shown to be monotonic in information loss. This notion and its properties are of independent interest. ",truth serum large scale evaluations major challenge obtain large scale evaluations product service review online platforms label image grade online course etc elicit honest responses agents absence verifiability propose new reward mechanism strong incentive properties applicable wide variety settings mechanism simple intuitive output agreement structure agent get reward response evaluation match peer instead reward across different answer inversely proportional popularity index answer index second order population statistic capture frequently two agents perform evaluation agree particular answer rare agreements thus earn higher reward agreements relatively common regime large number evaluation task show truthful behavior strict bay nash equilibrium game induce mechanism show truthful equilibrium approximately optimal term expect payoffs agents across symmetric equilibria approximation error vanish number evaluation task moreover mild condition strategy space show symmetric equilibrium give higher expect payoff truthful equilibrium must close fully informative number evaluations large last two result drive new notion agreement measure show monotonic information loss notion properties independent interest,155,11,1507.07045.txt
http://arxiv.org/abs/1507.07091,The Wiretap Channel with Generalized Feedback: Secure Communication and   Key Generation,"  It is a well-known fact that feedback does not increase the capacity of point-to-point memoryless channels, however, its effect in secure communications is not fully understood yet. In this work, an achievable scheme for the wiretap channel with generalized feedback is presented. This scheme, which uses the feedback signal to generate a shared secret key between the legitimate users, encrypts the message to be sent at the bit level. New capacity results for a class of channels are provided, as well as some new insights into the secret key agreement problem. Moreover, this scheme recovers previously reported rate regions from the literature, and thus it can be seen as a generalization that unifies several results in the field. ",Computer Science - Information Theory ; ,"Bassi, Germán ; Piantanida, Pablo ; Shamai, Shlomo ; ","The Wiretap Channel with Generalized Feedback: Secure Communication and   Key Generation  It is a well-known fact that feedback does not increase the capacity of point-to-point memoryless channels, however, its effect in secure communications is not fully understood yet. In this work, an achievable scheme for the wiretap channel with generalized feedback is presented. This scheme, which uses the feedback signal to generate a shared secret key between the legitimate users, encrypts the message to be sent at the bit level. New capacity results for a class of channels are provided, as well as some new insights into the secret key agreement problem. Moreover, this scheme recovers previously reported rate regions from the literature, and thus it can be seen as a generalization that unifies several results in the field. ",wiretap channel generalize feedback secure communication key generation well know fact feedback increase capacity point point memoryless channel however effect secure communications fully understand yet work achievable scheme wiretap channel generalize feedback present scheme use feedback signal generate share secret key legitimate users encrypt message send bite level new capacity result class channel provide well new insights secret key agreement problem moreover scheme recover previously report rate regions literature thus see generalization unify several result field,76,12,1507.07091.txt
http://arxiv.org/abs/1507.07402,Partial resampling to approximate covering integer programs,"  We consider column-sparse covering integer programs, a generalization of set cover, which have attracted a long line of research developing (randomized) approximation algorithms. We develop a new rounding scheme based on the Partial Resampling variant of the Lov\'{a}sz Local Lemma developed by Harris & Srinivasan (2013).   This achieves an approximation ratio of $1 + \frac{\ln (\Delta_1+1)}{a_{\text{min}}} + O\Big( \log(1 + \sqrt{ \frac{\log (\Delta_1+1)}{a_{\text{min}}}}) \Big)$, where $a_{\text{min}}$ is the minimum covering constraint and $\Delta_1$ is the maximum $\ell_1$-norm of any column of the covering matrix (whose entries are scaled to lie in $[0,1]$). When there are additional constraints on the sizes of the variables, we show an approximation ratio of $\ln \Delta_0 + O(\log \log \Delta_0)$ (where $\Delta_0$ is the maximum number of non-zero entries in any column of the covering matrix). We also show nearly-matching inapproximability and integrality-gap lower bounds. These results improve asymptotically, in several different ways, over results shown by Srinivasan (2006) and Kolliopoulos & Young (2005).   We show also that the rounding process leads to negative correlation among the variables. This allows us to automatically handle multi-criteria programs, efficiently achieving approximation ratios which are essentially equivalent to the single-criterion case and apply even when the number of criteria is large. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; ,"Chen, Antares ; Harris, David G. ; Srinivasan, Aravind ; ","Partial resampling to approximate covering integer programs  We consider column-sparse covering integer programs, a generalization of set cover, which have attracted a long line of research developing (randomized) approximation algorithms. We develop a new rounding scheme based on the Partial Resampling variant of the Lov\'{a}sz Local Lemma developed by Harris & Srinivasan (2013).   This achieves an approximation ratio of $1 + \frac{\ln (\Delta_1+1)}{a_{\text{min}}} + O\Big( \log(1 + \sqrt{ \frac{\log (\Delta_1+1)}{a_{\text{min}}}}) \Big)$, where $a_{\text{min}}$ is the minimum covering constraint and $\Delta_1$ is the maximum $\ell_1$-norm of any column of the covering matrix (whose entries are scaled to lie in $[0,1]$). When there are additional constraints on the sizes of the variables, we show an approximation ratio of $\ln \Delta_0 + O(\log \log \Delta_0)$ (where $\Delta_0$ is the maximum number of non-zero entries in any column of the covering matrix). We also show nearly-matching inapproximability and integrality-gap lower bounds. These results improve asymptotically, in several different ways, over results shown by Srinivasan (2006) and Kolliopoulos & Young (2005).   We show also that the rounding process leads to negative correlation among the variables. This allows us to automatically handle multi-criteria programs, efficiently achieving approximation ratios which are essentially equivalent to the single-criterion case and apply even when the number of criteria is large. ",partial resampling approximate cover integer program consider column sparse cover integer program generalization set cover attract long line research develop randomize approximation algorithms develop new round scheme base partial resampling variant lov sz local lemma develop harris srinivasan achieve approximation ratio frac ln delta text min big log sqrt frac log delta text min big text min minimum cover constraint delta maximum ell norm column cover matrix whose entries scale lie additional constraints size variables show approximation ratio ln delta log log delta delta maximum number non zero entries column cover matrix also show nearly match inapproximability integrality gap lower bound result improve asymptotically several different ways result show srinivasan kolliopoulos young show also round process lead negative correlation among variables allow us automatically handle multi criteria program efficiently achieve approximation ratios essentially equivalent single criterion case apply even number criteria large,142,1,1507.07402.txt
http://arxiv.org/abs/1507.07583,Mapping Auto-context Decision Forests to Deep ConvNets for Semantic   Segmentation,"  We consider the task of pixel-wise semantic segmentation given a small set of labeled training images. Among two of the most popular techniques to address this task are Decision Forests (DF) and Neural Networks (NN). In this work, we explore the relationship between two special forms of these techniques: stacked DFs (namely Auto-context) and deep Convolutional Neural Networks (ConvNet). Our main contribution is to show that Auto-context can be mapped to a deep ConvNet with novel architecture, and thereby trained end-to-end. This mapping can be used as an initialization of a deep ConvNet, enabling training even in the face of very limited amounts of training data. We also demonstrate an approximate mapping back from the refined ConvNet to a second stacked DF, with improved performance over the original. We experimentally verify that these mappings outperform stacked DFs for two different applications in computer vision and biology: Kinect-based body part labeling from depth images, and somite segmentation in microscopy images of developing zebrafish. Finally, we revisit the core mapping from a Decision Tree (DT) to a NN, and show that it is also possible to map a fuzzy DT, with sigmoidal split decisions, to a NN. This addresses multiple limitations of the previous mapping, and yields new insights into the popular Rectified Linear Unit (ReLU), and more recently proposed concatenated ReLU (CReLU), activation functions. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Richmond, David L. ; Kainmueller, Dagmar ; Yang, Michael Y. ; Myers, Eugene W. ; Rother, Carsten ; ","Mapping Auto-context Decision Forests to Deep ConvNets for Semantic   Segmentation  We consider the task of pixel-wise semantic segmentation given a small set of labeled training images. Among two of the most popular techniques to address this task are Decision Forests (DF) and Neural Networks (NN). In this work, we explore the relationship between two special forms of these techniques: stacked DFs (namely Auto-context) and deep Convolutional Neural Networks (ConvNet). Our main contribution is to show that Auto-context can be mapped to a deep ConvNet with novel architecture, and thereby trained end-to-end. This mapping can be used as an initialization of a deep ConvNet, enabling training even in the face of very limited amounts of training data. We also demonstrate an approximate mapping back from the refined ConvNet to a second stacked DF, with improved performance over the original. We experimentally verify that these mappings outperform stacked DFs for two different applications in computer vision and biology: Kinect-based body part labeling from depth images, and somite segmentation in microscopy images of developing zebrafish. Finally, we revisit the core mapping from a Decision Tree (DT) to a NN, and show that it is also possible to map a fuzzy DT, with sigmoidal split decisions, to a NN. This addresses multiple limitations of the previous mapping, and yields new insights into the popular Rectified Linear Unit (ReLU), and more recently proposed concatenated ReLU (CReLU), activation functions. ",map auto context decision forest deep convnets semantic segmentation consider task pixel wise semantic segmentation give small set label train image among two popular techniques address task decision forest df neural network nn work explore relationship two special form techniques stack dfs namely auto context deep convolutional neural network convnet main contribution show auto context map deep convnet novel architecture thereby train end end map use initialization deep convnet enable train even face limit amount train data also demonstrate approximate map back refine convnet second stack df improve performance original experimentally verify mappings outperform stack dfs two different applications computer vision biology kinect base body part label depth image somite segmentation microscopy image develop zebrafish finally revisit core map decision tree dt nn show also possible map fuzzy dt sigmoidal split decisions nn address multiple limitations previous map yield new insights popular rectify linear unit relu recently propose concatenate relu crelu activation function,153,6,1507.07583.txt
http://arxiv.org/abs/1507.07622,Fully-Online Suffix Tree and Directed Acyclic Word Graph Construction   for Multiple Texts,"  We consider construction of the suffix tree and the directed acyclic word graph (DAWG) indexing data structures for a collection $\mathcal{T}$ of texts, where a new symbol may be appended to any text in $\mathcal{T} = \{T_1, \ldots, T_K\}$, at any time. This fully-online scenario, which arises in dynamically indexing multi-sensor data, is a natural generalization of the long solved semi-online text indexing problem, where texts $T_1, \ldots, T_{k}$ are permanently fixed before the next text $T_{k+1}$ is processed for each $1 \leq k < K$. We present fully-online algorithms that construct the suffix tree and the DAWG for $\mathcal{T}$ in $O(N \log \sigma)$ time and $O(N)$ space, where $N$ is the total lengths of the strings in $\mathcal{T}$ and $\sigma$ is their alphabet size. The standard explicit representation of the suffix tree leaf edges and some DAWG edges must be relaxed in our fully-online scenario, since too many updates on these edges are required in the worst case. Instead, we provide access to the updated suffix tree leaf edge labels and the DAWG edges to be redirected via auxiliary data structures, in $O(\log \sigma)$ time per added character. ",Computer Science - Data Structures and Algorithms ; ,"Takagi, Takuya ; Inenaga, Shunsuke ; Arimura, Hiroki ; Breslauer, Dany ; Hendrian, Diptarama ; ","Fully-Online Suffix Tree and Directed Acyclic Word Graph Construction   for Multiple Texts  We consider construction of the suffix tree and the directed acyclic word graph (DAWG) indexing data structures for a collection $\mathcal{T}$ of texts, where a new symbol may be appended to any text in $\mathcal{T} = \{T_1, \ldots, T_K\}$, at any time. This fully-online scenario, which arises in dynamically indexing multi-sensor data, is a natural generalization of the long solved semi-online text indexing problem, where texts $T_1, \ldots, T_{k}$ are permanently fixed before the next text $T_{k+1}$ is processed for each $1 \leq k < K$. We present fully-online algorithms that construct the suffix tree and the DAWG for $\mathcal{T}$ in $O(N \log \sigma)$ time and $O(N)$ space, where $N$ is the total lengths of the strings in $\mathcal{T}$ and $\sigma$ is their alphabet size. The standard explicit representation of the suffix tree leaf edges and some DAWG edges must be relaxed in our fully-online scenario, since too many updates on these edges are required in the worst case. Instead, we provide access to the updated suffix tree leaf edge labels and the DAWG edges to be redirected via auxiliary data structures, in $O(\log \sigma)$ time per added character. ",fully online suffix tree direct acyclic word graph construction multiple texts consider construction suffix tree direct acyclic word graph dawg index data structure collection mathcal texts new symbol may append text mathcal ldots time fully online scenario arise dynamically index multi sensor data natural generalization long solve semi online text index problem texts ldots permanently fix next text process leq present fully online algorithms construct suffix tree dawg mathcal log sigma time space total lengths string mathcal sigma alphabet size standard explicit representation suffix tree leaf edge dawg edge must relax fully online scenario since many update edge require worst case instead provide access update suffix tree leaf edge label dawg edge redirect via auxiliary data structure log sigma time per add character,123,1,1507.07622.txt
http://arxiv.org/abs/1507.07855,Generalized Twisted Gabidulin Codes,"  Let $\mathcal{C}$ be a set of $m$ by $n$ matrices over $\mathbb{F}_q$ such that the rank of $A-B$ is at least $d$ for all distinct $A,B\in \mathcal{C}$. Suppose that $m\leqslant n$. If $\#\mathcal{C}= q^{n(m-d+1)}$, then $\mathcal{C}$ is a maximum rank distance (MRD for short) code. Until 2016, there were only two known constructions of MRD codes for arbitrary $1<d<m-1$. One was found by Delsarte (1978) and Gabidulin (1985) independently, and it was later generalized by Kshevetskiy and Gabidulin (2005). We often call them (generalized) Gabidulin codes. Another family was recently obtained by Sheekey (2016), and its elements are called twisted Gabidulin codes. In the same paper, Sheekey also proposed a generalization of the twisted Gabidulin codes. However the equivalence problem for it is not considered, whence it is not clear whether there exist new MRD codes in this generalization. We call the members of this putative larger family generalized twisted Gabidulin codes. In this paper, we first compute the Delsarte duals and adjoint codes of them, then we completely determine the equivalence between different generalized twisted Gabidulin codes. In particular, it can be proven that, up to equivalence, generalized Gabidulin codes and twisted Gabidulin codes are both proper subsets of this family. ",Mathematics - Combinatorics ; Computer Science - Information Theory ; ,"Lunardon, Guglielmo ; Trombetti, Rocco ; Zhou, Yue ; ","Generalized Twisted Gabidulin Codes  Let $\mathcal{C}$ be a set of $m$ by $n$ matrices over $\mathbb{F}_q$ such that the rank of $A-B$ is at least $d$ for all distinct $A,B\in \mathcal{C}$. Suppose that $m\leqslant n$. If $\#\mathcal{C}= q^{n(m-d+1)}$, then $\mathcal{C}$ is a maximum rank distance (MRD for short) code. Until 2016, there were only two known constructions of MRD codes for arbitrary $1<d<m-1$. One was found by Delsarte (1978) and Gabidulin (1985) independently, and it was later generalized by Kshevetskiy and Gabidulin (2005). We often call them (generalized) Gabidulin codes. Another family was recently obtained by Sheekey (2016), and its elements are called twisted Gabidulin codes. In the same paper, Sheekey also proposed a generalization of the twisted Gabidulin codes. However the equivalence problem for it is not considered, whence it is not clear whether there exist new MRD codes in this generalization. We call the members of this putative larger family generalized twisted Gabidulin codes. In this paper, we first compute the Delsarte duals and adjoint codes of them, then we completely determine the equivalence between different generalized twisted Gabidulin codes. In particular, it can be proven that, up to equivalence, generalized Gabidulin codes and twisted Gabidulin codes are both proper subsets of this family. ",generalize twist gabidulin cod let mathcal set matrices mathbb rank least distinct mathcal suppose leqslant mathcal mathcal maximum rank distance mrd short code two know constructions mrd cod arbitrary one find delsarte gabidulin independently later generalize kshevetskiy gabidulin often call generalize gabidulin cod another family recently obtain sheekey elements call twist gabidulin cod paper sheekey also propose generalization twist gabidulin cod however equivalence problem consider whence clear whether exist new mrd cod generalization call members putative larger family generalize twist gabidulin cod paper first compute delsarte duals adjoint cod completely determine equivalence different generalize twist gabidulin cod particular prove equivalence generalize gabidulin cod twist gabidulin cod proper subsets family,109,5,1507.07855.txt
http://arxiv.org/abs/1507.08861,Mobile Multi-View Object Image Search,"  High user interaction capability of mobile devices can help improve the accuracy of mobile visual search systems. At query time, it is possible to capture multiple views of an object from different viewing angles and at different scales with the mobile device camera to obtain richer information about the object compared to a single view and hence return more accurate results. Motivated by this, we developed a mobile multi-view object image search system, using a client-server architecture. Multi-view images of objects acquired by the mobile clients are processed and local features are sent to the server, which combines the query image representations with early/late fusion methods based on bag-of-visual-words and sends back the query results. We performed a comprehensive analysis of early and late fusion approaches using various similarity functions, on an existing single view and a new multi-view object image database. The experimental results show that multi-view search provides significantly better retrieval accuracy compared to single view search. ",Computer Science - Multimedia ; Computer Science - Computer Vision and Pattern Recognition ; ,"Calisir, Fatih ; Bastan, Muhammet ; Ulusoy, Ozgur ; Gudukbay, Ugur ; ","Mobile Multi-View Object Image Search  High user interaction capability of mobile devices can help improve the accuracy of mobile visual search systems. At query time, it is possible to capture multiple views of an object from different viewing angles and at different scales with the mobile device camera to obtain richer information about the object compared to a single view and hence return more accurate results. Motivated by this, we developed a mobile multi-view object image search system, using a client-server architecture. Multi-view images of objects acquired by the mobile clients are processed and local features are sent to the server, which combines the query image representations with early/late fusion methods based on bag-of-visual-words and sends back the query results. We performed a comprehensive analysis of early and late fusion approaches using various similarity functions, on an existing single view and a new multi-view object image database. The experimental results show that multi-view search provides significantly better retrieval accuracy compared to single view search. ",mobile multi view object image search high user interaction capability mobile devices help improve accuracy mobile visual search systems query time possible capture multiple view object different view angle different scale mobile device camera obtain richer information object compare single view hence return accurate result motivate develop mobile multi view object image search system use client server architecture multi view image object acquire mobile clients process local feature send server combine query image representations early late fusion methods base bag visual word send back query result perform comprehensive analysis early late fusion approach use various similarity function exist single view new multi view object image database experimental result show multi view search provide significantly better retrieval accuracy compare single view search,121,2,1507.08861.txt
http://arxiv.org/abs/1508.00217,Indexing of CNN Features for Large Scale Image Search,"  The convolutional neural network (CNN) features can give a good description of image content, which usually represent images with unique global vectors. Although they are compact compared to local descriptors, they still cannot efficiently deal with large-scale image retrieval due to the cost of the linear incremental computation and storage. To address this issue, we build a simple but effective indexing framework based on inverted table, which significantly decreases both the search time and memory usage. In addition, several strategies are fully investigated under an indexing framework to adapt it to CNN features and compensate for quantization errors. First, we use multiple assignment for the query and database images to increase the probability of relevant images' co-existing in the same Voronoi cells obtained via the clustering algorithm. Then, we introduce embedding codes to further improve precision by removing false matches during a search. We demonstrate that by using hashing schemes to calculate the embedding codes and by changing the ranking rule, indexing framework speeds can be greatly improved. Extensive experiments conducted on several unsupervised and supervised benchmarks support these results and the superiority of the proposed indexing framework. We also provide a fair comparison between the popular CNN features. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Liu, Ruoyu ; Zhao, Yao ; Wei, Shikui ; Yang, Yi ; ","Indexing of CNN Features for Large Scale Image Search  The convolutional neural network (CNN) features can give a good description of image content, which usually represent images with unique global vectors. Although they are compact compared to local descriptors, they still cannot efficiently deal with large-scale image retrieval due to the cost of the linear incremental computation and storage. To address this issue, we build a simple but effective indexing framework based on inverted table, which significantly decreases both the search time and memory usage. In addition, several strategies are fully investigated under an indexing framework to adapt it to CNN features and compensate for quantization errors. First, we use multiple assignment for the query and database images to increase the probability of relevant images' co-existing in the same Voronoi cells obtained via the clustering algorithm. Then, we introduce embedding codes to further improve precision by removing false matches during a search. We demonstrate that by using hashing schemes to calculate the embedding codes and by changing the ranking rule, indexing framework speeds can be greatly improved. Extensive experiments conducted on several unsupervised and supervised benchmarks support these results and the superiority of the proposed indexing framework. We also provide a fair comparison between the popular CNN features. ",index cnn feature large scale image search convolutional neural network cnn feature give good description image content usually represent image unique global vectors although compact compare local descriptors still cannot efficiently deal large scale image retrieval due cost linear incremental computation storage address issue build simple effective index framework base invert table significantly decrease search time memory usage addition several strategies fully investigate index framework adapt cnn feature compensate quantization errors first use multiple assignment query database image increase probability relevant image co exist voronoi cells obtain via cluster algorithm introduce embed cod improve precision remove false match search demonstrate use hash scheme calculate embed cod change rank rule index framework speed greatly improve extensive experiment conduct several unsupervised supervise benchmarks support result superiority propose index framework also provide fair comparison popular cnn feature,134,11,1508.00217.txt
http://arxiv.org/abs/1508.00315,Low-rank spectral optimization via gauge duality,"  Various applications in signal processing and machine learning give rise to highly structured spectral optimization problems characterized by low-rank solutions. Two important examples that motivate this work are optimization problems from phase retrieval and from blind deconvolution, which are designed to yield rank-1 solutions. An algorithm is described that is based on solving a certain constrained eigenvalue optimization problem that corresponds to the gauge dual which, unlike the more typical Lagrange dual, has an especially simple constraint. The dominant cost at each iteration is the computation of rightmost eigenpairs of a Hermitian operator. A range of numerical examples illustrate the scalability of the approach. ","Mathematics - Optimization and Control ; Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; 90C15, 90C25 ; ","Friedlander, Michael P. ; Macedo, Ives ; ","Low-rank spectral optimization via gauge duality  Various applications in signal processing and machine learning give rise to highly structured spectral optimization problems characterized by low-rank solutions. Two important examples that motivate this work are optimization problems from phase retrieval and from blind deconvolution, which are designed to yield rank-1 solutions. An algorithm is described that is based on solving a certain constrained eigenvalue optimization problem that corresponds to the gauge dual which, unlike the more typical Lagrange dual, has an especially simple constraint. The dominant cost at each iteration is the computation of rightmost eigenpairs of a Hermitian operator. A range of numerical examples illustrate the scalability of the approach. ",low rank spectral optimization via gauge duality various applications signal process machine learn give rise highly structure spectral optimization problems characterize low rank solutions two important examples motivate work optimization problems phase retrieval blind deconvolution design yield rank solutions algorithm describe base solve certain constrain eigenvalue optimization problem correspond gauge dual unlike typical lagrange dual especially simple constraint dominant cost iteration computation rightmost eigenpairs hermitian operator range numerical examples illustrate scalability approach,72,9,1508.00315.txt
http://arxiv.org/abs/1508.00510,Proving the Turing Universality of Oritatami Co-Transcriptional Folding   (Full Text),"  We study the oritatami model for molecular co-transcriptional folding. In oritatami systems, the transcript (the ""molecule"") folds as it is synthesized (transcribed), according to a local energy optimisation process, which is similar to how actual biomolecules such as RNA fold into complex shapes and functions as they are transcribed. We prove that there is an oritatami system embedding universal computation in the folding process itself.   Our result relies on the development of a generic toolbox, which is easily reusable for future work to design complex functions in oritatami systems. We develop ""low-level"" tools that allow to easily spread apart the encoding of different ""functions"" in the transcript, even if they are required to be applied at the same geometrical location in the folding. We build upon these low-level tools, a programming framework with increasing levels of abstraction, from encoding of instructions into the transcript to logical analysis. This framework is similar to the hardware-to-algorithm levels of abstractions in standard algorithm theory. These various levels of abstractions allow to separate the proof of correctness of the global behavior of our system, from the proof of correctness of its implementation. Thanks to this framework, we were able to computerize the proof of correctness of its implementation and produce certificates, in the form of a relatively small number of proof trees, compact and easily readable and checkable by human, while encapsulating huge case enumerations. We believe this particular type of certificates can be generalized to other discrete dynamical systems, where proofs involve large case enumerations as well. ",Computer Science - Computational Geometry ; Computer Science - Computational Complexity ; Computer Science - Emerging Technologies ; ,"Geary, Cody ; Meunier, Pierre-Étienne ; Schabanel, Nicolas ; Seki, Shinnosuke ; ","Proving the Turing Universality of Oritatami Co-Transcriptional Folding   (Full Text)  We study the oritatami model for molecular co-transcriptional folding. In oritatami systems, the transcript (the ""molecule"") folds as it is synthesized (transcribed), according to a local energy optimisation process, which is similar to how actual biomolecules such as RNA fold into complex shapes and functions as they are transcribed. We prove that there is an oritatami system embedding universal computation in the folding process itself.   Our result relies on the development of a generic toolbox, which is easily reusable for future work to design complex functions in oritatami systems. We develop ""low-level"" tools that allow to easily spread apart the encoding of different ""functions"" in the transcript, even if they are required to be applied at the same geometrical location in the folding. We build upon these low-level tools, a programming framework with increasing levels of abstraction, from encoding of instructions into the transcript to logical analysis. This framework is similar to the hardware-to-algorithm levels of abstractions in standard algorithm theory. These various levels of abstractions allow to separate the proof of correctness of the global behavior of our system, from the proof of correctness of its implementation. Thanks to this framework, we were able to computerize the proof of correctness of its implementation and produce certificates, in the form of a relatively small number of proof trees, compact and easily readable and checkable by human, while encapsulating huge case enumerations. We believe this particular type of certificates can be generalized to other discrete dynamical systems, where proofs involve large case enumerations as well. ",prove turing universality oritatami co transcriptional fold full text study oritatami model molecular co transcriptional fold oritatami systems transcript molecule fold synthesize transcribe accord local energy optimisation process similar actual biomolecules rna fold complex shape function transcribe prove oritatami system embed universal computation fold process result rely development generic toolbox easily reusable future work design complex function oritatami systems develop low level tool allow easily spread apart encode different function transcript even require apply geometrical location fold build upon low level tool program framework increase level abstraction encode instructions transcript logical analysis framework similar hardware algorithm level abstractions standard algorithm theory various level abstractions allow separate proof correctness global behavior system proof correctness implementation thank framework able computerize proof correctness implementation produce certificate form relatively small number proof tree compact easily readable checkable human encapsulate huge case enumerations believe particular type certificate generalize discrete dynamical systems proof involve large case enumerations well,152,8,1508.00510.txt
http://arxiv.org/abs/1508.00641,Episodic Multi-armed Bandits,"  We introduce a new class of reinforcement learning methods referred to as {\em episodic multi-armed bandits} (eMAB). In eMAB the learner proceeds in {\em episodes}, each composed of several {\em steps}, in which it chooses an action and observes a feedback signal. Moreover, in each step, it can take a special action, called the $stop$ action, that ends the current episode. After the $stop$ action is taken, the learner collects a terminal reward, and observes the costs and terminal rewards associated with each step of the episode. The goal of the learner is to maximize its cumulative gain (i.e., the terminal reward minus costs) over all episodes by learning to choose the best sequence of actions based on the feedback. First, we define an {\em oracle} benchmark, which sequentially selects the actions that maximize the expected immediate gain. Then, we propose our online learning algorithm, named {\em FeedBack Adaptive Learning} (FeedBAL), and prove that its regret with respect to the benchmark is bounded with high probability and increases logarithmically in expectation. Moreover, the regret only has polynomial dependence on the number of steps, actions and states. eMAB can be used to model applications that involve humans in the loop, ranging from personalized medical screening to personalized web-based education, where sequences of actions are taken in each episode, and optimal behavior requires adapting the chosen actions based on the feedback. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Tekin, Cem ; van der Schaar, Mihaela ; ","Episodic Multi-armed Bandits  We introduce a new class of reinforcement learning methods referred to as {\em episodic multi-armed bandits} (eMAB). In eMAB the learner proceeds in {\em episodes}, each composed of several {\em steps}, in which it chooses an action and observes a feedback signal. Moreover, in each step, it can take a special action, called the $stop$ action, that ends the current episode. After the $stop$ action is taken, the learner collects a terminal reward, and observes the costs and terminal rewards associated with each step of the episode. The goal of the learner is to maximize its cumulative gain (i.e., the terminal reward minus costs) over all episodes by learning to choose the best sequence of actions based on the feedback. First, we define an {\em oracle} benchmark, which sequentially selects the actions that maximize the expected immediate gain. Then, we propose our online learning algorithm, named {\em FeedBack Adaptive Learning} (FeedBAL), and prove that its regret with respect to the benchmark is bounded with high probability and increases logarithmically in expectation. Moreover, the regret only has polynomial dependence on the number of steps, actions and states. eMAB can be used to model applications that involve humans in the loop, ranging from personalized medical screening to personalized web-based education, where sequences of actions are taken in each episode, and optimal behavior requires adapting the chosen actions based on the feedback. ",episodic multi arm bandits introduce new class reinforcement learn methods refer em episodic multi arm bandits emab emab learner proceed em episodes compose several em step choose action observe feedback signal moreover step take special action call stop action end current episode stop action take learner collect terminal reward observe cost terminal reward associate step episode goal learner maximize cumulative gain terminal reward minus cost episodes learn choose best sequence action base feedback first define em oracle benchmark sequentially select action maximize expect immediate gain propose online learn algorithm name em feedback adaptive learn feedbal prove regret respect benchmark bound high probability increase logarithmically expectation moreover regret polynomial dependence number step action state emab use model applications involve humans loop range personalize medical screen personalize web base education sequence action take episode optimal behavior require adapt choose action base feedback,140,9,1508.00641.txt
http://arxiv.org/abs/1508.00688,Accelerating R with high performance linear algebra libraries,"  Linear algebra routines are basic building blocks for the statistical software. In this paper we analyzed how can we can improve R performance for matrix computations. We benchmarked few matrix operations using the standard linear algebra libraries included in the R distribution and high performance libraries like OpenBLAS, GotoBLAS and MKL. Our tests showed the the best results are obtained with the MKL library, the other two libraries having similar performances, but lower than MKL ",Computer Science - Mathematical Software ; 68N99 ; H.3.4 ; ,"Oancea, Bogdan ; Andrei, Tudorel ; Dragoescu, Raluca Mariana ; ","Accelerating R with high performance linear algebra libraries  Linear algebra routines are basic building blocks for the statistical software. In this paper we analyzed how can we can improve R performance for matrix computations. We benchmarked few matrix operations using the standard linear algebra libraries included in the R distribution and high performance libraries like OpenBLAS, GotoBLAS and MKL. Our tests showed the the best results are obtained with the MKL library, the other two libraries having similar performances, but lower than MKL ",accelerate high performance linear algebra libraries linear algebra routines basic build block statistical software paper analyze improve performance matrix computations benchmarked matrix operations use standard linear algebra libraries include distribution high performance libraries like openblas gotoblas mkl test show best result obtain mkl library two libraries similar performances lower mkl,50,9,1508.00688.txt
http://arxiv.org/abs/1508.00945,Structured Prediction: From Gaussian Perturbations to Linear-Time   Principled Algorithms,"  Margin-based structured prediction commonly uses a maximum loss over all possible structured outputs \cite{Altun03,Collins04b,Taskar03}. In natural language processing, recent work \cite{Zhang14,Zhang15} has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution. This method is linear-time in the number of random structured outputs and trivially parallelizable. We study this family of loss functions in the PAC-Bayes framework under Gaussian perturbations \cite{McAllester07}. Under some technical conditions and up to statistical accuracy, we show that this family of loss functions produces a tighter upper bound of the Gibbs decoder distortion than commonly used methods. Thus, using the maximum loss over random structured outputs is a principled way of learning the parameter of structured prediction models. Besides explaining the experimental success of \cite{Zhang14,Zhang15}, our theoretical results show that more general techniques are possible. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; ,"Honorio, Jean ; Jaakkola, Tommi ; ","Structured Prediction: From Gaussian Perturbations to Linear-Time   Principled Algorithms  Margin-based structured prediction commonly uses a maximum loss over all possible structured outputs \cite{Altun03,Collins04b,Taskar03}. In natural language processing, recent work \cite{Zhang14,Zhang15} has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution. This method is linear-time in the number of random structured outputs and trivially parallelizable. We study this family of loss functions in the PAC-Bayes framework under Gaussian perturbations \cite{McAllester07}. Under some technical conditions and up to statistical accuracy, we show that this family of loss functions produces a tighter upper bound of the Gibbs decoder distortion than commonly used methods. Thus, using the maximum loss over random structured outputs is a principled way of learning the parameter of structured prediction models. Besides explaining the experimental success of \cite{Zhang14,Zhang15}, our theoretical results show that more general techniques are possible. ",structure prediction gaussian perturbations linear time principled algorithms margin base structure prediction commonly use maximum loss possible structure output cite altun collins taskar natural language process recent work cite zhang zhang propose use maximum loss random structure output sample independently proposal distribution method linear time number random structure output trivially parallelizable study family loss function pac bay framework gaussian perturbations cite mcallester technical condition statistical accuracy show family loss function produce tighter upper bind gibbs decoder distortion commonly use methods thus use maximum loss random structure output principled way learn parameter structure prediction model besides explain experimental success cite zhang zhang theoretical result show general techniques possible,107,9,1508.00945.txt
http://arxiv.org/abs/1508.01014,Computational complexity of distance edge labeling,"  The problem of Distance Edge Labeling is a variant of Distance Vertex Labeling (also known as $L_{2,1}$ labeling) that has been studied for more than twenty years and has many applications, such as frequency assignment.   The Distance Edge Labeling problem asks whether the edges of a given graph can be labeled such that the labels of adjacent edges differ by at least two and the labels of edges of distance two differ by at least one. Labels are chosen from the set $\{0,1,\dots,\lambda\}$ for $\lambda$ fixed.   We present a full classification of its computational complexity - a dichotomy between the polynomially solvable cases and the remaining cases which are NP-complete. We characterise graphs with $\lambda \le 4$ which leads to a polynomial-time algorithm recognizing the class and we show NP-completeness for $\lambda \ge 5$ by several reductions from Monotone Not All Equal 3-SAT. ",Computer Science - Discrete Mathematics ; Computer Science - Computational Complexity ; G.2.2 ; F.2.2 ; ,"Knop, Dušan ; Masařík, Tomáš ; ","Computational complexity of distance edge labeling  The problem of Distance Edge Labeling is a variant of Distance Vertex Labeling (also known as $L_{2,1}$ labeling) that has been studied for more than twenty years and has many applications, such as frequency assignment.   The Distance Edge Labeling problem asks whether the edges of a given graph can be labeled such that the labels of adjacent edges differ by at least two and the labels of edges of distance two differ by at least one. Labels are chosen from the set $\{0,1,\dots,\lambda\}$ for $\lambda$ fixed.   We present a full classification of its computational complexity - a dichotomy between the polynomially solvable cases and the remaining cases which are NP-complete. We characterise graphs with $\lambda \le 4$ which leads to a polynomial-time algorithm recognizing the class and we show NP-completeness for $\lambda \ge 5$ by several reductions from Monotone Not All Equal 3-SAT. ",computational complexity distance edge label problem distance edge label variant distance vertex label also know label study twenty years many applications frequency assignment distance edge label problem ask whether edge give graph label label adjacent edge differ least two label edge distance two differ least one label choose set dot lambda lambda fix present full classification computational complexity dichotomy polynomially solvable case remain case np complete characterise graph lambda le lead polynomial time algorithm recognize class show np completeness lambda ge several reductions monotone equal sit,86,3,1508.01014.txt
http://arxiv.org/abs/1508.01059,Offline and Online Models of Budget Allocation for Maximizing Influence   Spread,"  The research of influence propagation in social networks via word-of-mouth processes has been given considerable attention in recent years. Arguably, the most fundamental problem in this domain is influence maximization, where the goal is to identify a seed set of individuals that can trigger a large cascade of influence in the network. While there has been significant progress regarding this problem and its variants, one basic shortcoming of the models is that they lack the flexibility in the way the budget is allocated to individuals. Indeed, budget allocation is a critical issue in advertising and viral marketing. Taking the other point of view, known models allowing flexible budget allocation do not take into account the influence spread in the network. We introduce a generalized model that captures both budgets and influence propagation simultaneously.   For the offline setting, we identify a large family of budget-based propagation functions that admit tight approximation guarantee. This family extends most of the previously studied influence models, including the well-known Triggering model. We establish that any function in this family implies an instance of a monotone submodular function maximization over the integer lattice subject to a knapsack constraint. This problem is known to admit an optimal (1-1/e)-approximation. We also study the price of anarchy of the multi-player game that extends the model and establish tight results.   For the online setting, in which an unknown subset of agents arrive in a random order and the algorithm needs to make an irrevocable budget allocation in each step, we develop a 1/(15e)-competitive algorithm. This setting extends the secretary problem, and its variant, the submodular knapsack secretary problem. Notably, our algorithm improves over the best known approximation for the latter problem, even though it applies to a more general setting. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computer Science and Game Theory ; Computer Science - Social and Information Networks ; ,"Avigdor-Elgrabli, Noa ; Blocq, Gideon ; Gamzu, Iftah ; Orda, Ariel ; ","Offline and Online Models of Budget Allocation for Maximizing Influence   Spread  The research of influence propagation in social networks via word-of-mouth processes has been given considerable attention in recent years. Arguably, the most fundamental problem in this domain is influence maximization, where the goal is to identify a seed set of individuals that can trigger a large cascade of influence in the network. While there has been significant progress regarding this problem and its variants, one basic shortcoming of the models is that they lack the flexibility in the way the budget is allocated to individuals. Indeed, budget allocation is a critical issue in advertising and viral marketing. Taking the other point of view, known models allowing flexible budget allocation do not take into account the influence spread in the network. We introduce a generalized model that captures both budgets and influence propagation simultaneously.   For the offline setting, we identify a large family of budget-based propagation functions that admit tight approximation guarantee. This family extends most of the previously studied influence models, including the well-known Triggering model. We establish that any function in this family implies an instance of a monotone submodular function maximization over the integer lattice subject to a knapsack constraint. This problem is known to admit an optimal (1-1/e)-approximation. We also study the price of anarchy of the multi-player game that extends the model and establish tight results.   For the online setting, in which an unknown subset of agents arrive in a random order and the algorithm needs to make an irrevocable budget allocation in each step, we develop a 1/(15e)-competitive algorithm. This setting extends the secretary problem, and its variant, the submodular knapsack secretary problem. Notably, our algorithm improves over the best known approximation for the latter problem, even though it applies to a more general setting. ",offline online model budget allocation maximize influence spread research influence propagation social network via word mouth process give considerable attention recent years arguably fundamental problem domain influence maximization goal identify seed set individuals trigger large cascade influence network significant progress regard problem variants one basic shortcoming model lack flexibility way budget allocate individuals indeed budget allocation critical issue advertise viral market take point view know model allow flexible budget allocation take account influence spread network introduce generalize model capture budget influence propagation simultaneously offline set identify large family budget base propagation function admit tight approximation guarantee family extend previously study influence model include well know trigger model establish function family imply instance monotone submodular function maximization integer lattice subject knapsack constraint problem know admit optimal approximation also study price anarchy multi player game extend model establish tight result online set unknown subset agents arrive random order algorithm need make irrevocable budget allocation step develop competitive algorithm set extend secretary problem variant submodular knapsack secretary problem notably algorithm improve best know approximation latter problem even though apply general set,178,0,1508.01059.txt
http://arxiv.org/abs/1508.01775,Cascading Power Outages Propagate Locally in an Influence Graph that is   not the Actual Grid Topology,"  In a cascading power transmission outage, component outages propagate non-locally, after one component outages, the next failure may be very distant, both topologically and geographically. As a result, simple models of topological contagion do not accurately represent the propagation of cascades in power systems. However, cascading power outages do follow patterns, some of which are useful in understanding and reducing blackout risk. This paper describes a method by which the data from many cascading failure simulations can be transformed into a graph-based model of influences that provides actionable information about the many ways that cascades propagate in a particular system. The resulting ""influence graph"" model is Markovian, in that component outage probabilities depend only on the outages that occurred in the prior generation. To validate the model we compare the distribution of cascade sizes resulting from $n-2$ contingencies in a $2896$ branch test case to cascade sizes in the influence graph. The two distributions are remarkably similar. In addition, we derive an equation with which one can quickly identify modifications to the proposed system that will substantially reduce cascade propagation. With this equation one can quickly identify critical components that can be improved to substantially reduce the risk of large cascading blackouts. ",Physics - Physics and Society ; Computer Science - Systems and Control ; ,"Hines, Paul D. H. ; Dobson, Ian ; Rezaei, Pooya ; ","Cascading Power Outages Propagate Locally in an Influence Graph that is   not the Actual Grid Topology  In a cascading power transmission outage, component outages propagate non-locally, after one component outages, the next failure may be very distant, both topologically and geographically. As a result, simple models of topological contagion do not accurately represent the propagation of cascades in power systems. However, cascading power outages do follow patterns, some of which are useful in understanding and reducing blackout risk. This paper describes a method by which the data from many cascading failure simulations can be transformed into a graph-based model of influences that provides actionable information about the many ways that cascades propagate in a particular system. The resulting ""influence graph"" model is Markovian, in that component outage probabilities depend only on the outages that occurred in the prior generation. To validate the model we compare the distribution of cascade sizes resulting from $n-2$ contingencies in a $2896$ branch test case to cascade sizes in the influence graph. The two distributions are remarkably similar. In addition, we derive an equation with which one can quickly identify modifications to the proposed system that will substantially reduce cascade propagation. With this equation one can quickly identify critical components that can be improved to substantially reduce the risk of large cascading blackouts. ",cascade power outages propagate locally influence graph actual grid topology cascade power transmission outage component outages propagate non locally one component outages next failure may distant topologically geographically result simple model topological contagion accurately represent propagation cascade power systems however cascade power outages follow pattern useful understand reduce blackout risk paper describe method data many cascade failure simulations transform graph base model influence provide actionable information many ways cascade propagate particular system result influence graph model markovian component outage probabilities depend outages occur prior generation validate model compare distribution cascade size result contingencies branch test case cascade size influence graph two distributions remarkably similar addition derive equation one quickly identify modifications propose system substantially reduce cascade propagation equation one quickly identify critical components improve substantially reduce risk large cascade blackouts,130,4,1508.01775.txt
http://arxiv.org/abs/1508.01841,Hypergraph coloring up to condensation,"  Improving a result of Dyer, Frieze and Greenhill [Journal of Combinatorial Theory, Series B, 2015], we determine the $q$-colorability threshold in random $k$-uniform hypergraphs up to an additive error of $\ln 2+\varepsilon_q$, where $\lim_{q\to\infty}\varepsilon_q=0$. The new lower bound on the threshold matches the ""condensation phase transition"" predicted by statistical physics considerations [Krzakala et al., PNAS 2007]. ",Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Ayre, Peter ; Coja-Oghlan, Amin ; Greenhill, Catherine ; ","Hypergraph coloring up to condensation  Improving a result of Dyer, Frieze and Greenhill [Journal of Combinatorial Theory, Series B, 2015], we determine the $q$-colorability threshold in random $k$-uniform hypergraphs up to an additive error of $\ln 2+\varepsilon_q$, where $\lim_{q\to\infty}\varepsilon_q=0$. The new lower bound on the threshold matches the ""condensation phase transition"" predicted by statistical physics considerations [Krzakala et al., PNAS 2007]. ",hypergraph color condensation improve result dyer frieze greenhill journal combinatorial theory series determine colorability threshold random uniform hypergraphs additive error ln varepsilon lim infty varepsilon new lower bind threshold match condensation phase transition predict statistical physics considerations krzakala et al pnas,41,8,1508.01841.txt
http://arxiv.org/abs/1508.01950,Defending Against Stealthy Attacks on Multiple Nodes with Limited   Resources: A Game-Theoretic Analysis,"  Stealthy attacks are a major cyber-security threat. In practice, both attackers and defenders have resource constraints that could limit their capabilities. Hence, to develop robust defense strategies, a promising approach is to utilize game theory to understand the fundamental trade-offs involved. Previous works in this direction, however, mainly focus on the single-node case without considering strict resource constraints. In this paper, a game-theoretic model for protecting a system of multiple nodes against stealthy attacks is proposed. We consider the practical setting where the frequencies of both attack and defense are constrained by limited resources, and an asymmetric feedback structure where the attacker can fully observe the states of nodes while largely hiding its actions from the defender. We characterize the best response strategies for both attacker and defender in the space of both non-adaptive and adaptive strategies, and study the Nash Equilibria of the game. We further study a sequential game where the defender first announces its strategy and the attacker then responds accordingly, and design an algorithm that finds a nearly optimal strategy for the defender to commit to. ",Computer Science - Computer Science and Game Theory ; ,"Zhang, Ming ; Zheng, Zizhan ; Shroff, Ness B. ; ","Defending Against Stealthy Attacks on Multiple Nodes with Limited   Resources: A Game-Theoretic Analysis  Stealthy attacks are a major cyber-security threat. In practice, both attackers and defenders have resource constraints that could limit their capabilities. Hence, to develop robust defense strategies, a promising approach is to utilize game theory to understand the fundamental trade-offs involved. Previous works in this direction, however, mainly focus on the single-node case without considering strict resource constraints. In this paper, a game-theoretic model for protecting a system of multiple nodes against stealthy attacks is proposed. We consider the practical setting where the frequencies of both attack and defense are constrained by limited resources, and an asymmetric feedback structure where the attacker can fully observe the states of nodes while largely hiding its actions from the defender. We characterize the best response strategies for both attacker and defender in the space of both non-adaptive and adaptive strategies, and study the Nash Equilibria of the game. We further study a sequential game where the defender first announces its strategy and the attacker then responds accordingly, and design an algorithm that finds a nearly optimal strategy for the defender to commit to. ",defend stealthy attack multiple nod limit resources game theoretic analysis stealthy attack major cyber security threat practice attackers defenders resource constraints could limit capabilities hence develop robust defense strategies promise approach utilize game theory understand fundamental trade off involve previous work direction however mainly focus single node case without consider strict resource constraints paper game theoretic model protect system multiple nod stealthy attack propose consider practical set frequencies attack defense constrain limit resources asymmetric feedback structure attacker fully observe state nod largely hide action defender characterize best response strategies attacker defender space non adaptive adaptive strategies study nash equilibria game study sequential game defender first announce strategy attacker respond accordingly design algorithm find nearly optimal strategy defender commit,118,8,1508.01950.txt
http://arxiv.org/abs/1508.01993,Improving Decision Analytics with Deep Learning: The Case of Financial   Disclosures,"  Decision analytics commonly focuses on the text mining of financial news sources in order to provide managerial decision support and to predict stock market movements. Existing predictive frameworks almost exclusively apply traditional machine learning methods, whereas recent research indicates that traditional machine learning methods are not sufficiently capable of extracting suitable features and capturing the non-linear nature of complex tasks. As a remedy, novel deep learning models aim to overcome this issue by extending traditional neural network models with additional hidden layers. Indeed, deep learning has been shown to outperform traditional methods in terms of predictive performance. In this paper, we adapt the novel deep learning technique to financial decision support. In this instance, we aim to predict the direction of stock movements following financial disclosures. As a result, we show how deep learning can outperform the accuracy of random forests as a benchmark for machine learning by 5.66%. ",Statistics - Machine Learning ; Computer Science - Computation and Language ; Computer Science - Machine Learning ; ,"Feuerriegel, Stefan ; Fehrer, Ralph ; ","Improving Decision Analytics with Deep Learning: The Case of Financial   Disclosures  Decision analytics commonly focuses on the text mining of financial news sources in order to provide managerial decision support and to predict stock market movements. Existing predictive frameworks almost exclusively apply traditional machine learning methods, whereas recent research indicates that traditional machine learning methods are not sufficiently capable of extracting suitable features and capturing the non-linear nature of complex tasks. As a remedy, novel deep learning models aim to overcome this issue by extending traditional neural network models with additional hidden layers. Indeed, deep learning has been shown to outperform traditional methods in terms of predictive performance. In this paper, we adapt the novel deep learning technique to financial decision support. In this instance, we aim to predict the direction of stock movements following financial disclosures. As a result, we show how deep learning can outperform the accuracy of random forests as a benchmark for machine learning by 5.66%. ",improve decision analytics deep learn case financial disclosures decision analytics commonly focus text mine financial news source order provide managerial decision support predict stock market movements exist predictive frameworks almost exclusively apply traditional machine learn methods whereas recent research indicate traditional machine learn methods sufficiently capable extract suitable feature capture non linear nature complex task remedy novel deep learn model aim overcome issue extend traditional neural network model additional hide layer indeed deep learn show outperform traditional methods term predictive performance paper adapt novel deep learn technique financial decision support instance aim predict direction stock movements follow financial disclosures result show deep learn outperform accuracy random forest benchmark machine learn,110,10,1508.01993.txt
http://arxiv.org/abs/1508.02340,The Pontryagin Maximum Principle for Nonlinear Infinite Horizon Optimal   Control Problems with State Constraints,"  The famous proof of the Pontryagin maximum principle for control problems on a finite horizon bases on the needle variation technique, as well as the separability concept of cones created by disturbances of the trajectories. In this preprint, we develop this method for infinite horizon optimal control problems. The results are necessary conditions for a strong local minimizer in form of the Pontryagin maximum principle, Arrow type sufficiency conditions and the validity of diverse transversality conditions. ","Mathematics - Optimization and Control ; Computer Science - Systems and Control ; 34, 46, 49 ; ","Tauchnitz, Nico ; ","The Pontryagin Maximum Principle for Nonlinear Infinite Horizon Optimal   Control Problems with State Constraints  The famous proof of the Pontryagin maximum principle for control problems on a finite horizon bases on the needle variation technique, as well as the separability concept of cones created by disturbances of the trajectories. In this preprint, we develop this method for infinite horizon optimal control problems. The results are necessary conditions for a strong local minimizer in form of the Pontryagin maximum principle, Arrow type sufficiency conditions and the validity of diverse transversality conditions. ",pontryagin maximum principle nonlinear infinite horizon optimal control problems state constraints famous proof pontryagin maximum principle control problems finite horizon base needle variation technique well separability concept con create disturbances trajectories preprint develop method infinite horizon optimal control problems result necessary condition strong local minimizer form pontryagin maximum principle arrow type sufficiency condition validity diverse transversality condition,57,0,1508.02340.txt
http://arxiv.org/abs/1508.02521,Topology Control of wireless sensor network using Quantum Inspired   Genetic algorithm,"  In this work, an evolving Linked Quantum register has been introduced, which are group vector of binary pair of genes, which in its local proximity represent those nodes that will have high connectivity and keep the energy consumption at low, and which are taken into account for topology control. The register works in higher dimension. Here order-2 Quantum inspired genetic algorithm has been used and also higher order can be used to achieve greater versatility in topology control of nodes. Numerical result has been obtained, analysis is done as how the result has previously been obtained with Quantum genetic algorithm and results are compared too. For future work, factor is hinted which would exploit the algorithm to work in more computational intensive problem. ",Computer Science - Neural and Evolutionary Computing ; Computer Science - Networking and Internet Architecture ; ,"Ullah, Sajid ; Wahid, Mussarat ; ","Topology Control of wireless sensor network using Quantum Inspired   Genetic algorithm  In this work, an evolving Linked Quantum register has been introduced, which are group vector of binary pair of genes, which in its local proximity represent those nodes that will have high connectivity and keep the energy consumption at low, and which are taken into account for topology control. The register works in higher dimension. Here order-2 Quantum inspired genetic algorithm has been used and also higher order can be used to achieve greater versatility in topology control of nodes. Numerical result has been obtained, analysis is done as how the result has previously been obtained with Quantum genetic algorithm and results are compared too. For future work, factor is hinted which would exploit the algorithm to work in more computational intensive problem. ",topology control wireless sensor network use quantum inspire genetic algorithm work evolve link quantum register introduce group vector binary pair genes local proximity represent nod high connectivity keep energy consumption low take account topology control register work higher dimension order quantum inspire genetic algorithm use also higher order use achieve greater versatility topology control nod numerical result obtain analysis do result previously obtain quantum genetic algorithm result compare future work factor hint would exploit algorithm work computational intensive problem,79,4,1508.02521.txt
http://arxiv.org/abs/1508.02570,A Combinatorial Model of Interference in Frequency Hopping Schemes,"  In a frequency hopping (FH) scheme users communicate simultaneously using FH sequences defined on the same set of frequency channels. An FH sequence specifies the frequency channel to be used as communication progresses. Much of the research on the performance of FH schemes is based on either pairwise mutual interference or adversarial interference but not both. In this paper, we evaluate the performance of an FH scheme with respect to both group-wise mutual interference and adversarial interference (jamming), bearing in mind that more than two users may be transmitting simultaneously in the presence of a jammer. We establish a correspondence between a cover-free code and an FH scheme. This gives a lower bound on the transmission capacity. Furthermore, we specify a jammer model and consider what additional properties a cover-free code should have to resist the jammer. We demonstrate that a purely combinatorial approach is inadequate against such a jammer, but that with the use of pseudorandomness, we can have a system that has high throughput as well as security against jamming. ","Computer Science - Information Theory ; Mathematics - Combinatorics ; 94A05, 94A55, 94B60 ; ","Nyirenda, Mwawi M. ; Ng, Siaw-Lynn ; Martin, Keith M. ; ","A Combinatorial Model of Interference in Frequency Hopping Schemes  In a frequency hopping (FH) scheme users communicate simultaneously using FH sequences defined on the same set of frequency channels. An FH sequence specifies the frequency channel to be used as communication progresses. Much of the research on the performance of FH schemes is based on either pairwise mutual interference or adversarial interference but not both. In this paper, we evaluate the performance of an FH scheme with respect to both group-wise mutual interference and adversarial interference (jamming), bearing in mind that more than two users may be transmitting simultaneously in the presence of a jammer. We establish a correspondence between a cover-free code and an FH scheme. This gives a lower bound on the transmission capacity. Furthermore, we specify a jammer model and consider what additional properties a cover-free code should have to resist the jammer. We demonstrate that a purely combinatorial approach is inadequate against such a jammer, but that with the use of pseudorandomness, we can have a system that has high throughput as well as security against jamming. ",combinatorial model interference frequency hop scheme frequency hop fh scheme users communicate simultaneously use fh sequence define set frequency channel fh sequence specify frequency channel use communication progress much research performance fh scheme base either pairwise mutual interference adversarial interference paper evaluate performance fh scheme respect group wise mutual interference adversarial interference jam bear mind two users may transmit simultaneously presence jammer establish correspondence cover free code fh scheme give lower bind transmission capacity furthermore specify jammer model consider additional properties cover free code resist jammer demonstrate purely combinatorial approach inadequate jammer use pseudorandomness system high throughput well security jam,100,12,1508.02570.txt
http://arxiv.org/abs/1508.02759,Technical Note: Split Algorithm in O(n) for the Capacitated Vehicle   Routing Problem,"  The Split algorithm is an essential building block of route-first cluster-second heuristics and modern genetic algorithms for vehicle routing problems. The algorithm is used to partition a solution, represented as a giant tour without occurrences of the depot, into separate routes with minimum cost. As highlighted by the recent survey of [Prins, Lacomme and Prodhon, Transport Res. C (40), 179-200], no less than 70 recent articles use this technique. In the vehicle routing literature, Split is usually assimilated to the search for a shortest path in a directed acyclic graph $\mathcal{G}$ and solved in $O(nB)$ using Bellman's algorithm, where $n$ is the number of delivery points and $B$ is the average number of feasible routes that start with a given customer in the giant tour. Some linear-time algorithms are also known for this problem as a consequence of a Monge property of $\mathcal{G}$. In this article, we highlight a stronger property of this graph, leading to a simple alternative algorithm in $O(n)$. Experimentally, we observe that the approach is faster than the classical Split for problem instances of practical size. We also extend the method to deal with a limited fleet and soft capacity constraints. ",Computer Science - Data Structures and Algorithms ; ,"Vidal, Thibaut ; ","Technical Note: Split Algorithm in O(n) for the Capacitated Vehicle   Routing Problem  The Split algorithm is an essential building block of route-first cluster-second heuristics and modern genetic algorithms for vehicle routing problems. The algorithm is used to partition a solution, represented as a giant tour without occurrences of the depot, into separate routes with minimum cost. As highlighted by the recent survey of [Prins, Lacomme and Prodhon, Transport Res. C (40), 179-200], no less than 70 recent articles use this technique. In the vehicle routing literature, Split is usually assimilated to the search for a shortest path in a directed acyclic graph $\mathcal{G}$ and solved in $O(nB)$ using Bellman's algorithm, where $n$ is the number of delivery points and $B$ is the average number of feasible routes that start with a given customer in the giant tour. Some linear-time algorithms are also known for this problem as a consequence of a Monge property of $\mathcal{G}$. In this article, we highlight a stronger property of this graph, leading to a simple alternative algorithm in $O(n)$. Experimentally, we observe that the approach is faster than the classical Split for problem instances of practical size. We also extend the method to deal with a limited fleet and soft capacity constraints. ",technical note split algorithm capacitate vehicle rout problem split algorithm essential build block route first cluster second heuristics modern genetic algorithms vehicle rout problems algorithm use partition solution represent giant tour without occurrences depot separate rout minimum cost highlight recent survey prins lacomme prodhon transport res less recent article use technique vehicle rout literature split usually assimilate search shortest path direct acyclic graph mathcal solve nb use bellman algorithm number delivery point average number feasible rout start give customer giant tour linear time algorithms also know problem consequence monge property mathcal article highlight stronger property graph lead simple alternative algorithm experimentally observe approach faster classical split problem instance practical size also extend method deal limit fleet soft capacity constraints,119,4,1508.02759.txt
http://arxiv.org/abs/1508.02793,A generalized Goulden-Jackson cluster method and lattice path   enumeration,"  The Goulden-Jackson cluster method is a powerful tool for obtaining generating functions for counting words in a free monoid by occurrences of a set of subwords. We introduce a generalization of the cluster method for monoid networks, which generalize the combinatorial framework of free monoids. As a sample application of the generalized cluster method, we compute bivariate and multivariate generating functions counting Motzkin paths---both with height bounded and unbounded---by statistics corresponding to the number of occurrences of various subwords, yielding both closed-form and continued fraction formulae. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; Computer Science - Formal Languages and Automata Theory ; 05A15, 05A05, 05C50, 68R05 ; ","Zhuang, Yan ; ","A generalized Goulden-Jackson cluster method and lattice path   enumeration  The Goulden-Jackson cluster method is a powerful tool for obtaining generating functions for counting words in a free monoid by occurrences of a set of subwords. We introduce a generalization of the cluster method for monoid networks, which generalize the combinatorial framework of free monoids. As a sample application of the generalized cluster method, we compute bivariate and multivariate generating functions counting Motzkin paths---both with height bounded and unbounded---by statistics corresponding to the number of occurrences of various subwords, yielding both closed-form and continued fraction formulae. ",generalize goulden jackson cluster method lattice path enumeration goulden jackson cluster method powerful tool obtain generate function count word free monoid occurrences set subwords introduce generalization cluster method monoid network generalize combinatorial framework free monoids sample application generalize cluster method compute bivariate multivariate generate function count motzkin paths height bound unbounded statistics correspond number occurrences various subwords yield close form continue fraction formulae,63,14,1508.02793.txt
http://arxiv.org/abs/1508.03117,Optimized Projections for Compressed Sensing via Direct Mutual Coherence   Minimization,"  Compressed Sensing (CS) is a novel technique for simultaneous signal sampling and compression based on the existence of a sparse representation of signal and a projected dictionary $PD$, where $P\in\mathbb{R}^{m\times d}$ is the projection matrix and $D\in\mathbb{R}^{d\times n}$ is the dictionary. To exactly recover the signal with a small number of measurements $m$, the projected dictionary $PD$ is expected to be of low mutual coherence. Several previous methods attempt to find the projection $P$ such that the mutual coherence of $PD$ can be as low as possible. However, they do not minimize the mutual coherence directly and thus their methods are far from optimal. Also the solvers they used lack of the convergence guarantee and thus there has no guarantee on the quality of their obtained solutions. This work aims to address these issues. We propose to find an optimal projection by minimizing the mutual coherence of $PD$ directly. This leads to a nonconvex nonsmooth minimization problem. We then approximate it by smoothing and solve it by alternate minimization. We further prove the convergence of our algorithm. To the best of our knowledge, this is the first work which directly minimizes the mutual coherence of the projected dictionary with a convergence guarantee. Numerical experiments demonstrate that the proposed method can recover sparse signals better than existing methods. ",Computer Science - Information Theory ; Computer Science - Machine Learning ; ,"Lu, Canyi ; Li, Huan ; Lin, Zhouchen ; ","Optimized Projections for Compressed Sensing via Direct Mutual Coherence   Minimization  Compressed Sensing (CS) is a novel technique for simultaneous signal sampling and compression based on the existence of a sparse representation of signal and a projected dictionary $PD$, where $P\in\mathbb{R}^{m\times d}$ is the projection matrix and $D\in\mathbb{R}^{d\times n}$ is the dictionary. To exactly recover the signal with a small number of measurements $m$, the projected dictionary $PD$ is expected to be of low mutual coherence. Several previous methods attempt to find the projection $P$ such that the mutual coherence of $PD$ can be as low as possible. However, they do not minimize the mutual coherence directly and thus their methods are far from optimal. Also the solvers they used lack of the convergence guarantee and thus there has no guarantee on the quality of their obtained solutions. This work aims to address these issues. We propose to find an optimal projection by minimizing the mutual coherence of $PD$ directly. This leads to a nonconvex nonsmooth minimization problem. We then approximate it by smoothing and solve it by alternate minimization. We further prove the convergence of our algorithm. To the best of our knowledge, this is the first work which directly minimizes the mutual coherence of the projected dictionary with a convergence guarantee. Numerical experiments demonstrate that the proposed method can recover sparse signals better than existing methods. ",optimize projections compress sense via direct mutual coherence minimization compress sense cs novel technique simultaneous signal sample compression base existence sparse representation signal project dictionary pd mathbb time projection matrix mathbb time dictionary exactly recover signal small number measurements project dictionary pd expect low mutual coherence several previous methods attempt find projection mutual coherence pd low possible however minimize mutual coherence directly thus methods far optimal also solvers use lack convergence guarantee thus guarantee quality obtain solutions work aim address issue propose find optimal projection minimize mutual coherence pd directly lead nonconvex nonsmooth minimization problem approximate smooth solve alternate minimization prove convergence algorithm best knowledge first work directly minimize mutual coherence project dictionary convergence guarantee numerical experiment demonstrate propose method recover sparse signal better exist methods,126,9,1508.03117.txt
http://arxiv.org/abs/1508.03263,Logic Programming with Macro Connectives,"  Logic programming such as Prolog is often sequential and slow because each execution step processes only a single, $micro$ connective. To fix this problem, we propose to use $macro$ connectives as the means of improving both readability and performance. ",Computer Science - Programming Languages ; ,"Kwon, Keehang ; ","Logic Programming with Macro Connectives  Logic programming such as Prolog is often sequential and slow because each execution step processes only a single, $micro$ connective. To fix this problem, we propose to use $macro$ connectives as the means of improving both readability and performance. ",logic program macro connectives logic program prolog often sequential slow execution step process single micro connective fix problem propose use macro connectives mean improve readability performance,26,8,1508.03263.txt
http://arxiv.org/abs/1508.03269,A New Approach to an Old Problem: The Reconstruction of a Go Game   through a Series of Photographs,"  Given a series of photographs taken during a Go game, we describe the techniques we successfully employ for pinpointing the grid lines of the Go board and for tracking their small movements between consecutive photographs; then we discuss how to approximate the location and orientation of the observer's point of view, in order to compensate for projection effects. Finally we describe the different criteria that jointly form the algorithm for stones' detection, thus enabling us to automatically reconstruct the whole move sequence. ",Computer Science - Computer Vision and Pattern Recognition ; I.2.10 ; I.4.8 ; I.5.5 ; ,"Corsolini, Mario ; Carta, Andrea ; ","A New Approach to an Old Problem: The Reconstruction of a Go Game   through a Series of Photographs  Given a series of photographs taken during a Go game, we describe the techniques we successfully employ for pinpointing the grid lines of the Go board and for tracking their small movements between consecutive photographs; then we discuss how to approximate the location and orientation of the observer's point of view, in order to compensate for projection effects. Finally we describe the different criteria that jointly form the algorithm for stones' detection, thus enabling us to automatically reconstruct the whole move sequence. ",new approach old problem reconstruction go game series photograph give series photograph take go game describe techniques successfully employ pinpoint grid line go board track small movements consecutive photograph discuss approximate location orientation observer point view order compensate projection effect finally describe different criteria jointly form algorithm stone detection thus enable us automatically reconstruct whole move sequence,57,11,1508.03269.txt
http://arxiv.org/abs/1508.03401,Binary Compressive Sensing via Analog Fountain Coding,"  In this paper, a compressive sensing (CS) approach is proposed for sparse binary signals' compression and reconstruction based on analog fountain codes (AFCs). In the proposed scheme, referred to as the analog fountain compressive sensing (AFCS), each measurement is generated from a linear combination of L randomly selected binary signal elements with real weight coefficients. The weight coefficients are chosen from a finite weight set and L, called measurement degree, is obtained based on a predefined degree distribution function. We propose a simple verification based reconstruction algorithm for the AFCS in the noiseless case. The proposed verification based decoder is analyzed through SUM-OR tree analytical approach and an optimization problem is formulated to find the optimum measurement degree to minimize the number of measurements required for the reconstruction of binary sparse signals. We show that in the AFCS, the number of required measurements is of O(-n log(1-k/n)), where n is the signal length and k is the signal sparsity level. We then consider the signal reconstruction of AFCS in the presence of additive white Gaussian noise (AWGN) and the standard message passing decoder is then used for the signal recovery. Simulation results show that the AFCS can perfectly recover all non-zero elements of the sparse binary signal with a significantly reduced number of measurements, compared to the conventional binary CS and L1-minimization approaches in a wide range of signal to noise ratios (SNRs). Finally, we show a practical application of the AFCS for the sparse event detection in wireless sensor networks (WSNs), where the sensors' readings can be treated as measurements from the CS point of view. ",Computer Science - Information Theory ; ,"Shirvanimoghaddam, Mahyar ; Li, Yonghui ; Vucetic, Branka ; Yuan, Jinhong ; ","Binary Compressive Sensing via Analog Fountain Coding  In this paper, a compressive sensing (CS) approach is proposed for sparse binary signals' compression and reconstruction based on analog fountain codes (AFCs). In the proposed scheme, referred to as the analog fountain compressive sensing (AFCS), each measurement is generated from a linear combination of L randomly selected binary signal elements with real weight coefficients. The weight coefficients are chosen from a finite weight set and L, called measurement degree, is obtained based on a predefined degree distribution function. We propose a simple verification based reconstruction algorithm for the AFCS in the noiseless case. The proposed verification based decoder is analyzed through SUM-OR tree analytical approach and an optimization problem is formulated to find the optimum measurement degree to minimize the number of measurements required for the reconstruction of binary sparse signals. We show that in the AFCS, the number of required measurements is of O(-n log(1-k/n)), where n is the signal length and k is the signal sparsity level. We then consider the signal reconstruction of AFCS in the presence of additive white Gaussian noise (AWGN) and the standard message passing decoder is then used for the signal recovery. Simulation results show that the AFCS can perfectly recover all non-zero elements of the sparse binary signal with a significantly reduced number of measurements, compared to the conventional binary CS and L1-minimization approaches in a wide range of signal to noise ratios (SNRs). Finally, we show a practical application of the AFCS for the sparse event detection in wireless sensor networks (WSNs), where the sensors' readings can be treated as measurements from the CS point of view. ",binary compressive sense via analog fountain cod paper compressive sense cs approach propose sparse binary signal compression reconstruction base analog fountain cod afcs propose scheme refer analog fountain compressive sense afcs measurement generate linear combination randomly select binary signal elements real weight coefficients weight coefficients choose finite weight set call measurement degree obtain base predefined degree distribution function propose simple verification base reconstruction algorithm afcs noiseless case propose verification base decoder analyze sum tree analytical approach optimization problem formulate find optimum measurement degree minimize number measurements require reconstruction binary sparse signal show afcs number require measurements log signal length signal sparsity level consider signal reconstruction afcs presence additive white gaussian noise awgn standard message pass decoder use signal recovery simulation result show afcs perfectly recover non zero elements sparse binary signal significantly reduce number measurements compare conventional binary cs minimization approach wide range signal noise ratios snrs finally show practical application afcs sparse event detection wireless sensor network wsns sensors read treat measurements cs point view,166,9,1508.03401.txt
http://arxiv.org/abs/1508.03773,No acute tetrahedron is an 8-reptile,"  An $r$-gentiling is a dissection of a shape into $r \geq 2$ parts which are all similar to the original shape. An $r$-reptiling is an $r$-gentiling of which all parts are mutually congruent. This article shows that no acute tetrahedron is an $r$-gentile or $r$-reptile for any $r < 9$, by showing that no acute spherical diangle can be dissected into less than nine acute spherical triangles. ",Computer Science - Computational Geometry ; Mathematics - Metric Geometry ; ,"Haverkort, Herman ; ","No acute tetrahedron is an 8-reptile  An $r$-gentiling is a dissection of a shape into $r \geq 2$ parts which are all similar to the original shape. An $r$-reptiling is an $r$-gentiling of which all parts are mutually congruent. This article shows that no acute tetrahedron is an $r$-gentile or $r$-reptile for any $r < 9$, by showing that no acute spherical diangle can be dissected into less than nine acute spherical triangles. ",acute tetrahedron reptile gentiling dissection shape geq part similar original shape reptiling gentiling part mutually congruent article show acute tetrahedron gentile reptile show acute spherical diangle dissect less nine acute spherical triangles,32,4,1508.03773.txt
http://arxiv.org/abs/1508.03837,Incorporating User Interaction into Imperative Languages,"  In this paper, we present two new forms of the $write$ statement: one of the form $write(x);G$ where $G$ is a statement and the other of the form $write(x);D$ where $D$ is a module. The former is a generalization of traditional $write$ statement and is quite useful. The latter is useful for implementing interactive modules. ",Computer Science - Programming Languages ; ,"Kwon, Keehang ; ","Incorporating User Interaction into Imperative Languages  In this paper, we present two new forms of the $write$ statement: one of the form $write(x);G$ where $G$ is a statement and the other of the form $write(x);D$ where $D$ is a module. The former is a generalization of traditional $write$ statement and is quite useful. The latter is useful for implementing interactive modules. ",incorporate user interaction imperative languages paper present two new form write statement one form write statement form write module former generalization traditional write statement quite useful latter useful implement interactive modules,31,8,1508.03837.txt
http://arxiv.org/abs/1508.03878,A Pessimistic Approximation for the Fisher Information Measure,"  The problem of determining the intrinsic quality of a signal processing system with respect to the inference of an unknown deterministic parameter $\theta$ is considered. While the Fisher information measure $F(\theta)$ forms a classical tool for such a problem, direct computation of the information measure can become difficult in various situations. For the estimation theoretic performance analysis of nonlinear measurement systems, the form of the likelihood function can make the calculation of the information measure $F(\theta)$ challenging. In situations where no closed-form expression of the statistical system model is available, the analytical derivation of $F(\theta)$ is not possible at all. Based on the Cauchy-Schwarz inequality, we derive an alternative information measure $S(\theta)$. It provides a lower bound on the Fisher information $F(\theta)$ and has the property of being evaluated with the mean, the variance, the skewness and the kurtosis of the system model at hand. These entities usually exhibit good mathematical tractability or can be determined at low-complexity by real-world measurements in a calibrated setup. With various examples, we show that $S(\theta)$ provides a good conservative approximation for $F(\theta)$ and outline different estimation theoretic problems where the presented information bound turns out to be useful. ",Computer Science - Information Theory ; Electrical Engineering and Systems Science - Signal Processing ; ,"Stein, Manuel ; Nossek, Josef A. ; ","A Pessimistic Approximation for the Fisher Information Measure  The problem of determining the intrinsic quality of a signal processing system with respect to the inference of an unknown deterministic parameter $\theta$ is considered. While the Fisher information measure $F(\theta)$ forms a classical tool for such a problem, direct computation of the information measure can become difficult in various situations. For the estimation theoretic performance analysis of nonlinear measurement systems, the form of the likelihood function can make the calculation of the information measure $F(\theta)$ challenging. In situations where no closed-form expression of the statistical system model is available, the analytical derivation of $F(\theta)$ is not possible at all. Based on the Cauchy-Schwarz inequality, we derive an alternative information measure $S(\theta)$. It provides a lower bound on the Fisher information $F(\theta)$ and has the property of being evaluated with the mean, the variance, the skewness and the kurtosis of the system model at hand. These entities usually exhibit good mathematical tractability or can be determined at low-complexity by real-world measurements in a calibrated setup. With various examples, we show that $S(\theta)$ provides a good conservative approximation for $F(\theta)$ and outline different estimation theoretic problems where the presented information bound turns out to be useful. ",pessimistic approximation fisher information measure problem determine intrinsic quality signal process system respect inference unknown deterministic parameter theta consider fisher information measure theta form classical tool problem direct computation information measure become difficult various situations estimation theoretic performance analysis nonlinear measurement systems form likelihood function make calculation information measure theta challenge situations close form expression statistical system model available analytical derivation theta possible base cauchy schwarz inequality derive alternative information measure theta provide lower bind fisher information theta property evaluate mean variance skewness kurtosis system model hand entities usually exhibit good mathematical tractability determine low complexity real world measurements calibrate setup various examples show theta provide good conservative approximation theta outline different estimation theoretic problems present information bind turn useful,120,9,1508.03878.txt
http://arxiv.org/abs/1508.03891,REBA: A Refinement-Based Architecture for Knowledge Representation and   Reasoning in Robotics,"  This paper describes an architecture for robots that combines the complementary strengths of probabilistic graphical models and declarative programming to represent and reason with logic-based and probabilistic descriptions of uncertainty and domain knowledge. An action language is extended to support non-boolean fluents and non-deterministic causal laws. This action language is used to describe tightly-coupled transition diagrams at two levels of granularity, with a fine-resolution transition diagram defined as a refinement of a coarse-resolution transition diagram of the domain. The coarse-resolution system description, and a history that includes (prioritized) defaults, are translated into an Answer Set Prolog (ASP) program. For any given goal, inference in the ASP program provides a plan of abstract actions. To implement each such abstract action, the robot automatically zooms to the part of the fine-resolution transition diagram relevant to this action. A probabilistic representation of the uncertainty in sensing and actuation is then included in this zoomed fine-resolution system description, and used to construct a partially observable Markov decision process (POMDP). The policy obtained by solving the POMDP is invoked repeatedly to implement the abstract action as a sequence of concrete actions, with the corresponding observations being recorded in the coarse-resolution history and used for subsequent reasoning. The architecture is evaluated in simulation and on a mobile robot moving objects in an indoor domain, to show that it supports reasoning with violation of defaults, noisy observations and unreliable actions, in complex domains. ",Computer Science - Robotics ; Computer Science - Artificial Intelligence ; Computer Science - Logic in Computer Science ; ,"Sridharan, Mohan ; Gelfond, Michael ; Zhang, Shiqi ; Wyatt, Jeremy ; ","REBA: A Refinement-Based Architecture for Knowledge Representation and   Reasoning in Robotics  This paper describes an architecture for robots that combines the complementary strengths of probabilistic graphical models and declarative programming to represent and reason with logic-based and probabilistic descriptions of uncertainty and domain knowledge. An action language is extended to support non-boolean fluents and non-deterministic causal laws. This action language is used to describe tightly-coupled transition diagrams at two levels of granularity, with a fine-resolution transition diagram defined as a refinement of a coarse-resolution transition diagram of the domain. The coarse-resolution system description, and a history that includes (prioritized) defaults, are translated into an Answer Set Prolog (ASP) program. For any given goal, inference in the ASP program provides a plan of abstract actions. To implement each such abstract action, the robot automatically zooms to the part of the fine-resolution transition diagram relevant to this action. A probabilistic representation of the uncertainty in sensing and actuation is then included in this zoomed fine-resolution system description, and used to construct a partially observable Markov decision process (POMDP). The policy obtained by solving the POMDP is invoked repeatedly to implement the abstract action as a sequence of concrete actions, with the corresponding observations being recorded in the coarse-resolution history and used for subsequent reasoning. The architecture is evaluated in simulation and on a mobile robot moving objects in an indoor domain, to show that it supports reasoning with violation of defaults, noisy observations and unreliable actions, in complex domains. ",reba refinement base architecture knowledge representation reason robotics paper describe architecture robots combine complementary strengths probabilistic graphical model declarative program represent reason logic base probabilistic descriptions uncertainty domain knowledge action language extend support non boolean fluents non deterministic causal laws action language use describe tightly couple transition diagram two level granularity fine resolution transition diagram define refinement coarse resolution transition diagram domain coarse resolution system description history include prioritize default translate answer set prolog asp program give goal inference asp program provide plan abstract action implement abstract action robot automatically zoom part fine resolution transition diagram relevant action probabilistic representation uncertainty sense actuation include zoom fine resolution system description use construct partially observable markov decision process pomdp policy obtain solve pomdp invoke repeatedly implement abstract action sequence concrete action correspond observations record coarse resolution history use subsequent reason architecture evaluate simulation mobile robot move object indoor domain show support reason violation default noisy observations unreliable action complex domains,158,8,1508.03891.txt
http://arxiv.org/abs/1508.04095,Algorithmic Aspects of Optimal Channel Coding,"  A central question in information theory is to determine the maximum success probability that can be achieved in sending a fixed number of messages over a noisy channel. This was first studied in the pioneering work of Shannon who established a simple expression characterizing this quantity in the limit of multiple independent uses of the channel. Here we consider the general setting with only one use of the channel. We observe that the maximum success probability can be expressed as the maximum value of a submodular function. Using this connection, we establish the following results:   1. There is a simple greedy polynomial-time algorithm that computes a code achieving a (1-1/e)-approximation of the maximum success probability. Moreover, for this problem it is NP-hard to obtain an approximation ratio strictly better than (1-1/e).   2. Shared quantum entanglement between the sender and the receiver can increase the success probability by a factor of at most 1/(1-1/e). In addition, this factor is tight if one allows an arbitrary non-signaling box between the sender and the receiver.   3. We give tight bounds on the one-shot performance of the meta-converse of Polyanskiy-Poor-Verdu. ",Computer Science - Information Theory ; Computer Science - Data Structures and Algorithms ; Quantum Physics ; ,"Barman, Siddharth ; Fawzi, Omar ; ","Algorithmic Aspects of Optimal Channel Coding  A central question in information theory is to determine the maximum success probability that can be achieved in sending a fixed number of messages over a noisy channel. This was first studied in the pioneering work of Shannon who established a simple expression characterizing this quantity in the limit of multiple independent uses of the channel. Here we consider the general setting with only one use of the channel. We observe that the maximum success probability can be expressed as the maximum value of a submodular function. Using this connection, we establish the following results:   1. There is a simple greedy polynomial-time algorithm that computes a code achieving a (1-1/e)-approximation of the maximum success probability. Moreover, for this problem it is NP-hard to obtain an approximation ratio strictly better than (1-1/e).   2. Shared quantum entanglement between the sender and the receiver can increase the success probability by a factor of at most 1/(1-1/e). In addition, this factor is tight if one allows an arbitrary non-signaling box between the sender and the receiver.   3. We give tight bounds on the one-shot performance of the meta-converse of Polyanskiy-Poor-Verdu. ",algorithmic aspects optimal channel cod central question information theory determine maximum success probability achieve send fix number message noisy channel first study pioneer work shannon establish simple expression characterize quantity limit multiple independent use channel consider general set one use channel observe maximum success probability express maximum value submodular function use connection establish follow result simple greedy polynomial time algorithm compute code achieve approximation maximum success probability moreover problem np hard obtain approximation ratio strictly better share quantum entanglement sender receiver increase success probability factor addition factor tight one allow arbitrary non signal box sender receiver give tight bound one shoot performance meta converse polyanskiy poor verdu,107,12,1508.04095.txt
http://arxiv.org/abs/1508.04417,Social Influence in the Concurrent Diffusion of Information and   Behaviors in Online Social Networks,"  The emergence of online social networks has greatly facilitated the diffusion of information and behaviors. While the two diffusion processes are often intertwined, ""talking the talk"" does not necessarily mean ""walking the talk""--those who share information about an action may not actually participate in it. We do not know if the diffusion of information and behaviors are similar, or if social influence plays an equally important role in these processes. Integrating text mining, social network analyses, and survival analysis, this research examines the concurrent spread of information and behaviors related to the Ice Bucket Challenge on Twitter. We show that the two processes follow different patterns. Unilateral social influence contributes to the diffusion of information, but not to the diffusion of behaviors; bilateral influence conveyed via the communication process is a significant and positive predictor of the diffusion of behaviors, but not of information. These results have implications for theories of social influence, social networks, and contagion. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Zhao, Kang ; Wang, Shiyao ; Vasi, Ion B. ; Zhang, Qi ; ","Social Influence in the Concurrent Diffusion of Information and   Behaviors in Online Social Networks  The emergence of online social networks has greatly facilitated the diffusion of information and behaviors. While the two diffusion processes are often intertwined, ""talking the talk"" does not necessarily mean ""walking the talk""--those who share information about an action may not actually participate in it. We do not know if the diffusion of information and behaviors are similar, or if social influence plays an equally important role in these processes. Integrating text mining, social network analyses, and survival analysis, this research examines the concurrent spread of information and behaviors related to the Ice Bucket Challenge on Twitter. We show that the two processes follow different patterns. Unilateral social influence contributes to the diffusion of information, but not to the diffusion of behaviors; bilateral influence conveyed via the communication process is a significant and positive predictor of the diffusion of behaviors, but not of information. These results have implications for theories of social influence, social networks, and contagion. ",social influence concurrent diffusion information behaviors online social network emergence online social network greatly facilitate diffusion information behaviors two diffusion process often intertwine talk talk necessarily mean walk talk share information action may actually participate know diffusion information behaviors similar social influence play equally important role process integrate text mine social network analyse survival analysis research examine concurrent spread information behaviors relate ice bucket challenge twitter show two process follow different pattern unilateral social influence contribute diffusion information diffusion behaviors bilateral influence convey via communication process significant positive predictor diffusion behaviors information result implications theories social influence social network contagion,100,0,1508.04417.txt
http://arxiv.org/abs/1508.04596,Large scale three-dimensional topology optimisation of heat sinks cooled   by natural convection,"  This work presents the application of density-based topology optimisation to the design of three-dimensional heat sinks cooled by natural convection. The governing equations are the steady-state incompressible Navier-Stokes equations coupled to the thermal convection-diffusion equation through the Bousinessq approximation. The fully coupled non-linear multiphysics system is solved using stabilised trilinear equal-order finite elements in a parallel framework allowing for the optimisation of large scale problems with order of 40-330 million state degrees of freedom. The flow is assumed to be laminar and several optimised designs are presented for Grashof numbers between $10^3$ and $10^6$. Interestingly, it is observed that the number of branches in the optimised design increases with increasing Grashof numbers, which is opposite to two-dimensional optimised designs. ","Physics - Fluid Dynamics ; Computer Science - Computational Engineering, Finance, and Science ; ","Alexandersen, Joe ; Sigmund, Ole ; Aage, Niels ; ","Large scale three-dimensional topology optimisation of heat sinks cooled   by natural convection  This work presents the application of density-based topology optimisation to the design of three-dimensional heat sinks cooled by natural convection. The governing equations are the steady-state incompressible Navier-Stokes equations coupled to the thermal convection-diffusion equation through the Bousinessq approximation. The fully coupled non-linear multiphysics system is solved using stabilised trilinear equal-order finite elements in a parallel framework allowing for the optimisation of large scale problems with order of 40-330 million state degrees of freedom. The flow is assumed to be laminar and several optimised designs are presented for Grashof numbers between $10^3$ and $10^6$. Interestingly, it is observed that the number of branches in the optimised design increases with increasing Grashof numbers, which is opposite to two-dimensional optimised designs. ",large scale three dimensional topology optimisation heat sink cool natural convection work present application density base topology optimisation design three dimensional heat sink cool natural convection govern equations steady state incompressible navier stoke equations couple thermal convection diffusion equation bousinessq approximation fully couple non linear multiphysics system solve use stabilise trilinear equal order finite elements parallel framework allow optimisation large scale problems order million state degrees freedom flow assume laminar several optimise design present grashof number interestingly observe number branch optimise design increase increase grashof number opposite two dimensional optimise design,91,9,1508.04596.txt
http://arxiv.org/abs/1508.04606,Distributed Event-Triggered Control for Asymptotic Synchronization of   Dynamical Networks,"  This paper studies synchronization of dynamical networks with event-based communication. Firstly, two estimators are introduced into each node, one to estimate its own state, and the other to estimate the average state of its neighbours. Then, with these two estimators, a distributed event-triggering rule (ETR) with a dwell time is designed such that the network achieves synchronization asymptotically with no Zeno behaviours. The designed ETR only depends on the information that each node can obtain, and thus can be implemented in a decentralized way. ",Computer Science - Systems and Control ; Computer Science - Multiagent Systems ; Mathematics - Dynamical Systems ; ,"Liu, Tao ; Cao, Ming ; De Persis, Claudio ; Hendrickx, Julien M. ; ","Distributed Event-Triggered Control for Asymptotic Synchronization of   Dynamical Networks  This paper studies synchronization of dynamical networks with event-based communication. Firstly, two estimators are introduced into each node, one to estimate its own state, and the other to estimate the average state of its neighbours. Then, with these two estimators, a distributed event-triggering rule (ETR) with a dwell time is designed such that the network achieves synchronization asymptotically with no Zeno behaviours. The designed ETR only depends on the information that each node can obtain, and thus can be implemented in a decentralized way. ",distribute event trigger control asymptotic synchronization dynamical network paper study synchronization dynamical network event base communication firstly two estimators introduce node one estimate state estimate average state neighbour two estimators distribute event trigger rule etr dwell time design network achieve synchronization asymptotically zeno behaviours design etr depend information node obtain thus implement decentralize way,54,6,1508.04606.txt
http://arxiv.org/abs/1508.04720,Quickest Detection for Changes in Maximal kNN Coherence of Random   Matrices,"  This paper addresses the problem of quickest detection of a change in the maximal coherence between columns of a $n\times p$ random matrix based on a sequence of matrix observations having a single unknown change point. The random matrix is assumed to have identically distributed rows and the maximal coherence is defined as the largest of the $p \choose 2$ correlation coefficients associated with any row. Likewise, the $k$ nearest neighbor (kNN) coherence is defined as the $k$-th largest of these correlation coefficients. The forms of the pre- and post-change distributions of the observed matrices are assumed to belong to the family of elliptically contoured densities with sparse dispersion matrices but are otherwise unknown. A non-parametric stopping rule is proposed that is based on the maximal k-nearest neighbor sample coherence between columns of each observed random matrix. This is a summary statistic that is related to a test of the existence of a hub vertex in a sample correlation graph having a degree at least $k$. Performance bounds on the delay and false alarm performance of the proposed stopping rule are obtained in the purely high dimensional regime where $p\rightarrow \infty$ and $n$ is fixed. When the pre-change dispersion matrix is diagonal it is shown that, among all functions of the proposed summary statistic, the proposed stopping rule is asymptotically optimal under a minimax quickest change detection (QCD) model as the stopping threshold approaches infinity. The theory developed also applies to sequential hypothesis testing and fixed sample size tests. ",Mathematics - Statistics Theory ; Computer Science - Information Theory ; Mathematics - Probability ; ,"Banerjee, Taposh ; Firouzi, Hamed ; Hero III, Alfred O. ; ","Quickest Detection for Changes in Maximal kNN Coherence of Random   Matrices  This paper addresses the problem of quickest detection of a change in the maximal coherence between columns of a $n\times p$ random matrix based on a sequence of matrix observations having a single unknown change point. The random matrix is assumed to have identically distributed rows and the maximal coherence is defined as the largest of the $p \choose 2$ correlation coefficients associated with any row. Likewise, the $k$ nearest neighbor (kNN) coherence is defined as the $k$-th largest of these correlation coefficients. The forms of the pre- and post-change distributions of the observed matrices are assumed to belong to the family of elliptically contoured densities with sparse dispersion matrices but are otherwise unknown. A non-parametric stopping rule is proposed that is based on the maximal k-nearest neighbor sample coherence between columns of each observed random matrix. This is a summary statistic that is related to a test of the existence of a hub vertex in a sample correlation graph having a degree at least $k$. Performance bounds on the delay and false alarm performance of the proposed stopping rule are obtained in the purely high dimensional regime where $p\rightarrow \infty$ and $n$ is fixed. When the pre-change dispersion matrix is diagonal it is shown that, among all functions of the proposed summary statistic, the proposed stopping rule is asymptotically optimal under a minimax quickest change detection (QCD) model as the stopping threshold approaches infinity. The theory developed also applies to sequential hypothesis testing and fixed sample size tests. ",quickest detection change maximal knn coherence random matrices paper address problem quickest detection change maximal coherence columns time random matrix base sequence matrix observations single unknown change point random matrix assume identically distribute row maximal coherence define largest choose correlation coefficients associate row likewise nearest neighbor knn coherence define th largest correlation coefficients form pre post change distributions observe matrices assume belong family elliptically contour densities sparse dispersion matrices otherwise unknown non parametric stop rule propose base maximal nearest neighbor sample coherence columns observe random matrix summary statistic relate test existence hub vertex sample correlation graph degree least performance bound delay false alarm performance propose stop rule obtain purely high dimensional regime rightarrow infty fix pre change dispersion matrix diagonal show among function propose summary statistic propose stop rule asymptotically optimal minimax quickest change detection qcd model stop threshold approach infinity theory develop also apply sequential hypothesis test fix sample size test,152,12,1508.04720.txt
http://arxiv.org/abs/1508.05117,The backtracking survey propagation algorithm for solving random K-SAT   problems,"  Discrete combinatorial optimization has a central role in many scientific disciplines, however, for hard problems we lack linear time algorithms that would allow us to solve very large instances. Moreover, it is still unclear what are the key features that make a discrete combinatorial optimization problem hard to solve. Here we study random K-satisfiability problems with $K=3,4$, which are known to be very hard close to the SAT-UNSAT threshold, where problems stop having solutions. We show that the backtracking survey propagation algorithm, in a time practically linear in the problem size, is able to find solutions very close to the threshold, in a region unreachable by any other algorithm. All solutions found have no frozen variables, thus supporting the conjecture that only unfrozen solutions can be found in linear time, and that a problem becomes impossible to solve in linear time when all solutions contain frozen variables. ",Computer Science - Computational Complexity ; Computer Science - Artificial Intelligence ; Computer Science - Data Structures and Algorithms ; ,"Marino, Raffaele ; Parisi, Giorgio ; Ricci-Tersenghi, Federico ; ","The backtracking survey propagation algorithm for solving random K-SAT   problems  Discrete combinatorial optimization has a central role in many scientific disciplines, however, for hard problems we lack linear time algorithms that would allow us to solve very large instances. Moreover, it is still unclear what are the key features that make a discrete combinatorial optimization problem hard to solve. Here we study random K-satisfiability problems with $K=3,4$, which are known to be very hard close to the SAT-UNSAT threshold, where problems stop having solutions. We show that the backtracking survey propagation algorithm, in a time practically linear in the problem size, is able to find solutions very close to the threshold, in a region unreachable by any other algorithm. All solutions found have no frozen variables, thus supporting the conjecture that only unfrozen solutions can be found in linear time, and that a problem becomes impossible to solve in linear time when all solutions contain frozen variables. ",backtrack survey propagation algorithm solve random sit problems discrete combinatorial optimization central role many scientific discipline however hard problems lack linear time algorithms would allow us solve large instance moreover still unclear key feature make discrete combinatorial optimization problem hard solve study random satisfiability problems know hard close sit unsat threshold problems stop solutions show backtrack survey propagation algorithm time practically linear problem size able find solutions close threshold region unreachable algorithm solutions find freeze variables thus support conjecture unfreeze solutions find linear time problem become impossible solve linear time solutions contain freeze variables,94,1,1508.05117.txt
http://arxiv.org/abs/1508.05232,Variable-mixing parameter quantized kernel robust mixed-norm algorithms   for combating impulsive interference,"  Although the kernel robust mixed-norm (KRMN) algorithm outperforms the kernel least mean square (KLMS) algorithm in impulsive noise, it still has two major problems as follows: (1) The choice of the mixing parameter in the KRMN is crucial to obtain satisfactory performance. (2) The structure of the KRMN algorithm grows linearly as the iteration goes on, thus it has high computational complexity and memory requirements. To solve the parameter selection problem, two variable-mixing parameter KRMN (VPKRMN) algorithms are developed in this paper. Moreover, a sparsification algorithm, quantized VPKRMN (QVPKRMN) algorithm is introduced for nonlinear system identification with impulsive interferences. The energy conservation relation (ECR) and convergence property of the QVPKRMN algorithm are analyzed. Simulation results in the context of nonlinear system identification under impulsive interference demonstrate the superior performance of the proposed VPKRMN and QVPKRMN algorithms as compared with the existing algorithms. ",Computer Science - Systems and Control ; ,"Lu, Lu ; Zhao, Haiquan ; Chen, Badong ; ","Variable-mixing parameter quantized kernel robust mixed-norm algorithms   for combating impulsive interference  Although the kernel robust mixed-norm (KRMN) algorithm outperforms the kernel least mean square (KLMS) algorithm in impulsive noise, it still has two major problems as follows: (1) The choice of the mixing parameter in the KRMN is crucial to obtain satisfactory performance. (2) The structure of the KRMN algorithm grows linearly as the iteration goes on, thus it has high computational complexity and memory requirements. To solve the parameter selection problem, two variable-mixing parameter KRMN (VPKRMN) algorithms are developed in this paper. Moreover, a sparsification algorithm, quantized VPKRMN (QVPKRMN) algorithm is introduced for nonlinear system identification with impulsive interferences. The energy conservation relation (ECR) and convergence property of the QVPKRMN algorithm are analyzed. Simulation results in the context of nonlinear system identification under impulsive interference demonstrate the superior performance of the proposed VPKRMN and QVPKRMN algorithms as compared with the existing algorithms. ",variable mix parameter quantize kernel robust mix norm algorithms combat impulsive interference although kernel robust mix norm krmn algorithm outperform kernel least mean square klms algorithm impulsive noise still two major problems follow choice mix parameter krmn crucial obtain satisfactory performance structure krmn algorithm grow linearly iteration go thus high computational complexity memory requirements solve parameter selection problem two variable mix parameter krmn vpkrmn algorithms develop paper moreover sparsification algorithm quantize vpkrmn qvpkrmn algorithm introduce nonlinear system identification impulsive interferences energy conservation relation ecr convergence property qvpkrmn algorithm analyze simulation result context nonlinear system identification impulsive interference demonstrate superior performance propose vpkrmn qvpkrmn algorithms compare exist algorithms,107,11,1508.05232.txt
http://arxiv.org/abs/1508.05465,A representation of antimatroids by Horn rules and its application to   educational systems,"  We study a representation of an antimatroid by Horn rules, motivated by its recent application to computer-aided educational systems. We associate any set $\mathcal{R}$ of Horn rules with the unique maximal antimatroid $\mathcal{A}(\mathcal{R})$ that is contained in the union-closed family $\mathcal{K}(\mathcal{R})$ naturally determined by ${\cal R}$. We address algorithmic and Boolean function theoretic aspects on the association ${\cal R} \mapsto \mathcal{A}(\mathcal{R})$, where ${\cal R}$ is viewed as the input. We present linear time algorithms to solve the membership problem and the inference problem for ${\cal A}({\cal R})$. We also provide efficient algorithms for generating all members and all implicates of ${\cal A}({\cal R})$. We show that this representation is essentially equivalent to the Korte-Lov\'{a}sz representation of antimatroids by rooted sets. Based on the equivalence, we provide a quadratic time algorithm to construct the uniquely-determined minimal representation. % These results have potential applications to computer-aided educational systems, where an antimatroid is used as a model of the space of possible knowledge states of learners, and is constructed by giving Horn queries to a human expert. ",Mathematics - Combinatorics ; Computer Science - Logic in Computer Science ; ,"Yoshikawa, Hiyori ; Hirai, Hiroshi ; Makino, Kazuhisa ; ","A representation of antimatroids by Horn rules and its application to   educational systems  We study a representation of an antimatroid by Horn rules, motivated by its recent application to computer-aided educational systems. We associate any set $\mathcal{R}$ of Horn rules with the unique maximal antimatroid $\mathcal{A}(\mathcal{R})$ that is contained in the union-closed family $\mathcal{K}(\mathcal{R})$ naturally determined by ${\cal R}$. We address algorithmic and Boolean function theoretic aspects on the association ${\cal R} \mapsto \mathcal{A}(\mathcal{R})$, where ${\cal R}$ is viewed as the input. We present linear time algorithms to solve the membership problem and the inference problem for ${\cal A}({\cal R})$. We also provide efficient algorithms for generating all members and all implicates of ${\cal A}({\cal R})$. We show that this representation is essentially equivalent to the Korte-Lov\'{a}sz representation of antimatroids by rooted sets. Based on the equivalence, we provide a quadratic time algorithm to construct the uniquely-determined minimal representation. % These results have potential applications to computer-aided educational systems, where an antimatroid is used as a model of the space of possible knowledge states of learners, and is constructed by giving Horn queries to a human expert. ",representation antimatroids horn rule application educational systems study representation antimatroid horn rule motivate recent application computer aid educational systems associate set mathcal horn rule unique maximal antimatroid mathcal mathcal contain union close family mathcal mathcal naturally determine cal address algorithmic boolean function theoretic aspects association cal mapsto mathcal mathcal cal view input present linear time algorithms solve membership problem inference problem cal cal also provide efficient algorithms generate members implicate cal cal show representation essentially equivalent korte lov sz representation antimatroids root set base equivalence provide quadratic time algorithm construct uniquely determine minimal representation result potential applications computer aid educational systems antimatroid use model space possible knowledge state learners construct give horn query human expert,115,7,1508.05465.txt
http://arxiv.org/abs/1508.05559,Structured Interactive Music Scores,"  Interactive Scores is a formalism for the design and performance of interactive scenarios that provides temporal relations (TRs) among the objects of the scenario. We can model TRs among objects in Time Stream Petri nets, but it is difficult to represent global constraints. This can be done explicitly in the Non-deterministic Timed Concurrent Constraint (ntcc) calculus. We want to formalize a heterogeneous system that controls in one subsystem the concurrent execution of the objects using ntcc, and audio and video processing in the other. We also plan to develop an automatic verifier for ntcc. ",Computer Science - Logic in Computer Science ; D.1.6 ; F.4.1 ; ,"Toro, Mauricio ; ","Structured Interactive Music Scores  Interactive Scores is a formalism for the design and performance of interactive scenarios that provides temporal relations (TRs) among the objects of the scenario. We can model TRs among objects in Time Stream Petri nets, but it is difficult to represent global constraints. This can be done explicitly in the Non-deterministic Timed Concurrent Constraint (ntcc) calculus. We want to formalize a heterogeneous system that controls in one subsystem the concurrent execution of the objects using ntcc, and audio and video processing in the other. We also plan to develop an automatic verifier for ntcc. ",structure interactive music score interactive score formalism design performance interactive scenarios provide temporal relations trs among object scenario model trs among object time stream petri net difficult represent global constraints do explicitly non deterministic time concurrent constraint ntcc calculus want formalize heterogeneous system control one subsystem concurrent execution object use ntcc audio video process also plan develop automatic verifier ntcc,60,11,1508.05559.txt
http://arxiv.org/abs/1508.05766,$n$-permutability and linear Datalog implies symmetric Datalog,"  We show that if $\mathbb A$ is a core relational structure such that CSP($\mathbb A$) can be solved by a linear Datalog program, and $\mathbb A$ is $n$-permutable for some $n$, then CSP($\mathbb A$) can be solved by a symmetric Datalog program (and thus CSP($\mathbb A$) lies in deterministic logspace). At the moment, it is not known for which structures $\mathbb A$ will CSP($\mathbb A$) be solvable by a linear Datalog program. However, once somebody obtains a characterization of linear Datalog, our result immediately gives a characterization of symmetric Datalog. ","Computer Science - Computational Complexity ; 68Q17, 68R05, 03C05 ; ","Kazda, Alexandr ; ","$n$-permutability and linear Datalog implies symmetric Datalog  We show that if $\mathbb A$ is a core relational structure such that CSP($\mathbb A$) can be solved by a linear Datalog program, and $\mathbb A$ is $n$-permutable for some $n$, then CSP($\mathbb A$) can be solved by a symmetric Datalog program (and thus CSP($\mathbb A$) lies in deterministic logspace). At the moment, it is not known for which structures $\mathbb A$ will CSP($\mathbb A$) be solvable by a linear Datalog program. However, once somebody obtains a characterization of linear Datalog, our result immediately gives a characterization of symmetric Datalog. ",permutability linear datalog imply symmetric datalog show mathbb core relational structure csp mathbb solve linear datalog program mathbb permutable csp mathbb solve symmetric datalog program thus csp mathbb lie deterministic logspace moment know structure mathbb csp mathbb solvable linear datalog program however somebody obtain characterization linear datalog result immediately give characterization symmetric datalog,53,8,1508.05766.txt
http://arxiv.org/abs/1508.06269,A systematic process for evaluating structured perfect Bayesian   equilibria in dynamic games with asymmetric information,"  We consider finite-horizon and infinite-horizon versions of a dynamic game with $N$ selfish players who observe their types privately and take actions that are publicly observed. Players' types evolve as conditionally independent Markov processes, conditioned on their current actions. Their actions and types jointly determine their instantaneous rewards. In dynamic games with asymmetric information, a widely used concept of equilibrium is perfect Bayesian equilibrium (PBE), which consists of a strategy and belief pair that simultaneously satisfy sequential rationality and belief consistency. In general, there does not exist a universal algorithm that decouples the interdependence of strategies and beliefs over time in calculating PBE. In this paper, for the finite-horizon game with independent types we develop a two-step backward-forward recursive algorithm that sequentially decomposes the problem (w.r.t. time) to obtain a subset of PBEs, which we refer to as structured Bayesian perfect equilibria (SPBE). In such equilibria, a player's strategy depends on its history only through a common public belief and its current private type. The backward recursive part of this algorithm defines an equilibrium generating function. Each period in the backward recursion involves solving a fixed-point equation on the space of probability simplexes for every possible belief on types. Using this function, equilibrium strategies and beliefs are generated through a forward recursion. We then extend this methodology to the infinite-horizon model, where we propose a time-invariant single-shot fixed-point equation, which in conjunction with a forward recursive step, generates the SPBE. Sufficient conditions for the existence of SPBE are provided. With our proposed method, we find equilibria that exhibit signaling behavior. This is illustrated with the help of a concrete public goods example. ",Mathematics - Optimization and Control ; Computer Science - Computer Science and Game Theory ; Computer Science - Systems and Control ; ,"Vasal, Deepanshu ; Sinha, Abhinav ; Anastasopoulos, Achilleas ; ","A systematic process for evaluating structured perfect Bayesian   equilibria in dynamic games with asymmetric information  We consider finite-horizon and infinite-horizon versions of a dynamic game with $N$ selfish players who observe their types privately and take actions that are publicly observed. Players' types evolve as conditionally independent Markov processes, conditioned on their current actions. Their actions and types jointly determine their instantaneous rewards. In dynamic games with asymmetric information, a widely used concept of equilibrium is perfect Bayesian equilibrium (PBE), which consists of a strategy and belief pair that simultaneously satisfy sequential rationality and belief consistency. In general, there does not exist a universal algorithm that decouples the interdependence of strategies and beliefs over time in calculating PBE. In this paper, for the finite-horizon game with independent types we develop a two-step backward-forward recursive algorithm that sequentially decomposes the problem (w.r.t. time) to obtain a subset of PBEs, which we refer to as structured Bayesian perfect equilibria (SPBE). In such equilibria, a player's strategy depends on its history only through a common public belief and its current private type. The backward recursive part of this algorithm defines an equilibrium generating function. Each period in the backward recursion involves solving a fixed-point equation on the space of probability simplexes for every possible belief on types. Using this function, equilibrium strategies and beliefs are generated through a forward recursion. We then extend this methodology to the infinite-horizon model, where we propose a time-invariant single-shot fixed-point equation, which in conjunction with a forward recursive step, generates the SPBE. Sufficient conditions for the existence of SPBE are provided. With our proposed method, we find equilibria that exhibit signaling behavior. This is illustrated with the help of a concrete public goods example. ",systematic process evaluate structure perfect bayesian equilibria dynamic game asymmetric information consider finite horizon infinite horizon versions dynamic game selfish players observe type privately take action publicly observe players type evolve conditionally independent markov process condition current action action type jointly determine instantaneous reward dynamic game asymmetric information widely use concept equilibrium perfect bayesian equilibrium pbe consist strategy belief pair simultaneously satisfy sequential rationality belief consistency general exist universal algorithm decouple interdependence strategies beliefs time calculate pbe paper finite horizon game independent type develop two step backward forward recursive algorithm sequentially decompose problem time obtain subset pbes refer structure bayesian perfect equilibria spbe equilibria player strategy depend history common public belief current private type backward recursive part algorithm define equilibrium generate function period backward recursion involve solve fix point equation space probability simplexes every possible belief type use function equilibrium strategies beliefs generate forward recursion extend methodology infinite horizon model propose time invariant single shoot fix point equation conjunction forward recursive step generate spbe sufficient condition existence spbe provide propose method find equilibria exhibit signal behavior illustrate help concrete public goods example,182,8,1508.06269.txt
http://arxiv.org/abs/1508.06464,SPF-CellTracker: Tracking multiple cells with strongly-correlated moves   using a spatial particle filter,"  Tracking many cells in time-lapse 3D image sequences is an important challenging task of bioimage informatics. Motivated by a study of brain-wide 4D imaging of neural activity in C. elegans, we present a new method of multi-cell tracking. Data types to which the method is applicable are characterized as follows: (i) cells are imaged as globular-like objects, (ii) it is difficult to distinguish cells based only on shape and size, (iii) the number of imaged cells ranges in several hundreds, (iv) moves of nearly-located cells are strongly correlated and (v) cells do not divide. We developed a tracking software suite which we call SPF-CellTracker. Incorporating dependency on cells' moves into prediction model is the key to reduce the tracking errors: cell-switching and coalescence of tracked positions. We model target cells' correlated moves as a Markov random field and we also derive a fast computation algorithm, which we call spatial particle filter. With the live-imaging data of nuclei of C. elegans neurons in which approximately 120 nuclei of neurons are imaged, we demonstrate an advantage of the proposed method over the standard particle filter and a method developed by Tokunaga et al. (2014). ",Computer Science - Computer Vision and Pattern Recognition ; ,"Hirose, Osamu ; Kawaguchi, Shotaro ; Tokunaga, Terumasa ; Toyoshima, Yu ; Teramoto, Takayuki ; Kuge, Sayuri ; Ishihara, Takeshi ; Iino, Yuichi ; Yoshida, Ryo ; ","SPF-CellTracker: Tracking multiple cells with strongly-correlated moves   using a spatial particle filter  Tracking many cells in time-lapse 3D image sequences is an important challenging task of bioimage informatics. Motivated by a study of brain-wide 4D imaging of neural activity in C. elegans, we present a new method of multi-cell tracking. Data types to which the method is applicable are characterized as follows: (i) cells are imaged as globular-like objects, (ii) it is difficult to distinguish cells based only on shape and size, (iii) the number of imaged cells ranges in several hundreds, (iv) moves of nearly-located cells are strongly correlated and (v) cells do not divide. We developed a tracking software suite which we call SPF-CellTracker. Incorporating dependency on cells' moves into prediction model is the key to reduce the tracking errors: cell-switching and coalescence of tracked positions. We model target cells' correlated moves as a Markov random field and we also derive a fast computation algorithm, which we call spatial particle filter. With the live-imaging data of nuclei of C. elegans neurons in which approximately 120 nuclei of neurons are imaged, we demonstrate an advantage of the proposed method over the standard particle filter and a method developed by Tokunaga et al. (2014). ",spf celltracker track multiple cells strongly correlate move use spatial particle filter track many cells time lapse image sequence important challenge task bioimage informatics motivate study brain wide image neural activity elegans present new method multi cell track data type method applicable characterize follow cells image globular like object ii difficult distinguish cells base shape size iii number image cells range several hundreds iv move nearly locate cells strongly correlate cells divide develop track software suite call spf celltracker incorporate dependency cells move prediction model key reduce track errors cell switch coalescence track position model target cells correlate move markov random field also derive fast computation algorithm call spatial particle filter live image data nuclei elegans neurons approximately nuclei neurons image demonstrate advantage propose method standard particle filter method develop tokunaga et al,133,2,1508.06464.txt
http://arxiv.org/abs/1508.06589,Cooperative Spectrum Sharing Relaying Protocols With Energy Harvesting   Cognitive User,"  The theory of wireless information and power transfer in energy constrained wireless networks has caught the interest of researchers due to its potential in increasing the lifetime of sensor nodes and mitigate the environment hazards caused by conventional cell batteries. Similarly, the advancements in areas of cooperative spectrum sharing protocols has enabled efficient use of frequency spectrum between a licensed primary user and a secondary user. In this paper, we consider an energy constrained secondary user which harvests energy from the primary signal and relays the primary signal in exchange for the spectrum access. We consider Nakagami-m fading model and propose two key protocols, namely time-splitting cooperative spectrum sharing (TS-CSS) and power-sharing cooperative spectrum sharing (PS-CSS), and derive expressions for the outage probabilities of the primary and secondary user in decode-forward and amplify-forward relaying modes. From the obtained results, it has been shown that the secondary user can carry its own transmission without adversely affecting the performance of the primary user and that PS-CSS protocol outperforms the TS-PSS protocol in terms of outage probability over a wide range of Signal to noise ratio(SNRs). The effect of various system parameters on the outage performance of these protocols have also been studied. ",Computer Science - Networking and Internet Architecture ; Computer Science - Information Theory ; ,"Kalluri, Tarun ; Peer, Mansi ; Bohara, Vivek Ashok ; da Costa, Daniel B. ; Dias, Ugo S. ; ","Cooperative Spectrum Sharing Relaying Protocols With Energy Harvesting   Cognitive User  The theory of wireless information and power transfer in energy constrained wireless networks has caught the interest of researchers due to its potential in increasing the lifetime of sensor nodes and mitigate the environment hazards caused by conventional cell batteries. Similarly, the advancements in areas of cooperative spectrum sharing protocols has enabled efficient use of frequency spectrum between a licensed primary user and a secondary user. In this paper, we consider an energy constrained secondary user which harvests energy from the primary signal and relays the primary signal in exchange for the spectrum access. We consider Nakagami-m fading model and propose two key protocols, namely time-splitting cooperative spectrum sharing (TS-CSS) and power-sharing cooperative spectrum sharing (PS-CSS), and derive expressions for the outage probabilities of the primary and secondary user in decode-forward and amplify-forward relaying modes. From the obtained results, it has been shown that the secondary user can carry its own transmission without adversely affecting the performance of the primary user and that PS-CSS protocol outperforms the TS-PSS protocol in terms of outage probability over a wide range of Signal to noise ratio(SNRs). The effect of various system parameters on the outage performance of these protocols have also been studied. ",cooperative spectrum share relay protocols energy harvest cognitive user theory wireless information power transfer energy constrain wireless network catch interest researchers due potential increase lifetime sensor nod mitigate environment hazard cause conventional cell batteries similarly advancements areas cooperative spectrum share protocols enable efficient use frequency spectrum license primary user secondary user paper consider energy constrain secondary user harvest energy primary signal relay primary signal exchange spectrum access consider nakagami fade model propose two key protocols namely time split cooperative spectrum share ts css power share cooperative spectrum share ps css derive expressions outage probabilities primary secondary user decode forward amplify forward relay modes obtain result show secondary user carry transmission without adversely affect performance primary user ps css protocol outperform ts pss protocol term outage probability wide range signal noise ratio snrs effect various system parameters outage performance protocols also study,141,12,1508.06589.txt
http://arxiv.org/abs/1508.07065,A dual descent algorithm for node-capacitated multiflow problems and its   applications,"  In this paper, we develop an $O((m \log k) {\rm MSF} (n,m,1))$-time algorithm to find a half-integral node-capacitated multiflow of the maximum total flow-value in a network with $n$ nodes, $m$ edges, and $k$ terminals, where ${\rm MSF} (n',m',\gamma)$ denotes the time complexity of solving the maximum submodular flow problem in a network with $n'$ nodes, $m'$ edges, and the complexity $\gamma$ of computing the exchange capacity of the submodular function describing the problem. By using Fujishige-Zhang algorithm for submodular flow, we can find a maximum half-integral multiflow in $O(m n^3 \log k)$ time. This is the first combinatorial strongly polynomial time algorithm for this problem. Our algorithm is built on a developing theory of discrete convex functions on certain graph structures. Applications include ""ellipsoid-free"" combinatorial implementations of a 2-approximation algorithm for the minimum node-multiway cut problem by Garg, Vazirani, and Yannakakis. ","Computer Science - Data Structures and Algorithms ; Mathematics - Optimization and Control ; 90C27, 05C21 ; ","Hirai, Hiroshi ; ","A dual descent algorithm for node-capacitated multiflow problems and its   applications  In this paper, we develop an $O((m \log k) {\rm MSF} (n,m,1))$-time algorithm to find a half-integral node-capacitated multiflow of the maximum total flow-value in a network with $n$ nodes, $m$ edges, and $k$ terminals, where ${\rm MSF} (n',m',\gamma)$ denotes the time complexity of solving the maximum submodular flow problem in a network with $n'$ nodes, $m'$ edges, and the complexity $\gamma$ of computing the exchange capacity of the submodular function describing the problem. By using Fujishige-Zhang algorithm for submodular flow, we can find a maximum half-integral multiflow in $O(m n^3 \log k)$ time. This is the first combinatorial strongly polynomial time algorithm for this problem. Our algorithm is built on a developing theory of discrete convex functions on certain graph structures. Applications include ""ellipsoid-free"" combinatorial implementations of a 2-approximation algorithm for the minimum node-multiway cut problem by Garg, Vazirani, and Yannakakis. ",dual descent algorithm node capacitate multiflow problems applications paper develop log rm msf time algorithm find half integral node capacitate multiflow maximum total flow value network nod edge terminals rm msf gamma denote time complexity solve maximum submodular flow problem network nod edge complexity gamma compute exchange capacity submodular function describe problem use fujishige zhang algorithm submodular flow find maximum half integral multiflow log time first combinatorial strongly polynomial time algorithm problem algorithm build develop theory discrete convex function certain graph structure applications include ellipsoid free combinatorial implementations approximation algorithm minimum node multiway cut problem garg vazirani yannakakis,98,1,1508.07065.txt
http://arxiv.org/abs/1508.07435,Subdifferential-based implicit return-mapping operators in Mohr-Coulomb   plasticity,"  The paper is devoted to a constitutive solution, limit load analysis and Newton-like methods in elastoplastic problems containing the Mohr-Coulomb yield criterion. Within the constitutive problem, we introduce a self-contained derivation of the implicit return-mapping solution scheme using a recent subdifferential-based treatment. Unlike conventional techniques based on Koiter's rules, the presented scheme a priori detects a position of the unknown stress tensor on the yield surface even if the constitutive solution cannot be found in closed form. This fact eliminates blind guesswork from the scheme, enables to analyze properties of the constitutive operator, and simplifies construction of the consistent tangent operator which is important for the semismooth Newton method applied on the incremental boundary value elastoplastic problem. The incremental problem in Mohr-Coulomb plasticity is combined with the limit load analysis. Beside a conventional direct method of the incremental limit analysis, a recent indirect one is introduced and its advantages are described. The paper contains 2D and 3D numerical experiments on slope stability with publicly available Matlab implementations. ","Computer Science - Computational Engineering, Finance, and Science ; ","Sysala, Stanislav ; Cermak, Martin ; ","Subdifferential-based implicit return-mapping operators in Mohr-Coulomb   plasticity  The paper is devoted to a constitutive solution, limit load analysis and Newton-like methods in elastoplastic problems containing the Mohr-Coulomb yield criterion. Within the constitutive problem, we introduce a self-contained derivation of the implicit return-mapping solution scheme using a recent subdifferential-based treatment. Unlike conventional techniques based on Koiter's rules, the presented scheme a priori detects a position of the unknown stress tensor on the yield surface even if the constitutive solution cannot be found in closed form. This fact eliminates blind guesswork from the scheme, enables to analyze properties of the constitutive operator, and simplifies construction of the consistent tangent operator which is important for the semismooth Newton method applied on the incremental boundary value elastoplastic problem. The incremental problem in Mohr-Coulomb plasticity is combined with the limit load analysis. Beside a conventional direct method of the incremental limit analysis, a recent indirect one is introduced and its advantages are described. The paper contains 2D and 3D numerical experiments on slope stability with publicly available Matlab implementations. ",subdifferential base implicit return map operators mohr coulomb plasticity paper devote constitutive solution limit load analysis newton like methods elastoplastic problems contain mohr coulomb yield criterion within constitutive problem introduce self contain derivation implicit return map solution scheme use recent subdifferential base treatment unlike conventional techniques base koiter rule present scheme priori detect position unknown stress tensor yield surface even constitutive solution cannot find close form fact eliminate blind guesswork scheme enable analyze properties constitutive operator simplify construction consistent tangent operator important semismooth newton method apply incremental boundary value elastoplastic problem incremental problem mohr coulomb plasticity combine limit load analysis beside conventional direct method incremental limit analysis recent indirect one introduce advantage describe paper contain numerical experiment slope stability publicly available matlab implementations,123,12,1508.07435.txt
http://arxiv.org/abs/1509.00144,Automatic Software Diversity in the Light of Test Suites,"  A few works address the challenge of automating software diversification, and they all share one core idea: using automated test suites to drive diversification. However, there is is lack of solid understanding of how test suites, programs and transformations interact one with another in this process. We explore this intricate interplay in the context of a specific diversification technique called ""sosiefication"". Sosiefication generates sosie programs, i.e., variants of a program in which some statements are deleted, added or replaced but still pass the test suite of the original program. Our investigation of the influence of test suites on sosiefication exploits the following observation: test suites cover the different regions of programs in very unequal ways. Hence, we hypothesize that sosie synthesis has different performances on a statement that is covered by one hundred test case and on a statement that is covered by a single test case. We synthesize 24583 sosies on 6 popular open-source Java programs. Our results show that there are two dimensions for diversification. The first one lies in the specification: the more test cases cover a statement, the more difficult it is to synthesize sosies. Yet, to our surprise, we are also able to synthesize sosies on highly tested statements (up to 600 test cases), which indicates an intrinsic property of the programs we study. The second dimension is in the code: we manually explore dozens of sosies and characterize new types of forgiving code regions that are prone to diversification. ",Computer Science - Software Engineering ; D.2.5 ; ,"Baudry, Benoit ; Allier, Simon ; Rodriguez-Cancio, Marcelino ; Monperrus, Martin ; ","Automatic Software Diversity in the Light of Test Suites  A few works address the challenge of automating software diversification, and they all share one core idea: using automated test suites to drive diversification. However, there is is lack of solid understanding of how test suites, programs and transformations interact one with another in this process. We explore this intricate interplay in the context of a specific diversification technique called ""sosiefication"". Sosiefication generates sosie programs, i.e., variants of a program in which some statements are deleted, added or replaced but still pass the test suite of the original program. Our investigation of the influence of test suites on sosiefication exploits the following observation: test suites cover the different regions of programs in very unequal ways. Hence, we hypothesize that sosie synthesis has different performances on a statement that is covered by one hundred test case and on a statement that is covered by a single test case. We synthesize 24583 sosies on 6 popular open-source Java programs. Our results show that there are two dimensions for diversification. The first one lies in the specification: the more test cases cover a statement, the more difficult it is to synthesize sosies. Yet, to our surprise, we are also able to synthesize sosies on highly tested statements (up to 600 test cases), which indicates an intrinsic property of the programs we study. The second dimension is in the code: we manually explore dozens of sosies and characterize new types of forgiving code regions that are prone to diversification. ",automatic software diversity light test suit work address challenge automate software diversification share one core idea use automate test suit drive diversification however lack solid understand test suit program transformations interact one another process explore intricate interplay context specific diversification technique call sosiefication sosiefication generate sosie program variants program statements delete add replace still pass test suite original program investigation influence test suit sosiefication exploit follow observation test suit cover different regions program unequal ways hence hypothesize sosie synthesis different performances statement cover one hundred test case statement cover single test case synthesize sosies popular open source java program result show two dimension diversification first one lie specification test case cover statement difficult synthesize sosies yet surprise also able synthesize sosies highly test statements test case indicate intrinsic property program study second dimension code manually explore dozens sosies characterize new type forgive code regions prone diversification,146,8,1509.00144.txt
http://arxiv.org/abs/1509.00773,A Big Data Analyzer for Large Trace Logs,"  Current generation of Internet-based services are typically hosted on large data centers that take the form of warehouse-size structures housing tens of thousands of servers. Continued availability of a modern data center is the result of a complex orchestration among many internal and external actors including computing hardware, multiple layers of intricate software, networking and storage devices, electrical power and cooling plants. During the course of their operation, many of these components produce large amounts of data in the form of event and error logs that are essential not only for identifying and resolving problems but also for improving data center efficiency and management. Most of these activities would benefit significantly from data analytics techniques to exploit hidden statistical patterns and correlations that may be present in the data. The sheer volume of data to be analyzed makes uncovering these correlations and patterns a challenging task. This paper presents BiDAl, a prototype Java tool for log-data analysis that incorporates several Big Data technologies in order to simplify the task of extracting information from data traces produced by large clusters and server farms. BiDAl provides the user with several analysis languages (SQL, R and Hadoop MapReduce) and storage backends (HDFS and SQLite) that can be freely mixed and matched so that a custom tool for a specific task can be easily constructed. BiDAl has a modular architecture so that it can be extended with other backends and analysis languages in the future. In this paper we present the design of BiDAl and describe our experience using it to analyze publicly-available traces from Google data clusters, with the goal of building a realistic model of a complex data center. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Balliu, Alkida ; Olivetti, Dennis ; Babaoglu, Ozalp ; Marzolla, Moreno ; Sîrbu, Alina ; ","A Big Data Analyzer for Large Trace Logs  Current generation of Internet-based services are typically hosted on large data centers that take the form of warehouse-size structures housing tens of thousands of servers. Continued availability of a modern data center is the result of a complex orchestration among many internal and external actors including computing hardware, multiple layers of intricate software, networking and storage devices, electrical power and cooling plants. During the course of their operation, many of these components produce large amounts of data in the form of event and error logs that are essential not only for identifying and resolving problems but also for improving data center efficiency and management. Most of these activities would benefit significantly from data analytics techniques to exploit hidden statistical patterns and correlations that may be present in the data. The sheer volume of data to be analyzed makes uncovering these correlations and patterns a challenging task. This paper presents BiDAl, a prototype Java tool for log-data analysis that incorporates several Big Data technologies in order to simplify the task of extracting information from data traces produced by large clusters and server farms. BiDAl provides the user with several analysis languages (SQL, R and Hadoop MapReduce) and storage backends (HDFS and SQLite) that can be freely mixed and matched so that a custom tool for a specific task can be easily constructed. BiDAl has a modular architecture so that it can be extended with other backends and analysis languages in the future. In this paper we present the design of BiDAl and describe our experience using it to analyze publicly-available traces from Google data clusters, with the goal of building a realistic model of a complex data center. ",big data analyzer large trace log current generation internet base service typically host large data center take form warehouse size structure house tens thousands servers continue availability modern data center result complex orchestration among many internal external actors include compute hardware multiple layer intricate software network storage devices electrical power cool plant course operation many components produce large amount data form event error log essential identify resolve problems also improve data center efficiency management activities would benefit significantly data analytics techniques exploit hide statistical pattern correlations may present data sheer volume data analyze make uncover correlations pattern challenge task paper present bidal prototype java tool log data analysis incorporate several big data technologies order simplify task extract information data trace produce large cluster server farm bidal provide user several analysis languages sql hadoop mapreduce storage backends hdfs sqlite freely mix match custom tool specific task easily construct bidal modular architecture extend backends analysis languages future paper present design bidal describe experience use analyze publicly available trace google data cluster goal build realistic model complex data center,176,10,1509.00773.txt
http://arxiv.org/abs/1509.00864,Strong Pseudoprimes to Twelve Prime Bases,  Let $\psi_m$ be the smallest strong pseudoprime to the first $m$ prime bases. This value is known for $1 \leq m \leq 11$. We extend this by finding $\psi_{12}$ and $\psi_{13}$. We also present an algorithm to find all integers $n\le B$ that are strong pseudoprimes to the first $m$ prime bases; with a reasonable heuristic assumption we can show that it takes at most $B^{2/3+o(1)}$ time. ,"Mathematics - Number Theory ; Computer Science - Data Structures and Algorithms ; Computer Science - Mathematical Software ; Primary 11Y16, 11Y16, Secondary 11A41, 68W40, 68W10 ; ","Sorenson, Jonathan P. ; Webster, Jonathan ; ",Strong Pseudoprimes to Twelve Prime Bases  Let $\psi_m$ be the smallest strong pseudoprime to the first $m$ prime bases. This value is known for $1 \leq m \leq 11$. We extend this by finding $\psi_{12}$ and $\psi_{13}$. We also present an algorithm to find all integers $n\le B$ that are strong pseudoprimes to the first $m$ prime bases; with a reasonable heuristic assumption we can show that it takes at most $B^{2/3+o(1)}$ time. ,strong pseudoprimes twelve prime base let psi smallest strong pseudoprime first prime base value know leq leq extend find psi psi also present algorithm find integers le strong pseudoprimes first prime base reasonable heuristic assumption show take time,38,4,1509.00864.txt
http://arxiv.org/abs/1509.00926,Using Inclusion Diagrams as an Alternative to Venn Diagrams to Determine   the Validity of Categorical Syllogisms,"  Inclusion diagrams are introduced as an alternative to using Venn diagrams to determine the validity of categorical syllogisms, and are used here for the analysis of diverse categorical syllogisms. As a preliminary example of a possible generalization of the use of inclusion diagrams, consideration is given also to an argument that includes more than two premises and more than three terms, the classic major, middle and minor terms in categorical syllogisms. ","Computer Science - Logic in Computer Science ; Mathematics - Logic ; 03B05, 03B10, 97E30, 00A66 ; ","Skliar, Osvaldo ; Monge, Ricardo E. ; Gapper, Sherry ; ","Using Inclusion Diagrams as an Alternative to Venn Diagrams to Determine   the Validity of Categorical Syllogisms  Inclusion diagrams are introduced as an alternative to using Venn diagrams to determine the validity of categorical syllogisms, and are used here for the analysis of diverse categorical syllogisms. As a preliminary example of a possible generalization of the use of inclusion diagrams, consideration is given also to an argument that includes more than two premises and more than three terms, the classic major, middle and minor terms in categorical syllogisms. ",use inclusion diagram alternative venn diagram determine validity categorical syllogisms inclusion diagram introduce alternative use venn diagram determine validity categorical syllogisms use analysis diverse categorical syllogisms preliminary example possible generalization use inclusion diagram consideration give also argument include two premise three term classic major middle minor term categorical syllogisms,49,4,1509.00926.txt
http://arxiv.org/abs/1509.01347,Verificarlo: checking floating point accuracy through Monte Carlo   Arithmetic,"  Numerical accuracy of floating point computation is a well studied topic which has not made its way to the end-user in scientific computing. Yet, it has become a critical issue with the recent requirements for code modernization to harness new highly parallel hardware and perform higher resolution computation. To democratize numerical accuracy analysis, it is important to propose tools and methodologies to study large use cases in a reliable and automatic way. In this paper, we propose verificarlo, an extension to the LLVM compiler to automatically use Monte Carlo Arithmetic in a transparent way for the end-user. It supports all the major languages including C, C++, and Fortran. Unlike source-to-source approaches, our implementation captures the influence of compiler optimizations on the numerical accuracy. We illustrate how Monte Carlo Arithmetic using the verificarlo tool outperforms the existing approaches on various use cases and is a step toward automatic numerical analysis. ",Computer Science - Mathematical Software ; Computer Science - Numerical Analysis ; ,"Denis, Christophe ; Castro, Pablo De Oliveira ; Petit, Eric ; ","Verificarlo: checking floating point accuracy through Monte Carlo   Arithmetic  Numerical accuracy of floating point computation is a well studied topic which has not made its way to the end-user in scientific computing. Yet, it has become a critical issue with the recent requirements for code modernization to harness new highly parallel hardware and perform higher resolution computation. To democratize numerical accuracy analysis, it is important to propose tools and methodologies to study large use cases in a reliable and automatic way. In this paper, we propose verificarlo, an extension to the LLVM compiler to automatically use Monte Carlo Arithmetic in a transparent way for the end-user. It supports all the major languages including C, C++, and Fortran. Unlike source-to-source approaches, our implementation captures the influence of compiler optimizations on the numerical accuracy. We illustrate how Monte Carlo Arithmetic using the verificarlo tool outperforms the existing approaches on various use cases and is a step toward automatic numerical analysis. ",verificarlo check float point accuracy monte carlo arithmetic numerical accuracy float point computation well study topic make way end user scientific compute yet become critical issue recent requirements code modernization harness new highly parallel hardware perform higher resolution computation democratize numerical accuracy analysis important propose tool methodologies study large use case reliable automatic way paper propose verificarlo extension llvm compiler automatically use monte carlo arithmetic transparent way end user support major languages include fortran unlike source source approach implementation capture influence compiler optimizations numerical accuracy illustrate monte carlo arithmetic use verificarlo tool outperform exist approach various use case step toward automatic numerical analysis,103,4,1509.01347.txt
http://arxiv.org/abs/1509.01676,Optimum Traffic Allocation in Bundled Energy Efficient Ethernet Links,"  The energy demands of Ethernet links have been an active focus of research in the recent years. This work has enabled a new generation of Energy Efficient Ethernet (EEE) interfaces able to adapt their power consumption to the actual traffic demands, thus yielding significant energy savings. With the energy consumption of single network connections being a solved problem, in this paper we focus on the energy demands of link aggregates that are commonly used to increase the capacity of a network connection. We build on known energy models of single EEE links to derive the energy demands of the whole aggregate as a function on how the traffic load is spread among its powered links. We then provide a practical method to share the load that minimizes overall energy consumption with controlled packet delay, and prove that it is valid for a wide range of EEE links. Finally, we validate our method with both synthetic and real traffic traces captured in Internet backbones. ",Computer Science - Networking and Internet Architecture ; ,"Pérez, Miguel Rodríguez ; Veiga, Manuel Fernández ; Alonso, Sergio Herrería ; Hmila, Mariem ; García, Cándido López ; ","Optimum Traffic Allocation in Bundled Energy Efficient Ethernet Links  The energy demands of Ethernet links have been an active focus of research in the recent years. This work has enabled a new generation of Energy Efficient Ethernet (EEE) interfaces able to adapt their power consumption to the actual traffic demands, thus yielding significant energy savings. With the energy consumption of single network connections being a solved problem, in this paper we focus on the energy demands of link aggregates that are commonly used to increase the capacity of a network connection. We build on known energy models of single EEE links to derive the energy demands of the whole aggregate as a function on how the traffic load is spread among its powered links. We then provide a practical method to share the load that minimizes overall energy consumption with controlled packet delay, and prove that it is valid for a wide range of EEE links. Finally, we validate our method with both synthetic and real traffic traces captured in Internet backbones. ",optimum traffic allocation bundle energy efficient ethernet link energy demand ethernet link active focus research recent years work enable new generation energy efficient ethernet eee interfaces able adapt power consumption actual traffic demand thus yield significant energy save energy consumption single network connections solve problem paper focus energy demand link aggregate commonly use increase capacity network connection build know energy model single eee link derive energy demand whole aggregate function traffic load spread among power link provide practical method share load minimize overall energy consumption control packet delay prove valid wide range eee link finally validate method synthetic real traffic trace capture internet backbones,104,0,1509.01676.txt
http://arxiv.org/abs/1509.01683,Inference From Visible Information And Background Knowledge,"  We provide a wide-ranging study of the scenario where a subset of the relations in a relational vocabulary are visible to a user --- that is, their complete contents are known --- while the remaining relations are invisible. We also have a background theory --- invariants given by logical sentences --- which may relate the visible relations to invisible ones, and also may constrain both the visible and invisible relations in isolation. We want to determine whether some other information, given as a positive existential formula, can be inferred using only the visible information and the background theory. This formula whose inference we are concered with is denoted as the \emph{query}. We consider whether positive information about the query can be inferred, and also whether negative information -- the sentence does not hold -- can be inferred. We further consider both the instance-level version of the problem, where both the query and the visible instance are given, and the schema-level version, where we want to know whether truth or falsity of the query can be inferred in some instance of the schema. ",Computer Science - Logic in Computer Science ; ,"Benedikt, Michael ; Bourhis, Pierre ; Cate, Balder ten ; Puppis, Gabriele ; Boom, Michael Vanden ; ","Inference From Visible Information And Background Knowledge  We provide a wide-ranging study of the scenario where a subset of the relations in a relational vocabulary are visible to a user --- that is, their complete contents are known --- while the remaining relations are invisible. We also have a background theory --- invariants given by logical sentences --- which may relate the visible relations to invisible ones, and also may constrain both the visible and invisible relations in isolation. We want to determine whether some other information, given as a positive existential formula, can be inferred using only the visible information and the background theory. This formula whose inference we are concered with is denoted as the \emph{query}. We consider whether positive information about the query can be inferred, and also whether negative information -- the sentence does not hold -- can be inferred. We further consider both the instance-level version of the problem, where both the query and the visible instance are given, and the schema-level version, where we want to know whether truth or falsity of the query can be inferred in some instance of the schema. ",inference visible information background knowledge provide wide range study scenario subset relations relational vocabulary visible user complete content know remain relations invisible also background theory invariants give logical sentence may relate visible relations invisible ones also may constrain visible invisible relations isolation want determine whether information give positive existential formula infer use visible information background theory formula whose inference concered denote emph query consider whether positive information query infer also whether negative information sentence hold infer consider instance level version problem query visible instance give schema level version want know whether truth falsity query infer instance schema,97,10,1509.01683.txt
http://arxiv.org/abs/1509.02223,Diffusion tensor imaging with deterministic error bounds,"  Errors in the data and the forward operator of an inverse problem can be handily modelled using partial order in Banach lattices. We present some existing results of the theory of regularisation in this novel framework, where errors are represented as bounds by means of the appropriate partial order.   We apply the theory to Diffusion Tensor Imaging, where correct noise modelling is challenging: it involves the Rician distribution and the nonlinear Stejskal-Tanner equation. Linearisation of the latter in the statistical framework would complicate the noise model even further. We avoid this using the error bounds approach, which preserves simple error structure under monotone transformations. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Numerical Analysis ; ,"Gorokh, Artur ; Korolev, Yury ; Valkonen, Tuomo ; ","Diffusion tensor imaging with deterministic error bounds  Errors in the data and the forward operator of an inverse problem can be handily modelled using partial order in Banach lattices. We present some existing results of the theory of regularisation in this novel framework, where errors are represented as bounds by means of the appropriate partial order.   We apply the theory to Diffusion Tensor Imaging, where correct noise modelling is challenging: it involves the Rician distribution and the nonlinear Stejskal-Tanner equation. Linearisation of the latter in the statistical framework would complicate the noise model even further. We avoid this using the error bounds approach, which preserves simple error structure under monotone transformations. ",diffusion tensor image deterministic error bound errors data forward operator inverse problem handily model use partial order banach lattices present exist result theory regularisation novel framework errors represent bound mean appropriate partial order apply theory diffusion tensor image correct noise model challenge involve rician distribution nonlinear stejskal tanner equation linearisation latter statistical framework would complicate noise model even avoid use error bound approach preserve simple error structure monotone transformations,69,11,1509.02223.txt
http://arxiv.org/abs/1509.02479,Hofstadter's problem for curious readers,"  This document summarizes the proofs made during a Coq development inSummer 2015. This development investigates the function G introducedby Hofstadter in his famous ""G{\""o}del, Escher, Bach"" bookas well as a related infinite tree. The left/right flipped variantof this G tree has also been studied here, followingHofstadter's ""problem for the curious reader"".The initial G function is refered as sequence A005206 inOEIS, while the flipped version is the sequence A123070. ",Computer Science - Logic in Computer Science ; Mathematics - History and Overview ; ,"Letouzey, Pierre ; ","Hofstadter's problem for curious readers  This document summarizes the proofs made during a Coq development inSummer 2015. This development investigates the function G introducedby Hofstadter in his famous ""G{\""o}del, Escher, Bach"" bookas well as a related infinite tree. The left/right flipped variantof this G tree has also been studied here, followingHofstadter's ""problem for the curious reader"".The initial G function is refered as sequence A005206 inOEIS, while the flipped version is the sequence A123070. ",hofstadter problem curious readers document summarize proof make coq development insummer development investigate function introducedby hofstadter famous del escher bach bookas well relate infinite tree leave right flip variantof tree also study followinghofstadter problem curious reader initial function refer sequence inoeis flip version sequence,44,7,1509.02479.txt
http://arxiv.org/abs/1509.02601,On the $O_\beta$-hull of a planar point set,"  We study the $O_\beta$-hull of a planar point set, a generalization of the Orthogonal Convex Hull where the coordinate axes form an angle $\beta$. Given a set $P$ of $n$ points in the plane, we show how to maintain the $O_\beta$-hull of $P$ while $\beta$ runs from $0$ to $\pi$ in $O(n \log n)$ time and $O(n)$ space. With the same complexity, we also find the values of $\beta$ that maximize the area and the perimeter of the $O_\beta$-hull and, furthermore, we find the value of $\beta$ achieving the best fitting of the point set $P$ with a two-joint chain of alternate interior angle $\beta$. ",Computer Science - Computational Geometry ; 68U05 ; I.3.5 ; ,"Alegría-Galicia, Carlos ; Orden, David ; Seara, Carlos ; Urrutia, Jorge ; ","On the $O_\beta$-hull of a planar point set  We study the $O_\beta$-hull of a planar point set, a generalization of the Orthogonal Convex Hull where the coordinate axes form an angle $\beta$. Given a set $P$ of $n$ points in the plane, we show how to maintain the $O_\beta$-hull of $P$ while $\beta$ runs from $0$ to $\pi$ in $O(n \log n)$ time and $O(n)$ space. With the same complexity, we also find the values of $\beta$ that maximize the area and the perimeter of the $O_\beta$-hull and, furthermore, we find the value of $\beta$ achieving the best fitting of the point set $P$ with a two-joint chain of alternate interior angle $\beta$. ",beta hull planar point set study beta hull planar point set generalization orthogonal convex hull coordinate ax form angle beta give set point plane show maintain beta hull beta run pi log time space complexity also find value beta maximize area perimeter beta hull furthermore find value beta achieve best fit point set two joint chain alternate interior angle beta,60,4,1509.02601.txt
http://arxiv.org/abs/1509.02709,A Topological Approach to Meta-heuristics: Analytical Results on the BFS   vs. DFS Algorithm Selection Problem,"  Search is a central problem in artificial intelligence, and breadth-first search (BFS) and depth-first search (DFS) are the two most fundamental ways to search. In this paper we derive estimates for average BFS and DFS runtime. The average runtime estimates can be used to allocate resources or judge the hardness of a problem. They can also be used for selecting the best graph representation, and for selecting the faster algorithm out of BFS and DFS. They may also form the basis for an analysis of more advanced search methods. The paper treats both tree search and graph search. For tree search, we employ a probabilistic model of goal distribution; for graph search, the analysis depends on an additional statistic of path redundancy and average branching factor. As an application, we use the results to predict BFS and DFS runtime on two concrete grammar problems and on the N-puzzle. Experimental verification shows that our analytical approximations come close to empirical reality. ",Computer Science - Artificial Intelligence ; I.2.8 ; ,"Everitt, Tom ; Hutter, Marcus ; ","A Topological Approach to Meta-heuristics: Analytical Results on the BFS   vs. DFS Algorithm Selection Problem  Search is a central problem in artificial intelligence, and breadth-first search (BFS) and depth-first search (DFS) are the two most fundamental ways to search. In this paper we derive estimates for average BFS and DFS runtime. The average runtime estimates can be used to allocate resources or judge the hardness of a problem. They can also be used for selecting the best graph representation, and for selecting the faster algorithm out of BFS and DFS. They may also form the basis for an analysis of more advanced search methods. The paper treats both tree search and graph search. For tree search, we employ a probabilistic model of goal distribution; for graph search, the analysis depends on an additional statistic of path redundancy and average branching factor. As an application, we use the results to predict BFS and DFS runtime on two concrete grammar problems and on the N-puzzle. Experimental verification shows that our analytical approximations come close to empirical reality. ",topological approach meta heuristics analytical result bfs vs dfs algorithm selection problem search central problem artificial intelligence breadth first search bfs depth first search dfs two fundamental ways search paper derive estimate average bfs dfs runtime average runtime estimate use allocate resources judge hardness problem also use select best graph representation select faster algorithm bfs dfs may also form basis analysis advance search methods paper treat tree search graph search tree search employ probabilistic model goal distribution graph search analysis depend additional statistic path redundancy average branch factor application use result predict bfs dfs runtime two concrete grammar problems puzzle experimental verification show analytical approximations come close empirical reality,109,11,1509.02709.txt
http://arxiv.org/abs/1509.02900,"Statistical Inference, Learning and Models in Big Data","  The need for new methods to deal with big data is a common theme in most scientific fields, although its definition tends to vary with the context. Statistical ideas are an essential part of this, and as a partial response, a thematic program on statistical inference, learning, and models in big data was held in 2015 in Canada, under the general direction of the Canadian Statistical Sciences Institute, with major funding from, and most activities located at, the Fields Institute for Research in Mathematical Sciences. This paper gives an overview of the topics covered, describing challenges and strategies that seem common to many different areas of application, and including some examples of applications to make these challenges and strategies more concrete. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; 62-07 ; I.2.6 ; I.2.3 ; I.5.1 ; G.3 ; ,"Franke, Beate ; Plante, Jean-François ; Roscher, Ribana ; Lee, Annie ; Smyth, Cathal ; Hatefi, Armin ; Chen, Fuqi ; Gil, Einat ; Schwing, Alexander ; Selvitella, Alessandro ; Hoffman, Michael M. ; Grosse, Roger ; Hendricks, Dieter ; Reid, Nancy ; ","Statistical Inference, Learning and Models in Big Data  The need for new methods to deal with big data is a common theme in most scientific fields, although its definition tends to vary with the context. Statistical ideas are an essential part of this, and as a partial response, a thematic program on statistical inference, learning, and models in big data was held in 2015 in Canada, under the general direction of the Canadian Statistical Sciences Institute, with major funding from, and most activities located at, the Fields Institute for Research in Mathematical Sciences. This paper gives an overview of the topics covered, describing challenges and strategies that seem common to many different areas of application, and including some examples of applications to make these challenges and strategies more concrete. ",statistical inference learn model big data need new methods deal big data common theme scientific field although definition tend vary context statistical ideas essential part partial response thematic program statistical inference learn model big data hold canada general direction canadian statistical sciences institute major fund activities locate field institute research mathematical sciences paper give overview topics cover describe challenge strategies seem common many different areas application include examples applications make challenge strategies concrete,73,10,1509.02900.txt
http://arxiv.org/abs/1509.03258,Entropic CLT and phase transition in high-dimensional Wishart matrices,"  We consider high dimensional Wishart matrices $\mathbb{X} \mathbb{X}^{\top}$ where the entries of $\mathbb{X} \in {\mathbb{R}^{n \times d}}$ are i.i.d. from a log-concave distribution. We prove an information theoretic phase transition: such matrices are close in total variation distance to the corresponding Gaussian ensemble if and only if $d$ is much larger than $n^3$. Our proof is entropy-based, making use of the chain rule for relative entropy along with the recursive structure in the definition of the Wishart ensemble. The proof crucially relies on the well known relation between Fisher information and entropy, a variational representation for Fisher information, concentration bounds for the spectral norm of a random matrix, and certain small ball probability estimates for log-concave measures. ",Mathematics - Probability ; Computer Science - Information Theory ; Mathematics - Functional Analysis ; Mathematics - Statistics Theory ; ,"Bubeck, Sébastien ; Ganguly, Shirshendu ; ","Entropic CLT and phase transition in high-dimensional Wishart matrices  We consider high dimensional Wishart matrices $\mathbb{X} \mathbb{X}^{\top}$ where the entries of $\mathbb{X} \in {\mathbb{R}^{n \times d}}$ are i.i.d. from a log-concave distribution. We prove an information theoretic phase transition: such matrices are close in total variation distance to the corresponding Gaussian ensemble if and only if $d$ is much larger than $n^3$. Our proof is entropy-based, making use of the chain rule for relative entropy along with the recursive structure in the definition of the Wishart ensemble. The proof crucially relies on the well known relation between Fisher information and entropy, a variational representation for Fisher information, concentration bounds for the spectral norm of a random matrix, and certain small ball probability estimates for log-concave measures. ",entropic clt phase transition high dimensional wishart matrices consider high dimensional wishart matrices mathbb mathbb top entries mathbb mathbb time log concave distribution prove information theoretic phase transition matrices close total variation distance correspond gaussian ensemble much larger proof entropy base make use chain rule relative entropy along recursive structure definition wishart ensemble proof crucially rely well know relation fisher information entropy variational representation fisher information concentration bound spectral norm random matrix certain small ball probability estimate log concave measure,80,7,1509.03258.txt
http://arxiv.org/abs/1509.03476,Relational reasoning via probabilistic coupling,"  Probabilistic coupling is a powerful tool for analyzing pairs of probabilistic processes. Roughly, coupling two processes requires finding an appropriate witness process that models both processes in the same probability space. Couplings are powerful tools proving properties about the relation between two processes, include reasoning about convergence of distributions and stochastic dominance---a probabilistic version of a monotonicity property.   While the mathematical definition of coupling looks rather complex and cumbersome to manipulate, we show that the relational program logic pRHL---the logic underlying the EasyCrypt cryptographic proof assistant---already internalizes a generalization of probabilistic coupling. With this insight, constructing couplings is no harder than constructing logical proofs. We demonstrate how to express and verify classic examples of couplings in pRHL, and we mechanically verify several couplings in EasyCrypt. ",Computer Science - Logic in Computer Science ; Computer Science - Programming Languages ; ,"Barthe, Gilles ; Espitau, Thomas ; Grégoire, Benjamin ; Hsu, Justin ; Stefanesco, Léo ; Strub, Pierre-Yves ; ","Relational reasoning via probabilistic coupling  Probabilistic coupling is a powerful tool for analyzing pairs of probabilistic processes. Roughly, coupling two processes requires finding an appropriate witness process that models both processes in the same probability space. Couplings are powerful tools proving properties about the relation between two processes, include reasoning about convergence of distributions and stochastic dominance---a probabilistic version of a monotonicity property.   While the mathematical definition of coupling looks rather complex and cumbersome to manipulate, we show that the relational program logic pRHL---the logic underlying the EasyCrypt cryptographic proof assistant---already internalizes a generalization of probabilistic coupling. With this insight, constructing couplings is no harder than constructing logical proofs. We demonstrate how to express and verify classic examples of couplings in pRHL, and we mechanically verify several couplings in EasyCrypt. ",relational reason via probabilistic couple probabilistic couple powerful tool analyze pair probabilistic process roughly couple two process require find appropriate witness process model process probability space couple powerful tool prove properties relation two process include reason convergence distributions stochastic dominance probabilistic version monotonicity property mathematical definition couple look rather complex cumbersome manipulate show relational program logic prhl logic underlie easycrypt cryptographic proof assistant already internalize generalization probabilistic couple insight construct couple harder construct logical proof demonstrate express verify classic examples couple prhl mechanically verify several couple easycrypt,87,8,1509.03476.txt
http://arxiv.org/abs/1509.03484,Local structure can identify and quantify influential global spreaders   in large scale social networks,"  Measuring and optimizing the influence of nodes in big-data online social networks are important for many practical applications, such as the viral marketing and the adoption of new products. As the viral spreading on social network is a global process, it is commonly believed that measuring the influence of nodes inevitably requires the knowledge of the entire network. Employing percolation theory, we show that the spreading process displays a nucleation behavior: once a piece of information spread from the seeds to more than a small characteristic number of nodes, it reaches a point of no return and will quickly reach the percolation cluster, regardless of the entire network structure, otherwise the spreading will be contained locally. Thus, we find that, without the knowledge of entire network, any nodes' global influence can be accurately measured using this characteristic number, which is independent of the network size. This motivates an efficient algorithm with constant time complexity on the long standing problem of best seed spreaders selection, with performance remarkably close to the true optimum. ",Physics - Physics and Society ; Computer Science - Computers and Society ; Computer Science - Data Structures and Algorithms ; Computer Science - Social and Information Networks ; ,"Hu, Yanqing ; Ji, Shenggong ; Jin, Yuliang ; Feng, Ling ; Stanley, H. Eugene ; Havlin, Shlomo ; ","Local structure can identify and quantify influential global spreaders   in large scale social networks  Measuring and optimizing the influence of nodes in big-data online social networks are important for many practical applications, such as the viral marketing and the adoption of new products. As the viral spreading on social network is a global process, it is commonly believed that measuring the influence of nodes inevitably requires the knowledge of the entire network. Employing percolation theory, we show that the spreading process displays a nucleation behavior: once a piece of information spread from the seeds to more than a small characteristic number of nodes, it reaches a point of no return and will quickly reach the percolation cluster, regardless of the entire network structure, otherwise the spreading will be contained locally. Thus, we find that, without the knowledge of entire network, any nodes' global influence can be accurately measured using this characteristic number, which is independent of the network size. This motivates an efficient algorithm with constant time complexity on the long standing problem of best seed spreaders selection, with performance remarkably close to the true optimum. ",local structure identify quantify influential global spreaders large scale social network measure optimize influence nod big data online social network important many practical applications viral market adoption new products viral spread social network global process commonly believe measure influence nod inevitably require knowledge entire network employ percolation theory show spread process display nucleation behavior piece information spread seed small characteristic number nod reach point return quickly reach percolation cluster regardless entire network structure otherwise spread contain locally thus find without knowledge entire network nod global influence accurately measure use characteristic number independent network size motivate efficient algorithm constant time complexity long stand problem best seed spreaders selection performance remarkably close true optimum,112,6,1509.03484.txt
http://arxiv.org/abs/1509.03784,Solving underdetermined systems with error-correcting codes,"  In an underdetermined system of equations $Ax=y$, where $A$ is an $m\times n$ matrix, only $u$ of the entries of $y$ with $u < m$ are known. Thus $E_jw$, called `measurements', are known for certain $j\in J \subset \{0,1,\ldots,m-1\}$ where $\{E_i, i=0,1,\ldots, m-1\}$ are the rows of $A$ and $|J|=u$. It is required, if possible, to solve the system uniquely when $x$ has at most $t$ non-zero entries with $u\geq 2t$.   Here such systems are considered from an error-correcting coding point of view. The unknown $x$ can be shown to be the error vector of a code subject to certain conditions on the rows of the matrix $A$. This reduces the problem to finding a suitable decoding algorithm which then finds $x$.   Decoding workable algorithms are shown to exist, from which the unknown $x$ may be determined, in cases where the known $2t$ values are evenly spaced (that is, when the elements of $J$ are in arithmetic progression) for classes of matrices satisfying certain row properties. These cases include Fourier $n\times n $ matrices where the arithmetic difference $k$ satisfies $\gcd(n,k)=1$, and classes of Vandermonde matrices $V(x_1,x_2,\ldots,x_n)$ (with $x_i\neq 0$) with arithmetic difference $k$ where the ratios $x_i/x_j$ for $i\neq j$ are not $k^{th}$ roots of unity. The decoding algorithm has complexity $O(nt)$ and in some cases, including the Fourier matrix cases, the complexity is $O(t^2)$.   Matrices which have the property that the determinant of any square submatrix is non-zero are of particular interest. Randomly choosing rows of such matrices can then give $t$ error-correcting pairs to generate a `measuring' code $C^\perp=\{E_j | j\in J\}$ with a decoding algorithm which finds $x$.   This has applications to signal processing and compressed sensing. ","Computer Science - Information Theory ; 94A99, 15B99 ; ","Hurley, Ted ; ","Solving underdetermined systems with error-correcting codes  In an underdetermined system of equations $Ax=y$, where $A$ is an $m\times n$ matrix, only $u$ of the entries of $y$ with $u < m$ are known. Thus $E_jw$, called `measurements', are known for certain $j\in J \subset \{0,1,\ldots,m-1\}$ where $\{E_i, i=0,1,\ldots, m-1\}$ are the rows of $A$ and $|J|=u$. It is required, if possible, to solve the system uniquely when $x$ has at most $t$ non-zero entries with $u\geq 2t$.   Here such systems are considered from an error-correcting coding point of view. The unknown $x$ can be shown to be the error vector of a code subject to certain conditions on the rows of the matrix $A$. This reduces the problem to finding a suitable decoding algorithm which then finds $x$.   Decoding workable algorithms are shown to exist, from which the unknown $x$ may be determined, in cases where the known $2t$ values are evenly spaced (that is, when the elements of $J$ are in arithmetic progression) for classes of matrices satisfying certain row properties. These cases include Fourier $n\times n $ matrices where the arithmetic difference $k$ satisfies $\gcd(n,k)=1$, and classes of Vandermonde matrices $V(x_1,x_2,\ldots,x_n)$ (with $x_i\neq 0$) with arithmetic difference $k$ where the ratios $x_i/x_j$ for $i\neq j$ are not $k^{th}$ roots of unity. The decoding algorithm has complexity $O(nt)$ and in some cases, including the Fourier matrix cases, the complexity is $O(t^2)$.   Matrices which have the property that the determinant of any square submatrix is non-zero are of particular interest. Randomly choosing rows of such matrices can then give $t$ error-correcting pairs to generate a `measuring' code $C^\perp=\{E_j | j\in J\}$ with a decoding algorithm which finds $x$.   This has applications to signal processing and compressed sensing. ",solve underdetermined systems error correct cod underdetermined system equations ax time matrix entries know thus jw call measurements know certain subset ldots ldots row require possible solve system uniquely non zero entries geq systems consider error correct cod point view unknown show error vector code subject certain condition row matrix reduce problem find suitable decode algorithm find decode workable algorithms show exist unknown may determine case know value evenly space elements arithmetic progression class matrices satisfy certain row properties case include fourier time matrices arithmetic difference satisfy gcd class vandermonde matrices ldots neq arithmetic difference ratios neq th root unity decode algorithm complexity nt case include fourier matrix case complexity matrices property determinant square submatrix non zero particular interest randomly choose row matrices give error correct pair generate measure code perp decode algorithm find applications signal process compress sense,139,7,1509.03784.txt
http://arxiv.org/abs/1509.03915,An Impossibility Result for Housing Markets with Fractional Endowments,"  The housing market setting constitutes a fundamental model of exchange economies of goods. Most of the work concerning housing markets does not cater for randomized assignments or allocation of time-shares. House allocation with fractional endowments of houses was considered by Athanassoglou and Sethuraman (2011) who posed the open problem whether individual rationality, weak strategyproofness, and efficiency are compatible for the setting. We show that the three axioms are incompatible. ","Computer Science - Computer Science and Game Theory ; Computer Science - Data Structures and Algorithms ; 91A12, 68Q15 ; F.2 ; J.4 ; ","Aziz, Haris ; ","An Impossibility Result for Housing Markets with Fractional Endowments  The housing market setting constitutes a fundamental model of exchange economies of goods. Most of the work concerning housing markets does not cater for randomized assignments or allocation of time-shares. House allocation with fractional endowments of houses was considered by Athanassoglou and Sethuraman (2011) who posed the open problem whether individual rationality, weak strategyproofness, and efficiency are compatible for the setting. We show that the three axioms are incompatible. ",impossibility result house market fractional endowments house market set constitute fundamental model exchange economies goods work concern house market cater randomize assignments allocation time share house allocation fractional endowments house consider athanassoglou sethuraman pose open problem whether individual rationality weak strategyproofness efficiency compatible set show three axioms incompatible,48,11,1509.03915.txt
http://arxiv.org/abs/1509.04037,Measuring Partial Balance in Signed Networks,"  Is the enemy of an enemy necessarily a friend? If not, to what extent does this tend to hold? Such questions were formulated in terms of signed (social) networks and necessary and sufficient conditions for a network to be ""balanced"" were obtained around 1960. Since then the idea that signed networks tend over time to become more balanced has been widely used in several application areas. However, investigation of this hypothesis has been complicated by the lack of a standard measure of partial balance, since complete balance is almost never achieved in practice. We formalize the concept of a measure of partial balance, discuss various measures, compare the measures on synthetic datasets, and investigate their axiomatic properties. The synthetic data involves Erd\H{o}s-R\'enyi and specially structured random graphs. We show that some measures behave better than others in terms of axioms and ability to differentiate between graphs. We also use well-known datasets from the sociology and biology literature, such as Read's New Guinean tribes, gene regulatory networks related to two organisms, and a network involving senate bill co-sponsorship. Our results show that substantially different levels of partial balance is observed under cycle-based, eigenvalue-based, and frustration-based measures. We make some recommendations for measures to be used in future work. ","Computer Science - Social and Information Networks ; Physics - Physics and Society ; 05C22, 05C38, 91D30, 90B10 ; ","Aref, Samin ; Wilson, Mark C. ; ","Measuring Partial Balance in Signed Networks  Is the enemy of an enemy necessarily a friend? If not, to what extent does this tend to hold? Such questions were formulated in terms of signed (social) networks and necessary and sufficient conditions for a network to be ""balanced"" were obtained around 1960. Since then the idea that signed networks tend over time to become more balanced has been widely used in several application areas. However, investigation of this hypothesis has been complicated by the lack of a standard measure of partial balance, since complete balance is almost never achieved in practice. We formalize the concept of a measure of partial balance, discuss various measures, compare the measures on synthetic datasets, and investigate their axiomatic properties. The synthetic data involves Erd\H{o}s-R\'enyi and specially structured random graphs. We show that some measures behave better than others in terms of axioms and ability to differentiate between graphs. We also use well-known datasets from the sociology and biology literature, such as Read's New Guinean tribes, gene regulatory networks related to two organisms, and a network involving senate bill co-sponsorship. Our results show that substantially different levels of partial balance is observed under cycle-based, eigenvalue-based, and frustration-based measures. We make some recommendations for measures to be used in future work. ",measure partial balance sign network enemy enemy necessarily friend extent tend hold question formulate term sign social network necessary sufficient condition network balance obtain around since idea sign network tend time become balance widely use several application areas however investigation hypothesis complicate lack standard measure partial balance since complete balance almost never achieve practice formalize concept measure partial balance discuss various measure compare measure synthetic datasets investigate axiomatic properties synthetic data involve erd enyi specially structure random graph show measure behave better others term axioms ability differentiate graph also use well know datasets sociology biology literature read new guinean tribes gene regulatory network relate two organisms network involve senate bill co sponsorship result show substantially different level partial balance observe cycle base eigenvalue base frustration base measure make recommendations measure use future work,133,6,1509.04037.txt
http://arxiv.org/abs/1509.04634,Modeling and interpolation of the ambient magnetic field by Gaussian   processes,"  Anomalies in the ambient magnetic field can be used as features in indoor positioning and navigation. By using Maxwell's equations, we derive and present a Bayesian non-parametric probabilistic modeling approach for interpolation and extrapolation of the magnetic field. We model the magnetic field components jointly by imposing a Gaussian process (GP) prior on the latent scalar potential of the magnetic field. By rewriting the GP model in terms of a Hilbert space representation, we circumvent the computational pitfalls associated with GP modeling and provide a computationally efficient and physically justified modeling tool for the ambient magnetic field. The model allows for sequential updating of the estimate and time-dependent changes in the magnetic field. The model is shown to work well in practice in different applications: we demonstrate mapping of the magnetic field both with an inexpensive Raspberry Pi powered robot and on foot using a standard smartphone. ",Computer Science - Robotics ; Statistics - Machine Learning ; ,"Solin, Arno ; Kok, Manon ; Wahlström, Niklas ; Schön, Thomas B. ; Särkkä, Simo ; ","Modeling and interpolation of the ambient magnetic field by Gaussian   processes  Anomalies in the ambient magnetic field can be used as features in indoor positioning and navigation. By using Maxwell's equations, we derive and present a Bayesian non-parametric probabilistic modeling approach for interpolation and extrapolation of the magnetic field. We model the magnetic field components jointly by imposing a Gaussian process (GP) prior on the latent scalar potential of the magnetic field. By rewriting the GP model in terms of a Hilbert space representation, we circumvent the computational pitfalls associated with GP modeling and provide a computationally efficient and physically justified modeling tool for the ambient magnetic field. The model allows for sequential updating of the estimate and time-dependent changes in the magnetic field. The model is shown to work well in practice in different applications: we demonstrate mapping of the magnetic field both with an inexpensive Raspberry Pi powered robot and on foot using a standard smartphone. ",model interpolation ambient magnetic field gaussian process anomalies ambient magnetic field use feature indoor position navigation use maxwell equations derive present bayesian non parametric probabilistic model approach interpolation extrapolation magnetic field model magnetic field components jointly impose gaussian process gp prior latent scalar potential magnetic field rewrite gp model term hilbert space representation circumvent computational pitfalls associate gp model provide computationally efficient physically justify model tool ambient magnetic field model allow sequential update estimate time dependent change magnetic field model show work well practice different applications demonstrate map magnetic field inexpensive raspberry pi power robot foot use standard smartphone,99,0,1509.04634.txt
http://arxiv.org/abs/1509.04857,Markov modeling of online inter-arrival times,"  In this paper, we investigate the arising communication patterns on social media, and in particular the series of events happening for a single user. While the distribution of inter-event times is often assimilated to power-law density functions, a debate persists on the nature of an underlying model that explains the observed distribution. In the present, we propose an intuitive explanation to understand the observed dependence of subsequent waiting times. Our contribution is twofold. The first idea consists of separating the short waiting times -- out of scope for power-law distributions -- from the long ones. The model is further enhanced by introducing a two-state Markovian process to incorporate memory. ","Statistics - Applications ; Computer Science - Social and Information Networks ; Physics - Physics and Society ; 62P25 (Primary), 91C20(Secondary) ; ","Kerckhove, Corentin Vande ; Gerencsér, Balázs ; Hendrickx, Julien M. ; Blondel, Vincent D. ; ","Markov modeling of online inter-arrival times  In this paper, we investigate the arising communication patterns on social media, and in particular the series of events happening for a single user. While the distribution of inter-event times is often assimilated to power-law density functions, a debate persists on the nature of an underlying model that explains the observed distribution. In the present, we propose an intuitive explanation to understand the observed dependence of subsequent waiting times. Our contribution is twofold. The first idea consists of separating the short waiting times -- out of scope for power-law distributions -- from the long ones. The model is further enhanced by introducing a two-state Markovian process to incorporate memory. ",markov model online inter arrival time paper investigate arise communication pattern social media particular series events happen single user distribution inter event time often assimilate power law density function debate persist nature underlie model explain observe distribution present propose intuitive explanation understand observe dependence subsequent wait time contribution twofold first idea consist separate short wait time scope power law distributions long ones model enhance introduce two state markovian process incorporate memory,71,11,1509.04857.txt
http://arxiv.org/abs/1509.04880,An FPT 2-Approximation for Tree-Cut Decomposition,"  The tree-cut width of a graph is a graph parameter defined by Wollan [J. Comb. Theory, Ser. B, 110:47-66, 2015] with the help of tree-cut decompositions. In certain cases, tree-cut width appears to be more adequate than treewidth as an invariant that, when bounded, can accelerate the resolution of intractable problems. While designing algorithms for problems with bounded tree-cut width, it is important to have a parametrically tractable way to compute the exact value of this parameter or, at least, some constant approximation of it. In this paper we give a parameterized 2-approximation algorithm for the computation of tree-cut width; for an input $n$-vertex graph $G$ and an integer $w$, our algorithm either confirms that the tree-cut width of $G$ is more than $w$ or returns a tree-cut decomposition of $G$ certifying that its tree-cut width is at most $2w$, in time $2^{O(w^2\log w)} \cdot n^2$. Prior to this work, no constructive parameterized algorithms, even approximated ones, existed for computing the tree-cut width of a graph. As a consequence of the Graph Minors series by Robertson and Seymour, only the existence of a decision algorithm was known. ","Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; 68R10, 05C85 ; G.2.2 ; F.2.2 ; ","Kim, Eunjung ; Oum, Sang-il ; Paul, Christophe ; Sau, Ignasi ; Thilikos, Dimitrios M. ; ","An FPT 2-Approximation for Tree-Cut Decomposition  The tree-cut width of a graph is a graph parameter defined by Wollan [J. Comb. Theory, Ser. B, 110:47-66, 2015] with the help of tree-cut decompositions. In certain cases, tree-cut width appears to be more adequate than treewidth as an invariant that, when bounded, can accelerate the resolution of intractable problems. While designing algorithms for problems with bounded tree-cut width, it is important to have a parametrically tractable way to compute the exact value of this parameter or, at least, some constant approximation of it. In this paper we give a parameterized 2-approximation algorithm for the computation of tree-cut width; for an input $n$-vertex graph $G$ and an integer $w$, our algorithm either confirms that the tree-cut width of $G$ is more than $w$ or returns a tree-cut decomposition of $G$ certifying that its tree-cut width is at most $2w$, in time $2^{O(w^2\log w)} \cdot n^2$. Prior to this work, no constructive parameterized algorithms, even approximated ones, existed for computing the tree-cut width of a graph. As a consequence of the Graph Minors series by Robertson and Seymour, only the existence of a decision algorithm was known. ",fpt approximation tree cut decomposition tree cut width graph graph parameter define wollan comb theory ser help tree cut decompositions certain case tree cut width appear adequate treewidth invariant bound accelerate resolution intractable problems design algorithms problems bound tree cut width important parametrically tractable way compute exact value parameter least constant approximation paper give parameterized approximation algorithm computation tree cut width input vertex graph integer algorithm either confirm tree cut width return tree cut decomposition certify tree cut width time log cdot prior work constructive parameterized algorithms even approximate ones exist compute tree cut width graph consequence graph minors series robertson seymour existence decision algorithm know,106,3,1509.04880.txt
http://arxiv.org/abs/1509.05001,Solving constrained quadratic binary problems via quantum adiabatic   evolution,"  Quantum adiabatic evolution is perceived as useful for binary quadratic programming problems that are a priori unconstrained. For constrained problems, it is a common practice to relax linear equality constraints as penalty terms in the objective function. However, there has not yet been proposed a method for efficiently dealing with inequality constraints using the quantum adiabatic approach. In this paper, we give a method for solving the Lagrangian dual of a binary quadratic programming (BQP) problem in the presence of inequality constraints and employ this procedure within a branch-and-bound framework for constrained BQP (CBQP) problems. ",Mathematics - Optimization and Control ; Computer Science - Emerging Technologies ; Quantum Physics ; ,"Ronagh, Pooya ; Woods, Brad ; Iranmanesh, Ehsan ; ","Solving constrained quadratic binary problems via quantum adiabatic   evolution  Quantum adiabatic evolution is perceived as useful for binary quadratic programming problems that are a priori unconstrained. For constrained problems, it is a common practice to relax linear equality constraints as penalty terms in the objective function. However, there has not yet been proposed a method for efficiently dealing with inequality constraints using the quantum adiabatic approach. In this paper, we give a method for solving the Lagrangian dual of a binary quadratic programming (BQP) problem in the presence of inequality constraints and employ this procedure within a branch-and-bound framework for constrained BQP (CBQP) problems. ",solve constrain quadratic binary problems via quantum adiabatic evolution quantum adiabatic evolution perceive useful binary quadratic program problems priori unconstrained constrain problems common practice relax linear equality constraints penalty term objective function however yet propose method efficiently deal inequality constraints use quantum adiabatic approach paper give method solve lagrangian dual binary quadratic program bqp problem presence inequality constraints employ procedure within branch bind framework constrain bqp cbqp problems,68,8,1509.05001.txt
http://arxiv.org/abs/1509.05498,Supervisor Localization of Discrete-Event Systems under Partial   Observation,"  Recently we developed supervisor localization, a top-down approach to distributed control of discrete-event systems. Its essence is the allocation of monolithic (global) control action among the local control strategies of individual agents. In this paper, we extend supervisor localization by considering partial observation; namely not all events are observable. Specifically, we employ the recently proposed concept of relative observability to compute a partial-observation monolithic supervisor, and then design a suitable localization procedure to decompose the supervisor into a set of local controllers. In the resulting local controllers, only observable events can cause state change. Further, to deal with large-scale systems, we combine the partial-observation supervisor localization with an efficient architectural synthesis approach: first compute a heterarchical array of partial-observation decentralized supervisors and coordinators, and then localize each of these supervisors/coordinators into local controllers. ",Computer Science - Systems and Control ; ,"Zhang, Renyuan ; Cai, Kai ; Wonham, W. M. ; ","Supervisor Localization of Discrete-Event Systems under Partial   Observation  Recently we developed supervisor localization, a top-down approach to distributed control of discrete-event systems. Its essence is the allocation of monolithic (global) control action among the local control strategies of individual agents. In this paper, we extend supervisor localization by considering partial observation; namely not all events are observable. Specifically, we employ the recently proposed concept of relative observability to compute a partial-observation monolithic supervisor, and then design a suitable localization procedure to decompose the supervisor into a set of local controllers. In the resulting local controllers, only observable events can cause state change. Further, to deal with large-scale systems, we combine the partial-observation supervisor localization with an efficient architectural synthesis approach: first compute a heterarchical array of partial-observation decentralized supervisors and coordinators, and then localize each of these supervisors/coordinators into local controllers. ",supervisor localization discrete event systems partial observation recently develop supervisor localization top approach distribute control discrete event systems essence allocation monolithic global control action among local control strategies individual agents paper extend supervisor localization consider partial observation namely events observable specifically employ recently propose concept relative observability compute partial observation monolithic supervisor design suitable localization procedure decompose supervisor set local controllers result local controllers observable events cause state change deal large scale systems combine partial observation supervisor localization efficient architectural synthesis approach first compute heterarchical array partial observation decentralize supervisors coordinators localize supervisors coordinators local controllers,96,12,1509.05498.txt
http://arxiv.org/abs/1509.05572,Randomised enumeration of small witnesses using a decision oracle,"  Many combinatorial problems involve determining whether a universe of $n$ elements contains a witness consisting of $k$ elements which have some specified property. In this paper we investigate the relationship between the decision and enumeration versions of such problems: efficient methods are known for transforming a decision algorithm into a search procedure that finds a single witness, but even finding a second witness is not so straightforward in general. We show that, if the decision version of the problem can be solved in time $f(k) \cdot poly(n)$, there is a randomised algorithm which enumerates all witnesses in time $e^{k + o(k)} \cdot f(k) \cdot poly(n) \cdot N$, where $N$ is the total number of witnesses. If the decision version of the problem is solved by a randomised algorithm which may return false negatives, then the same method allows us to output a list of witnesses in which any given witness will be included with high probability. The enumeration algorithm also gives rise to an efficient algorithm to count the total number of witnesses when this number is small. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; ,"Meeks, Kitty ; ","Randomised enumeration of small witnesses using a decision oracle  Many combinatorial problems involve determining whether a universe of $n$ elements contains a witness consisting of $k$ elements which have some specified property. In this paper we investigate the relationship between the decision and enumeration versions of such problems: efficient methods are known for transforming a decision algorithm into a search procedure that finds a single witness, but even finding a second witness is not so straightforward in general. We show that, if the decision version of the problem can be solved in time $f(k) \cdot poly(n)$, there is a randomised algorithm which enumerates all witnesses in time $e^{k + o(k)} \cdot f(k) \cdot poly(n) \cdot N$, where $N$ is the total number of witnesses. If the decision version of the problem is solved by a randomised algorithm which may return false negatives, then the same method allows us to output a list of witnesses in which any given witness will be included with high probability. The enumeration algorithm also gives rise to an efficient algorithm to count the total number of witnesses when this number is small. ",randomise enumeration small witness use decision oracle many combinatorial problems involve determine whether universe elements contain witness consist elements specify property paper investigate relationship decision enumeration versions problems efficient methods know transform decision algorithm search procedure find single witness even find second witness straightforward general show decision version problem solve time cdot poly randomise algorithm enumerate witness time cdot cdot poly cdot total number witness decision version problem solve randomise algorithm may return false negative method allow us output list witness give witness include high probability enumeration algorithm also give rise efficient algorithm count total number witness number small,99,8,1509.05572.txt
http://arxiv.org/abs/1509.05664,Automated Synthesis of Distributed Self-Stabilizing Protocols,"  In this paper, we introduce an SMT-based method that automatically synthesizes a distributed self-stabilizing protocol from a given high-level specification and network topology. Unlike existing approaches, where synthesis algorithms require the explicit description of the set of legitimate states, our technique only needs the temporal behavior of the protocol. We extend our approach to synthesize ideal-stabilizing protocols, where every state is legitimate. We also extend our technique to synthesize monotonic-stabilizing protocols, where during recovery, each process can execute an most once one action. Our proposed methods are fully implemented and we report successful synthesis of well-known protocols such as Dijkstra's token ring, a self-stabilizing version of Raymond's mutual exclusion algorithm, ideal-stabilizing leader election and local mutual exclusion, as well as monotonic-stabilizing maximal independent set and distributed Grundy coloring. ","Computer Science - Software Engineering ; Computer Science - Distributed, Parallel, and Cluster Computing ; ","Faghih, Fathiyeh ; Bonakdarpour, Borzoo ; Tixeuil, Sebastien ; Kulkarni, Sandeep ; ","Automated Synthesis of Distributed Self-Stabilizing Protocols  In this paper, we introduce an SMT-based method that automatically synthesizes a distributed self-stabilizing protocol from a given high-level specification and network topology. Unlike existing approaches, where synthesis algorithms require the explicit description of the set of legitimate states, our technique only needs the temporal behavior of the protocol. We extend our approach to synthesize ideal-stabilizing protocols, where every state is legitimate. We also extend our technique to synthesize monotonic-stabilizing protocols, where during recovery, each process can execute an most once one action. Our proposed methods are fully implemented and we report successful synthesis of well-known protocols such as Dijkstra's token ring, a self-stabilizing version of Raymond's mutual exclusion algorithm, ideal-stabilizing leader election and local mutual exclusion, as well as monotonic-stabilizing maximal independent set and distributed Grundy coloring. ",automate synthesis distribute self stabilize protocols paper introduce smt base method automatically synthesize distribute self stabilize protocol give high level specification network topology unlike exist approach synthesis algorithms require explicit description set legitimate state technique need temporal behavior protocol extend approach synthesize ideal stabilize protocols every state legitimate also extend technique synthesize monotonic stabilize protocols recovery process execute one action propose methods fully implement report successful synthesis well know protocols dijkstra token ring self stabilize version raymond mutual exclusion algorithm ideal stabilize leader election local mutual exclusion well monotonic stabilize maximal independent set distribute grundy color,96,2,1509.05664.txt
http://arxiv.org/abs/1509.05821,New bounds on curve tangencies and orthogonalities,"  We establish new bounds on the number of tangencies and orthogonal intersections determined by an arrangement of curves. First, given a set of $n$ algebraic plane curves, we show that there are $O(n^{3/2})$ points where two or more curves are tangent. In particular, if no three curves are mutually tangent at a common point, then there are $O(n^{3/2})$ curve-curve tangencies. Second, given a family of algebraic plane curves and a set of $n$ curves from this family, we show that either there are $O(n^{3/2})$ points where two or more curves are orthogonal, or the family of curves has certain special properties.   We obtain these bounds by transforming the arrangement of plane curves into an arrangement of space curves so that tangency (or orthogonality) of the original plane curves corresponds to intersection of space curves. We then bound the number of intersections of the corresponding space curves. For the case of curve-curve tangency, we use a polynomial method technique that is reminiscent of Guth and Katz's proof of the joints theorem. For the case of orthogonal curve intersections, we employ a bound of Guth and the third author to control the number of two-rich points in space curve arrangements. ",Mathematics - Combinatorics ; Computer Science - Computational Geometry ; ,"Ellenberg, Jordan S. ; Solymosi, Jozsef ; Zahl, Joshua ; ","New bounds on curve tangencies and orthogonalities  We establish new bounds on the number of tangencies and orthogonal intersections determined by an arrangement of curves. First, given a set of $n$ algebraic plane curves, we show that there are $O(n^{3/2})$ points where two or more curves are tangent. In particular, if no three curves are mutually tangent at a common point, then there are $O(n^{3/2})$ curve-curve tangencies. Second, given a family of algebraic plane curves and a set of $n$ curves from this family, we show that either there are $O(n^{3/2})$ points where two or more curves are orthogonal, or the family of curves has certain special properties.   We obtain these bounds by transforming the arrangement of plane curves into an arrangement of space curves so that tangency (or orthogonality) of the original plane curves corresponds to intersection of space curves. We then bound the number of intersections of the corresponding space curves. For the case of curve-curve tangency, we use a polynomial method technique that is reminiscent of Guth and Katz's proof of the joints theorem. For the case of orthogonal curve intersections, we employ a bound of Guth and the third author to control the number of two-rich points in space curve arrangements. ",new bound curve tangencies orthogonalities establish new bound number tangencies orthogonal intersections determine arrangement curve first give set algebraic plane curve show point two curve tangent particular three curve mutually tangent common point curve curve tangencies second give family algebraic plane curve set curve family show either point two curve orthogonal family curve certain special properties obtain bound transform arrangement plane curve arrangement space curve tangency orthogonality original plane curve correspond intersection space curve bind number intersections correspond space curve case curve curve tangency use polynomial method technique reminiscent guth katz proof joint theorem case orthogonal curve intersections employ bind guth third author control number two rich point space curve arrangements,111,4,1509.05821.txt
http://arxiv.org/abs/1509.06191,Product Space Models of Correlation: Between Noise Stability and   Additive Combinatorics,"  There is a common theme to some research questions in additive combinatorics and noise stability. Both study the following basic question: Let $\mathcal{P}$ be a probability distribution over a space $\Omega^\ell$ with all $\ell$ marginals equal. Let $\underline{X}^{(1)}, \ldots, \underline{X}^{(\ell)}$ where $\underline{X}^{(j)} = (X_1^{(j)}, \ldots, X_n^{(j)})$ be random vectors such that for every coordinate $i \in [n]$ the tuples $(X_i^{(1)}, \ldots, X_i^{(\ell)})$ are i.i.d. according to $\mathcal{P}$.   A central question that is addressed in both areas is:   - Does there exist a function $c_{\mathcal{P}}()$ independent of $n$ such that for every $f: \Omega^n \to [0, 1]$ with $\mathrm{E}[f(X^{(1)})] = \mu > 0$: \begin{align*} \mathrm{E} \left[ \prod_{j=1}^\ell f(X^{(j)}) \right]   \ge c(\mu) > 0 \, ? \end{align*}   Instances of this question include the finite field model version of Roth's and Szemer\'edi's theorems as well as Borell's result about the optimality of noise stability of half-spaces.   Our goal in this paper is to interpolate between the noise stability theory and the finite field additive combinatorics theory and address the question above in further generality than considered before. In particular, we settle the question for $\ell = 2$ and when $\ell > 2$ and $\mathcal{P}$ has bounded correlation $\rho(\mathcal{P}) < 1$. Under the same conditions we also characterize the _obstructions_ for similar lower bounds in the case of $\ell$ different functions. Part of the novelty in our proof is the combination of analytic arguments from the theories of influences and hyper-contraction with arguments from additive combinatorics. ",Computer Science - Discrete Mathematics ; ,"Hązła, Jan ; Holenstein, Thomas ; Mossel, Elchanan ; ","Product Space Models of Correlation: Between Noise Stability and   Additive Combinatorics  There is a common theme to some research questions in additive combinatorics and noise stability. Both study the following basic question: Let $\mathcal{P}$ be a probability distribution over a space $\Omega^\ell$ with all $\ell$ marginals equal. Let $\underline{X}^{(1)}, \ldots, \underline{X}^{(\ell)}$ where $\underline{X}^{(j)} = (X_1^{(j)}, \ldots, X_n^{(j)})$ be random vectors such that for every coordinate $i \in [n]$ the tuples $(X_i^{(1)}, \ldots, X_i^{(\ell)})$ are i.i.d. according to $\mathcal{P}$.   A central question that is addressed in both areas is:   - Does there exist a function $c_{\mathcal{P}}()$ independent of $n$ such that for every $f: \Omega^n \to [0, 1]$ with $\mathrm{E}[f(X^{(1)})] = \mu > 0$: \begin{align*} \mathrm{E} \left[ \prod_{j=1}^\ell f(X^{(j)}) \right]   \ge c(\mu) > 0 \, ? \end{align*}   Instances of this question include the finite field model version of Roth's and Szemer\'edi's theorems as well as Borell's result about the optimality of noise stability of half-spaces.   Our goal in this paper is to interpolate between the noise stability theory and the finite field additive combinatorics theory and address the question above in further generality than considered before. In particular, we settle the question for $\ell = 2$ and when $\ell > 2$ and $\mathcal{P}$ has bounded correlation $\rho(\mathcal{P}) < 1$. Under the same conditions we also characterize the _obstructions_ for similar lower bounds in the case of $\ell$ different functions. Part of the novelty in our proof is the combination of analytic arguments from the theories of influences and hyper-contraction with arguments from additive combinatorics. ",product space model correlation noise stability additive combinatorics common theme research question additive combinatorics noise stability study follow basic question let mathcal probability distribution space omega ell ell marginals equal let underline ldots underline ell underline ldots random vectors every coordinate tuples ldots ell accord mathcal central question address areas exist function mathcal independent every omega mathrm mu begin align mathrm leave prod ell right ge mu end align instance question include finite field model version roth szemer edi theorems well borell result optimality noise stability half space goal paper interpolate noise stability theory finite field additive combinatorics theory address question generality consider particular settle question ell ell mathcal bound correlation rho mathcal condition also characterize obstructions similar lower bound case ell different function part novelty proof combination analytic arguments theories influence hyper contraction arguments additive combinatorics,137,7,1509.06191.txt
http://arxiv.org/abs/1509.06357,Using Contracted Solution Graphs for Solving Reconfiguration Problems,"  We introduce in a general setting a dynamic programming method for solving reconfiguration problems. Our method is based on contracted solution graphs, which are obtained from solution graphs by performing an appropriate series of edge contractions that decrease the graph size without losing any critical information needed to solve the reconfiguration problem under consideration. Our general framework captures the approach behind known reconfiguration results of Bonsma (2012) and Hatanaka, Ito and Zhou (2014). As a third example, we apply the method to the following problem: given two $k$-colorings $\alpha$ and $\beta$ of a graph $G$, can $\alpha$ be modified into $\beta$ by recoloring one vertex of $G$ at a time, while maintaining a $k$-coloring throughout? This problem is known to be PSPACE-hard even for bipartite planar graphs and $k=4$. By applying our method in combination with a thorough exploitation of the graph structure we obtain a polynomial time algorithm for $(k-2)$-connected chordal graphs. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; ,"Bonsma, Paul ; Paulusma, Daniel ; ","Using Contracted Solution Graphs for Solving Reconfiguration Problems  We introduce in a general setting a dynamic programming method for solving reconfiguration problems. Our method is based on contracted solution graphs, which are obtained from solution graphs by performing an appropriate series of edge contractions that decrease the graph size without losing any critical information needed to solve the reconfiguration problem under consideration. Our general framework captures the approach behind known reconfiguration results of Bonsma (2012) and Hatanaka, Ito and Zhou (2014). As a third example, we apply the method to the following problem: given two $k$-colorings $\alpha$ and $\beta$ of a graph $G$, can $\alpha$ be modified into $\beta$ by recoloring one vertex of $G$ at a time, while maintaining a $k$-coloring throughout? This problem is known to be PSPACE-hard even for bipartite planar graphs and $k=4$. By applying our method in combination with a thorough exploitation of the graph structure we obtain a polynomial time algorithm for $(k-2)$-connected chordal graphs. ",use contract solution graph solve reconfiguration problems introduce general set dynamic program method solve reconfiguration problems method base contract solution graph obtain solution graph perform appropriate series edge contractions decrease graph size without lose critical information need solve reconfiguration problem consideration general framework capture approach behind know reconfiguration result bonsma hatanaka ito zhou third example apply method follow problem give two color alpha beta graph alpha modify beta recoloring one vertex time maintain color throughout problem know pspace hard even bipartite planar graph apply method combination thorough exploitation graph structure obtain polynomial time algorithm connect chordal graph,97,3,1509.06357.txt
http://arxiv.org/abs/1509.06559,Shape Aware Matching of Implicit Surfaces based on Thin Shell Energies,"  A shape sensitive, variational approach for the matching of surfaces considered as thin elastic shells is investigated. The elasticity functional to be minimized takes into account two different types of nonlinear energies: a membrane energy measuring the rate of tangential distortion when deforming the reference shell into the template shell, and a bending energy measuring the bending under the deformation in terms of the change of the shape operators from the undeformed into the deformed configuration. The variational method applies to surfaces described as level sets. It is mathematically well-posed and an existence proof of an optimal matching deformation is given. The variational model is implemented using a finite element discretization combined with a narrow band approach on an efficient hierarchical grid structure. For the optimization a regularized nonlinear conjugate gradient scheme and a cascadic multilevel strategy are used. The features of the proposed approach are studied for synthetic test cases and a collection of geometry processing examples. ","Mathematics - Optimization and Control ; Computer Science - Computational Geometry ; 65D18, 49J45, 74K25 ; ","Iglesias, José A. ; Rumpf, Martin ; Scherzer, Otmar ; ","Shape Aware Matching of Implicit Surfaces based on Thin Shell Energies  A shape sensitive, variational approach for the matching of surfaces considered as thin elastic shells is investigated. The elasticity functional to be minimized takes into account two different types of nonlinear energies: a membrane energy measuring the rate of tangential distortion when deforming the reference shell into the template shell, and a bending energy measuring the bending under the deformation in terms of the change of the shape operators from the undeformed into the deformed configuration. The variational method applies to surfaces described as level sets. It is mathematically well-posed and an existence proof of an optimal matching deformation is given. The variational model is implemented using a finite element discretization combined with a narrow band approach on an efficient hierarchical grid structure. For the optimization a regularized nonlinear conjugate gradient scheme and a cascadic multilevel strategy are used. The features of the proposed approach are studied for synthetic test cases and a collection of geometry processing examples. ",shape aware match implicit surface base thin shell energies shape sensitive variational approach match surface consider thin elastic shell investigate elasticity functional minimize take account two different type nonlinear energies membrane energy measure rate tangential distortion deform reference shell template shell bend energy measure bend deformation term change shape operators undeformed deform configuration variational method apply surface describe level set mathematically well pose existence proof optimal match deformation give variational model implement use finite element discretization combine narrow band approach efficient hierarchical grid structure optimization regularize nonlinear conjugate gradient scheme cascadic multilevel strategy use feature propose approach study synthetic test case collection geometry process examples,105,8,1509.06559.txt
http://arxiv.org/abs/1509.07127,Universal recovery maps and approximate sufficiency of quantum relative   entropy,"  The data processing inequality states that the quantum relative entropy between two states $\rho$ and $\sigma$ can never increase by applying the same quantum channel $\mathcal{N}$ to both states. This inequality can be strengthened with a remainder term in the form of a distance between $\rho$ and the closest recovered state $(\mathcal{R} \circ \mathcal{N})(\rho)$, where $\mathcal{R}$ is a recovery map with the property that $\sigma = (\mathcal{R} \circ \mathcal{N})(\sigma)$. We show the existence of an explicit recovery map that is universal in the sense that it depends only on $\sigma$ and the quantum channel $\mathcal{N}$ to be reversed. This result gives an alternate, information-theoretic characterization of the conditions for approximate quantum error correction. ",Quantum Physics ; Computer Science - Information Theory ; Mathematical Physics ; ,"Junge, Marius ; Renner, Renato ; Sutter, David ; Wilde, Mark M. ; Winter, Andreas ; ","Universal recovery maps and approximate sufficiency of quantum relative   entropy  The data processing inequality states that the quantum relative entropy between two states $\rho$ and $\sigma$ can never increase by applying the same quantum channel $\mathcal{N}$ to both states. This inequality can be strengthened with a remainder term in the form of a distance between $\rho$ and the closest recovered state $(\mathcal{R} \circ \mathcal{N})(\rho)$, where $\mathcal{R}$ is a recovery map with the property that $\sigma = (\mathcal{R} \circ \mathcal{N})(\sigma)$. We show the existence of an explicit recovery map that is universal in the sense that it depends only on $\sigma$ and the quantum channel $\mathcal{N}$ to be reversed. This result gives an alternate, information-theoretic characterization of the conditions for approximate quantum error correction. ",universal recovery map approximate sufficiency quantum relative entropy data process inequality state quantum relative entropy two state rho sigma never increase apply quantum channel mathcal state inequality strengthen remainder term form distance rho closest recover state mathcal circ mathcal rho mathcal recovery map property sigma mathcal circ mathcal sigma show existence explicit recovery map universal sense depend sigma quantum channel mathcal reverse result give alternate information theoretic characterization condition approximate quantum error correction,73,7,1509.07127.txt
http://arxiv.org/abs/1509.07314,Adaptive-Robust Control of a Class of Uncertain Nonlinear Systems   Utilizing Time-Delayed Input and Position Feedback,"  In this paper, the tracking control problem of a class of Euler-Lagrange systems subjected to unknown uncertainties is addressed and an adaptive-robust control strategy, christened as Time-Delayed Adaptive Robust Control (TARC) is presented. The proposed control strategy approximates the unknown dynamics through time-delayed logic, and the switching logic provides robustness against the approximation error. The novel adaptation law for the switching gain, in contrast to the conventional adaptive-robust control methodologies, does not require either nominal modelling or predefined bounds of the uncertainties. Also, the proposed adaptive law circumvents the overestimation-underestimation problem of switching gain. The state derivatives in the proposed control law is estimated from past data of the state to alleviate the measurement error when state derivatives are not available directly. Moreover, a new stability notion for time-delayed control is proposed which in turn provides a selection criterion for controller gain and sampling interval. Experimental result of the proposed methodology using a nonholonomic wheeled mobile robot (WMR) is presented and improved tracking accuracy of the proposed control law is noted compared to time-delayed control and adaptive sliding mode control. ",Computer Science - Systems and Control ; ,"Roy, Spandan ; Kar, Indra Narayan ; ","Adaptive-Robust Control of a Class of Uncertain Nonlinear Systems   Utilizing Time-Delayed Input and Position Feedback  In this paper, the tracking control problem of a class of Euler-Lagrange systems subjected to unknown uncertainties is addressed and an adaptive-robust control strategy, christened as Time-Delayed Adaptive Robust Control (TARC) is presented. The proposed control strategy approximates the unknown dynamics through time-delayed logic, and the switching logic provides robustness against the approximation error. The novel adaptation law for the switching gain, in contrast to the conventional adaptive-robust control methodologies, does not require either nominal modelling or predefined bounds of the uncertainties. Also, the proposed adaptive law circumvents the overestimation-underestimation problem of switching gain. The state derivatives in the proposed control law is estimated from past data of the state to alleviate the measurement error when state derivatives are not available directly. Moreover, a new stability notion for time-delayed control is proposed which in turn provides a selection criterion for controller gain and sampling interval. Experimental result of the proposed methodology using a nonholonomic wheeled mobile robot (WMR) is presented and improved tracking accuracy of the proposed control law is noted compared to time-delayed control and adaptive sliding mode control. ",adaptive robust control class uncertain nonlinear systems utilize time delay input position feedback paper track control problem class euler lagrange systems subject unknown uncertainties address adaptive robust control strategy christen time delay adaptive robust control tarc present propose control strategy approximate unknown dynamics time delay logic switch logic provide robustness approximation error novel adaptation law switch gain contrast conventional adaptive robust control methodologies require either nominal model predefined bound uncertainties also propose adaptive law circumvent overestimation underestimation problem switch gain state derivatives propose control law estimate past data state alleviate measurement error state derivatives available directly moreover new stability notion time delay control propose turn provide selection criterion controller gain sample interval experimental result propose methodology use nonholonomic wheel mobile robot wmr present improve track accuracy propose control law note compare time delay control adaptive slide mode control,138,11,1509.07314.txt
http://arxiv.org/abs/1509.07330,Pricing Policies for Selling Indivisible Storable Goods to Strategic   Consumers,"  We study the dynamic pricing problem faced by a monopolistic retailer who sells a storable product to forward-looking consumers. In this framework, the two major pricing policies (or mechanisms) studied in the literature are the preannounced (commitment) pricing policy and the contingent (threat or history dependent) pricing policy. We analyse and compare these pricing policies in the setting where the good can be purchased along a finite time horizon in indivisible atomic quantities. First, we show that, given linear storage costs, the retailer can compute an optimal preannounced pricing policy in polynomial time by solving a dynamic program. Moreover, under such a policy, we show that consumers do not need to store units in order to anticipate price rises. Second, under the contingent pricing policy rather than the preannounced pricing mechanism, (i) prices could be lower, (ii) retailer revenues could be higher, and (iii) consumer surplus could be higher. This result is surprising, in that these three facts are in complete contrast to the case of a retailer selling divisible storable goods Dudine et al. (2006). Third, we quantify exactly how much more profitable a contingent policy could be with respect to a preannounced policy. Specifically, for a market with $N$ consumers, a contingent policy can produce a multiplicative factor of $\Omega(\log N)$ more revenues than a preannounced policy, and this bound is tight. ",Computer Science - Computer Science and Game Theory ; ,"Berbeglia, Gerardo ; Rayaprolu, Gautam ; Vetta, Adrian ; ","Pricing Policies for Selling Indivisible Storable Goods to Strategic   Consumers  We study the dynamic pricing problem faced by a monopolistic retailer who sells a storable product to forward-looking consumers. In this framework, the two major pricing policies (or mechanisms) studied in the literature are the preannounced (commitment) pricing policy and the contingent (threat or history dependent) pricing policy. We analyse and compare these pricing policies in the setting where the good can be purchased along a finite time horizon in indivisible atomic quantities. First, we show that, given linear storage costs, the retailer can compute an optimal preannounced pricing policy in polynomial time by solving a dynamic program. Moreover, under such a policy, we show that consumers do not need to store units in order to anticipate price rises. Second, under the contingent pricing policy rather than the preannounced pricing mechanism, (i) prices could be lower, (ii) retailer revenues could be higher, and (iii) consumer surplus could be higher. This result is surprising, in that these three facts are in complete contrast to the case of a retailer selling divisible storable goods Dudine et al. (2006). Third, we quantify exactly how much more profitable a contingent policy could be with respect to a preannounced policy. Specifically, for a market with $N$ consumers, a contingent policy can produce a multiplicative factor of $\Omega(\log N)$ more revenues than a preannounced policy, and this bound is tight. ",price policies sell indivisible storable goods strategic consumers study dynamic price problem face monopolistic retailer sell storable product forward look consumers framework two major price policies mechanisms study literature preannounced commitment price policy contingent threat history dependent price policy analyse compare price policies set good purchase along finite time horizon indivisible atomic quantities first show give linear storage cost retailer compute optimal preannounced price policy polynomial time solve dynamic program moreover policy show consumers need store units order anticipate price rise second contingent price policy rather preannounced price mechanism price could lower ii retailer revenues could higher iii consumer surplus could higher result surprise three facts complete contrast case retailer sell divisible storable goods dudine et al third quantify exactly much profitable contingent policy could respect preannounced policy specifically market consumers contingent policy produce multiplicative factor omega log revenues preannounced policy bind tight,143,0,1509.07330.txt
http://arxiv.org/abs/1509.07417,Deterministic Sparse Suffix Sorting in the Restore Model,"  Given a text $T$ of length $n$, we propose a deterministic online algorithm computing the sparse suffix array and the sparse longest common prefix array of $T$ in $O(c \sqrt{\lg n} + m \lg m \lg n \lg^* n)$ time with $O(m)$ words of space under the premise that the space of $T$ is rewritable, where $m \le n$ is the number of suffixes to be sorted (provided online and arbitrarily), and $c$ is the number of characters with $m \le c \le n$ that must be compared for distinguishing the designated suffixes. ",Computer Science - Data Structures and Algorithms ; ,"Fischer, Johannes ; I, Tomohiro ; Köppl, Dominik ; ","Deterministic Sparse Suffix Sorting in the Restore Model  Given a text $T$ of length $n$, we propose a deterministic online algorithm computing the sparse suffix array and the sparse longest common prefix array of $T$ in $O(c \sqrt{\lg n} + m \lg m \lg n \lg^* n)$ time with $O(m)$ words of space under the premise that the space of $T$ is rewritable, where $m \le n$ is the number of suffixes to be sorted (provided online and arbitrarily), and $c$ is the number of characters with $m \le c \le n$ that must be compared for distinguishing the designated suffixes. ",deterministic sparse suffix sort restore model give text length propose deterministic online algorithm compute sparse suffix array sparse longest common prefix array sqrt lg lg lg lg time word space premise space rewritable le number suffix sort provide online arbitrarily number character le le must compare distinguish designate suffix,49,9,1509.07417.txt
http://arxiv.org/abs/1509.07552,Formal Support for Standardizing Protocols with State,"  Many cryptographic protocols are designed to achieve their goals using only messages passed over an open network. Numerous tools, based on well-understood foundations, exist for the design and analysis of protocols that rely purely on message passing. However, these tools encounter difficulties when faced with protocols that rely on non-local, mutable state to coordinate several local sessions.   We adapt one of these tools, {\cpsa}, to provide automated support for reasoning about state. We use Ryan's Envelope Protocol as an example to demonstrate how the message-passing reasoning can be integrated with state reasoning to yield interesting and powerful results.   Keywords: protocol analysis tools, stateful protocols, TPM, PKCS#11. ",Computer Science - Cryptography and Security ; ,"Guttman, Joshua D. ; Liskov, Moses D. ; Ramsdell, John D. ; Rowe, Paul D. ; ","Formal Support for Standardizing Protocols with State  Many cryptographic protocols are designed to achieve their goals using only messages passed over an open network. Numerous tools, based on well-understood foundations, exist for the design and analysis of protocols that rely purely on message passing. However, these tools encounter difficulties when faced with protocols that rely on non-local, mutable state to coordinate several local sessions.   We adapt one of these tools, {\cpsa}, to provide automated support for reasoning about state. We use Ryan's Envelope Protocol as an example to demonstrate how the message-passing reasoning can be integrated with state reasoning to yield interesting and powerful results.   Keywords: protocol analysis tools, stateful protocols, TPM, PKCS#11. ",formal support standardize protocols state many cryptographic protocols design achieve goals use message pass open network numerous tool base well understand foundations exist design analysis protocols rely purely message pass however tool encounter difficulties face protocols rely non local mutable state coordinate several local sessions adapt one tool cpsa provide automate support reason state use ryan envelope protocol example demonstrate message pass reason integrate state reason yield interest powerful result keywords protocol analysis tool stateful protocols tpm pkcs,78,4,1509.07552.txt
http://arxiv.org/abs/1509.07808,A (1+epsilon)-Approximation for Makespan Scheduling with Precedence   Constraints using LP Hierarchies,"  In a classical problem in scheduling, one has $n$ unit size jobs with a precedence order and the goal is to find a schedule of those jobs on $m$ identical machines as to minimize the makespan. It is one of the remaining four open problems from the book of Garey & Johnson whether or not this problem is $\mathbf{NP}$-hard for $m=3$.   We prove that for any fixed $\varepsilon$ and $m$, an LP-hierarchy lift of the time-indexed LP with a slightly super poly-logarithmic number of $r = (\log(n))^{\Theta(\log \log n)}$ rounds provides a $(1 + \varepsilon)$-approximation. For example Sherali-Adams suffices as hierarchy. This implies an algorithm that yields a $(1+\varepsilon)$-approximation in time $n^{O(r)}$. The previously best approximation algorithms guarantee a $2 - \frac{7}{3m+1}$-approximation in polynomial time for $m \geq 4$ and $\frac{4}{3}$ for $m=3$. Our algorithm is based on a recursive scheduling approach where in each step we reduce the correlation in form of long chains. Our method adds to the rather short list of examples where hierarchies are actually useful to obtain better approximation algorithms. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; ,"Levey, Elaine ; Rothvoss, Thomas ; ","A (1+epsilon)-Approximation for Makespan Scheduling with Precedence   Constraints using LP Hierarchies  In a classical problem in scheduling, one has $n$ unit size jobs with a precedence order and the goal is to find a schedule of those jobs on $m$ identical machines as to minimize the makespan. It is one of the remaining four open problems from the book of Garey & Johnson whether or not this problem is $\mathbf{NP}$-hard for $m=3$.   We prove that for any fixed $\varepsilon$ and $m$, an LP-hierarchy lift of the time-indexed LP with a slightly super poly-logarithmic number of $r = (\log(n))^{\Theta(\log \log n)}$ rounds provides a $(1 + \varepsilon)$-approximation. For example Sherali-Adams suffices as hierarchy. This implies an algorithm that yields a $(1+\varepsilon)$-approximation in time $n^{O(r)}$. The previously best approximation algorithms guarantee a $2 - \frac{7}{3m+1}$-approximation in polynomial time for $m \geq 4$ and $\frac{4}{3}$ for $m=3$. Our algorithm is based on a recursive scheduling approach where in each step we reduce the correlation in form of long chains. Our method adds to the rather short list of examples where hierarchies are actually useful to obtain better approximation algorithms. ",epsilon approximation makespan schedule precedence constraints use lp hierarchies classical problem schedule one unit size job precedence order goal find schedule job identical machine minimize makespan one remain four open problems book garey johnson whether problem mathbf np hard prove fix varepsilon lp hierarchy lift time index lp slightly super poly logarithmic number log theta log log round provide varepsilon approximation example sherali adams suffice hierarchy imply algorithm yield varepsilon approximation time previously best approximation algorithms guarantee frac approximation polynomial time geq frac algorithm base recursive schedule approach step reduce correlation form long chain method add rather short list examples hierarchies actually useful obtain better approximation algorithms,107,1,1509.07808.txt
http://arxiv.org/abs/1509.08102,Discriminative Learning of the Prototype Set for Nearest Neighbor   Classification,"  The nearest neighbor rule is a classic yet essential classification model, particularly in problems where the supervising information is given by pairwise dissimilarities and the embedding function are not easily obtained. Prototype selection provides means of generalization and improving efficiency of the nearest neighbor model, but many existing methods assume and rely on the analyses of the input vector space. In this paper, we explore a dissimilarity-based, parametrized model of the nearest neighbor rule. In the proposed model, the selection of the nearest prototypes is influenced by the parameters of the respective prototypes. It provides a formulation for minimizing the violation of the extended nearest neighbor rule over the training set in a tractable form to exploit numerical techniques. We show that the minimization problem reduces to a large-margin principle learning and demonstrate its advantage by empirical comparisons with other prototype selection methods. ",Computer Science - Machine Learning ; ,"Ando, Shin ; ","Discriminative Learning of the Prototype Set for Nearest Neighbor   Classification  The nearest neighbor rule is a classic yet essential classification model, particularly in problems where the supervising information is given by pairwise dissimilarities and the embedding function are not easily obtained. Prototype selection provides means of generalization and improving efficiency of the nearest neighbor model, but many existing methods assume and rely on the analyses of the input vector space. In this paper, we explore a dissimilarity-based, parametrized model of the nearest neighbor rule. In the proposed model, the selection of the nearest prototypes is influenced by the parameters of the respective prototypes. It provides a formulation for minimizing the violation of the extended nearest neighbor rule over the training set in a tractable form to exploit numerical techniques. We show that the minimization problem reduces to a large-margin principle learning and demonstrate its advantage by empirical comparisons with other prototype selection methods. ",discriminative learn prototype set nearest neighbor classification nearest neighbor rule classic yet essential classification model particularly problems supervise information give pairwise dissimilarities embed function easily obtain prototype selection provide mean generalization improve efficiency nearest neighbor model many exist methods assume rely analyse input vector space paper explore dissimilarity base parametrized model nearest neighbor rule propose model selection nearest prototypes influence parameters respective prototypes provide formulation minimize violation extend nearest neighbor rule train set tractable form exploit numerical techniques show minimization problem reduce large margin principle learn demonstrate advantage empirical comparisons prototype selection methods,93,11,1509.08102.txt
http://arxiv.org/abs/1509.08346,UB-ANC Drone: A Flexible Airborne Networking and Communications Testbed,"  We present the University at Buffalo's Airborne Networking and Communications Testbed (UB-ANC Drone). UB-ANC Drone is an open software/hardware platform that aims to facilitate rapid testing and repeatable comparative evaluation of airborne networking and communications protocols at different layers of the protocol stack. It combines quadcopters capable of autonomous flight with sophisticated command and control capabilities and embedded software-defined radios (SDRs), which enable flexible deployment of novel communications and networking protocols. This is in contrast to existing airborne network testbeds, which rely on standard inflexible wireless technologies, e.g., Wi-Fi or Zigbee. UB-ANC Drone is designed with emphasis on modularity and extensibility, and is built around popular open-source projects and standards developed by the research and hobby communities. This makes UB-ANC Drone highly customizable, while also simplifying its adoption. In this paper, we describe UB-ANC Drone's hardware and software architecture. ",Computer Science - Networking and Internet Architecture ; Computer Science - Robotics ; ,"Modares, Jalil ; Mastronarde, Nicholas ; ","UB-ANC Drone: A Flexible Airborne Networking and Communications Testbed  We present the University at Buffalo's Airborne Networking and Communications Testbed (UB-ANC Drone). UB-ANC Drone is an open software/hardware platform that aims to facilitate rapid testing and repeatable comparative evaluation of airborne networking and communications protocols at different layers of the protocol stack. It combines quadcopters capable of autonomous flight with sophisticated command and control capabilities and embedded software-defined radios (SDRs), which enable flexible deployment of novel communications and networking protocols. This is in contrast to existing airborne network testbeds, which rely on standard inflexible wireless technologies, e.g., Wi-Fi or Zigbee. UB-ANC Drone is designed with emphasis on modularity and extensibility, and is built around popular open-source projects and standards developed by the research and hobby communities. This makes UB-ANC Drone highly customizable, while also simplifying its adoption. In this paper, we describe UB-ANC Drone's hardware and software architecture. ",ub anc drone flexible airborne network communications testbed present university buffalo airborne network communications testbed ub anc drone ub anc drone open software hardware platform aim facilitate rapid test repeatable comparative evaluation airborne network communications protocols different layer protocol stack combine quadcopters capable autonomous flight sophisticate command control capabilities embed software define radio sdrs enable flexible deployment novel communications network protocols contrast exist airborne network testbeds rely standard inflexible wireless technologies wi fi zigbee ub anc drone design emphasis modularity extensibility build around popular open source project standards develop research hobby communities make ub anc drone highly customizable also simplify adoption paper describe ub anc drone hardware software architecture,109,6,1509.08346.txt
http://arxiv.org/abs/1509.08690,Kempe's Universality Theorem for Rational Space Curves,  We prove that every bounded rational space curve of degree d and circularity c can be drawn by a linkage with 9/2 d - 6c + 1 revolute joints. Our proof is based on two ingredients. The first one is the factorization theory of motion polynomials. The second one is the construction of a motion polynomial of minimum degree with given orbit. Our proof also gives the explicity construction of the linkage. ,"Computer Science - Computational Geometry ; Computer Science - Robotics ; Computer Science - Symbolic Computation ; Mathematics - Algebraic Geometry ; Mathematics - Rings and Algebras ; 70B05, 13F20, 65D17, 68U07 ; ","Li, Zijia ; Schicho, Josef ; Schröcker, Hans-Peter ; ",Kempe's Universality Theorem for Rational Space Curves  We prove that every bounded rational space curve of degree d and circularity c can be drawn by a linkage with 9/2 d - 6c + 1 revolute joints. Our proof is based on two ingredients. The first one is the factorization theory of motion polynomials. The second one is the construction of a motion polynomial of minimum degree with given orbit. Our proof also gives the explicity construction of the linkage. ,kempe universality theorem rational space curve prove every bound rational space curve degree circularity draw linkage revolute joint proof base two ingredients first one factorization theory motion polynomials second one construction motion polynomial minimum degree give orbit proof also give explicity construction linkage,43,4,1509.08690.txt
http://arxiv.org/abs/1509.08764,On the Min-cost Traveling Salesman Problem with Drone,"  Over the past few years, unmanned aerial vehicles (UAV), also known as drones, have been adopted as part of a new logistic method in the commercial sector called ""last-mile delivery"". In this novel approach, they are deployed alongside trucks to deliver goods to customers to improve the quality of service and reduce the transportation cost. This approach gives rise to a new variant of the traveling salesman problem (TSP), called TSP with drone (TSP-D). A variant of this problem that aims to minimize the time at which truck and drone finish the service (or, in other words, to maximize the quality of service) was studied in the work of Murray and Chu (2015). In contrast, this paper considers a new variant of TSP-D in which the objective is to minimize operational costs including total transportation cost and one created by waste time a vehicle has to wait for the other. The problem is first formulated mathematically. Then, two algorithms are proposed for the solution. The first algorithm (TSP-LS) was adapted from the approach proposed by Murray and Chu (2015), in which an optimal TSP solution is converted to a feasible TSP-D solution by local searches. The second algorithm, a Greedy Randomized Adaptive Search Procedure (GRASP), is based on a new split procedure that optimally splits any TSP tour into a TSP-D solution. After a TSP-D solution has been generated, it is then improved through local search operators. Numerical results obtained on various instances of both objective functions with different sizes and characteristics are presented. The results show that GRASP outperforms TSP-LS in terms of solution quality under an acceptable running time. ",Computer Science - Artificial Intelligence ; ,"Ha, Quang Minh ; Deville, Yves ; Pham, Quang Dung ; Hà, Minh Hoàng ; ","On the Min-cost Traveling Salesman Problem with Drone  Over the past few years, unmanned aerial vehicles (UAV), also known as drones, have been adopted as part of a new logistic method in the commercial sector called ""last-mile delivery"". In this novel approach, they are deployed alongside trucks to deliver goods to customers to improve the quality of service and reduce the transportation cost. This approach gives rise to a new variant of the traveling salesman problem (TSP), called TSP with drone (TSP-D). A variant of this problem that aims to minimize the time at which truck and drone finish the service (or, in other words, to maximize the quality of service) was studied in the work of Murray and Chu (2015). In contrast, this paper considers a new variant of TSP-D in which the objective is to minimize operational costs including total transportation cost and one created by waste time a vehicle has to wait for the other. The problem is first formulated mathematically. Then, two algorithms are proposed for the solution. The first algorithm (TSP-LS) was adapted from the approach proposed by Murray and Chu (2015), in which an optimal TSP solution is converted to a feasible TSP-D solution by local searches. The second algorithm, a Greedy Randomized Adaptive Search Procedure (GRASP), is based on a new split procedure that optimally splits any TSP tour into a TSP-D solution. After a TSP-D solution has been generated, it is then improved through local search operators. Numerical results obtained on various instances of both objective functions with different sizes and characteristics are presented. The results show that GRASP outperforms TSP-LS in terms of solution quality under an acceptable running time. ",min cost travel salesman problem drone past years unman aerial vehicles uav also know drone adopt part new logistic method commercial sector call last mile delivery novel approach deploy alongside truck deliver goods customers improve quality service reduce transportation cost approach give rise new variant travel salesman problem tsp call tsp drone tsp variant problem aim minimize time truck drone finish service word maximize quality service study work murray chu contrast paper consider new variant tsp objective minimize operational cost include total transportation cost one create waste time vehicle wait problem first formulate mathematically two algorithms propose solution first algorithm tsp ls adapt approach propose murray chu optimal tsp solution convert feasible tsp solution local search second algorithm greedy randomize adaptive search procedure grasp base new split procedure optimally split tsp tour tsp solution tsp solution generate improve local search operators numerical result obtain various instance objective function different size characteristics present result show grasp outperform tsp ls term solution quality acceptable run time,164,11,1509.08764.txt
http://arxiv.org/abs/1509.08892,A data-dependent weighted LASSO under Poisson noise,"  Sparse linear inverse problems appear in a variety of settings, but often the noise contaminating observations cannot accurately be described as bounded by or arising from a Gaussian distribution. Poisson observations in particular are a feature of several real-world applications. Previous work on sparse Poisson inverse problems encountered several limiting technical hurdles. This paper describes a novel alternative analysis approach for sparse Poisson inverse problems that (a) sidesteps the technical challenges in previous work, (b) admits estimators that can readily be computed using off-the-shelf LASSO algorithms, and (c) hints at a general framework for broad classes of noise in sparse linear inverse problems. At the heart of this new approach lies a weighted LASSO estimator for which data-dependent weights are based on Poisson concentration inequalities. Unlike previous analyses of the weighted LASSO, the proposed analysis depends on conditions which can be checked or shown to hold in general settings with high probability. ",Mathematics - Statistics Theory ; Computer Science - Information Theory ; ,"Jiang, Xin ; Reynaud-Bouret, Patricia ; Rivoirard, Vincent ; Sansonnet, Laure ; Willett, Rebecca ; ","A data-dependent weighted LASSO under Poisson noise  Sparse linear inverse problems appear in a variety of settings, but often the noise contaminating observations cannot accurately be described as bounded by or arising from a Gaussian distribution. Poisson observations in particular are a feature of several real-world applications. Previous work on sparse Poisson inverse problems encountered several limiting technical hurdles. This paper describes a novel alternative analysis approach for sparse Poisson inverse problems that (a) sidesteps the technical challenges in previous work, (b) admits estimators that can readily be computed using off-the-shelf LASSO algorithms, and (c) hints at a general framework for broad classes of noise in sparse linear inverse problems. At the heart of this new approach lies a weighted LASSO estimator for which data-dependent weights are based on Poisson concentration inequalities. Unlike previous analyses of the weighted LASSO, the proposed analysis depends on conditions which can be checked or shown to hold in general settings with high probability. ",data dependent weight lasso poisson noise sparse linear inverse problems appear variety settings often noise contaminate observations cannot accurately describe bound arise gaussian distribution poisson observations particular feature several real world applications previous work sparse poisson inverse problems encounter several limit technical hurdle paper describe novel alternative analysis approach sparse poisson inverse problems sidestep technical challenge previous work admit estimators readily compute use shelf lasso algorithms hint general framework broad class noise sparse linear inverse problems heart new approach lie weight lasso estimator data dependent weight base poisson concentration inequalities unlike previous analyse weight lasso propose analysis depend condition check show hold general settings high probability,106,9,1509.08892.txt
http://arxiv.org/abs/1509.08979,Fixpoint Node Selection Query Languages for Trees,"  The study of node selection query languages for (finite) trees has been a major topic in the recent research on query languages for Web documents. On one hand, there has been an extensive study of XPath and its various extensions. On the other hand, query languages based on classical logics, such as first-order logic (FO) or Monadic Second-Order Logic (MSO), have been considered. Results in this area typically relate an XPath-based language to a classical logic. What has yet to emerge is an XPath-related language that is as expressive as MSO, and at the same time enjoys the computational properties of XPath, which are linear time query evaluation and exponential time query-containment test. In this paper we propose muXPath, which is the alternation-free fragment of XPath extended with fixpoint operators. Using two-way alternating automata, we show that this language does combine desired expressiveness and computational properties, placing it as an attractive candidate for the definite node-selection query language for trees. ",Computer Science - Databases ; Computer Science - Logic in Computer Science ; ,"Calvanese, Diego ; De Giacomo, Giuseppe ; Lenzerini, Maurizio ; Vardi, Moshe Y. ; ","Fixpoint Node Selection Query Languages for Trees  The study of node selection query languages for (finite) trees has been a major topic in the recent research on query languages for Web documents. On one hand, there has been an extensive study of XPath and its various extensions. On the other hand, query languages based on classical logics, such as first-order logic (FO) or Monadic Second-Order Logic (MSO), have been considered. Results in this area typically relate an XPath-based language to a classical logic. What has yet to emerge is an XPath-related language that is as expressive as MSO, and at the same time enjoys the computational properties of XPath, which are linear time query evaluation and exponential time query-containment test. In this paper we propose muXPath, which is the alternation-free fragment of XPath extended with fixpoint operators. Using two-way alternating automata, we show that this language does combine desired expressiveness and computational properties, placing it as an attractive candidate for the definite node-selection query language for trees. ",fixpoint node selection query languages tree study node selection query languages finite tree major topic recent research query languages web document one hand extensive study xpath various extensions hand query languages base classical logics first order logic fo monadic second order logic mso consider result area typically relate xpath base language classical logic yet emerge xpath relate language expressive mso time enjoy computational properties xpath linear time query evaluation exponential time query containment test paper propose muxpath alternation free fragment xpath extend fixpoint operators use two way alternate automata show language combine desire expressiveness computational properties place attractive candidate definite node selection query language tree,105,1,1509.08979.txt
http://arxiv.org/abs/1509.09121,"The ""handedness"" of language: Directional symmetry breaking of sign   usage in words","  Language, which allows complex ideas to be communicated through symbolic sequences, is a characteristic feature of our species and manifested in a multitude of forms. Using large written corpora for many different languages and scripts, we show that the occurrence probability distributions of signs at the left and right ends of words have a distinct heterogeneous nature. Characterizing this asymmetry using quantitative inequality measures, viz. information entropy and the Gini index, we show that the beginning of a word is less restrictive in sign usage than the end. This property is not simply attributable to the use of common affixes as it is seen even when only word roots are considered. We use the existence of this asymmetry to infer the direction of writing in undeciphered inscriptions that agrees with the archaeological evidence. Unlike traditional investigations of phonotactic constraints which focus on language-specific patterns, our study reveals a property valid across languages and writing systems. As both language and writing are unique aspects of our species, this universal signature may reflect an innate feature of the human cognitive phenomenon. ",Computer Science - Computation and Language ; ,"Ashraf, Md Izhar ; Sinha, Sitabhra ; ","The ""handedness"" of language: Directional symmetry breaking of sign   usage in words  Language, which allows complex ideas to be communicated through symbolic sequences, is a characteristic feature of our species and manifested in a multitude of forms. Using large written corpora for many different languages and scripts, we show that the occurrence probability distributions of signs at the left and right ends of words have a distinct heterogeneous nature. Characterizing this asymmetry using quantitative inequality measures, viz. information entropy and the Gini index, we show that the beginning of a word is less restrictive in sign usage than the end. This property is not simply attributable to the use of common affixes as it is seen even when only word roots are considered. We use the existence of this asymmetry to infer the direction of writing in undeciphered inscriptions that agrees with the archaeological evidence. Unlike traditional investigations of phonotactic constraints which focus on language-specific patterns, our study reveals a property valid across languages and writing systems. As both language and writing are unique aspects of our species, this universal signature may reflect an innate feature of the human cognitive phenomenon. ",handedness language directional symmetry break sign usage word language allow complex ideas communicate symbolic sequence characteristic feature species manifest multitude form use large write corpora many different languages script show occurrence probability distributions sign leave right end word distinct heterogeneous nature characterize asymmetry use quantitative inequality measure viz information entropy gini index show begin word less restrictive sign usage end property simply attributable use common affix see even word root consider use existence asymmetry infer direction write undeciphered inscriptions agree archaeological evidence unlike traditional investigations phonotactic constraints focus language specific pattern study reveal property valid across languages write systems language write unique aspects species universal signature may reflect innate feature human cognitive phenomenon,113,14,1509.09121.txt
http://arxiv.org/abs/1509.09188,Approximate Spectral Clustering: Efficiency and Guarantees,"  Approximate Spectral Clustering (ASC) is a popular and successful heuristic for partitioning the nodes of a graph $G$ into clusters for which the ratio of outside connections compared to the volume (sum of degrees) is small. ASC consists of the following two subroutines: i) compute an approximate Spectral Embedding via the Power method; and ii) partition the resulting vector set with an approximate $k$-means clustering algorithm. The resulting $k$-means partition naturally induces a $k$-way node partition of $G$.   We give a comprehensive analysis of ASC building on the work of Peng et al.~(SICOMP'17), Boutsidis et al.~(ICML'15) and Ostrovsky et al.~(JACM'13). We show that ASC i) runs efficiently, and ii) yields a good approximation of an optimal $k$-way node partition of $G$. Moreover, we strengthen the quality guarantees of a structural result of Peng et al. by a factor of $k$, and simultaneously weaken the eigenvalue gap assumption. Further, we show that ASC finds a $k$-way node partition of $G$ with the strengthened quality guarantees. ",Computer Science - Discrete Mathematics ; ,"Kolev, Pavel ; Mehlhorn, Kurt ; ","Approximate Spectral Clustering: Efficiency and Guarantees  Approximate Spectral Clustering (ASC) is a popular and successful heuristic for partitioning the nodes of a graph $G$ into clusters for which the ratio of outside connections compared to the volume (sum of degrees) is small. ASC consists of the following two subroutines: i) compute an approximate Spectral Embedding via the Power method; and ii) partition the resulting vector set with an approximate $k$-means clustering algorithm. The resulting $k$-means partition naturally induces a $k$-way node partition of $G$.   We give a comprehensive analysis of ASC building on the work of Peng et al.~(SICOMP'17), Boutsidis et al.~(ICML'15) and Ostrovsky et al.~(JACM'13). We show that ASC i) runs efficiently, and ii) yields a good approximation of an optimal $k$-way node partition of $G$. Moreover, we strengthen the quality guarantees of a structural result of Peng et al. by a factor of $k$, and simultaneously weaken the eigenvalue gap assumption. Further, we show that ASC finds a $k$-way node partition of $G$ with the strengthened quality guarantees. ",approximate spectral cluster efficiency guarantee approximate spectral cluster asc popular successful heuristic partition nod graph cluster ratio outside connections compare volume sum degrees small asc consist follow two subroutines compute approximate spectral embed via power method ii partition result vector set approximate mean cluster algorithm result mean partition naturally induce way node partition give comprehensive analysis asc build work peng et al sicomp boutsidis et al icml ostrovsky et al jacm show asc run efficiently ii yield good approximation optimal way node partition moreover strengthen quality guarantee structural result peng et al factor simultaneously weaken eigenvalue gap assumption show asc find way node partition strengthen quality guarantee,107,4,1509.09188.txt
http://arxiv.org/abs/1509.09236,On the Complexity of Robust PCA and $\ell_1$-norm Low-Rank Matrix   Approximation,"  The low-rank matrix approximation problem with respect to the component-wise $\ell_1$-norm ($\ell_1$-LRA), which is closely related to robust principal component analysis (PCA), has become a very popular tool in data mining and machine learning. Robust PCA aims at recovering a low-rank matrix that was perturbed with sparse noise, with applications for example in foreground-background video separation. Although $\ell_1$-LRA is strongly believed to be NP-hard, there is, to the best of our knowledge, no formal proof of this fact. In this paper, we prove that $\ell_1$-LRA is NP-hard, already in the rank-one case, using a reduction from MAX CUT. Our derivations draw interesting connections between $\ell_1$-LRA and several other well-known problems, namely, robust PCA, $\ell_0$-LRA, binary matrix factorization, a particular densest bipartite subgraph problem, the computation of the cut norm of $\{-1,+1\}$ matrices, and the discrete basis problem, which we all prove to be NP-hard. ",Computer Science - Machine Learning ; Computer Science - Computational Complexity ; Mathematics - Numerical Analysis ; Mathematics - Optimization and Control ; ,"Gillis, Nicolas ; Vavasis, Stephen A. ; ","On the Complexity of Robust PCA and $\ell_1$-norm Low-Rank Matrix   Approximation  The low-rank matrix approximation problem with respect to the component-wise $\ell_1$-norm ($\ell_1$-LRA), which is closely related to robust principal component analysis (PCA), has become a very popular tool in data mining and machine learning. Robust PCA aims at recovering a low-rank matrix that was perturbed with sparse noise, with applications for example in foreground-background video separation. Although $\ell_1$-LRA is strongly believed to be NP-hard, there is, to the best of our knowledge, no formal proof of this fact. In this paper, we prove that $\ell_1$-LRA is NP-hard, already in the rank-one case, using a reduction from MAX CUT. Our derivations draw interesting connections between $\ell_1$-LRA and several other well-known problems, namely, robust PCA, $\ell_0$-LRA, binary matrix factorization, a particular densest bipartite subgraph problem, the computation of the cut norm of $\{-1,+1\}$ matrices, and the discrete basis problem, which we all prove to be NP-hard. ",complexity robust pca ell norm low rank matrix approximation low rank matrix approximation problem respect component wise ell norm ell lra closely relate robust principal component analysis pca become popular tool data mine machine learn robust pca aim recover low rank matrix perturb sparse noise applications example foreground background video separation although ell lra strongly believe np hard best knowledge formal proof fact paper prove ell lra np hard already rank one case use reduction max cut derivations draw interest connections ell lra several well know problems namely robust pca ell lra binary matrix factorization particular densest bipartite subgraph problem computation cut norm matrices discrete basis problem prove np hard,110,7,1509.09236.txt
http://arxiv.org/abs/1510.00215,FPT Approximation Schemes for Maximizing Submodular Functions,"  We investigate the existence of approximation algorithms for maximization of submodular functions, that run in fixed parameter tractable (FPT) time. Given a non-decreasing submodular set function $v: 2^X \to \mathbb{R}$ the goal is to select a subset $S$ of $K$ elements from $X$ such that $v(S)$ is maximized. We identify three properties of set functions, referred to as $p$-separability properties, and we argue that many real-life problems can be expressed as maximization of submodular, $p$-separable functions, with low values of the parameter $p$. We present FPT approximation schemes for the minimization and maximization variants of the problem, for several parameters that depend on characteristics of the optimized set function, such as $p$ and $K$. We confirm that our algorithms are applicable to a broad class of problems, in particular to problems from computational social choice, such as item selection or winner determination under several multiwinner election systems. ",Computer Science - Data Structures and Algorithms ; ,"Skowron, Piotr ; ","FPT Approximation Schemes for Maximizing Submodular Functions  We investigate the existence of approximation algorithms for maximization of submodular functions, that run in fixed parameter tractable (FPT) time. Given a non-decreasing submodular set function $v: 2^X \to \mathbb{R}$ the goal is to select a subset $S$ of $K$ elements from $X$ such that $v(S)$ is maximized. We identify three properties of set functions, referred to as $p$-separability properties, and we argue that many real-life problems can be expressed as maximization of submodular, $p$-separable functions, with low values of the parameter $p$. We present FPT approximation schemes for the minimization and maximization variants of the problem, for several parameters that depend on characteristics of the optimized set function, such as $p$ and $K$. We confirm that our algorithms are applicable to a broad class of problems, in particular to problems from computational social choice, such as item selection or winner determination under several multiwinner election systems. ",fpt approximation scheme maximize submodular function investigate existence approximation algorithms maximization submodular function run fix parameter tractable fpt time give non decrease submodular set function mathbb goal select subset elements maximize identify three properties set function refer separability properties argue many real life problems express maximization submodular separable function low value parameter present fpt approximation scheme minimization maximization variants problem several parameters depend characteristics optimize set function confirm algorithms applicable broad class problems particular problems computational social choice item selection winner determination several multiwinner election systems,86,7,1510.00215.txt
http://arxiv.org/abs/1510.00347,Coding Theorem and Converse for Abstract Channels with Time Structure   and Memory,"  A coding theorem and converse are proved for a large class of abstract stationary channels with time structure including the result by Kadota and Wyner (1972) on continuous-time real-valued channels as special cases. As main contribution the coding theorem is proved for a significantly weaker condition on the channel output memory - called total ergodicity w.r.t. finite alphabet block-memoryless input sources - and under a crucial relaxation of the measurability requirement for the channel. These improvements are achieved by introducing a suitable characterization of information rate capacity. It is shown that the $\psi$-mixing output memory condition used by Kadota and Wyner is quite restrictive and excludes important channel models, in particular for the class of Gaussian channels. In fact, it is proved that for Gaussian (e.g., fading or additive noise) channels the $\psi$-mixing condition is equivalent to finite output memory. Further, it is demonstrated that the measurability requirement of Kadota and Wyner is not satisfied for relevant continuous-time channel models such as linear filters, whereas the condition used in this paper is satisfied for these models. Moreover, a weak converse is derived for all stationary channels with time structure. Intersymbol interference as well as input constraints are taken into account in a general and flexible way, including amplitude and average power constraints as special case. Formulated in rigorous mathematical terms complete, explicit, and transparent proofs are presented. As a side product a gap in the proof of Kadota and Wyner - illustrated by a counterexample - is closed by providing a corrected proof of a lemma on the monotonicity of some sequence of normalized mutual information quantities. An abstract framework is established to treat discrete- and continuous-time channels with memory and arbitrary alphabets in a unified way. ",Computer Science - Information Theory ; ,"Mittelbach, Martin ; Jorswieck, Eduard A. ; ","Coding Theorem and Converse for Abstract Channels with Time Structure   and Memory  A coding theorem and converse are proved for a large class of abstract stationary channels with time structure including the result by Kadota and Wyner (1972) on continuous-time real-valued channels as special cases. As main contribution the coding theorem is proved for a significantly weaker condition on the channel output memory - called total ergodicity w.r.t. finite alphabet block-memoryless input sources - and under a crucial relaxation of the measurability requirement for the channel. These improvements are achieved by introducing a suitable characterization of information rate capacity. It is shown that the $\psi$-mixing output memory condition used by Kadota and Wyner is quite restrictive and excludes important channel models, in particular for the class of Gaussian channels. In fact, it is proved that for Gaussian (e.g., fading or additive noise) channels the $\psi$-mixing condition is equivalent to finite output memory. Further, it is demonstrated that the measurability requirement of Kadota and Wyner is not satisfied for relevant continuous-time channel models such as linear filters, whereas the condition used in this paper is satisfied for these models. Moreover, a weak converse is derived for all stationary channels with time structure. Intersymbol interference as well as input constraints are taken into account in a general and flexible way, including amplitude and average power constraints as special case. Formulated in rigorous mathematical terms complete, explicit, and transparent proofs are presented. As a side product a gap in the proof of Kadota and Wyner - illustrated by a counterexample - is closed by providing a corrected proof of a lemma on the monotonicity of some sequence of normalized mutual information quantities. An abstract framework is established to treat discrete- and continuous-time channels with memory and arbitrary alphabets in a unified way. ",cod theorem converse abstract channel time structure memory cod theorem converse prove large class abstract stationary channel time structure include result kadota wyner continuous time real value channel special case main contribution cod theorem prove significantly weaker condition channel output memory call total ergodicity finite alphabet block memoryless input source crucial relaxation measurability requirement channel improvements achieve introduce suitable characterization information rate capacity show psi mix output memory condition use kadota wyner quite restrictive exclude important channel model particular class gaussian channel fact prove gaussian fade additive noise channel psi mix condition equivalent finite output memory demonstrate measurability requirement kadota wyner satisfy relevant continuous time channel model linear filter whereas condition use paper satisfy model moreover weak converse derive stationary channel time structure intersymbol interference well input constraints take account general flexible way include amplitude average power constraints special case formulate rigorous mathematical term complete explicit transparent proof present side product gap proof kadota wyner illustrate counterexample close provide correct proof lemma monotonicity sequence normalize mutual information quantities abstract framework establish treat discrete continuous time channel memory arbitrary alphabets unify way,181,5,1510.00347.txt
http://arxiv.org/abs/1510.00549,Bishellable drawings of $K_n$,"  The Harary--Hill conjecture, still open after more than 50 years, asserts that the crossing number of the complete graph $K_n$ is $ H(n) = \frac 1 4 \left\lfloor\frac{\mathstrut n}{\mathstrut 2}\right\rfloor \left\lfloor\frac{\mathstrut n-1}{\mathstrut 2}\right\rfloor \left\lfloor\frac{\mathstrut n-2}{\mathstrut 2}\right\rfloor \left\lfloor\frac{\mathstrut n-3}{\mathstrut 2}\right \rfloor$. \'Abrego et al. introduced the notion of shellability of a drawing $D$ of $K_n$. They proved that if $D$ is $s$-shellable for some $s\geq\lfloor\frac{n}{2}\rfloor$, then $D$ has at least $H(n)$ crossings. This is the first combinatorial condition on a drawing that guarantees at least $H(n)$ crossings. In this work, we generalize the concept of $s$-shellability to bishellability, where the former implies the latter in the sense that every $s$-shellable drawing is, for any $b \leq s-2$, also $b$-bishellable. Our main result is that $(\lfloor \frac{n}{2} \rfloor\!-\!2)$-bishellability of a drawing $D$ of $K_n$ also guarantees, with a simpler proof than for $s$-shellability, that $D$ has at least $H(n)$ crossings. We exhibit a drawing of $K_{11}$ that has $H(11)$ crossings, is 3-bishellable, and is not $s$-shellable for any $s\geq5$. This shows that we have properly extended the class of drawings for which the Harary-Hill Conjecture is proved. Moreover, we provide an infinite family of drawings of $K_n$ that are $(\lfloor \frac{n}{2} \rfloor\!-\!2)$-bishellable, but not $s$-shellable for any $s\geq\lfloor\frac{n}{2}\rfloor$. ","Mathematics - Combinatorics ; Computer Science - Computational Geometry ; 05C10, 68R10 ; ","Ábrego, Bernardo M. ; Aichholzer, Oswin ; Fernández-Merchant, Silvia ; McQuillan, Dan ; Mohar, Bojan ; Mutzel, Petra ; Ramos, Pedro ; Richter, R. Bruce ; Vogtenhuber, Birgit ; ","Bishellable drawings of $K_n$  The Harary--Hill conjecture, still open after more than 50 years, asserts that the crossing number of the complete graph $K_n$ is $ H(n) = \frac 1 4 \left\lfloor\frac{\mathstrut n}{\mathstrut 2}\right\rfloor \left\lfloor\frac{\mathstrut n-1}{\mathstrut 2}\right\rfloor \left\lfloor\frac{\mathstrut n-2}{\mathstrut 2}\right\rfloor \left\lfloor\frac{\mathstrut n-3}{\mathstrut 2}\right \rfloor$. \'Abrego et al. introduced the notion of shellability of a drawing $D$ of $K_n$. They proved that if $D$ is $s$-shellable for some $s\geq\lfloor\frac{n}{2}\rfloor$, then $D$ has at least $H(n)$ crossings. This is the first combinatorial condition on a drawing that guarantees at least $H(n)$ crossings. In this work, we generalize the concept of $s$-shellability to bishellability, where the former implies the latter in the sense that every $s$-shellable drawing is, for any $b \leq s-2$, also $b$-bishellable. Our main result is that $(\lfloor \frac{n}{2} \rfloor\!-\!2)$-bishellability of a drawing $D$ of $K_n$ also guarantees, with a simpler proof than for $s$-shellability, that $D$ has at least $H(n)$ crossings. We exhibit a drawing of $K_{11}$ that has $H(11)$ crossings, is 3-bishellable, and is not $s$-shellable for any $s\geq5$. This shows that we have properly extended the class of drawings for which the Harary-Hill Conjecture is proved. Moreover, we provide an infinite family of drawings of $K_n$ that are $(\lfloor \frac{n}{2} \rfloor\!-\!2)$-bishellable, but not $s$-shellable for any $s\geq\lfloor\frac{n}{2}\rfloor$. ",bishellable draw harary hill conjecture still open years assert cross number complete graph frac leave lfloor frac mathstrut mathstrut right rfloor leave lfloor frac mathstrut mathstrut right rfloor leave lfloor frac mathstrut mathstrut right rfloor leave lfloor frac mathstrut mathstrut right rfloor abrego et al introduce notion shellability draw prove shellable geq lfloor frac rfloor least cross first combinatorial condition draw guarantee least cross work generalize concept shellability bishellability former imply latter sense every shellable draw leq also bishellable main result lfloor frac rfloor bishellability draw also guarantee simpler proof shellability least cross exhibit draw cross bishellable shellable geq show properly extend class draw harary hill conjecture prove moreover provide infinite family draw lfloor frac rfloor bishellable shellable geq lfloor frac rfloor,122,8,1510.00549.txt
http://arxiv.org/abs/1510.01072,Routing in Unit Disk Graphs,"  Let $S \subset \mathbb{R}^2$ be a set of $n$ sites. The unit disk graph $\text{UD}(S)$ on $S$ has vertex set $S$ and an edge between two distinct sites $s,t \in S$ if and only if $s$ and $t$ have Euclidean distance $|st| \leq 1$.   A routing scheme $R$ for $\text{UD}(S)$ assigns to each site $s \in S$ a label $\ell(s)$ and a routing table $\rho(s)$. For any two sites $s, t \in S$, the scheme $R$ must be able to route a packet from $s$ to $t$ in the following way: given a current site $r$ (initially, $r = s$), a header $h$ (initially empty), and the label $\ell(t)$ of the target, the scheme $R$ consults the routing table $\rho(r)$ to compute a neighbor $r'$ of $r$, a new header $h'$, and the label $\ell(t')$ of an intermediate target $t'$. (The label of the original target may be stored at the header $h'$.) The packet is then routed to $r'$, and the procedure is repeated until the packet reaches $t$. The resulting sequence of sites is called the routing path. The stretch of $R$ is the maximum ratio of the (Euclidean) length of the routing path produced by $R$ and the shortest path in $\text{UD}(S)$, over all pairs of distinct sites in $S$.   For any given $\varepsilon > 0$, we show how to construct a routing scheme for $\text{UD}(S)$ with stretch $1+\varepsilon$ using labels of $O(\log n)$ bits and routing tables of $O(\varepsilon^{-5}\log^2 n \log^2 D)$ bits, where $D$ is the (Euclidean) diameter of $\text{UD}(S)$. The header size is $O(\log n \log D)$ bits. ",Computer Science - Computational Geometry ; Computer Science - Data Structures and Algorithms ; ,"Kaplan, Haim ; Mulzer, Wolfgang ; Roditty, Liam ; Seiferth, Paul ; ","Routing in Unit Disk Graphs  Let $S \subset \mathbb{R}^2$ be a set of $n$ sites. The unit disk graph $\text{UD}(S)$ on $S$ has vertex set $S$ and an edge between two distinct sites $s,t \in S$ if and only if $s$ and $t$ have Euclidean distance $|st| \leq 1$.   A routing scheme $R$ for $\text{UD}(S)$ assigns to each site $s \in S$ a label $\ell(s)$ and a routing table $\rho(s)$. For any two sites $s, t \in S$, the scheme $R$ must be able to route a packet from $s$ to $t$ in the following way: given a current site $r$ (initially, $r = s$), a header $h$ (initially empty), and the label $\ell(t)$ of the target, the scheme $R$ consults the routing table $\rho(r)$ to compute a neighbor $r'$ of $r$, a new header $h'$, and the label $\ell(t')$ of an intermediate target $t'$. (The label of the original target may be stored at the header $h'$.) The packet is then routed to $r'$, and the procedure is repeated until the packet reaches $t$. The resulting sequence of sites is called the routing path. The stretch of $R$ is the maximum ratio of the (Euclidean) length of the routing path produced by $R$ and the shortest path in $\text{UD}(S)$, over all pairs of distinct sites in $S$.   For any given $\varepsilon > 0$, we show how to construct a routing scheme for $\text{UD}(S)$ with stretch $1+\varepsilon$ using labels of $O(\log n)$ bits and routing tables of $O(\varepsilon^{-5}\log^2 n \log^2 D)$ bits, where $D$ is the (Euclidean) diameter of $\text{UD}(S)$. The header size is $O(\log n \log D)$ bits. ",rout unit disk graph let subset mathbb set sit unit disk graph text ud vertex set edge two distinct sit euclidean distance st leq rout scheme text ud assign site label ell rout table rho two sit scheme must able route packet follow way give current site initially header initially empty label ell target scheme consult rout table rho compute neighbor new header label ell intermediate target label original target may store header packet rout procedure repeat packet reach result sequence sit call rout path stretch maximum ratio euclidean length rout path produce shortest path text ud pair distinct sit give varepsilon show construct rout scheme text ud stretch varepsilon use label log bits rout table varepsilon log log bits euclidean diameter text ud header size log log bits,129,4,1510.01072.txt
http://arxiv.org/abs/1510.01210,Trading Networks with Bilateral Contracts,"  We consider a model of matching in trading networks in which firms can enter into bilateral contracts. In trading networks, stable outcomes, which are immune to deviations of arbitrary sets of firms, may not exist. We define a new solution concept called trail stability. Trail-stable outcomes are immune to consecutive, pairwise deviations between linked firms. We show that any trading network with bilateral contracts has a trail-stable outcome whenever firms' choice functions satisfy the full substitutability condition. For trail-stable outcomes, we prove results on the lattice structure, the rural hospitals theorem, strategy-proofness, and comparative statics of firm entry and exit. We also introduce weak trail stability which is implied by trail stability under full substitutability. We describe relationships between the solution concepts. ",Computer Science - Computer Science and Game Theory ; Economics - General Economics ; J.4 ; G.2.1 ; ,"Fleiner, Tamás ; Jankó, Zsuzsanna ; Tamura, Akihisa ; Teytelboym, Alexander ; ","Trading Networks with Bilateral Contracts  We consider a model of matching in trading networks in which firms can enter into bilateral contracts. In trading networks, stable outcomes, which are immune to deviations of arbitrary sets of firms, may not exist. We define a new solution concept called trail stability. Trail-stable outcomes are immune to consecutive, pairwise deviations between linked firms. We show that any trading network with bilateral contracts has a trail-stable outcome whenever firms' choice functions satisfy the full substitutability condition. For trail-stable outcomes, we prove results on the lattice structure, the rural hospitals theorem, strategy-proofness, and comparative statics of firm entry and exit. We also introduce weak trail stability which is implied by trail stability under full substitutability. We describe relationships between the solution concepts. ",trade network bilateral contract consider model match trade network firm enter bilateral contract trade network stable outcomes immune deviations arbitrary set firm may exist define new solution concept call trail stability trail stable outcomes immune consecutive pairwise deviations link firm show trade network bilateral contract trail stable outcome whenever firm choice function satisfy full substitutability condition trail stable outcomes prove result lattice structure rural hospitals theorem strategy proofness comparative statics firm entry exit also introduce weak trail stability imply trail stability full substitutability describe relationships solution concepts,87,7,1510.01210.txt
http://arxiv.org/abs/1510.01429,Distance-2 MDS codes and latin colorings in the Doob graphs,"  The maximum independent sets in the Doob graphs D(m,n) are analogs of the distance-2 MDS codes in Hamming graphs and of the latin hypercubes. We prove the characterization of these sets stating that every such set is semilinear or reducible. As related objects, we study vertex sets with maximum cut (edge boundary) in D(m,n) and prove some facts on their structure. We show that the considered two classes (the maximum independent sets and the maximum-cut sets) can be defined as classes of completely regular sets with specified 2-by-2 quotient matrices. It is notable that for a set from the considered classes, the eigenvalues of the quotient matrix are the maximum and the minimum eigenvalues of the graph. For D(m,0), we show the existence of a third, intermediate, class of completely regular sets with the same property. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; Computer Science - Information Theory ; 05B30 ; ,"Krotov, Denis ; Bespalov, Evgeny ; ","Distance-2 MDS codes and latin colorings in the Doob graphs  The maximum independent sets in the Doob graphs D(m,n) are analogs of the distance-2 MDS codes in Hamming graphs and of the latin hypercubes. We prove the characterization of these sets stating that every such set is semilinear or reducible. As related objects, we study vertex sets with maximum cut (edge boundary) in D(m,n) and prove some facts on their structure. We show that the considered two classes (the maximum independent sets and the maximum-cut sets) can be defined as classes of completely regular sets with specified 2-by-2 quotient matrices. It is notable that for a set from the considered classes, the eigenvalues of the quotient matrix are the maximum and the minimum eigenvalues of the graph. For D(m,0), we show the existence of a third, intermediate, class of completely regular sets with the same property. ",distance mds cod latin color doob graph maximum independent set doob graph analogs distance mds cod ham graph latin hypercubes prove characterization set state every set semilinear reducible relate object study vertex set maximum cut edge boundary prove facts structure show consider two class maximum independent set maximum cut set define class completely regular set specify quotient matrices notable set consider class eigenvalues quotient matrix maximum minimum eigenvalues graph show existence third intermediate class completely regular set property,78,3,1510.01429.txt
http://arxiv.org/abs/1510.01518,DC Decomposition of Nonconvex Polynomials with Algebraic Techniques,"  We consider the problem of decomposing a multivariate polynomial as the difference of two convex polynomials. We introduce algebraic techniques which reduce this task to linear, second order cone, and semidefinite programming. This allows us to optimize over subsets of valid difference of convex decompositions (dcds) and find ones that speed up the convex-concave procedure (CCP). We prove, however, that optimizing over the entire set of dcds is NP-hard. ",Mathematics - Optimization and Control ; Computer Science - Data Structures and Algorithms ; Statistics - Machine Learning ; ,"Ahmadi, Amir Ali ; Hall, Georgina ; ","DC Decomposition of Nonconvex Polynomials with Algebraic Techniques  We consider the problem of decomposing a multivariate polynomial as the difference of two convex polynomials. We introduce algebraic techniques which reduce this task to linear, second order cone, and semidefinite programming. This allows us to optimize over subsets of valid difference of convex decompositions (dcds) and find ones that speed up the convex-concave procedure (CCP). We prove, however, that optimizing over the entire set of dcds is NP-hard. ",dc decomposition nonconvex polynomials algebraic techniques consider problem decompose multivariate polynomial difference two convex polynomials introduce algebraic techniques reduce task linear second order cone semidefinite program allow us optimize subsets valid difference convex decompositions dcds find ones speed convex concave procedure ccp prove however optimize entire set dcds np hard,50,7,1510.01518.txt
http://arxiv.org/abs/1510.01591,On the codes over the Z_3+vZ_3+v^2Z_3,"  In this paper, we study the structure of cyclic, quasi-cyclic, constacyclic codes and their skew codes over the finite ring R=Z_3+vZ_3+v^2Z_3, v^3=v. The Gray images of cyclic, quasi-cyclic, skew cyclic, skew quasi-cyclic and skew constacyclic codes over R are obtained. A necessary and sufficient condition for cyclic (negacyclic) codes over R that contains its dual has been given. The parameters of quantum error correcting codes are obtained from both cyclic and negacyclic codes over R. It is given some examples. Firstly, quasi-constacyclic and skew quasi-constacyclic codes are introduced. By giving two product, it is investigated their duality. A sufficient condition for 1-generator skew quasi-constacyclic codes to be free is determined. ",Computer Science - Information Theory ; Quantum Physics ; ,"Dertli, Abdullah ; Cengellenmis, Yasein ; Eren, Senol ; ","On the codes over the Z_3+vZ_3+v^2Z_3  In this paper, we study the structure of cyclic, quasi-cyclic, constacyclic codes and their skew codes over the finite ring R=Z_3+vZ_3+v^2Z_3, v^3=v. The Gray images of cyclic, quasi-cyclic, skew cyclic, skew quasi-cyclic and skew constacyclic codes over R are obtained. A necessary and sufficient condition for cyclic (negacyclic) codes over R that contains its dual has been given. The parameters of quantum error correcting codes are obtained from both cyclic and negacyclic codes over R. It is given some examples. Firstly, quasi-constacyclic and skew quasi-constacyclic codes are introduced. By giving two product, it is investigated their duality. A sufficient condition for 1-generator skew quasi-constacyclic codes to be free is determined. ",cod vz paper study structure cyclic quasi cyclic constacyclic cod skew cod finite ring vz gray image cyclic quasi cyclic skew cyclic skew quasi cyclic skew constacyclic cod obtain necessary sufficient condition cyclic negacyclic cod contain dual give parameters quantum error correct cod obtain cyclic negacyclic cod give examples firstly quasi constacyclic skew quasi constacyclic cod introduce give two product investigate duality sufficient condition generator skew quasi constacyclic cod free determine,71,5,1510.01591.txt
http://arxiv.org/abs/1510.01671,Cross-boundary Behavioural Reprogrammability Reveals Evidence of   Pervasive Universality,"  We exhaustively explore the reprogrammability capabilities and the intrinsic universality of the Cartesian product $P \times C$ of the space $P$ of all possible computer programs of increasing size and the space $C$ of all possible compilers of increasing length such that $p \in P$ emulates $p^\prime \in P$ with $T|p^\prime|=|p|$ under a coarse-graining transformation $T$. Our approach yields a novel perspective on the complexity, controllability, causality and (re)programmability discrete dynamical systems. We find evidence that the density of (qualitatively different) computer programs that can be reprogrammed grows asymptotically as a function of program and compiler size. To illustrate these findings we show a series of behavioural boundary crossing results, including emulations (for all initial conditions) of Wolfram class 2 Elementary Cellular Automata (ECA) by Class 1 ECA, emulations of Classes 1, 2 and 3 ECA by Class 2 and 3 ECA, and of Classes 1, 2 and 3 by Class 3 ECA, along with results of even greater emulability for general CA (neighbourhood $r=3/2$), including Class 1 CA emulating Classes 2 and 3, and Classes 3 and 4 emulating all other classes (1, 2, 3 and 4). The emulations occur with only a linear overhead and can be considered computationally efficient. We also found that there is no hacking strategy to compress the search space based on compiler profiling in terms of e.g. similarity or complexity, suggesting that no strategy other than exhaustive search is viable. We also introduce emulation networks, derive a topologically-based measure of complexity based upon out- and in-degree connectivity, and establish bridges to fundamental ideas of complexity, universality, causality and dynamical systems. ",Computer Science - Formal Languages and Automata Theory ; Computer Science - Computational Complexity ; Nonlinear Sciences - Cellular Automata and Lattice Gases ; ,"Riedel, Jürgen ; Zenil, Hector ; ","Cross-boundary Behavioural Reprogrammability Reveals Evidence of   Pervasive Universality  We exhaustively explore the reprogrammability capabilities and the intrinsic universality of the Cartesian product $P \times C$ of the space $P$ of all possible computer programs of increasing size and the space $C$ of all possible compilers of increasing length such that $p \in P$ emulates $p^\prime \in P$ with $T|p^\prime|=|p|$ under a coarse-graining transformation $T$. Our approach yields a novel perspective on the complexity, controllability, causality and (re)programmability discrete dynamical systems. We find evidence that the density of (qualitatively different) computer programs that can be reprogrammed grows asymptotically as a function of program and compiler size. To illustrate these findings we show a series of behavioural boundary crossing results, including emulations (for all initial conditions) of Wolfram class 2 Elementary Cellular Automata (ECA) by Class 1 ECA, emulations of Classes 1, 2 and 3 ECA by Class 2 and 3 ECA, and of Classes 1, 2 and 3 by Class 3 ECA, along with results of even greater emulability for general CA (neighbourhood $r=3/2$), including Class 1 CA emulating Classes 2 and 3, and Classes 3 and 4 emulating all other classes (1, 2, 3 and 4). The emulations occur with only a linear overhead and can be considered computationally efficient. We also found that there is no hacking strategy to compress the search space based on compiler profiling in terms of e.g. similarity or complexity, suggesting that no strategy other than exhaustive search is viable. We also introduce emulation networks, derive a topologically-based measure of complexity based upon out- and in-degree connectivity, and establish bridges to fundamental ideas of complexity, universality, causality and dynamical systems. ",cross boundary behavioural reprogrammability reveal evidence pervasive universality exhaustively explore reprogrammability capabilities intrinsic universality cartesian product time space possible computer program increase size space possible compilers increase length emulate prime prime coarse grain transformation approach yield novel perspective complexity controllability causality programmability discrete dynamical systems find evidence density qualitatively different computer program reprogrammed grow asymptotically function program compiler size illustrate find show series behavioural boundary cross result include emulations initial condition wolfram class elementary cellular automata eca class eca emulations class eca class eca class class eca along result even greater emulability general ca neighbourhood include class ca emulate class class emulate class emulations occur linear overhead consider computationally efficient also find hack strategy compress search space base compiler profile term similarity complexity suggest strategy exhaustive search viable also introduce emulation network derive topologically base measure complexity base upon degree connectivity establish bridge fundamental ideas complexity universality causality dynamical systems,150,8,1510.01671.txt
http://arxiv.org/abs/1510.01819,Balanced Islands in Two Colored Point Sets in the Plane,"  Let $S$ be a set of $n$ points in general position in the plane, $r$ of which are red and $b$ of which are blue. In this paper we prove that there exist: for every $\alpha \in \left [ 0,\frac{1}{2} \right ]$, a convex set containing exactly $\lceil \alpha r\rceil$ red points and exactly $\lceil \alpha b \rceil$ blue points of $S$; a convex set containing exactly $\left \lceil \frac{r+1}{2}\right \rceil$ red points and exactly $\left \lceil \frac{b+1}{2}\right \rceil$ blue points of $S$. Furthermore, we present polynomial time algorithms to find these convex sets. In the first case we provide an $O(n^4)$ time algorithm and an $O(n^2\log n)$ time algorithm in the second case. Finally, if $\lceil \alpha r\rceil+\lceil \alpha b\rceil$ is small, that is, not much larger than $\frac{1}{3}n$, we improve the running time to $O(n \log n)$. ",Computer Science - Computational Geometry ; ,"Aichholzer, Oswin ; Atienza, Nieves ; Fabila-Monroy, Ruy ; Perez-Lantero, Pablo ; Dıaz-Báñez, Jose M. ; Flores-Peñaloza, David ; Vogtenhuber, Birgit ; Urrutia, Jorge ; ","Balanced Islands in Two Colored Point Sets in the Plane  Let $S$ be a set of $n$ points in general position in the plane, $r$ of which are red and $b$ of which are blue. In this paper we prove that there exist: for every $\alpha \in \left [ 0,\frac{1}{2} \right ]$, a convex set containing exactly $\lceil \alpha r\rceil$ red points and exactly $\lceil \alpha b \rceil$ blue points of $S$; a convex set containing exactly $\left \lceil \frac{r+1}{2}\right \rceil$ red points and exactly $\left \lceil \frac{b+1}{2}\right \rceil$ blue points of $S$. Furthermore, we present polynomial time algorithms to find these convex sets. In the first case we provide an $O(n^4)$ time algorithm and an $O(n^2\log n)$ time algorithm in the second case. Finally, if $\lceil \alpha r\rceil+\lceil \alpha b\rceil$ is small, that is, not much larger than $\frac{1}{3}n$, we improve the running time to $O(n \log n)$. ",balance islands two color point set plane let set point general position plane red blue paper prove exist every alpha leave frac right convex set contain exactly lceil alpha rceil red point exactly lceil alpha rceil blue point convex set contain exactly leave lceil frac right rceil red point exactly leave lceil frac right rceil blue point furthermore present polynomial time algorithms find convex set first case provide time algorithm log time algorithm second case finally lceil alpha rceil lceil alpha rceil small much larger frac improve run time log,90,1,1510.01819.txt
http://arxiv.org/abs/1510.01844,Linear Bounds between Contraction Coefficients for $f$-Divergences,"  Data processing inequalities for $f$-divergences can be sharpened using constants called ""contraction coefficients"" to produce strong data processing inequalities. For any discrete source-channel pair, the contraction coefficients for $f$-divergences are lower bounded by the contraction coefficient for $\chi^2$-divergence. In this paper, we elucidate that this lower bound can be achieved by driving the input $f$-divergences of the contraction coefficients to zero. Then, we establish a linear upper bound on the contraction coefficients for a certain class of $f$-divergences using the contraction coefficient for $\chi^2$-divergence, and refine this upper bound for the salient special case of Kullback-Leibler (KL) divergence. Furthermore, we present an alternative proof of the fact that the contraction coefficients for KL and $\chi^2$-divergences are equal for a Gaussian source with an additive Gaussian noise channel (where the former coefficient can be power constrained). Finally, we generalize the well-known result that contraction coefficients of channels (after extremizing over all possible sources) for all $f$-divergences with non-linear operator convex $f$ are equal. In particular, we prove that the so called ""less noisy"" preorder over channels can be equivalently characterized by any non-linear operator convex $f$-divergence. ",Computer Science - Information Theory ; Mathematics - Probability ; Mathematics - Statistics Theory ; ,"Makur, Anuran ; Zheng, Lizhong ; ","Linear Bounds between Contraction Coefficients for $f$-Divergences  Data processing inequalities for $f$-divergences can be sharpened using constants called ""contraction coefficients"" to produce strong data processing inequalities. For any discrete source-channel pair, the contraction coefficients for $f$-divergences are lower bounded by the contraction coefficient for $\chi^2$-divergence. In this paper, we elucidate that this lower bound can be achieved by driving the input $f$-divergences of the contraction coefficients to zero. Then, we establish a linear upper bound on the contraction coefficients for a certain class of $f$-divergences using the contraction coefficient for $\chi^2$-divergence, and refine this upper bound for the salient special case of Kullback-Leibler (KL) divergence. Furthermore, we present an alternative proof of the fact that the contraction coefficients for KL and $\chi^2$-divergences are equal for a Gaussian source with an additive Gaussian noise channel (where the former coefficient can be power constrained). Finally, we generalize the well-known result that contraction coefficients of channels (after extremizing over all possible sources) for all $f$-divergences with non-linear operator convex $f$ are equal. In particular, we prove that the so called ""less noisy"" preorder over channels can be equivalently characterized by any non-linear operator convex $f$-divergence. ",linear bound contraction coefficients divergences data process inequalities divergences sharpen use constants call contraction coefficients produce strong data process inequalities discrete source channel pair contraction coefficients divergences lower bound contraction coefficient chi divergence paper elucidate lower bind achieve drive input divergences contraction coefficients zero establish linear upper bind contraction coefficients certain class divergences use contraction coefficient chi divergence refine upper bind salient special case kullback leibler kl divergence furthermore present alternative proof fact contraction coefficients kl chi divergences equal gaussian source additive gaussian noise channel former coefficient power constrain finally generalize well know result contraction coefficients channel extremizing possible source divergences non linear operator convex equal particular prove call less noisy preorder channel equivalently characterize non linear operator convex divergence,120,9,1510.01844.txt
http://arxiv.org/abs/1510.01913,On the Uniform Computational Content of the Baire Category Theorem,"  We study the uniform computational content of different versions of the Baire Category Theorem in the Weihrauch lattice. The Baire Category Theorem can be seen as a pigeonhole principle that states that a complete (i.e., ""large"") metric space cannot be decomposed into countably many nowhere dense (i.e., ""small"") pieces. The Baire Category Theorem is an illuminating example of a theorem that can be used to demonstrate that one classical theorem can have several different computational interpretations. For one, we distinguish two different logical versions of the theorem, where one can be seen as the contrapositive form of the other one. The first version aims to find an uncovered point in the space, given a sequence of nowhere dense closed sets. The second version aims to find the index of a closed set that is somewhere dense, given a sequence of closed sets that cover the space. Even though the two statements behind these versions are equivalent to each other in classical logic, they are not equivalent in intuitionistic logic and likewise they exhibit different computational behavior in the Weihrauch lattice. Besides this logical distinction, we also consider different ways how the sequence of closed sets is ""given"". Essentially, we can distinguish between positive and negative information on closed sets. We discuss all the four resulting versions of the Baire Category Theorem. Somewhat surprisingly it turns out that the difference in providing the input information can also be expressed with the jump operation. Finally, we also relate the Baire Category Theorem to notions of genericity and computably comeager sets. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; ,"Brattka, Vasco ; Hendtlass, Matthew ; Kreuzer, Alexander P. ; ","On the Uniform Computational Content of the Baire Category Theorem  We study the uniform computational content of different versions of the Baire Category Theorem in the Weihrauch lattice. The Baire Category Theorem can be seen as a pigeonhole principle that states that a complete (i.e., ""large"") metric space cannot be decomposed into countably many nowhere dense (i.e., ""small"") pieces. The Baire Category Theorem is an illuminating example of a theorem that can be used to demonstrate that one classical theorem can have several different computational interpretations. For one, we distinguish two different logical versions of the theorem, where one can be seen as the contrapositive form of the other one. The first version aims to find an uncovered point in the space, given a sequence of nowhere dense closed sets. The second version aims to find the index of a closed set that is somewhere dense, given a sequence of closed sets that cover the space. Even though the two statements behind these versions are equivalent to each other in classical logic, they are not equivalent in intuitionistic logic and likewise they exhibit different computational behavior in the Weihrauch lattice. Besides this logical distinction, we also consider different ways how the sequence of closed sets is ""given"". Essentially, we can distinguish between positive and negative information on closed sets. We discuss all the four resulting versions of the Baire Category Theorem. Somewhat surprisingly it turns out that the difference in providing the input information can also be expressed with the jump operation. Finally, we also relate the Baire Category Theorem to notions of genericity and computably comeager sets. ",uniform computational content baire category theorem study uniform computational content different versions baire category theorem weihrauch lattice baire category theorem see pigeonhole principle state complete large metric space cannot decompose countably many nowhere dense small piece baire category theorem illuminate example theorem use demonstrate one classical theorem several different computational interpretations one distinguish two different logical versions theorem one see contrapositive form one first version aim find uncover point space give sequence nowhere dense close set second version aim find index close set somewhere dense give sequence close set cover space even though two statements behind versions equivalent classical logic equivalent intuitionistic logic likewise exhibit different computational behavior weihrauch lattice besides logical distinction also consider different ways sequence close set give essentially distinguish positive negative information close set discuss four result versions baire category theorem somewhat surprisingly turn difference provide input information also express jump operation finally also relate baire category theorem notions genericity computably comeager set,157,8,1510.01913.txt
http://arxiv.org/abs/1510.02438,Decomposing 1-Sperner hypergraphs,"  A hypergraph is Sperner if no hyperedge contains another one. A Sperner hypergraph is equilizable (resp., threshold) if the characteristic vectors of its hyperedges are the (minimal) binary solutions to a linear equation (resp., inequality) with positive coefficients. These combinatorial notions have many applications and are motivated by the theory of Boolean functions and integer programming. We introduce in this paper the class of $1$-Sperner hypergraphs, defined by the property that for every two hyperedges the smallest of their two set differences is of size one. We characterize this class of Sperner hypergraphs by a decomposition theorem and derive several consequences from it. In particular, we obtain bounds on the size of $1$-Sperner hypergraphs and their transversal hypergraphs, show that the characteristic vectors of the hyperedges are linearly independent over the reals, and prove that $1$-Sperner hypergraphs are both threshold and equilizable. The study of $1$-Sperner hypergraphs is motivated also by their applications in graph theory, which we present in a companion paper. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C65, 94C10 ; ","Boros, Endre ; Gurvich, Vladimir ; Milanič, Martin ; ","Decomposing 1-Sperner hypergraphs  A hypergraph is Sperner if no hyperedge contains another one. A Sperner hypergraph is equilizable (resp., threshold) if the characteristic vectors of its hyperedges are the (minimal) binary solutions to a linear equation (resp., inequality) with positive coefficients. These combinatorial notions have many applications and are motivated by the theory of Boolean functions and integer programming. We introduce in this paper the class of $1$-Sperner hypergraphs, defined by the property that for every two hyperedges the smallest of their two set differences is of size one. We characterize this class of Sperner hypergraphs by a decomposition theorem and derive several consequences from it. In particular, we obtain bounds on the size of $1$-Sperner hypergraphs and their transversal hypergraphs, show that the characteristic vectors of the hyperedges are linearly independent over the reals, and prove that $1$-Sperner hypergraphs are both threshold and equilizable. The study of $1$-Sperner hypergraphs is motivated also by their applications in graph theory, which we present in a companion paper. ",decompose sperner hypergraphs hypergraph sperner hyperedge contain another one sperner hypergraph equilizable resp threshold characteristic vectors hyperedges minimal binary solutions linear equation resp inequality positive coefficients combinatorial notions many applications motivate theory boolean function integer program introduce paper class sperner hypergraphs define property every two hyperedges smallest two set differences size one characterize class sperner hypergraphs decomposition theorem derive several consequences particular obtain bound size sperner hypergraphs transversal hypergraphs show characteristic vectors hyperedges linearly independent reals prove sperner hypergraphs threshold equilizable study sperner hypergraphs motivate also applications graph theory present companion paper,92,7,1510.02438.txt
http://arxiv.org/abs/1510.02659,Windrose Planarity: Embedding Graphs with Direction-Constrained Edges,"  Given a planar graph $G$ and a partition of the neighbors of each vertex $v$ in four sets $UR(v)$, $UL(v)$, $DL(v)$, and $DR(v)$, the problem Windrose Planarity asks to decide whether $G$ admits a windrose-planar drawing, that is, a planar drawing in which (i) each neighbor $u \in UR(v)$ is above and to the right of $v$, (ii) each neighbor $u \in UL(v)$ is above and to the left of $v$, (iii) each neighbor $u \in DL(v)$ is below and to the left of $v$, (iv) each neighbor $u \in DR(v)$ is below and to the right of $v$, and (v) edges are represented by curves that are monotone with respect to each axis. By exploiting both the horizontal and the vertical relationship among vertices, windrose-planar drawings allow to simultaneously visualize two partial orders defined by means of the edges of the graph.   Although the problem is NP-hard in the general case, we give a polynomial-time algorithm for testing whether there exists a windrose-planar drawing that respects a given combinatorial embedding. This algorithm is based on a characterization of the plane triangulations admitting a windrose-planar drawing. Furthermore, for any embedded graph with $n$ vertices that has a windrose-planar drawing, we can construct one with at most one bend per edge and with at most $2n-5$ bends in total, which lies on the $3n \times 3n$ grid. The latter result contrasts with the fact that straight-line windrose-planar drawings may require exponential area. ",Computer Science - Computational Geometry ; ,"Angelini, Patrizio ; Da Lozzo, Giordano ; Di Battista, Giuseppe ; Di Donato, Valentino ; Kindermann, Philipp ; Rote, Günter ; Rutter, Ignaz ; ","Windrose Planarity: Embedding Graphs with Direction-Constrained Edges  Given a planar graph $G$ and a partition of the neighbors of each vertex $v$ in four sets $UR(v)$, $UL(v)$, $DL(v)$, and $DR(v)$, the problem Windrose Planarity asks to decide whether $G$ admits a windrose-planar drawing, that is, a planar drawing in which (i) each neighbor $u \in UR(v)$ is above and to the right of $v$, (ii) each neighbor $u \in UL(v)$ is above and to the left of $v$, (iii) each neighbor $u \in DL(v)$ is below and to the left of $v$, (iv) each neighbor $u \in DR(v)$ is below and to the right of $v$, and (v) edges are represented by curves that are monotone with respect to each axis. By exploiting both the horizontal and the vertical relationship among vertices, windrose-planar drawings allow to simultaneously visualize two partial orders defined by means of the edges of the graph.   Although the problem is NP-hard in the general case, we give a polynomial-time algorithm for testing whether there exists a windrose-planar drawing that respects a given combinatorial embedding. This algorithm is based on a characterization of the plane triangulations admitting a windrose-planar drawing. Furthermore, for any embedded graph with $n$ vertices that has a windrose-planar drawing, we can construct one with at most one bend per edge and with at most $2n-5$ bends in total, which lies on the $3n \times 3n$ grid. The latter result contrasts with the fact that straight-line windrose-planar drawings may require exponential area. ",windrose planarity embed graph direction constrain edge give planar graph partition neighbor vertex four set ur ul dl dr problem windrose planarity ask decide whether admit windrose planar draw planar draw neighbor ur right ii neighbor ul leave iii neighbor dl leave iv neighbor dr right edge represent curve monotone respect axis exploit horizontal vertical relationship among vertices windrose planar draw allow simultaneously visualize two partial order define mean edge graph although problem np hard general case give polynomial time algorithm test whether exist windrose planar draw respect give combinatorial embed algorithm base characterization plane triangulations admit windrose planar draw furthermore embed graph vertices windrose planar draw construct one one bend per edge bend total lie time grid latter result contrast fact straight line windrose planar draw may require exponential area,131,3,1510.02659.txt
http://arxiv.org/abs/1510.02786,Recovering a Hidden Community Beyond the Kesten-Stigum Threshold in   $O(|E| \log^*|V|)$ Time,"  Community detection is considered for a stochastic block model graph of n vertices, with K vertices in the planted community, edge probability p for pairs of vertices both in the community, and edge probability q for other pairs of vertices.   The main focus of the paper is on weak recovery of the community based on the graph G, with o(K) misclassified vertices on average, in the sublinear regime $n^{1-o(1)} \leq K \leq o(n).$ A critical parameter is the effective signal-to-noise ratio $\lambda=K^2(p-q)^2/((n-K)q)$, with $\lambda=1$ corresponding to the Kesten-Stigum threshold. We show that a belief propagation algorithm achieves weak recovery if $\lambda>1/e$, beyond the Kesten-Stigum threshold by a factor of $1/e.$ The belief propagation algorithm only needs to run for $\log^\ast n+O(1) $ iterations, with the total time complexity $O(|E| \log^*n)$, where $\log^*n$ is the iterated logarithm of $n.$ Conversely, if $\lambda \leq 1/e$, no local algorithm can asymptotically outperform trivial random guessing. Furthermore, a linear message-passing algorithm that corresponds to applying power iteration to the non-backtracking matrix of the graph is shown to attain weak recovery if and only if $\lambda>1$. In addition, the belief propagation algorithm can be combined with a linear-time voting procedure to achieve the information limit of exact recovery (correctly classify all vertices with high probability) for all $K \ge \frac{n}{\log n} \left( \rho_{\rm BP} +o(1) \right),$ where $\rho_{\rm BP}$ is a function of $p/q$. ",Statistics - Machine Learning ; Computer Science - Computational Complexity ; Computer Science - Social and Information Networks ; Mathematics - Probability ; ,"Hajek, Bruce ; Wu, Yihong ; Xu, Jiaming ; ","Recovering a Hidden Community Beyond the Kesten-Stigum Threshold in   $O(|E| \log^*|V|)$ Time  Community detection is considered for a stochastic block model graph of n vertices, with K vertices in the planted community, edge probability p for pairs of vertices both in the community, and edge probability q for other pairs of vertices.   The main focus of the paper is on weak recovery of the community based on the graph G, with o(K) misclassified vertices on average, in the sublinear regime $n^{1-o(1)} \leq K \leq o(n).$ A critical parameter is the effective signal-to-noise ratio $\lambda=K^2(p-q)^2/((n-K)q)$, with $\lambda=1$ corresponding to the Kesten-Stigum threshold. We show that a belief propagation algorithm achieves weak recovery if $\lambda>1/e$, beyond the Kesten-Stigum threshold by a factor of $1/e.$ The belief propagation algorithm only needs to run for $\log^\ast n+O(1) $ iterations, with the total time complexity $O(|E| \log^*n)$, where $\log^*n$ is the iterated logarithm of $n.$ Conversely, if $\lambda \leq 1/e$, no local algorithm can asymptotically outperform trivial random guessing. Furthermore, a linear message-passing algorithm that corresponds to applying power iteration to the non-backtracking matrix of the graph is shown to attain weak recovery if and only if $\lambda>1$. In addition, the belief propagation algorithm can be combined with a linear-time voting procedure to achieve the information limit of exact recovery (correctly classify all vertices with high probability) for all $K \ge \frac{n}{\log n} \left( \rho_{\rm BP} +o(1) \right),$ where $\rho_{\rm BP}$ is a function of $p/q$. ",recover hide community beyond kesten stigum threshold log time community detection consider stochastic block model graph vertices vertices plant community edge probability pair vertices community edge probability pair vertices main focus paper weak recovery community base graph misclassified vertices average sublinear regime leq leq critical parameter effective signal noise ratio lambda lambda correspond kesten stigum threshold show belief propagation algorithm achieve weak recovery lambda beyond kesten stigum threshold factor belief propagation algorithm need run log ast iterations total time complexity log log iterate logarithm conversely lambda leq local algorithm asymptotically outperform trivial random guess furthermore linear message pass algorithm correspond apply power iteration non backtrack matrix graph show attain weak recovery lambda addition belief propagation algorithm combine linear time vote procedure achieve information limit exact recovery correctly classify vertices high probability ge frac log leave rho rm bp right rho rm bp function,143,1,1510.02786.txt
http://arxiv.org/abs/1510.02787,The grounding for Continuum,"  It is a ubiquitous opinion among mathematicians that a real number is just a point in the line. If this rough definition is not enough, then a mathematician may provide a formal definition of the real numbers in the set theoretic and axiomatic fashion, i.e. via Cauchy sequences or Dedekind cuts, or as the collection of axioms characterizing exactly (up to isomorphism) the set of real numbers as the complete and totally ordered Archimedean field. Actually, the above notions of the real numbers are abstract and do not have a constructive grounding. Definition of Cauchy sequences, and equivalence classes of these sequences explicitly use the actual infinity. The same is for Dedekind cuts, where the set of rational numbers is used as actual infinity. Although there is no direct constructive grounding for these abstract notions, there are so called intuitions on which they are based. A rigorous approach to express these very intuition in a constructive way is proposed. It is based on the concept of the adjacency relation that seems to be a missing primitive concept in type theory. The approach corresponds to the intuitionistic view of Continuum proposed by Brouwer. The famous and controversial Brouwer Continuity Theorem is discussed on the basis of different principle than the Axiom of Continuity. The real numbers are the cornerstone of Calculus, Homotopy theory, Riemannian geometry, and many others important subjects. The proposed grounding of Continuum is discussed in the context of these subjects. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03D ; F.4.1 ; ,"Ambroszkiewicz, Stanislaw ; ","The grounding for Continuum  It is a ubiquitous opinion among mathematicians that a real number is just a point in the line. If this rough definition is not enough, then a mathematician may provide a formal definition of the real numbers in the set theoretic and axiomatic fashion, i.e. via Cauchy sequences or Dedekind cuts, or as the collection of axioms characterizing exactly (up to isomorphism) the set of real numbers as the complete and totally ordered Archimedean field. Actually, the above notions of the real numbers are abstract and do not have a constructive grounding. Definition of Cauchy sequences, and equivalence classes of these sequences explicitly use the actual infinity. The same is for Dedekind cuts, where the set of rational numbers is used as actual infinity. Although there is no direct constructive grounding for these abstract notions, there are so called intuitions on which they are based. A rigorous approach to express these very intuition in a constructive way is proposed. It is based on the concept of the adjacency relation that seems to be a missing primitive concept in type theory. The approach corresponds to the intuitionistic view of Continuum proposed by Brouwer. The famous and controversial Brouwer Continuity Theorem is discussed on the basis of different principle than the Axiom of Continuity. The real numbers are the cornerstone of Calculus, Homotopy theory, Riemannian geometry, and many others important subjects. The proposed grounding of Continuum is discussed in the context of these subjects. ",ground continuum ubiquitous opinion among mathematicians real number point line rough definition enough mathematician may provide formal definition real number set theoretic axiomatic fashion via cauchy sequence dedekind cut collection axioms characterize exactly isomorphism set real number complete totally order archimedean field actually notions real number abstract constructive ground definition cauchy sequence equivalence class sequence explicitly use actual infinity dedekind cut set rational number use actual infinity although direct constructive ground abstract notions call intuitions base rigorous approach express intuition constructive way propose base concept adjacency relation seem miss primitive concept type theory approach correspond intuitionistic view continuum propose brouwer famous controversial brouwer continuity theorem discuss basis different principle axiom continuity real number cornerstone calculus homotopy theory riemannian geometry many others important subject propose ground continuum discuss context subject,129,4,1510.02787.txt
http://arxiv.org/abs/1510.02807,Avoiding fractional powers over the natural numbers,"  We study the lexicographically least infinite $a/b$-power-free word on the alphabet of non-negative integers. Frequently this word is a fixed point of a uniform morphism, or closely related to one. For example, the lexicographically least $7/4$-power-free word is a fixed point of a $50847$-uniform morphism. We identify the structure of the lexicographically least $a/b$-power-free word for three infinite families of rationals $a/b$ as well many ""sporadic"" rationals that do not seem to belong to general families. To accomplish this, we develop an automated procedure for proving $a/b$-power-freeness for morphisms of a certain form, both for explicit and symbolic rational numbers $a/b$. Finally, we establish a connection to words on a finite alphabet. Namely, the lexicographically least $27/23$-power-free word is in fact a word on the finite alphabet $\{0, 1, 2\}$, and its sequence of letters is $353$-automatic. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Pudwell, Lara ; Rowland, Eric ; ","Avoiding fractional powers over the natural numbers  We study the lexicographically least infinite $a/b$-power-free word on the alphabet of non-negative integers. Frequently this word is a fixed point of a uniform morphism, or closely related to one. For example, the lexicographically least $7/4$-power-free word is a fixed point of a $50847$-uniform morphism. We identify the structure of the lexicographically least $a/b$-power-free word for three infinite families of rationals $a/b$ as well many ""sporadic"" rationals that do not seem to belong to general families. To accomplish this, we develop an automated procedure for proving $a/b$-power-freeness for morphisms of a certain form, both for explicit and symbolic rational numbers $a/b$. Finally, we establish a connection to words on a finite alphabet. Namely, the lexicographically least $27/23$-power-free word is in fact a word on the finite alphabet $\{0, 1, 2\}$, and its sequence of letters is $353$-automatic. ",avoid fractional power natural number study lexicographically least infinite power free word alphabet non negative integers frequently word fix point uniform morphism closely relate one example lexicographically least power free word fix point uniform morphism identify structure lexicographically least power free word three infinite families rationals well many sporadic rationals seem belong general families accomplish develop automate procedure prove power freeness morphisms certain form explicit symbolic rational number finally establish connection word finite alphabet namely lexicographically least power free word fact word finite alphabet sequence letter automatic,87,14,1510.02807.txt
http://arxiv.org/abs/1510.02833,On the Definiteness of Earth Mover's Distance and Its Relation to Set   Intersection,"  Positive definite kernels are an important tool in machine learning that enable efficient solutions to otherwise difficult or intractable problems by implicitly linearizing the problem geometry. In this paper we develop a set-theoretic interpretation of the Earth Mover's Distance (EMD) and propose Earth Mover's Intersection (EMI), a positive definite analog to EMD for sets of different sizes. We provide conditions under which EMD or certain approximations to EMD are negative definite. We also present a positive-definite-preserving transformation that can be applied to any kernel and can also be used to derive positive definite EMD-based kernels and show that the Jaccard index is simply the result of this transformation. Finally, we evaluate kernels based on EMI and the proposed transformation versus EMD in various computer vision tasks and show that EMD is generally inferior even with indefinite kernel techniques. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Gardner, Andrew ; Duncan, Christian A. ; Kanno, Jinko ; Selmic, Rastko R. ; ","On the Definiteness of Earth Mover's Distance and Its Relation to Set   Intersection  Positive definite kernels are an important tool in machine learning that enable efficient solutions to otherwise difficult or intractable problems by implicitly linearizing the problem geometry. In this paper we develop a set-theoretic interpretation of the Earth Mover's Distance (EMD) and propose Earth Mover's Intersection (EMI), a positive definite analog to EMD for sets of different sizes. We provide conditions under which EMD or certain approximations to EMD are negative definite. We also present a positive-definite-preserving transformation that can be applied to any kernel and can also be used to derive positive definite EMD-based kernels and show that the Jaccard index is simply the result of this transformation. Finally, we evaluate kernels based on EMI and the proposed transformation versus EMD in various computer vision tasks and show that EMD is generally inferior even with indefinite kernel techniques. ",definiteness earth mover distance relation set intersection positive definite kernels important tool machine learn enable efficient solutions otherwise difficult intractable problems implicitly linearize problem geometry paper develop set theoretic interpretation earth mover distance emd propose earth mover intersection emi positive definite analog emd set different size provide condition emd certain approximations emd negative definite also present positive definite preserve transformation apply kernel also use derive positive definite emd base kernels show jaccard index simply result transformation finally evaluate kernels base emi propose transformation versus emd various computer vision task show emd generally inferior even indefinite kernel techniques,97,11,1510.02833.txt
http://arxiv.org/abs/1510.02923,On 1-Laplacian Elliptic Equations Modeling Magnetic Resonance Image   Rician Denoising,"  Modeling magnitude Magnetic Resonance Images (MRI) rician denoising in a Bayesian or generalized Tikhonov framework using Total Variation (TV) leads naturally to the consideration of nonlinear elliptic equations. These involve the so called $1$-Laplacian operator and special care is needed to properly formulate the problem. The rician statistics of the data are introduced through a singular equation with a reaction term defined in terms of modified first order Bessel functions. An existence theory is provided here together with other qualitative properties of the solutions. Remarkably, each positive global minimum of the associated functional is one of such solutions. Moreover, we directly solve this non--smooth non--convex minimization problem using a convergent Proximal Point Algorithm. Numerical results based on synthetic and real MRI demonstrate a better performance of the proposed method when compared to previous TV based models for rician denoising which regularize or convexify the problem. Finally, an application on real Diffusion Tensor Images, a strongly affected by rician noise MRI modality, is presented and discussed. ",Mathematics - Analysis of PDEs ; Computer Science - Computer Vision and Pattern Recognition ; Mathematics - Numerical Analysis ; ,"Martin, Adrian ; Schiavi, Emanuele ; de Leon, Sergio Segura ; ","On 1-Laplacian Elliptic Equations Modeling Magnetic Resonance Image   Rician Denoising  Modeling magnitude Magnetic Resonance Images (MRI) rician denoising in a Bayesian or generalized Tikhonov framework using Total Variation (TV) leads naturally to the consideration of nonlinear elliptic equations. These involve the so called $1$-Laplacian operator and special care is needed to properly formulate the problem. The rician statistics of the data are introduced through a singular equation with a reaction term defined in terms of modified first order Bessel functions. An existence theory is provided here together with other qualitative properties of the solutions. Remarkably, each positive global minimum of the associated functional is one of such solutions. Moreover, we directly solve this non--smooth non--convex minimization problem using a convergent Proximal Point Algorithm. Numerical results based on synthetic and real MRI demonstrate a better performance of the proposed method when compared to previous TV based models for rician denoising which regularize or convexify the problem. Finally, an application on real Diffusion Tensor Images, a strongly affected by rician noise MRI modality, is presented and discussed. ",laplacian elliptic equations model magnetic resonance image rician denoising model magnitude magnetic resonance image mri rician denoising bayesian generalize tikhonov framework use total variation tv lead naturally consideration nonlinear elliptic equations involve call laplacian operator special care need properly formulate problem rician statistics data introduce singular equation reaction term define term modify first order bessel function existence theory provide together qualitative properties solutions remarkably positive global minimum associate functional one solutions moreover directly solve non smooth non convex minimization problem use convergent proximal point algorithm numerical result base synthetic real mri demonstrate better performance propose method compare previous tv base model rician denoising regularize convexify problem finally application real diffusion tensor image strongly affect rician noise mri modality present discuss,120,7,1510.02923.txt
http://arxiv.org/abs/1510.03170,Fair and Square: Cake-Cutting in Two Dimensions,"  We consider the classic problem of fairly dividing a heterogeneous good (""cake"") among several agents with different valuations. Classic cake-cutting procedures either allocate each agent a collection of disconnected pieces, or assume that the cake is a one-dimensional interval. In practice, however, the two-dimensional shape of the allotted pieces is important. In particular, when building a house or designing an advertisement in printed or electronic media, squares are more usable than long and narrow rectangles. We thus introduce and study the problem of fair two-dimensional division wherein the allotted pieces must be of some restricted two-dimensional geometric shape(s), particularly squares and fat rectangles. Adding such geometric constraints re-opens most questions and challenges related to cake-cutting. Indeed, even the most elementary fairness criterion --- proportionality --- can no longer be guaranteed. In this paper we thus examine the level of proportionality that can be guaranteed, providing both impossibility results and constructive division procedures. ",Computer Science - Computer Science and Game Theory ; Computer Science - Computational Geometry ; ,"Segal-Halevi, Erel ; Nitzan, Shmuel ; Hassidim, Avinatan ; Aumann, Yonatan ; ","Fair and Square: Cake-Cutting in Two Dimensions  We consider the classic problem of fairly dividing a heterogeneous good (""cake"") among several agents with different valuations. Classic cake-cutting procedures either allocate each agent a collection of disconnected pieces, or assume that the cake is a one-dimensional interval. In practice, however, the two-dimensional shape of the allotted pieces is important. In particular, when building a house or designing an advertisement in printed or electronic media, squares are more usable than long and narrow rectangles. We thus introduce and study the problem of fair two-dimensional division wherein the allotted pieces must be of some restricted two-dimensional geometric shape(s), particularly squares and fat rectangles. Adding such geometric constraints re-opens most questions and challenges related to cake-cutting. Indeed, even the most elementary fairness criterion --- proportionality --- can no longer be guaranteed. In this paper we thus examine the level of proportionality that can be guaranteed, providing both impossibility results and constructive division procedures. ",fair square cake cut two dimension consider classic problem fairly divide heterogeneous good cake among several agents different valuations classic cake cut procedures either allocate agent collection disconnect piece assume cake one dimensional interval practice however two dimensional shape allot piece important particular build house design advertisement print electronic media square usable long narrow rectangles thus introduce study problem fair two dimensional division wherein allot piece must restrict two dimensional geometric shape particularly square fat rectangles add geometric constraints open question challenge relate cake cut indeed even elementary fairness criterion proportionality longer guarantee paper thus examine level proportionality guarantee provide impossibility result constructive division procedures,105,7,1510.03170.txt
http://arxiv.org/abs/1510.03271,A Core Model for Choreographic Programming,"  Choreographic Programming is a programming paradigm for building concurrent programs that are deadlock-free by construction, as a result of programming communications declaratively and then synthesising process implementations automatically. Despite strong interest on choreographies, a foundational model that explains which computations can be performed with the hallmark constructs of choreographies is still missing.   In this work, we introduce Core Choreographies (CC), a model that includes only the core primitives of choreographic programming. Every computable function can be implemented as a choreography in CC, from which we can synthesise a process implementation where independent computations run in parallel. We discuss the design of CC and argue that it constitutes a canonical model for choreographic programming. ",Computer Science - Programming Languages ; ,"Cruz-Filipe, Luís ; Montesi, Fabrizio ; ","A Core Model for Choreographic Programming  Choreographic Programming is a programming paradigm for building concurrent programs that are deadlock-free by construction, as a result of programming communications declaratively and then synthesising process implementations automatically. Despite strong interest on choreographies, a foundational model that explains which computations can be performed with the hallmark constructs of choreographies is still missing.   In this work, we introduce Core Choreographies (CC), a model that includes only the core primitives of choreographic programming. Every computable function can be implemented as a choreography in CC, from which we can synthesise a process implementation where independent computations run in parallel. We discuss the design of CC and argue that it constitutes a canonical model for choreographic programming. ",core model choreographic program choreographic program program paradigm build concurrent program deadlock free construction result program communications declaratively synthesise process implementations automatically despite strong interest choreographies foundational model explain computations perform hallmark construct choreographies still miss work introduce core choreographies cc model include core primitives choreographic program every computable function implement choreography cc synthesise process implementation independent computations run parallel discuss design cc argue constitute canonical model choreographic program,69,8,1510.03271.txt
http://arxiv.org/abs/1510.03637,Applied Choreographies,"  Choreographic Programming is a correct-by-construction paradigm for distributed programming, where global declarative descriptions of communications (choreographies) are used to synthesise deadlock-free processes. Choreographies are global descriptions of communications in concurrent systems, which have been used in different methodologies for the verification or synthesis of programs. However, there is no formalisation that provides a chain of correctness from choreographies to their implementations. This problem originates from the gap between previous theoretical models, which abstract communications using channel names (\`a la CCS/$\pi$-calculus), and their implementations, which use low-level mechanisms for message routing. As a solution, we propose the framework of Applied Choreographies (AC). In AC, programmers write choreographies in a language that follows the standard syntax and semantics of previous works. Then, choreographies are compiled to a real-world execution model for Service-Oriented Computing (SOC). To manage the complexity of this task, our compilation happens in three steps, respectively dealing with: implementing name-based communications using the concrete mechanism found in SOC, projecting a choreography to a set of processes, and translating processes to a distributed implementation in terms of services. For each step a suitable correspondence result guarantees that the behaviour is preserved, thus ensuring the correctness of the global compilation process. This is the first correctness result of an end-to-end translation from standard choreographies to programs based on a ""real-world"" communication mechanism. ",Computer Science - Programming Languages ; ,"Giallorenzo, Saverio ; Montesi, Fabrizio ; Gabbrielli, Maurizio ; ","Applied Choreographies  Choreographic Programming is a correct-by-construction paradigm for distributed programming, where global declarative descriptions of communications (choreographies) are used to synthesise deadlock-free processes. Choreographies are global descriptions of communications in concurrent systems, which have been used in different methodologies for the verification or synthesis of programs. However, there is no formalisation that provides a chain of correctness from choreographies to their implementations. This problem originates from the gap between previous theoretical models, which abstract communications using channel names (\`a la CCS/$\pi$-calculus), and their implementations, which use low-level mechanisms for message routing. As a solution, we propose the framework of Applied Choreographies (AC). In AC, programmers write choreographies in a language that follows the standard syntax and semantics of previous works. Then, choreographies are compiled to a real-world execution model for Service-Oriented Computing (SOC). To manage the complexity of this task, our compilation happens in three steps, respectively dealing with: implementing name-based communications using the concrete mechanism found in SOC, projecting a choreography to a set of processes, and translating processes to a distributed implementation in terms of services. For each step a suitable correspondence result guarantees that the behaviour is preserved, thus ensuring the correctness of the global compilation process. This is the first correctness result of an end-to-end translation from standard choreographies to programs based on a ""real-world"" communication mechanism. ",apply choreographies choreographic program correct construction paradigm distribute program global declarative descriptions communications choreographies use synthesise deadlock free process choreographies global descriptions communications concurrent systems use different methodologies verification synthesis program however formalisation provide chain correctness choreographies implementations problem originate gap previous theoretical model abstract communications use channel name la ccs pi calculus implementations use low level mechanisms message rout solution propose framework apply choreographies ac ac programmers write choreographies language follow standard syntax semantics previous work choreographies compile real world execution model service orient compute soc manage complexity task compilation happen three step respectively deal implement name base communications use concrete mechanism find soc project choreography set process translate process distribute implementation term service step suitable correspondence result guarantee behaviour preserve thus ensure correctness global compilation process first correctness result end end translation standard choreographies program base real world communication mechanism,142,8,1510.03637.txt
http://arxiv.org/abs/1510.03840,Dynamic Spectrum Sensing Through Accelerated Particle Swarm Optimization,"  In this paper, a novel optimization algorithm, called the acceleration-aided particle swarm optimization (AAPSO), is proposed for reliable dynamic spectrum sensing in cognitive radio networks. In A-APSO, the acceleration variable of the particles in the swarm is also considered in the search space of the optimization problem. We show that the proposed A-APSO based spectrum sensing technique is more efficient in terms of performance than the corresponding one based on the standard particle swarm optimization algorithm. ",Mathematics - Optimization and Control ; Computer Science - Information Theory ; Statistics - Applications ; ,"Paschos, Alexandros E. ; Kapinas, Vasileios M. ; Ntouni, Georgia D. ; Hadjileontiadis, Leontios J. ; Karagiannidis, George K. ; ","Dynamic Spectrum Sensing Through Accelerated Particle Swarm Optimization  In this paper, a novel optimization algorithm, called the acceleration-aided particle swarm optimization (AAPSO), is proposed for reliable dynamic spectrum sensing in cognitive radio networks. In A-APSO, the acceleration variable of the particles in the swarm is also considered in the search space of the optimization problem. We show that the proposed A-APSO based spectrum sensing technique is more efficient in terms of performance than the corresponding one based on the standard particle swarm optimization algorithm. ",dynamic spectrum sense accelerate particle swarm optimization paper novel optimization algorithm call acceleration aid particle swarm optimization aapso propose reliable dynamic spectrum sense cognitive radio network apso acceleration variable particles swarm also consider search space optimization problem show propose apso base spectrum sense technique efficient term performance correspond one base standard particle swarm optimization algorithm,55,2,1510.03840.txt
http://arxiv.org/abs/1510.03895,A faster subquadratic algorithm for finding outlier correlations,"  We study the problem of detecting outlier pairs of strongly correlated variables among a collection of $n$ variables with otherwise weak pairwise correlations. After normalization, this task amounts to the geometric task where we are given as input a set of $n$ vectors with unit Euclidean norm and dimension $d$, and for some constants $0<\tau<\rho<1$, we are asked to find all the outlier pairs of vectors whose inner product is at least $\rho$ in absolute value, subject to the promise that all but at most $q$ pairs of vectors have inner product at most $\tau$ in absolute value.   Improving on an algorithm of G. Valiant [FOCS 2012; J. ACM 2015], we present a randomized algorithm that for Boolean inputs ($\{-1,1\}$-valued data normalized to unit Euclidean length) runs in time \[ \tilde O\bigl(n^{\max\,\{1-\gamma+M(\Delta\gamma,\gamma),\,M(1-\gamma,2\Delta\gamma)\}}+qdn^{2\gamma}\bigr)\,, \] where $0<\gamma<1$ is a constant tradeoff parameter and $M(\mu,\nu)$ is the exponent to multiply an $\lfloor n^\mu\rfloor\times\lfloor n^\nu\rfloor$ matrix with an $\lfloor n^\nu\rfloor\times \lfloor n^\mu\rfloor$ matrix and $\Delta=1/(1-\log_\tau\rho)$. As corollaries we obtain randomized algorithms that run in time \[ \tilde O\bigl(n^{\frac{2\omega}{3-\log_\tau\rho}}+qdn^{\frac{2(1-\log_\tau\rho)}{3-\log_\tau\rho}}\bigr) \] and in time \[ \tilde O\bigl(n^{\frac{4}{2+\alpha(1-\log_\tau\rho)}}+qdn^{\frac{2\alpha(1-\log_\tau\rho)}{2+\alpha(1-\log_\tau\rho)}}\bigr)\,, \] where $2\leq\omega<2.38$ is the exponent for square matrix multiplication and $0.3<\alpha\leq 1$ is the exponent for rectangular matrix multiplication. The notation $\tilde O(\cdot)$ hides polylogarithmic factors in $n$ and $d$ whose degree may depend on $\rho$ and $\tau$. We present further corollaries for the light bulb problem and for learning sparse Boolean functions. ","Computer Science - Data Structures and Algorithms ; 65F30, 68W20, 62H20, 68T05, 68Q32 ; F.2.1 ; I.1.2 ; G.3 ; H.2.8 ; H.3.3 ; I.2.6 ; ","Karppa, Matti ; Kaski, Petteri ; Kohonen, Jukka ; ","A faster subquadratic algorithm for finding outlier correlations  We study the problem of detecting outlier pairs of strongly correlated variables among a collection of $n$ variables with otherwise weak pairwise correlations. After normalization, this task amounts to the geometric task where we are given as input a set of $n$ vectors with unit Euclidean norm and dimension $d$, and for some constants $0<\tau<\rho<1$, we are asked to find all the outlier pairs of vectors whose inner product is at least $\rho$ in absolute value, subject to the promise that all but at most $q$ pairs of vectors have inner product at most $\tau$ in absolute value.   Improving on an algorithm of G. Valiant [FOCS 2012; J. ACM 2015], we present a randomized algorithm that for Boolean inputs ($\{-1,1\}$-valued data normalized to unit Euclidean length) runs in time \[ \tilde O\bigl(n^{\max\,\{1-\gamma+M(\Delta\gamma,\gamma),\,M(1-\gamma,2\Delta\gamma)\}}+qdn^{2\gamma}\bigr)\,, \] where $0<\gamma<1$ is a constant tradeoff parameter and $M(\mu,\nu)$ is the exponent to multiply an $\lfloor n^\mu\rfloor\times\lfloor n^\nu\rfloor$ matrix with an $\lfloor n^\nu\rfloor\times \lfloor n^\mu\rfloor$ matrix and $\Delta=1/(1-\log_\tau\rho)$. As corollaries we obtain randomized algorithms that run in time \[ \tilde O\bigl(n^{\frac{2\omega}{3-\log_\tau\rho}}+qdn^{\frac{2(1-\log_\tau\rho)}{3-\log_\tau\rho}}\bigr) \] and in time \[ \tilde O\bigl(n^{\frac{4}{2+\alpha(1-\log_\tau\rho)}}+qdn^{\frac{2\alpha(1-\log_\tau\rho)}{2+\alpha(1-\log_\tau\rho)}}\bigr)\,, \] where $2\leq\omega<2.38$ is the exponent for square matrix multiplication and $0.3<\alpha\leq 1$ is the exponent for rectangular matrix multiplication. The notation $\tilde O(\cdot)$ hides polylogarithmic factors in $n$ and $d$ whose degree may depend on $\rho$ and $\tau$. We present further corollaries for the light bulb problem and for learning sparse Boolean functions. ",faster subquadratic algorithm find outlier correlations study problem detect outlier pair strongly correlate variables among collection variables otherwise weak pairwise correlations normalization task amount geometric task give input set vectors unit euclidean norm dimension constants tau rho ask find outlier pair vectors whose inner product least rho absolute value subject promise pair vectors inner product tau absolute value improve algorithm valiant focs acm present randomize algorithm boolean input value data normalize unit euclidean length run time tilde bigl max gamma delta gamma gamma gamma delta gamma qdn gamma bigr gamma constant tradeoff parameter mu nu exponent multiply lfloor mu rfloor time lfloor nu rfloor matrix lfloor nu rfloor time lfloor mu rfloor matrix delta log tau rho corollaries obtain randomize algorithms run time tilde bigl frac omega log tau rho qdn frac log tau rho log tau rho bigr time tilde bigl frac alpha log tau rho qdn frac alpha log tau rho alpha log tau rho bigr leq omega exponent square matrix multiplication alpha leq exponent rectangular matrix multiplication notation tilde cdot hide polylogarithmic factor whose degree may depend rho tau present corollaries light bulb problem learn sparse boolean function,191,1,1510.03895.txt
http://arxiv.org/abs/1510.03903,Fair Cake-Cutting among Families,"  We study the fair division of a continuous resource, such as a land-estate or a time-interval, among pre-specified groups of agents, such as families. Each family is given a piece of the resource and this piece is used simultaneously by all family members, while different members may have different value functions. Three ways to assess the fairness of such a division are examined. (a) *Average fairness* means that each family's share is fair according to the ""family value function"", defined as the arithmetic mean of the value functions of the family members. (b) *Unanimous fairness* means that all members in all families feel that their family received a fair share according to their personal value function. (c) *Democratic fairness* means that in each family, at least half the members feel that their family's share is fair. We compare these criteria based on the number of connected components in the resulting division, and based on their compatibility with Pareto-efficiency. ",Computer Science - Computer Science and Game Theory ; ,"Segal-Halevi, Erel ; Nitzan, Shmuel ; ","Fair Cake-Cutting among Families  We study the fair division of a continuous resource, such as a land-estate or a time-interval, among pre-specified groups of agents, such as families. Each family is given a piece of the resource and this piece is used simultaneously by all family members, while different members may have different value functions. Three ways to assess the fairness of such a division are examined. (a) *Average fairness* means that each family's share is fair according to the ""family value function"", defined as the arithmetic mean of the value functions of the family members. (b) *Unanimous fairness* means that all members in all families feel that their family received a fair share according to their personal value function. (c) *Democratic fairness* means that in each family, at least half the members feel that their family's share is fair. We compare these criteria based on the number of connected components in the resulting division, and based on their compatibility with Pareto-efficiency. ",fair cake cut among families study fair division continuous resource land estate time interval among pre specify group agents families family give piece resource piece use simultaneously family members different members may different value function three ways assess fairness division examine average fairness mean family share fair accord family value function define arithmetic mean value function family members unanimous fairness mean members families feel family receive fair share accord personal value function democratic fairness mean family least half members feel family share fair compare criteria base number connect components result division base compatibility pareto efficiency,95,7,1510.03903.txt
http://arxiv.org/abs/1510.03935,Surface Approximation via Asymptotic Optimal Geometric Partition,"  In this paper, we present a surface remeshing method with high approximation quality based on Principal Component Analysis. Given a triangular mesh and a user assigned polygon/vertex budget, traditional methods usually require the extra curvature metric field for the desired anisotropy to best approximate the surface, even though the estimated curvature metric is known to be imperfect and already self-contained in the surface. In our approach, this anisotropic control is achieved through the optimal geometry partition without this explicit metric field. The minimization of our proposed partition energy has the following properties: Firstly, on a C2 surface, it is theoretically guaranteed to have the optimal aspect ratio and cluster size as specified in approximation theory for L1 piecewise linear approximation. Secondly, it captures sharp features on practical models without any pre-tagging. We develop an effective merging-swapping framework to seek the optimal partition and construct polygonal/triangular mesh afterwards. The effectiveness and efficiency of our method are demonstrated through the comparison with other state-of-the-art remeshing methods. ",Computer Science - Graphics ; ,"Cai, Yiqi ; Guo, Xiaohu ; Liu, Yang ; Wang, Wenping ; Mao, Weihua ; Zhong, Zichun ; ","Surface Approximation via Asymptotic Optimal Geometric Partition  In this paper, we present a surface remeshing method with high approximation quality based on Principal Component Analysis. Given a triangular mesh and a user assigned polygon/vertex budget, traditional methods usually require the extra curvature metric field for the desired anisotropy to best approximate the surface, even though the estimated curvature metric is known to be imperfect and already self-contained in the surface. In our approach, this anisotropic control is achieved through the optimal geometry partition without this explicit metric field. The minimization of our proposed partition energy has the following properties: Firstly, on a C2 surface, it is theoretically guaranteed to have the optimal aspect ratio and cluster size as specified in approximation theory for L1 piecewise linear approximation. Secondly, it captures sharp features on practical models without any pre-tagging. We develop an effective merging-swapping framework to seek the optimal partition and construct polygonal/triangular mesh afterwards. The effectiveness and efficiency of our method are demonstrated through the comparison with other state-of-the-art remeshing methods. ",surface approximation via asymptotic optimal geometric partition paper present surface remeshing method high approximation quality base principal component analysis give triangular mesh user assign polygon vertex budget traditional methods usually require extra curvature metric field desire anisotropy best approximate surface even though estimate curvature metric know imperfect already self contain surface approach anisotropic control achieve optimal geometry partition without explicit metric field minimization propose partition energy follow properties firstly surface theoretically guarantee optimal aspect ratio cluster size specify approximation theory piecewise linear approximation secondly capture sharp feature practical model without pre tag develop effective merge swap framework seek optimal partition construct polygonal triangular mesh afterwards effectiveness efficiency method demonstrate comparison state art remeshing methods,114,0,1510.03935.txt
http://arxiv.org/abs/1510.04249,Random Irregular Block-hierarchical Networks: Algorithms for Computation   of Main Properties,"  In this paper, the class of random irregular block-hierarchical networks is defined and algorithms for generation and calculation of network properties are described. The algorithms presented for this class of networks are more efficient than known algorithms both in computation time and memory usage and can be used to analyze topological properties of such networks. The algorithms are implemented in the system created by the authors for the study of topological and statistical properties of random networks. ",Computer Science - Data Structures and Algorithms ; ,"Avetisyan, Svetlana ; Samvelyan, Mikayel ; Karapetyan, Martun ; ","Random Irregular Block-hierarchical Networks: Algorithms for Computation   of Main Properties  In this paper, the class of random irregular block-hierarchical networks is defined and algorithms for generation and calculation of network properties are described. The algorithms presented for this class of networks are more efficient than known algorithms both in computation time and memory usage and can be used to analyze topological properties of such networks. The algorithms are implemented in the system created by the authors for the study of topological and statistical properties of random networks. ",random irregular block hierarchical network algorithms computation main properties paper class random irregular block hierarchical network define algorithms generation calculation network properties describe algorithms present class network efficient know algorithms computation time memory usage use analyze topological properties network algorithms implement system create author study topological statistical properties random network,50,6,1510.04249.txt
http://arxiv.org/abs/1510.04389,Sketch-based Manga Retrieval using Manga109 Dataset,"  Manga (Japanese comics) are popular worldwide. However, current e-manga archives offer very limited search support, including keyword-based search by title or author, or tag-based categorization. To make the manga search experience more intuitive, efficient, and enjoyable, we propose a content-based manga retrieval system. First, we propose a manga-specific image-describing framework. It consists of efficient margin labeling, edge orientation histogram feature description, and approximate nearest-neighbor search using product quantization. Second, we propose a sketch-based interface as a natural way to interact with manga content. The interface provides sketch-based querying, relevance feedback, and query retouch. For evaluation, we built a novel dataset of manga images, Manga109, which consists of 109 comic books of 21,142 pages drawn by professional manga artists. To the best of our knowledge, Manga109 is currently the biggest dataset of manga images available for research. We conducted a comparative study, a localization evaluation, and a large-scale qualitative study. From the experiments, we verified that: (1) the retrieval accuracy of the proposed method is higher than those of previous methods; (2) the proposed method can localize an object instance with reasonable runtime and accuracy; and (3) sketch querying is useful for manga search. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Information Retrieval ; Computer Science - Multimedia ; ,"Matsui, Yusuke ; Ito, Kota ; Aramaki, Yuji ; Yamasaki, Toshihiko ; Aizawa, Kiyoharu ; ","Sketch-based Manga Retrieval using Manga109 Dataset  Manga (Japanese comics) are popular worldwide. However, current e-manga archives offer very limited search support, including keyword-based search by title or author, or tag-based categorization. To make the manga search experience more intuitive, efficient, and enjoyable, we propose a content-based manga retrieval system. First, we propose a manga-specific image-describing framework. It consists of efficient margin labeling, edge orientation histogram feature description, and approximate nearest-neighbor search using product quantization. Second, we propose a sketch-based interface as a natural way to interact with manga content. The interface provides sketch-based querying, relevance feedback, and query retouch. For evaluation, we built a novel dataset of manga images, Manga109, which consists of 109 comic books of 21,142 pages drawn by professional manga artists. To the best of our knowledge, Manga109 is currently the biggest dataset of manga images available for research. We conducted a comparative study, a localization evaluation, and a large-scale qualitative study. From the experiments, we verified that: (1) the retrieval accuracy of the proposed method is higher than those of previous methods; (2) the proposed method can localize an object instance with reasonable runtime and accuracy; and (3) sketch querying is useful for manga search. ",sketch base manga retrieval use manga dataset manga japanese comics popular worldwide however current manga archive offer limit search support include keyword base search title author tag base categorization make manga search experience intuitive efficient enjoyable propose content base manga retrieval system first propose manga specific image describe framework consist efficient margin label edge orientation histogram feature description approximate nearest neighbor search use product quantization second propose sketch base interface natural way interact manga content interface provide sketch base query relevance feedback query retouch evaluation build novel dataset manga image manga consist comic book page draw professional manga artists best knowledge manga currently biggest dataset manga image available research conduct comparative study localization evaluation large scale qualitative study experiment verify retrieval accuracy propose method higher previous methods propose method localize object instance reasonable runtime accuracy sketch query useful manga search,140,11,1510.04389.txt
http://arxiv.org/abs/1510.04390,Dual Principal Component Pursuit,"  We consider the problem of learning a linear subspace from data corrupted by outliers. Classical approaches are typically designed for the case in which the subspace dimension is small relative to the ambient dimension. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement; as such, it is particularly suitable for subspaces whose dimension is close to the ambient dimension (subspaces of high relative dimension). We pose the problem of computing normal vectors to the inlier subspace as a non-convex $\ell_1$ minimization problem on the sphere, which we call Dual Principal Component Pursuit (DPCP) problem. We provide theoretical guarantees under which every global solution to DPCP is a vector in the orthogonal complement of the inlier subspace. Moreover, we relax the non-convex DPCP problem to a recursion of linear programs whose solutions are shown to converge in a finite number of steps to a vector orthogonal to the subspace. In particular, when the inlier subspace is a hyperplane, the solutions to the recursion of linear programs converge to the global minimum of the non-convex DPCP problem in a finite number of steps. We also propose algorithms based on alternating minimization and iteratively re-weighted least squares, which are suitable for dealing with large-scale data. Experiments on synthetic data show that the proposed methods are able to handle more outliers and higher relative dimensions than current state-of-the-art methods, while experiments in the context of the three-view geometry problem in computer vision suggest that the proposed methods can be a useful or even superior alternative to traditional RANSAC-based approaches for computer vision and other applications. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Machine Learning ; ,"Tsakiris, Manolis C. ; Vidal, Rene ; ","Dual Principal Component Pursuit  We consider the problem of learning a linear subspace from data corrupted by outliers. Classical approaches are typically designed for the case in which the subspace dimension is small relative to the ambient dimension. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement; as such, it is particularly suitable for subspaces whose dimension is close to the ambient dimension (subspaces of high relative dimension). We pose the problem of computing normal vectors to the inlier subspace as a non-convex $\ell_1$ minimization problem on the sphere, which we call Dual Principal Component Pursuit (DPCP) problem. We provide theoretical guarantees under which every global solution to DPCP is a vector in the orthogonal complement of the inlier subspace. Moreover, we relax the non-convex DPCP problem to a recursion of linear programs whose solutions are shown to converge in a finite number of steps to a vector orthogonal to the subspace. In particular, when the inlier subspace is a hyperplane, the solutions to the recursion of linear programs converge to the global minimum of the non-convex DPCP problem in a finite number of steps. We also propose algorithms based on alternating minimization and iteratively re-weighted least squares, which are suitable for dealing with large-scale data. Experiments on synthetic data show that the proposed methods are able to handle more outliers and higher relative dimensions than current state-of-the-art methods, while experiments in the context of the three-view geometry problem in computer vision suggest that the proposed methods can be a useful or even superior alternative to traditional RANSAC-based approaches for computer vision and other applications. ",dual principal component pursuit consider problem learn linear subspace data corrupt outliers classical approach typically design case subspace dimension small relative ambient dimension approach work dual representation subspace hence aim find orthogonal complement particularly suitable subspaces whose dimension close ambient dimension subspaces high relative dimension pose problem compute normal vectors inlier subspace non convex ell minimization problem sphere call dual principal component pursuit dpcp problem provide theoretical guarantee every global solution dpcp vector orthogonal complement inlier subspace moreover relax non convex dpcp problem recursion linear program whose solutions show converge finite number step vector orthogonal subspace particular inlier subspace hyperplane solutions recursion linear program converge global minimum non convex dpcp problem finite number step also propose algorithms base alternate minimization iteratively weight least square suitable deal large scale data experiment synthetic data show propose methods able handle outliers higher relative dimension current state art methods experiment context three view geometry problem computer vision suggest propose methods useful even superior alternative traditional ransac base approach computer vision applications,167,7,1510.04390.txt
http://arxiv.org/abs/1510.04524,"Old Bands, New Tracks---Revisiting the Band Model for Robust Hypothesis   Testing","  The density band model proposed by Kassam for robust hypothesis testing is revisited in this paper. First, a novel criterion for the general characterization of least favorable distributions is proposed, which unifies existing results. This criterion is then used to derive an implicit definition of the least favorable distributions under band uncertainties. In contrast to the existing solution, it only requires two scalar values to be determined and eliminates the need for case-by-case statements. Based on this definition, a generic fixed-point algorithm is proposed that iteratively calculates the least favorable distributions for arbitrary band specifications. Finally, three different types of robust tests that emerge from band models are discussed and a numerical example is presented to illustrate their potential use in practice. ",Computer Science - Information Theory ; 62C20 ; ,"Fauß, Michael ; Zoubir, Abdelhak M. ; ","Old Bands, New Tracks---Revisiting the Band Model for Robust Hypothesis   Testing  The density band model proposed by Kassam for robust hypothesis testing is revisited in this paper. First, a novel criterion for the general characterization of least favorable distributions is proposed, which unifies existing results. This criterion is then used to derive an implicit definition of the least favorable distributions under band uncertainties. In contrast to the existing solution, it only requires two scalar values to be determined and eliminates the need for case-by-case statements. Based on this definition, a generic fixed-point algorithm is proposed that iteratively calculates the least favorable distributions for arbitrary band specifications. Finally, three different types of robust tests that emerge from band models are discussed and a numerical example is presented to illustrate their potential use in practice. ",old band new track revisit band model robust hypothesis test density band model propose kassam robust hypothesis test revisit paper first novel criterion general characterization least favorable distributions propose unify exist result criterion use derive implicit definition least favorable distributions band uncertainties contrast exist solution require two scalar value determine eliminate need case case statements base definition generic fix point algorithm propose iteratively calculate least favorable distributions arbitrary band specifications finally three different type robust test emerge band model discuss numerical example present illustrate potential use practice,87,12,1510.04524.txt
http://arxiv.org/abs/1510.04780,A Graph Traversal Based Approach to Answer Non-Aggregation Questions   Over DBpedia,"  We present a question answering system over DBpedia, filling the gap between user information needs expressed in natural language and a structured query interface expressed in SPARQL over the underlying knowledge base (KB). Given the KB, our goal is to comprehend a natural language query and provide corresponding accurate answers. Focusing on solving the non-aggregation questions, in this paper, we construct a subgraph of the knowledge base from the detected entities and propose a graph traversal method to solve both the semantic item mapping problem and the disambiguation problem in a joint way. Compared with existing work, we simplify the process of query intention understanding and pay more attention to the answer path ranking. We evaluate our method on a non-aggregation question dataset and further on a complete dataset. Experimental results show that our method achieves best performance compared with several state-of-the-art systems. ",Computer Science - Computation and Language ; Computer Science - Information Retrieval ; ,"Zhu, Chenhao ; Ren, Kan ; Liu, Xuan ; Wang, Haofen ; Tian, Yiding ; Yu, Yong ; ","A Graph Traversal Based Approach to Answer Non-Aggregation Questions   Over DBpedia  We present a question answering system over DBpedia, filling the gap between user information needs expressed in natural language and a structured query interface expressed in SPARQL over the underlying knowledge base (KB). Given the KB, our goal is to comprehend a natural language query and provide corresponding accurate answers. Focusing on solving the non-aggregation questions, in this paper, we construct a subgraph of the knowledge base from the detected entities and propose a graph traversal method to solve both the semantic item mapping problem and the disambiguation problem in a joint way. Compared with existing work, we simplify the process of query intention understanding and pay more attention to the answer path ranking. We evaluate our method on a non-aggregation question dataset and further on a complete dataset. Experimental results show that our method achieves best performance compared with several state-of-the-art systems. ",graph traversal base approach answer non aggregation question dbpedia present question answer system dbpedia fill gap user information need express natural language structure query interface express sparql underlie knowledge base kb give kb goal comprehend natural language query provide correspond accurate answer focus solve non aggregation question paper construct subgraph knowledge base detect entities propose graph traversal method solve semantic item map problem disambiguation problem joint way compare exist work simplify process query intention understand pay attention answer path rank evaluate method non aggregation question dataset complete dataset experimental result show method achieve best performance compare several state art systems,100,10,1510.04780.txt
http://arxiv.org/abs/1510.05071,Distributed Robust Adaptive Frequency Control of Power Systems with   Dynamic Loads,  This paper investigates the frequency control of multi-machine power systems subject to uncertain and dynamic net loads. We propose distributed internal model controllers which coordinate synchronous generators and demand response to tackle the unpredictable nature of net loads. Frequency stability is formally guaranteed via Lyapunov analysis. Numerical simulations on the IEEE 68-bus test system as well as the Minni-WECC system demonstrate the effectiveness of the controllers and performance under a three-phase fault and load-switching during light/peak loads. ,Computer Science - Systems and Control ; ,"Kim, Hunmin ; Zhu, Minghui ; Lian, Jianming ; ",Distributed Robust Adaptive Frequency Control of Power Systems with   Dynamic Loads  This paper investigates the frequency control of multi-machine power systems subject to uncertain and dynamic net loads. We propose distributed internal model controllers which coordinate synchronous generators and demand response to tackle the unpredictable nature of net loads. Frequency stability is formally guaranteed via Lyapunov analysis. Numerical simulations on the IEEE 68-bus test system as well as the Minni-WECC system demonstrate the effectiveness of the controllers and performance under a three-phase fault and load-switching during light/peak loads. ,distribute robust adaptive frequency control power systems dynamic load paper investigate frequency control multi machine power systems subject uncertain dynamic net load propose distribute internal model controllers coordinate synchronous generators demand response tackle unpredictable nature net load frequency stability formally guarantee via lyapunov analysis numerical simulations ieee bus test system well minni wecc system demonstrate effectiveness controllers performance three phase fault load switch light peak load,66,2,1510.05071.txt
http://arxiv.org/abs/1510.05175,Top-k Query Processing on Encrypted Databases with Strong Security   Guarantees,"  Privacy concerns in outsourced cloud databases have become more and more important recently and many efficient and scalable query processing methods over encrypted data have been proposed. However, there is very limited work on how to securely process top-k ranking queries over encrypted databases in the cloud. In this paper, we focus exactly on this problem: secure and efficient processing of top-k queries over outsourced databases. In particular, we propose the first efficient and provable secure top-k query processing construction that achieves adaptively CQA security. We develop an encrypted data structure called EHL and describe several secure sub-protocols under our security model to answer top-k queries. Furthermore, we optimize our query algorithms for both space and time efficiency. Finally, in the experiments, we empirically analyze our protocol using real world datasets and demonstrate that our construction is efficient and practical. ",Computer Science - Cryptography and Security ; H.2.7 ; K.4.4 ; H.2.4 ; ,"Meng, Xianrui ; Zhu, Haohan ; Kollios, George ; ","Top-k Query Processing on Encrypted Databases with Strong Security   Guarantees  Privacy concerns in outsourced cloud databases have become more and more important recently and many efficient and scalable query processing methods over encrypted data have been proposed. However, there is very limited work on how to securely process top-k ranking queries over encrypted databases in the cloud. In this paper, we focus exactly on this problem: secure and efficient processing of top-k queries over outsourced databases. In particular, we propose the first efficient and provable secure top-k query processing construction that achieves adaptively CQA security. We develop an encrypted data structure called EHL and describe several secure sub-protocols under our security model to answer top-k queries. Furthermore, we optimize our query algorithms for both space and time efficiency. Finally, in the experiments, we empirically analyze our protocol using real world datasets and demonstrate that our construction is efficient and practical. ",top query process encrypt databases strong security guarantee privacy concern outsource cloud databases become important recently many efficient scalable query process methods encrypt data propose however limit work securely process top rank query encrypt databases cloud paper focus exactly problem secure efficient process top query outsource databases particular propose first efficient provable secure top query process construction achieve adaptively cqa security develop encrypt data structure call ehl describe several secure sub protocols security model answer top query furthermore optimize query algorithms space time efficiency finally experiment empirically analyze protocol use real world datasets demonstrate construction efficient practical,97,1,1510.05175.txt
http://arxiv.org/abs/1510.05229,Monotonicity and Competitive Equilibrium in Cake-cutting,"  We study the monotonicity properties of solutions in the classic problem of fair cake-cutting --- dividing a heterogeneous resource among agents with different preferences. Resource- and population-monotonicity relate to scenarios where the cake, or the number of participants who divide the cake, changes. It is required that the utility of all participants change in the same direction: either all of them are better-off (if there is more to share or fewer to share among) or all are worse-off (if there is less to share or more to share among).   We formally introduce these concepts to the cake-cutting problem and examine whether they are satisfied by various common division rules. We prove that the Nash-optimal rule, which maximizes the product of utilities, is resource-monotonic and population-monotonic, in addition to being Pareto-optimal, envy-free and satisfying a strong competitive-equilibrium condition. Moreover, we prove that it is the only rule among a natural family of welfare-maximizing rules that is both proportional and resource-monotonic. ",Computer Science - Computer Science and Game Theory ; ,"Segal-Halevi, Erel ; Sziklai, Balázs ; ","Monotonicity and Competitive Equilibrium in Cake-cutting  We study the monotonicity properties of solutions in the classic problem of fair cake-cutting --- dividing a heterogeneous resource among agents with different preferences. Resource- and population-monotonicity relate to scenarios where the cake, or the number of participants who divide the cake, changes. It is required that the utility of all participants change in the same direction: either all of them are better-off (if there is more to share or fewer to share among) or all are worse-off (if there is less to share or more to share among).   We formally introduce these concepts to the cake-cutting problem and examine whether they are satisfied by various common division rules. We prove that the Nash-optimal rule, which maximizes the product of utilities, is resource-monotonic and population-monotonic, in addition to being Pareto-optimal, envy-free and satisfying a strong competitive-equilibrium condition. Moreover, we prove that it is the only rule among a natural family of welfare-maximizing rules that is both proportional and resource-monotonic. ",monotonicity competitive equilibrium cake cut study monotonicity properties solutions classic problem fair cake cut divide heterogeneous resource among agents different preferences resource population monotonicity relate scenarios cake number participants divide cake change require utility participants change direction either better share fewer share among worse less share share among formally introduce concepts cake cut problem examine whether satisfy various common division rule prove nash optimal rule maximize product utilities resource monotonic population monotonic addition pareto optimal envy free satisfy strong competitive equilibrium condition moreover prove rule among natural family welfare maximize rule proportional resource monotonic,94,7,1510.05229.txt
http://arxiv.org/abs/1510.05278,A First Practical Fully Homomorphic Crypto-Processor Design: The Secret   Computer is Nearly Here,"  Following a sequence of hardware designs for a fully homomorphic crypto-processor - a general purpose processor that natively runs encrypted machine code on encrypted data in registers and memory, resulting in encrypted machine states - proposed by the authors in 2014, we discuss a working prototype of the first of those, a so-called `pseudo-homomorphic' design. This processor is in principle safe against physical or software-based attacks by the owner/operator of the processor on user processes running in it. The processor is intended as a more secure option for those emerging computing paradigms that require trust to be placed in computations carried out in remote locations or overseen by untrusted operators.   The prototype has a single-pipeline superscalar architecture that runs OpenRISC standard machine code in two distinct modes. The processor runs in the encrypted mode (the unprivileged, `user' mode, with a long pipeline) at 60-70% of the speed in the unencrypted mode (the privileged, `supervisor' mode, with a short pipeline), emitting a completed encrypted instruction every 1.67-1.8 cycles on average in real trials. ",Computer Science - Cryptography and Security ; ,"Breuer, Peter ; Bowen, Jonathan ; ","A First Practical Fully Homomorphic Crypto-Processor Design: The Secret   Computer is Nearly Here  Following a sequence of hardware designs for a fully homomorphic crypto-processor - a general purpose processor that natively runs encrypted machine code on encrypted data in registers and memory, resulting in encrypted machine states - proposed by the authors in 2014, we discuss a working prototype of the first of those, a so-called `pseudo-homomorphic' design. This processor is in principle safe against physical or software-based attacks by the owner/operator of the processor on user processes running in it. The processor is intended as a more secure option for those emerging computing paradigms that require trust to be placed in computations carried out in remote locations or overseen by untrusted operators.   The prototype has a single-pipeline superscalar architecture that runs OpenRISC standard machine code in two distinct modes. The processor runs in the encrypted mode (the unprivileged, `user' mode, with a long pipeline) at 60-70% of the speed in the unencrypted mode (the privileged, `supervisor' mode, with a short pipeline), emitting a completed encrypted instruction every 1.67-1.8 cycles on average in real trials. ",first practical fully homomorphic crypto processor design secret computer nearly follow sequence hardware design fully homomorphic crypto processor general purpose processor natively run encrypt machine code encrypt data register memory result encrypt machine state propose author discuss work prototype first call pseudo homomorphic design processor principle safe physical software base attack owner operator processor user process run processor intend secure option emerge compute paradigms require trust place computations carry remote locations oversee untrusted operators prototype single pipeline superscalar architecture run openrisc standard machine code two distinct modes processor run encrypt mode unprivileged user mode long pipeline speed unencrypted mode privilege supervisor mode short pipeline emit complete encrypt instruction every cycle average real trials,113,4,1510.05278.txt
http://arxiv.org/abs/1510.05461,Confidence Sets for the Source of a Diffusion in Regular Trees,"  We study the problem of identifying the source of a diffusion spreading over a regular tree. When the degree of each node is at least three, we show that it is possible to construct confidence sets for the diffusion source with size independent of the number of infected nodes. Our estimators are motivated by analogous results in the literature concerning identification of the root node in preferential attachment and uniform attachment trees. At the core of our proofs is a probabilistic analysis of P\'{o}lya urns corresponding to the number of uninfected neighbors in specific subtrees of the infection tree. We also provide an example illustrating the shortcomings of source estimation techniques in settings where the underlying graph is asymmetric. ",Mathematics - Statistics Theory ; Computer Science - Discrete Mathematics ; Computer Science - Social and Information Networks ; Mathematics - Probability ; Statistics - Machine Learning ; 62M99 ; ,"Khim, Justin ; Loh, Po-Ling ; ","Confidence Sets for the Source of a Diffusion in Regular Trees  We study the problem of identifying the source of a diffusion spreading over a regular tree. When the degree of each node is at least three, we show that it is possible to construct confidence sets for the diffusion source with size independent of the number of infected nodes. Our estimators are motivated by analogous results in the literature concerning identification of the root node in preferential attachment and uniform attachment trees. At the core of our proofs is a probabilistic analysis of P\'{o}lya urns corresponding to the number of uninfected neighbors in specific subtrees of the infection tree. We also provide an example illustrating the shortcomings of source estimation techniques in settings where the underlying graph is asymmetric. ",confidence set source diffusion regular tree study problem identify source diffusion spread regular tree degree node least three show possible construct confidence set diffusion source size independent number infect nod estimators motivate analogous result literature concern identification root node preferential attachment uniform attachment tree core proof probabilistic analysis lya urns correspond number uninfected neighbor specific subtrees infection tree also provide example illustrate shortcomings source estimation techniques settings underlie graph asymmetric,70,3,1510.05461.txt
http://arxiv.org/abs/1510.05751,Semi-Implicit Time Integration of Atmospheric Flows with   Characteristic-Based Flux Partitioning,"  This paper presents a characteristic-based flux partitioning for the semi-implicit time integration of atmospheric flows. Nonhydrostatic models require the solution of the compressible Euler equations. The acoustic time-scale is significantly faster than the advective scale, yet it is typically not relevant to atmospheric and weather phenomena. The acoustic and advective components of the hyperbolic flux are separated in the characteristic space. High-order, conservative additive Runge-Kutta methods are applied to the partitioned equations so that the acoustic component is integrated in time implicitly with an unconditionally stable method, while the advective component is integrated explicitly. The time step of the overall algorithm is thus determined by the advective scale. Benchmark flow problems are used to demonstrate the accuracy, stability, and convergence of the proposed algorithm. The computational cost of the partitioned semi-implicit approach is compared with that of explicit time integration. ","Computer Science - Computational Engineering, Finance, and Science ; Mathematics - Numerical Analysis ; 65M06, 86A10, 76N15 ; ","Ghosh, Debojyoti ; Constantinescu, Emil M. ; ","Semi-Implicit Time Integration of Atmospheric Flows with   Characteristic-Based Flux Partitioning  This paper presents a characteristic-based flux partitioning for the semi-implicit time integration of atmospheric flows. Nonhydrostatic models require the solution of the compressible Euler equations. The acoustic time-scale is significantly faster than the advective scale, yet it is typically not relevant to atmospheric and weather phenomena. The acoustic and advective components of the hyperbolic flux are separated in the characteristic space. High-order, conservative additive Runge-Kutta methods are applied to the partitioned equations so that the acoustic component is integrated in time implicitly with an unconditionally stable method, while the advective component is integrated explicitly. The time step of the overall algorithm is thus determined by the advective scale. Benchmark flow problems are used to demonstrate the accuracy, stability, and convergence of the proposed algorithm. The computational cost of the partitioned semi-implicit approach is compared with that of explicit time integration. ",semi implicit time integration atmospheric flow characteristic base flux partition paper present characteristic base flux partition semi implicit time integration atmospheric flow nonhydrostatic model require solution compressible euler equations acoustic time scale significantly faster advective scale yet typically relevant atmospheric weather phenomena acoustic advective components hyperbolic flux separate characteristic space high order conservative additive runge kutta methods apply partition equations acoustic component integrate time implicitly unconditionally stable method advective component integrate explicitly time step overall algorithm thus determine advective scale benchmark flow problems use demonstrate accuracy stability convergence propose algorithm computational cost partition semi implicit approach compare explicit time integration,100,11,1510.05751.txt
http://arxiv.org/abs/1510.06423,Optimization as Estimation with Gaussian Processes in Bandit Settings,"  Recently, there has been rising interest in Bayesian optimization -- the optimization of an unknown function with assumptions usually expressed by a Gaussian Process (GP) prior. We study an optimization strategy that directly uses an estimate of the argmax of the function. This strategy offers both practical and theoretical advantages: no tradeoff parameter needs to be selected, and, moreover, we establish close connections to the popular GP-UCB and GP-PI strategies. Our approach can be understood as automatically and adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We illustrate the effects of this adaptive tuning via bounds on the regret as well as an extensive empirical evaluation on robotics and vision tasks, demonstrating the robustness of this strategy for a range of performance criteria. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; ,"Wang, Zi ; Zhou, Bolei ; Jegelka, Stefanie ; ","Optimization as Estimation with Gaussian Processes in Bandit Settings  Recently, there has been rising interest in Bayesian optimization -- the optimization of an unknown function with assumptions usually expressed by a Gaussian Process (GP) prior. We study an optimization strategy that directly uses an estimate of the argmax of the function. This strategy offers both practical and theoretical advantages: no tradeoff parameter needs to be selected, and, moreover, we establish close connections to the popular GP-UCB and GP-PI strategies. Our approach can be understood as automatically and adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We illustrate the effects of this adaptive tuning via bounds on the regret as well as an extensive empirical evaluation on robotics and vision tasks, demonstrating the robustness of this strategy for a range of performance criteria. ",optimization estimation gaussian process bandit settings recently rise interest bayesian optimization optimization unknown function assumptions usually express gaussian process gp prior study optimization strategy directly use estimate argmax function strategy offer practical theoretical advantage tradeoff parameter need select moreover establish close connections popular gp ucb gp pi strategies approach understand automatically adaptively trade exploration exploitation gp ucb gp pi illustrate effect adaptive tune via bound regret well extensive empirical evaluation robotics vision task demonstrate robustness strategy range performance criteria,79,7,1510.06423.txt
http://arxiv.org/abs/1510.06482,Triangular Alignment (TAME): A Tensor-based Approach for Higher-order   Network Alignment,"  Network alignment has extensive applications in comparative interactomics. Traditional approaches aim to simultaneously maximize the number of conserved edges and the underlying similarity of aligned entities. We propose a novel formulation of the network alignment problem that extends topological similarity to higher-order structures and provides a new objective function that maximizes the number of aligned substructures. This objective function corresponds to an integer programming problem, which is NP-hard. Consequently, we identify a closely related surrogate function whose maximization results in a tensor eigenvector problem. Based on this formulation, we present an algorithm called Triangular AlignMEnt (TAME), which attempts to maximize the number of aligned triangles across networks. Using a case study on the NAPAbench dataset, we show that triangular alignment is capable of producing mappings with high node correctness. We further evaluate our method by aligning yeast and human interactomes. Our results indicate that TAME outperforms the state-of-art alignment methods in terms of conserved triangles. In addition, we show that the number of conserved triangles is more significantly correlated, compared to the conserved edge, with node correctness and co-expression of edges. Our formulation and resulting algorithms can be easily extended to arbitrary motifs. ","Computer Science - Computational Engineering, Finance, and Science ; ","Mohammadi, Shahin ; Gleich, David ; Kolda, Tamara ; Grama, Ananth ; ","Triangular Alignment (TAME): A Tensor-based Approach for Higher-order   Network Alignment  Network alignment has extensive applications in comparative interactomics. Traditional approaches aim to simultaneously maximize the number of conserved edges and the underlying similarity of aligned entities. We propose a novel formulation of the network alignment problem that extends topological similarity to higher-order structures and provides a new objective function that maximizes the number of aligned substructures. This objective function corresponds to an integer programming problem, which is NP-hard. Consequently, we identify a closely related surrogate function whose maximization results in a tensor eigenvector problem. Based on this formulation, we present an algorithm called Triangular AlignMEnt (TAME), which attempts to maximize the number of aligned triangles across networks. Using a case study on the NAPAbench dataset, we show that triangular alignment is capable of producing mappings with high node correctness. We further evaluate our method by aligning yeast and human interactomes. Our results indicate that TAME outperforms the state-of-art alignment methods in terms of conserved triangles. In addition, we show that the number of conserved triangles is more significantly correlated, compared to the conserved edge, with node correctness and co-expression of edges. Our formulation and resulting algorithms can be easily extended to arbitrary motifs. ",triangular alignment tame tensor base approach higher order network alignment network alignment extensive applications comparative interactomics traditional approach aim simultaneously maximize number conserve edge underlie similarity align entities propose novel formulation network alignment problem extend topological similarity higher order structure provide new objective function maximize number align substructures objective function correspond integer program problem np hard consequently identify closely relate surrogate function whose maximization result tensor eigenvector problem base formulation present algorithm call triangular alignment tame attempt maximize number align triangles across network use case study napabench dataset show triangular alignment capable produce mappings high node correctness evaluate method align yeast human interactomes result indicate tame outperform state art alignment methods term conserve triangles addition show number conserve triangles significantly correlate compare conserve edge node correctness co expression edge formulation result algorithms easily extend arbitrary motifs,136,11,1510.06482.txt
http://arxiv.org/abs/1510.06684,Dual Free Adaptive Mini-batch SDCA for Empirical Risk Minimization,"  In this paper we develop dual free mini-batch SDCA with adaptive probabilities for regularized empirical risk minimization. This work is motivated by recent work of Shai Shalev-Shwartz on dual free SDCA method, however, we allow a non-uniform selection of ""dual"" coordinates in SDCA. Moreover, the probability can change over time, making it more efficient than fix uniform or non-uniform selection. We also propose an efficient procedure to generate a random non-uniform mini-batch through iterative process. The work is concluded with multiple numerical experiments to show the efficiency of proposed algorithms. ",Mathematics - Optimization and Control ; Computer Science - Machine Learning ; ,"He, Xi ; Takáč, Martin ; ","Dual Free Adaptive Mini-batch SDCA for Empirical Risk Minimization  In this paper we develop dual free mini-batch SDCA with adaptive probabilities for regularized empirical risk minimization. This work is motivated by recent work of Shai Shalev-Shwartz on dual free SDCA method, however, we allow a non-uniform selection of ""dual"" coordinates in SDCA. Moreover, the probability can change over time, making it more efficient than fix uniform or non-uniform selection. We also propose an efficient procedure to generate a random non-uniform mini-batch through iterative process. The work is concluded with multiple numerical experiments to show the efficiency of proposed algorithms. ",dual free adaptive mini batch sdca empirical risk minimization paper develop dual free mini batch sdca adaptive probabilities regularize empirical risk minimization work motivate recent work shai shalev shwartz dual free sdca method however allow non uniform selection dual coordinate sdca moreover probability change time make efficient fix uniform non uniform selection also propose efficient procedure generate random non uniform mini batch iterative process work conclude multiple numerical experiment show efficiency propose algorithms,73,7,1510.06684.txt
http://arxiv.org/abs/1510.06750,Quantum vs Classical Proofs and Subset Verification,"  We study the ability of efficient quantum verifiers to decide properties of exponentially large subsets given either a classical or quantum witness. We develop a general framework that can be used to prove that QCMA machines, with only classical witnesses, cannot verify certain properties of subsets given implicitly via an oracle. We use this framework to prove an oracle separation between QCMA and QMA using an ""in-place"" permutation oracle, making the first progress on this question since Aaronson and Kuperberg in 2007. We also use the framework to prove a particularly simple standard oracle separation between QCMA and AM. ",Quantum Physics ; Computer Science - Computational Complexity ; ,"Fefferman, Bill ; Kimmel, Shelby ; ","Quantum vs Classical Proofs and Subset Verification  We study the ability of efficient quantum verifiers to decide properties of exponentially large subsets given either a classical or quantum witness. We develop a general framework that can be used to prove that QCMA machines, with only classical witnesses, cannot verify certain properties of subsets given implicitly via an oracle. We use this framework to prove an oracle separation between QCMA and QMA using an ""in-place"" permutation oracle, making the first progress on this question since Aaronson and Kuperberg in 2007. We also use the framework to prove a particularly simple standard oracle separation between QCMA and AM. ",quantum vs classical proof subset verification study ability efficient quantum verifiers decide properties exponentially large subsets give either classical quantum witness develop general framework use prove qcma machine classical witness cannot verify certain properties subsets give implicitly via oracle use framework prove oracle separation qcma qma use place permutation oracle make first progress question since aaronson kuperberg also use framework prove particularly simple standard oracle separation qcma,67,8,1510.06750.txt
http://arxiv.org/abs/1510.07135,Induced minors and well-quasi-ordering,"  A graph $H$ is an induced minor of a graph $G$ if it can be obtained from an induced subgraph of $G$ by contracting edges. Otherwise, $G$ is said to be $H$-induced minor-free. Robin Thomas showed that $K_4$-induced minor-free graphs are well-quasi-ordered by induced minors [Graphs without $K_4$ and well-quasi-ordering, Journal of Combinatorial Theory, Series B, 38(3):240 -- 247, 1985].   We provide a dichotomy theorem for $H$-induced minor-free graphs and show that the class of $H$-induced minor-free graphs is well-quasi-ordered by the induced minor relation if and only if $H$ is an induced minor of the gem (the path on 4 vertices plus a dominating vertex) or of the graph obtained by adding a vertex of degree 2 to the complete graph on 4 vertices. To this end we proved two decomposition theorems which are of independent interest.   Similar dichotomy results were previously given for subgraphs by Guoli Ding in [Subgraphs and well-quasi-ordering, Journal of Graph Theory, 16(5):489--502, 1992] and for induced subgraphs by Peter Damaschke in [Induced subgraphs and well-quasi-ordering, Journal of Graph Theory, 14(4):427--435, 1990]. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C, 06A07 ; G.2.2 ; ","Błasiok, Jarosław ; Kamiński, Marcin ; Raymond, Jean-Florent ; Trunck, Théophile ; ","Induced minors and well-quasi-ordering  A graph $H$ is an induced minor of a graph $G$ if it can be obtained from an induced subgraph of $G$ by contracting edges. Otherwise, $G$ is said to be $H$-induced minor-free. Robin Thomas showed that $K_4$-induced minor-free graphs are well-quasi-ordered by induced minors [Graphs without $K_4$ and well-quasi-ordering, Journal of Combinatorial Theory, Series B, 38(3):240 -- 247, 1985].   We provide a dichotomy theorem for $H$-induced minor-free graphs and show that the class of $H$-induced minor-free graphs is well-quasi-ordered by the induced minor relation if and only if $H$ is an induced minor of the gem (the path on 4 vertices plus a dominating vertex) or of the graph obtained by adding a vertex of degree 2 to the complete graph on 4 vertices. To this end we proved two decomposition theorems which are of independent interest.   Similar dichotomy results were previously given for subgraphs by Guoli Ding in [Subgraphs and well-quasi-ordering, Journal of Graph Theory, 16(5):489--502, 1992] and for induced subgraphs by Peter Damaschke in [Induced subgraphs and well-quasi-ordering, Journal of Graph Theory, 14(4):427--435, 1990]. ",induce minors well quasi order graph induce minor graph obtain induce subgraph contract edge otherwise say induce minor free robin thomas show induce minor free graph well quasi order induce minors graph without well quasi order journal combinatorial theory series provide dichotomy theorem induce minor free graph show class induce minor free graph well quasi order induce minor relation induce minor gem path vertices plus dominate vertex graph obtain add vertex degree complete graph vertices end prove two decomposition theorems independent interest similar dichotomy result previously give subgraphs guoli ding subgraphs well quasi order journal graph theory induce subgraphs peter damaschke induce subgraphs well quasi order journal graph theory,109,3,1510.07135.txt
http://arxiv.org/abs/1510.07357,Distributed Bare-Bones Communication in Wireless Networks,"  We consider wireless networks with the SINR model of interference when nodes have very limited individual knowledge and capabilities. In particular, nodes do not know their geographic coordinates and their neighborhoods, nor even the size $n$ of the network, nor can they sense collisions. Each node is equipped only with a unique integer name, where $N$ is an upper bound on their range. We study distributed algorithms for communication problems in the following three settings. In the single-node-start case, when one node starts an execution and the other nodes are awoken by receiving messages from already awoken nodes, we present a randomized broadcast algorithm which wakes up all the nodes in $O(n \log^2 N)$ rounds with high probability. Let $\Delta$ denote the maximum number of nodes that successfully receive a message transmitted by a node when no other nodes transmit. For the synchronized-start case, when all the nodes start an execution simultaneously, we give a randomized algorithm that computes a backbone of the network in $O(\Delta\log^{7} N)$ rounds with high probability. Finally, in the partly-coordinated-start case, when a number of nodes start an execution together and other nodes are awoken by receiving messages from the already awoken nodes, we develop an algorithm that creates a backbone network in time $O(n\log^2 N +\Delta\log^{7} N)$ with high probability. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Chlebus, Bogdan S. ; Kowalski, Dariusz R. ; Vaya, Shailesh ; ","Distributed Bare-Bones Communication in Wireless Networks  We consider wireless networks with the SINR model of interference when nodes have very limited individual knowledge and capabilities. In particular, nodes do not know their geographic coordinates and their neighborhoods, nor even the size $n$ of the network, nor can they sense collisions. Each node is equipped only with a unique integer name, where $N$ is an upper bound on their range. We study distributed algorithms for communication problems in the following three settings. In the single-node-start case, when one node starts an execution and the other nodes are awoken by receiving messages from already awoken nodes, we present a randomized broadcast algorithm which wakes up all the nodes in $O(n \log^2 N)$ rounds with high probability. Let $\Delta$ denote the maximum number of nodes that successfully receive a message transmitted by a node when no other nodes transmit. For the synchronized-start case, when all the nodes start an execution simultaneously, we give a randomized algorithm that computes a backbone of the network in $O(\Delta\log^{7} N)$ rounds with high probability. Finally, in the partly-coordinated-start case, when a number of nodes start an execution together and other nodes are awoken by receiving messages from the already awoken nodes, we develop an algorithm that creates a backbone network in time $O(n\log^2 N +\Delta\log^{7} N)$ with high probability. ",distribute bare bone communication wireless network consider wireless network sinr model interference nod limit individual knowledge capabilities particular nod know geographic coordinate neighborhoods even size network sense collisions node equip unique integer name upper bind range study distribute algorithms communication problems follow three settings single node start case one node start execution nod awake receive message already awake nod present randomize broadcast algorithm wake nod log round high probability let delta denote maximum number nod successfully receive message transmit node nod transmit synchronize start case nod start execution simultaneously give randomize algorithm compute backbone network delta log round high probability finally partly coordinate start case number nod start execution together nod awake receive message already awake nod develop algorithm create backbone network time log delta log high probability,128,6,1510.07357.txt
http://arxiv.org/abs/1510.07380,SLAP: Simultaneous Localization and Planning Under Uncertainty for   Physical Mobile Robots via Dynamic Replanning in Belief Space: Extended   version,"  Simultaneous localization and Planning (SLAP) is a crucial ability for an autonomous robot operating under uncertainty. In its most general form, SLAP induces a continuous POMDP (partially-observable Markov decision process), which needs to be repeatedly solved online. This paper addresses this problem and proposes a dynamic replanning scheme in belief space. The underlying POMDP, which is continuous in state, action, and observation space, is approximated offline via sampling-based methods, but operates in a replanning loop online to admit local improvements to the coarse offline policy. This construct enables the proposed method to combat changing environments and large localization errors, even when the change alters the homotopy class of the optimal trajectory. It further outperforms the state-of-the-art FIRM (Feedback-based Information RoadMap) method by eliminating unnecessary stabilization steps. Applying belief space planning to physical systems brings with it a plethora of challenges. A key focus of this paper is to implement the proposed planner on a physical robot and show the SLAP solution performance under uncertainty, in changing environments and in the presence of large disturbances, such as a kidnapped robot situation. ",Computer Science - Robotics ; Computer Science - Systems and Control ; ,"Agha-mohammadi, Ali-akbar ; Agarwal, Saurav ; Kim, Sung-Kyun ; Chakravorty, Suman ; Amato, Nancy M. ; ","SLAP: Simultaneous Localization and Planning Under Uncertainty for   Physical Mobile Robots via Dynamic Replanning in Belief Space: Extended   version  Simultaneous localization and Planning (SLAP) is a crucial ability for an autonomous robot operating under uncertainty. In its most general form, SLAP induces a continuous POMDP (partially-observable Markov decision process), which needs to be repeatedly solved online. This paper addresses this problem and proposes a dynamic replanning scheme in belief space. The underlying POMDP, which is continuous in state, action, and observation space, is approximated offline via sampling-based methods, but operates in a replanning loop online to admit local improvements to the coarse offline policy. This construct enables the proposed method to combat changing environments and large localization errors, even when the change alters the homotopy class of the optimal trajectory. It further outperforms the state-of-the-art FIRM (Feedback-based Information RoadMap) method by eliminating unnecessary stabilization steps. Applying belief space planning to physical systems brings with it a plethora of challenges. A key focus of this paper is to implement the proposed planner on a physical robot and show the SLAP solution performance under uncertainty, in changing environments and in the presence of large disturbances, such as a kidnapped robot situation. ",slap simultaneous localization plan uncertainty physical mobile robots via dynamic replanning belief space extend version simultaneous localization plan slap crucial ability autonomous robot operate uncertainty general form slap induce continuous pomdp partially observable markov decision process need repeatedly solve online paper address problem propose dynamic replanning scheme belief space underlie pomdp continuous state action observation space approximate offline via sample base methods operate replanning loop online admit local improvements coarse offline policy construct enable propose method combat change environments large localization errors even change alter homotopy class optimal trajectory outperform state art firm feedback base information roadmap method eliminate unnecessary stabilization step apply belief space plan physical systems bring plethora challenge key focus paper implement propose planner physical robot show slap solution performance uncertainty change environments presence large disturbances kidnap robot situation,132,11,1510.07380.txt
http://arxiv.org/abs/1510.07391,Vehicle Color Recognition using Convolutional Neural Network,"  Vehicle color information is one of the important elements in ITS (Intelligent Traffic System). In this paper, we present a vehicle color recognition method using convolutional neural network (CNN). Naturally, CNN is designed to learn classification method based on shape information, but we proved that CNN can also learn classification based on color distribution. In our method, we convert the input image to two different color spaces, HSV and CIE Lab, and run it to some CNN architecture. The training process follow procedure introduce by Krizhevsky, that learning rate is decreasing by factor of 10 after some iterations. To test our method, we use publicly vehicle color recognition dataset provided by Chen. The results, our model outperform the original system provide by Chen with 2% higher overall accuracy. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Rachmadi, Reza Fuad ; Purnama, I Ketut Eddy ; ","Vehicle Color Recognition using Convolutional Neural Network  Vehicle color information is one of the important elements in ITS (Intelligent Traffic System). In this paper, we present a vehicle color recognition method using convolutional neural network (CNN). Naturally, CNN is designed to learn classification method based on shape information, but we proved that CNN can also learn classification based on color distribution. In our method, we convert the input image to two different color spaces, HSV and CIE Lab, and run it to some CNN architecture. The training process follow procedure introduce by Krizhevsky, that learning rate is decreasing by factor of 10 after some iterations. To test our method, we use publicly vehicle color recognition dataset provided by Chen. The results, our model outperform the original system provide by Chen with 2% higher overall accuracy. ",vehicle color recognition use convolutional neural network vehicle color information one important elements intelligent traffic system paper present vehicle color recognition method use convolutional neural network cnn naturally cnn design learn classification method base shape information prove cnn also learn classification base color distribution method convert input image two different color space hsv cie lab run cnn architecture train process follow procedure introduce krizhevsky learn rate decrease factor iterations test method use publicly vehicle color recognition dataset provide chen result model outperform original system provide chen higher overall accuracy,89,13,1510.07391.txt
http://arxiv.org/abs/1510.07424,The Impossibility of Extending Random Dictatorship to Weak Preferences,"  Random dictatorship has been characterized as the only social decision scheme that satisfies efficiency and strategyproofness when individual preferences are strict. We show that no extension of random dictatorship to weak preferences satisfies these properties, even when significantly weakening the required degree of strategyproofness. ",Computer Science - Multiagent Systems ; Computer Science - Computer Science and Game Theory ; C.6 ; D.7 ; D.8 ; ,"Brandl, Florian ; Brandt, Felix ; Suksompong, Warut ; ","The Impossibility of Extending Random Dictatorship to Weak Preferences  Random dictatorship has been characterized as the only social decision scheme that satisfies efficiency and strategyproofness when individual preferences are strict. We show that no extension of random dictatorship to weak preferences satisfies these properties, even when significantly weakening the required degree of strategyproofness. ",impossibility extend random dictatorship weak preferences random dictatorship characterize social decision scheme satisfy efficiency strategyproofness individual preferences strict show extension random dictatorship weak preferences satisfy properties even significantly weaken require degree strategyproofness,32,7,1510.07424.txt
http://arxiv.org/abs/1510.08183,Raptor Codes in the Low SNR Regime,"  In this paper, we revisit the design of Raptor codes for binary input additive white Gaussian noise (BIAWGN) channels, where we are interested in very low signal to noise ratios (SNRs). A linear programming degree distribution optimization problem is defined for Raptor codes in the low SNR regime through several approximations. We also provide an exact expression for the polynomial representation of the degree distribution with infinite maximum degree in the low SNR regime, which enables us to calculate the exact value of the fractions of output nodes of small degrees. A more practical degree distribution design is also proposed for Raptor codes in the low SNR regime, where we include the rate efficiency and the decoding complexity in the optimization problem, and an upper bound on the maximum rate efficiency is derived for given design parameters. Simulation results show that the Raptor code with the designed degree distributions can approach rate efficiencies larger than 0.95 in the low SNR regime. ",Computer Science - Information Theory ; ,"Shirvanimoghaddam, Mahyar ; Johnson, Sarah J. ; ","Raptor Codes in the Low SNR Regime  In this paper, we revisit the design of Raptor codes for binary input additive white Gaussian noise (BIAWGN) channels, where we are interested in very low signal to noise ratios (SNRs). A linear programming degree distribution optimization problem is defined for Raptor codes in the low SNR regime through several approximations. We also provide an exact expression for the polynomial representation of the degree distribution with infinite maximum degree in the low SNR regime, which enables us to calculate the exact value of the fractions of output nodes of small degrees. A more practical degree distribution design is also proposed for Raptor codes in the low SNR regime, where we include the rate efficiency and the decoding complexity in the optimization problem, and an upper bound on the maximum rate efficiency is derived for given design parameters. Simulation results show that the Raptor code with the designed degree distributions can approach rate efficiencies larger than 0.95 in the low SNR regime. ",raptor cod low snr regime paper revisit design raptor cod binary input additive white gaussian noise biawgn channel interest low signal noise ratios snrs linear program degree distribution optimization problem define raptor cod low snr regime several approximations also provide exact expression polynomial representation degree distribution infinite maximum degree low snr regime enable us calculate exact value fraction output nod small degrees practical degree distribution design also propose raptor cod low snr regime include rate efficiency decode complexity optimization problem upper bind maximum rate efficiency derive give design parameters simulation result show raptor code design degree distributions approach rate efficiencies larger low snr regime,104,5,1510.08183.txt
http://arxiv.org/abs/1510.08202,Oblivious Fronthaul-Constrained Relay for a Gaussian Channel,"  We consider systems in which the transmitter conveys messages to the receiver through a capacity-limited relay station. The channel between the transmitter and the relay-station is assumed to be a frequency selective additive Gaussian noise channel. It is assumed that the transmitter can shape the spectrum and adapt the coding technique so as to optimize performance. The relay operation is oblivious (nomadic transmitters), that is, the specific codebooks used are unknown. We find the reliable information rate that can be achieved with Gaussian signaling in this setting, and to that end, employ Gaussian bottleneck results combined with Shannon's incremental frequency approach. We also prove that, unlike classical water-pouring, the allocated spectrum (power and bit-rate) of the optimal solution could frequently be discontinuous. These results can be applied to a MIMO transmission scheme. We also investigate the case of an entropy limited relay. We present lower and upper bounds on the optimal performance (in terms of mutual information), and derive an analytical approximation. ",Computer Science - Information Theory ; ,"Homri, Adi ; Peleg, Michael ; Shamai, Shlomo ; ","Oblivious Fronthaul-Constrained Relay for a Gaussian Channel  We consider systems in which the transmitter conveys messages to the receiver through a capacity-limited relay station. The channel between the transmitter and the relay-station is assumed to be a frequency selective additive Gaussian noise channel. It is assumed that the transmitter can shape the spectrum and adapt the coding technique so as to optimize performance. The relay operation is oblivious (nomadic transmitters), that is, the specific codebooks used are unknown. We find the reliable information rate that can be achieved with Gaussian signaling in this setting, and to that end, employ Gaussian bottleneck results combined with Shannon's incremental frequency approach. We also prove that, unlike classical water-pouring, the allocated spectrum (power and bit-rate) of the optimal solution could frequently be discontinuous. These results can be applied to a MIMO transmission scheme. We also investigate the case of an entropy limited relay. We present lower and upper bounds on the optimal performance (in terms of mutual information), and derive an analytical approximation. ",oblivious fronthaul constrain relay gaussian channel consider systems transmitter convey message receiver capacity limit relay station channel transmitter relay station assume frequency selective additive gaussian noise channel assume transmitter shape spectrum adapt cod technique optimize performance relay operation oblivious nomadic transmitters specific codebooks use unknown find reliable information rate achieve gaussian signal set end employ gaussian bottleneck result combine shannon incremental frequency approach also prove unlike classical water pour allocate spectrum power bite rate optimal solution could frequently discontinuous result apply mimo transmission scheme also investigate case entropy limit relay present lower upper bound optimal performance term mutual information derive analytical approximation,102,12,1510.08202.txt
http://arxiv.org/abs/1510.08368,Switching control for incremental stabilization of nonlinear systems via   contraction theory,"  In this paper we present a switching control strategy to incrementally stabilize a class of nonlinear dynamical systems. Exploiting recent results on contraction analysis of switched Filippov systems derived using regularization, sufficient conditions are presented to prove incremental stability of the closed-loop system. Furthermore, based on these sufficient conditions, a design procedure is proposed to design a switched control action that is active only where the open-loop system is not sufficiently incrementally stable in order to reduce the required control effort. The design procedure to either locally or globally incrementally stabilize a dynamical system is then illustrated by means of a representative example. ",Computer Science - Systems and Control ; ,"di Bernardo, Mario ; Fiore, Davide ; ","Switching control for incremental stabilization of nonlinear systems via   contraction theory  In this paper we present a switching control strategy to incrementally stabilize a class of nonlinear dynamical systems. Exploiting recent results on contraction analysis of switched Filippov systems derived using regularization, sufficient conditions are presented to prove incremental stability of the closed-loop system. Furthermore, based on these sufficient conditions, a design procedure is proposed to design a switched control action that is active only where the open-loop system is not sufficiently incrementally stable in order to reduce the required control effort. The design procedure to either locally or globally incrementally stabilize a dynamical system is then illustrated by means of a representative example. ",switch control incremental stabilization nonlinear systems via contraction theory paper present switch control strategy incrementally stabilize class nonlinear dynamical systems exploit recent result contraction analysis switch filippov systems derive use regularization sufficient condition present prove incremental stability close loop system furthermore base sufficient condition design procedure propose design switch control action active open loop system sufficiently incrementally stable order reduce require control effort design procedure either locally globally incrementally stabilize dynamical system illustrate mean representative example,76,7,1510.08368.txt
http://arxiv.org/abs/1510.08417,Monotone Projection Lower Bounds from Extended Formulation Lower Bounds,"  In this short note, we reduce lower bounds on monotone projections of polynomials to lower bounds on extended formulations of polytopes. Applying our reduction to the seminal extended formulation lower bounds of Fiorini, Massar, Pokutta, Tiwari, & de Wolf (STOC 2012; J. ACM, 2015) and Rothvoss (STOC 2014; J. ACM, 2017), we obtain the following interesting consequences.   1. The Hamiltonian Cycle polynomial is not a monotone subexponential-size projection of the permanent; this both rules out a natural attempt at a monotone lower bound on the Boolean permanent, and shows that the permanent is not complete for non-negative polynomials in VNP$_{{\mathbb R}}$ under monotone p-projections.   2. The cut polynomials and the perfect matching polynomial (or ""unsigned Pfaffian"") are not monotone p-projections of the permanent. The latter, over the Boolean and-or semi-ring, rules out monotone reductions in one of the natural approaches to reducing perfect matchings in general graphs to perfect matchings in bipartite graphs.   As the permanent is universal for monotone formulas, these results also imply exponential lower bounds on the monotone formula size and monotone circuit size of these polynomials. ","Computer Science - Computational Complexity ; 68Q15, 68Q17, 90C05, 15A15, 05C70 ; F.1.3 ; F.2.1 ; G.1.6 ; ","Grochow, Joshua A. ; ","Monotone Projection Lower Bounds from Extended Formulation Lower Bounds  In this short note, we reduce lower bounds on monotone projections of polynomials to lower bounds on extended formulations of polytopes. Applying our reduction to the seminal extended formulation lower bounds of Fiorini, Massar, Pokutta, Tiwari, & de Wolf (STOC 2012; J. ACM, 2015) and Rothvoss (STOC 2014; J. ACM, 2017), we obtain the following interesting consequences.   1. The Hamiltonian Cycle polynomial is not a monotone subexponential-size projection of the permanent; this both rules out a natural attempt at a monotone lower bound on the Boolean permanent, and shows that the permanent is not complete for non-negative polynomials in VNP$_{{\mathbb R}}$ under monotone p-projections.   2. The cut polynomials and the perfect matching polynomial (or ""unsigned Pfaffian"") are not monotone p-projections of the permanent. The latter, over the Boolean and-or semi-ring, rules out monotone reductions in one of the natural approaches to reducing perfect matchings in general graphs to perfect matchings in bipartite graphs.   As the permanent is universal for monotone formulas, these results also imply exponential lower bounds on the monotone formula size and monotone circuit size of these polynomials. ",monotone projection lower bound extend formulation lower bound short note reduce lower bound monotone projections polynomials lower bound extend formulations polytopes apply reduction seminal extend formulation lower bound fiorini massar pokutta tiwari de wolf stoc acm rothvoss stoc acm obtain follow interest consequences hamiltonian cycle polynomial monotone subexponential size projection permanent rule natural attempt monotone lower bind boolean permanent show permanent complete non negative polynomials vnp mathbb monotone projections cut polynomials perfect match polynomial unsigned pfaffian monotone projections permanent latter boolean semi ring rule monotone reductions one natural approach reduce perfect match general graph perfect match bipartite graph permanent universal monotone formulas result also imply exponential lower bound monotone formula size monotone circuit size polynomials,115,8,1510.08417.txt
http://arxiv.org/abs/1510.08578,My Reflections on the First Man vs. Machine No-Limit Texas Hold 'em   Competition,"  The first ever human vs. computer no-limit Texas hold 'em competition took place from April 24-May 8, 2015 at River's Casino in Pittsburgh, PA. In this article I present my thoughts on the competition design, agent architecture, and lessons learned. ",Computer Science - Computer Science and Game Theory ; Computer Science - Artificial Intelligence ; Computer Science - Multiagent Systems ; Economics - Theoretical Economics ; ,"Ganzfried, Sam ; ","My Reflections on the First Man vs. Machine No-Limit Texas Hold 'em   Competition  The first ever human vs. computer no-limit Texas hold 'em competition took place from April 24-May 8, 2015 at River's Casino in Pittsburgh, PA. In this article I present my thoughts on the competition design, agent architecture, and lessons learned. ",reflections first man vs machine limit texas hold em competition first ever human vs computer limit texas hold em competition take place april may river casino pittsburgh pa article present thoughts competition design agent architecture lessons learn,37,4,1510.08578.txt
http://arxiv.org/abs/1510.08779,Effect of Gromov-hyperbolicity Parameter on Cuts and Expansions in   Graphs and Some Algorithmic Implications,"  $\delta$-hyperbolic graphs, originally conceived by Gromov in 1987, occur often in many network applications; for fixed $\delta$, such graphs are simply called hyperbolic graphs and include non-trivial interesting classes of ""non-expander"" graphs. The main motivation of this paper is to investigate the effect of the hyperbolicity measure $\delta$ on expansion and cut-size bounds on graphs (here $\delta$ need not be a constant), and the asymptotic ranges of $\delta$ for which these results may provide improved approximation algorithms for related combinatorial problems. To this effect, we provide constructive bounds on node expansions for $\delta$-hyperbolic graphs as a function of $\delta$, and show that many witnesses (subsets of nodes) for such expansions can be computed efficiently even if the witnesses are required to be nested or sufficiently distinct from each other. To the best of our knowledge, these are the first such constructive bounds proven. We also show how to find a large family of s-t cuts with relatively small number of cut-edges when s and t are sufficiently far apart. We then provide algorithmic consequences of these bounds and their related proof techniques for two problems for $\delta$-hyperbolic graphs (where $\delta$ is a function $f$ of the number of nodes, the exact nature of growth of $f$ being dependent on the particular problem considered). ","Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; 68Q25, 68W25, 68W40, 05C85 ; F.2.2 ; G.2.2 ; I.1.2 ; ","DasGupta, Bhaskar ; Karpinski, Marek ; Mobasheri, Nasim ; Yahyanejad, Farzaneh ; ","Effect of Gromov-hyperbolicity Parameter on Cuts and Expansions in   Graphs and Some Algorithmic Implications  $\delta$-hyperbolic graphs, originally conceived by Gromov in 1987, occur often in many network applications; for fixed $\delta$, such graphs are simply called hyperbolic graphs and include non-trivial interesting classes of ""non-expander"" graphs. The main motivation of this paper is to investigate the effect of the hyperbolicity measure $\delta$ on expansion and cut-size bounds on graphs (here $\delta$ need not be a constant), and the asymptotic ranges of $\delta$ for which these results may provide improved approximation algorithms for related combinatorial problems. To this effect, we provide constructive bounds on node expansions for $\delta$-hyperbolic graphs as a function of $\delta$, and show that many witnesses (subsets of nodes) for such expansions can be computed efficiently even if the witnesses are required to be nested or sufficiently distinct from each other. To the best of our knowledge, these are the first such constructive bounds proven. We also show how to find a large family of s-t cuts with relatively small number of cut-edges when s and t are sufficiently far apart. We then provide algorithmic consequences of these bounds and their related proof techniques for two problems for $\delta$-hyperbolic graphs (where $\delta$ is a function $f$ of the number of nodes, the exact nature of growth of $f$ being dependent on the particular problem considered). ",effect gromov hyperbolicity parameter cut expansions graph algorithmic implications delta hyperbolic graph originally conceive gromov occur often many network applications fix delta graph simply call hyperbolic graph include non trivial interest class non expander graph main motivation paper investigate effect hyperbolicity measure delta expansion cut size bound graph delta need constant asymptotic range delta result may provide improve approximation algorithms relate combinatorial problems effect provide constructive bound node expansions delta hyperbolic graph function delta show many witness subsets nod expansions compute efficiently even witness require nest sufficiently distinct best knowledge first constructive bound prove also show find large family cut relatively small number cut edge sufficiently far apart provide algorithmic consequences bound relate proof techniques two problems delta hyperbolic graph delta function number nod exact nature growth dependent particular problem consider,131,3,1510.08779.txt
http://arxiv.org/abs/1510.08797,Convexity in Tree Spaces,"  We study the geometry of metrics and convexity structures on the space of phylogenetic trees, which is here realized as the tropical linear space of all \ ultrametrics. The ${\rm CAT}(0)$-metric of Billera-Holmes-Vogtman arises from the theory of orthant spaces. While its geodesics can be computed by the Owen-Provan algorithm, geodesic triangles are complicated. We show that the dimension of such a triangle can be arbitrarily high. Tropical convexity and the tropical metric behave better. They exhibit properties desirable for geometric statistics, such as geodesics of small depth. ",Mathematics - Metric Geometry ; Computer Science - Computational Geometry ; Mathematics - Combinatorics ; Quantitative Biology - Populations and Evolution ; ,"Lin, Bo ; Sturmfels, Bernd ; Tang, Xiaoxian ; Yoshida, Ruriko ; ","Convexity in Tree Spaces  We study the geometry of metrics and convexity structures on the space of phylogenetic trees, which is here realized as the tropical linear space of all \ ultrametrics. The ${\rm CAT}(0)$-metric of Billera-Holmes-Vogtman arises from the theory of orthant spaces. While its geodesics can be computed by the Owen-Provan algorithm, geodesic triangles are complicated. We show that the dimension of such a triangle can be arbitrarily high. Tropical convexity and the tropical metric behave better. They exhibit properties desirable for geometric statistics, such as geodesics of small depth. ",convexity tree space study geometry metrics convexity structure space phylogenetic tree realize tropical linear space ultrametrics rm cat metric billera holmes vogtman arise theory orthant space geodesics compute owen provan algorithm geodesic triangles complicate show dimension triangle arbitrarily high tropical convexity tropical metric behave better exhibit properties desirable geometric statistics geodesics small depth,53,4,1510.08797.txt
http://arxiv.org/abs/1510.08887,Phase Retrieval Using Unitary 2-Designs,"  We consider a variant of the phase retrieval problem, where vectors are replaced by unitary matrices, i.e., the unknown signal is a unitary matrix U, and the measurements consist of squared inner products |Tr(C*U)|^2 with unitary matrices C that are chosen by the observer. This problem has applications to quantum process tomography, when the unknown process is a unitary operation.   We show that PhaseLift, a convex programming algorithm for phase retrieval, can be adapted to this matrix setting, using measurements that are sampled from unitary 4- and 2-designs. In the case of unitary 4-design measurements, we show that PhaseLift can reconstruct all unitary matrices, using a near-optimal number of measurements. This extends previous work on PhaseLift using spherical 4-designs.   In the case of unitary 2-design measurements, we show that PhaseLift still works pretty well on average: it recovers almost all signals, up to a constant additive error, using a near-optimal number of measurements. These 2-design measurements are convenient for quantum process tomography, as they can be implemented via randomized benchmarking techniques. This is the first positive result on PhaseLift using 2-designs. ",Quantum Physics ; Computer Science - Information Theory ; Mathematics - Statistics Theory ; ,"Kimmel, Shelby ; Liu, Yi-Kai ; ","Phase Retrieval Using Unitary 2-Designs  We consider a variant of the phase retrieval problem, where vectors are replaced by unitary matrices, i.e., the unknown signal is a unitary matrix U, and the measurements consist of squared inner products |Tr(C*U)|^2 with unitary matrices C that are chosen by the observer. This problem has applications to quantum process tomography, when the unknown process is a unitary operation.   We show that PhaseLift, a convex programming algorithm for phase retrieval, can be adapted to this matrix setting, using measurements that are sampled from unitary 4- and 2-designs. In the case of unitary 4-design measurements, we show that PhaseLift can reconstruct all unitary matrices, using a near-optimal number of measurements. This extends previous work on PhaseLift using spherical 4-designs.   In the case of unitary 2-design measurements, we show that PhaseLift still works pretty well on average: it recovers almost all signals, up to a constant additive error, using a near-optimal number of measurements. These 2-design measurements are convenient for quantum process tomography, as they can be implemented via randomized benchmarking techniques. This is the first positive result on PhaseLift using 2-designs. ",phase retrieval use unitary design consider variant phase retrieval problem vectors replace unitary matrices unknown signal unitary matrix measurements consist square inner products tr unitary matrices choose observer problem applications quantum process tomography unknown process unitary operation show phaselift convex program algorithm phase retrieval adapt matrix set use measurements sample unitary design case unitary design measurements show phaselift reconstruct unitary matrices use near optimal number measurements extend previous work phaselift use spherical design case unitary design measurements show phaselift still work pretty well average recover almost signal constant additive error use near optimal number measurements design measurements convenient quantum process tomography implement via randomize benchmarking techniques first positive result phaselift use design,112,9,1510.08887.txt
http://arxiv.org/abs/1510.08983,Highway Long Short-Term Memory RNNs for Distant Speech Recognition,"  In this paper, we extend the deep long short-term memory (DLSTM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and $5.3\%$ relative improvement respectively. ",Computer Science - Neural and Evolutionary Computing ; Computer Science - Artificial Intelligence ; Computer Science - Computation and Language ; Computer Science - Machine Learning ; Electrical Engineering and Systems Science - Audio and Speech Processing ; ,"Zhang, Yu ; Chen, Guoguo ; Yu, Dong ; Yao, Kaisheng ; Khudanpur, Sanjeev ; Glass, James ; ","Highway Long Short-Term Memory RNNs for Distant Speech Recognition  In this paper, we extend the deep long short-term memory (DLSTM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and $5.3\%$ relative improvement respectively. ",highway long short term memory rnns distant speech recognition paper extend deep long short term memory dlstm recurrent neural network introduce gate direct connections memory cells adjacent layer direct link call highway connections enable unimpeded information flow across different layer thus alleviate gradient vanish problem build deeper lstms introduce latency control bidirectional lstms blstms exploit whole history keep latency control efficient algorithms propose train novel network use frame sequence discriminative criteria experiment ami distant speech recognition dsr task indicate train deeper lstms achieve better improvement sequence train highway lstms hlstms novel model obtain wer ami sdm dev eval set outperform previous work beat strong dnn dlstm baselines relative improvement respectively,110,6,1510.08983.txt
http://arxiv.org/abs/1510.08985,Prediction-Adaptation-Correction Recurrent Neural Networks for   Low-Resource Language Speech Recognition,"  In this paper, we investigate the use of prediction-adaptation-correction recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A PAC-RNN is comprised of a pair of neural networks in which a {\it correction} network uses auxiliary information given by a {\it prediction} network to help estimate the state probability. The information from the correction network is also used by the prediction network in a recurrent loop. Our model outperforms other state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks. Moreover, transfer learning from a language that is similar to the target language can help improve performance further. ",Computer Science - Computation and Language ; Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; Electrical Engineering and Systems Science - Audio and Speech Processing ; ,"Zhang, Yu ; Chuangsuwanich, Ekapol ; Glass, James ; Yu, Dong ; ","Prediction-Adaptation-Correction Recurrent Neural Networks for   Low-Resource Language Speech Recognition  In this paper, we investigate the use of prediction-adaptation-correction recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A PAC-RNN is comprised of a pair of neural networks in which a {\it correction} network uses auxiliary information given by a {\it prediction} network to help estimate the state probability. The information from the correction network is also used by the prediction network in a recurrent loop. Our model outperforms other state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks. Moreover, transfer learning from a language that is similar to the target language can help improve performance further. ",prediction adaptation correction recurrent neural network low resource language speech recognition paper investigate use prediction adaptation correction recurrent neural network pac rnns low resource speech recognition pac rnn comprise pair neural network correction network use auxiliary information give prediction network help estimate state probability information correction network also use prediction network recurrent loop model outperform state art neural network dnns lstms iarpa babel task moreover transfer learn language similar target language help improve performance,74,6,1510.08985.txt
http://arxiv.org/abs/1510.09102,Trace Refinement in Labelled Markov Decision Processes,"  Given two labelled Markov decision processes (MDPs), the trace-refinement problem asks whether for all strategies of the first MDP there exists a strategy of the second MDP such that the induced labelled Markov chains are trace-equivalent. We show that this problem is decidable in polynomial time if the second MDP is a Markov chain. The algorithm is based on new results on a particular notion of bisimulation between distributions over the states. However, we show that the general trace-refinement problem is undecidable, even if the first MDP is a Markov chain. Decidability of those problems was stated as open in 2008. We further study the decidability and complexity of the trace-refinement problem provided that the strategies are restricted to be memoryless. ",Computer Science - Logic in Computer Science ; ,"Fijalkow, Nathanaël ; Kiefer, Stefan ; Shirmohammadi, Mahsa ; ","Trace Refinement in Labelled Markov Decision Processes  Given two labelled Markov decision processes (MDPs), the trace-refinement problem asks whether for all strategies of the first MDP there exists a strategy of the second MDP such that the induced labelled Markov chains are trace-equivalent. We show that this problem is decidable in polynomial time if the second MDP is a Markov chain. The algorithm is based on new results on a particular notion of bisimulation between distributions over the states. However, we show that the general trace-refinement problem is undecidable, even if the first MDP is a Markov chain. Decidability of those problems was stated as open in 2008. We further study the decidability and complexity of the trace-refinement problem provided that the strategies are restricted to be memoryless. ",trace refinement label markov decision process give two label markov decision process mdps trace refinement problem ask whether strategies first mdp exist strategy second mdp induce label markov chain trace equivalent show problem decidable polynomial time second mdp markov chain algorithm base new result particular notion bisimulation distributions state however show general trace refinement problem undecidable even first mdp markov chain decidability problems state open study decidability complexity trace refinement problem provide strategies restrict memoryless,75,8,1510.09102.txt
http://arxiv.org/abs/1511.00158,Prediction of Dynamical time Series Using Kernel Based Regression and   Smooth Splines,"  Prediction of dynamical time series with additive noise using support vector machines or kernel based regression has been proved to be consistent for certain classes of discrete dynamical systems. Consistency implies that these methods are effective at computing the expected value of a point at a future time given the present coordinates. However, the present coordinates themselves are noisy, and therefore, these methods are not necessarily effective at removing noise. In this article, we consider denoising and prediction as separate problems for flows, as opposed to discrete time dynamical systems, and show that the use of smooth splines is more effective at removing noise. Combination of smooth splines and kernel based regression yields predictors that are more accurate on benchmarks typically by a factor of 2 or more. We prove that kernel based regression in combination with smooth splines converges to the exact predictor for time series extracted from any compact invariant set of any sufficiently smooth flow. As a consequence of convergence, one can find examples where the combination of kernel based regression with smooth splines is superior by even a factor of $100$. The predictors that we compute operate on delay coordinate data and not the full state vector, which is typically not observable. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; ,"Navarrete, Raymundo ; Viswanath, Divakar ; ","Prediction of Dynamical time Series Using Kernel Based Regression and   Smooth Splines  Prediction of dynamical time series with additive noise using support vector machines or kernel based regression has been proved to be consistent for certain classes of discrete dynamical systems. Consistency implies that these methods are effective at computing the expected value of a point at a future time given the present coordinates. However, the present coordinates themselves are noisy, and therefore, these methods are not necessarily effective at removing noise. In this article, we consider denoising and prediction as separate problems for flows, as opposed to discrete time dynamical systems, and show that the use of smooth splines is more effective at removing noise. Combination of smooth splines and kernel based regression yields predictors that are more accurate on benchmarks typically by a factor of 2 or more. We prove that kernel based regression in combination with smooth splines converges to the exact predictor for time series extracted from any compact invariant set of any sufficiently smooth flow. As a consequence of convergence, one can find examples where the combination of kernel based regression with smooth splines is superior by even a factor of $100$. The predictors that we compute operate on delay coordinate data and not the full state vector, which is typically not observable. ",prediction dynamical time series use kernel base regression smooth splines prediction dynamical time series additive noise use support vector machine kernel base regression prove consistent certain class discrete dynamical systems consistency imply methods effective compute expect value point future time give present coordinate however present coordinate noisy therefore methods necessarily effective remove noise article consider denoising prediction separate problems flow oppose discrete time dynamical systems show use smooth splines effective remove noise combination smooth splines kernel base regression yield predictors accurate benchmarks typically factor prove kernel base regression combination smooth splines converge exact predictor time series extract compact invariant set sufficiently smooth flow consequence convergence one find examples combination kernel base regression smooth splines superior even factor predictors compute operate delay coordinate data full state vector typically observable,128,11,1511.00158.txt
http://arxiv.org/abs/1511.00452,Stable Matching Mechanisms are Not Obviously Strategy-Proof,"  Many two-sided matching markets, from labor markets to school choice programs, use a clearinghouse based on the applicant-proposing deferred acceptance algorithm, which is well known to be strategy-proof for the applicants. Nonetheless, a growing amount of empirical evidence reveals that applicants misrepresent their preferences when this mechanism is used. This paper shows that no mechanism that implements a stable matching is ""obviously strategy-proof"" for any side of the market, a stronger incentive property than strategy-proofness that was introduced by Li (2017). A stable mechanism that is obviously strategy-proof for applicants is introduced for the case in which agents on the other side have acyclical preferences. ",Computer Science - Computer Science and Game Theory ; ,"Ashlagi, Itai ; Gonczarowski, Yannai A. ; ","Stable Matching Mechanisms are Not Obviously Strategy-Proof  Many two-sided matching markets, from labor markets to school choice programs, use a clearinghouse based on the applicant-proposing deferred acceptance algorithm, which is well known to be strategy-proof for the applicants. Nonetheless, a growing amount of empirical evidence reveals that applicants misrepresent their preferences when this mechanism is used. This paper shows that no mechanism that implements a stable matching is ""obviously strategy-proof"" for any side of the market, a stronger incentive property than strategy-proofness that was introduced by Li (2017). A stable mechanism that is obviously strategy-proof for applicants is introduced for the case in which agents on the other side have acyclical preferences. ",stable match mechanisms obviously strategy proof many two side match market labor market school choice program use clearinghouse base applicant propose defer acceptance algorithm well know strategy proof applicants nonetheless grow amount empirical evidence reveal applicants misrepresent preferences mechanism use paper show mechanism implement stable match obviously strategy proof side market stronger incentive property strategy proofness introduce li stable mechanism obviously strategy proof applicants introduce case agents side acyclical preferences,70,8,1511.00452.txt
http://arxiv.org/abs/1511.00493,"Uniqueness, Spatial Mixing, and Approximation for Ferromagnetic 2-Spin   Systems","  We give fully polynomial-time approximation schemes (FPTAS) for the partition function of ferromagnetic 2-spin systems in certain parameter regimes. The threshold we obtain is almost tight up to an integrality gap. Our technique is based on the correlation decay framework. The main technical contribution is a new potential function, with which we establish a new kind of spatial mixing. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; ,"Guo, Heng ; Lu, Pinyan ; ","Uniqueness, Spatial Mixing, and Approximation for Ferromagnetic 2-Spin   Systems  We give fully polynomial-time approximation schemes (FPTAS) for the partition function of ferromagnetic 2-spin systems in certain parameter regimes. The threshold we obtain is almost tight up to an integrality gap. Our technique is based on the correlation decay framework. The main technical contribution is a new potential function, with which we establish a new kind of spatial mixing. ",uniqueness spatial mix approximation ferromagnetic spin systems give fully polynomial time approximation scheme fptas partition function ferromagnetic spin systems certain parameter regimes threshold obtain almost tight integrality gap technique base correlation decay framework main technical contribution new potential function establish new kind spatial mix,44,11,1511.00493.txt
http://arxiv.org/abs/1511.00523,Minimizing Regret in Discounted-Sum Games,"  In this paper, we study the problem of minimizing regret in discounted-sum games played on weighted game graphs. We give algorithms for the general problem of computing the minimal regret of the controller (Eve) as well as several variants depending on which strategies the environment (Adam) is permitted to use. We also consider the problem of synthesizing regret-free strategies for Eve in each of these scenarios. ",Computer Science - Computer Science and Game Theory ; Computer Science - Formal Languages and Automata Theory ; Computer Science - Logic in Computer Science ; F.1.1 ; D.2.4 ; ,"Hunter, Paul ; Pérez, Guillermo A. ; Raskin, Jean-François ; ","Minimizing Regret in Discounted-Sum Games  In this paper, we study the problem of minimizing regret in discounted-sum games played on weighted game graphs. We give algorithms for the general problem of computing the minimal regret of the controller (Eve) as well as several variants depending on which strategies the environment (Adam) is permitted to use. We also consider the problem of synthesizing regret-free strategies for Eve in each of these scenarios. ",minimize regret discount sum game paper study problem minimize regret discount sum game play weight game graph give algorithms general problem compute minimal regret controller eve well several variants depend strategies environment adam permit use also consider problem synthesize regret free strategies eve scenarios,44,8,1511.00523.txt
http://arxiv.org/abs/1511.00546,An Impossibility Result for Reconstruction in a Degree-Corrected   Planted-Partition Model,"  We consider the Degree-Corrected Stochastic Block Model (DC-SBM): a random graph on $n$ nodes, having i.i.d. weights $(\phi_u)_{u=1}^n$ (possibly heavy-tailed), partitioned into $q \geq 2$ asymptotically equal-sized clusters. The model parameters are two constants $a,b > 0$ and the finite second moment of the weights $\Phi^{(2)}$. Vertices $u$ and $v$ are connected by an edge with probability $\frac{\phi_u \phi_v}{n}a$ when they are in the same class and with probability $\frac{\phi_u \phi_v}{n}b$ otherwise.   We prove that it is information-theoretically impossible to estimate the clusters in a way positively correlated with the true community structure when $(a-b)^2 \Phi^{(2)} \leq q(a+b)$.   As by-products of our proof we obtain $(1)$ a precise coupling result for local neighbourhoods in DC-SBM's, that we use in a follow up paper [Gulikers et al., 2017] to establish a law of large numbers for local-functionals and $(2)$ that long-range interactions are weak in (power-law) DC-SBM's. ",Mathematics - Probability ; Computer Science - Machine Learning ; Computer Science - Social and Information Networks ; Statistics - Machine Learning ; ,"Gulikers, Lennart ; Lelarge, Marc ; Massoulié, Laurent ; ","An Impossibility Result for Reconstruction in a Degree-Corrected   Planted-Partition Model  We consider the Degree-Corrected Stochastic Block Model (DC-SBM): a random graph on $n$ nodes, having i.i.d. weights $(\phi_u)_{u=1}^n$ (possibly heavy-tailed), partitioned into $q \geq 2$ asymptotically equal-sized clusters. The model parameters are two constants $a,b > 0$ and the finite second moment of the weights $\Phi^{(2)}$. Vertices $u$ and $v$ are connected by an edge with probability $\frac{\phi_u \phi_v}{n}a$ when they are in the same class and with probability $\frac{\phi_u \phi_v}{n}b$ otherwise.   We prove that it is information-theoretically impossible to estimate the clusters in a way positively correlated with the true community structure when $(a-b)^2 \Phi^{(2)} \leq q(a+b)$.   As by-products of our proof we obtain $(1)$ a precise coupling result for local neighbourhoods in DC-SBM's, that we use in a follow up paper [Gulikers et al., 2017] to establish a law of large numbers for local-functionals and $(2)$ that long-range interactions are weak in (power-law) DC-SBM's. ",impossibility result reconstruction degree correct plant partition model consider degree correct stochastic block model dc sbm random graph nod weight phi possibly heavy tail partition geq asymptotically equal size cluster model parameters two constants finite second moment weight phi vertices connect edge probability frac phi phi class probability frac phi phi otherwise prove information theoretically impossible estimate cluster way positively correlate true community structure phi leq products proof obtain precise couple result local neighbourhoods dc sbm use follow paper gulikers et al establish law large number local functionals long range interactions weak power law dc sbm,96,11,1511.00546.txt
http://arxiv.org/abs/1511.00725,Toward an Efficient Multi-class Classification in an Open Universe,"  Classification is a fundamental task in machine learning and data mining. Existing classification methods are designed to classify unknown instances within a set of previously known training classes. Such a classification takes the form of a prediction within a closed-set of classes. However, a more realistic scenario that fits real-world applications is to consider the possibility of encountering instances that do not belong to any of the training classes, $i.e.$, an open-set classification. In such situation, existing closed-set classifiers will assign a training label to these instances resulting in a misclassification. In this paper, we introduce Galaxy-X, a novel multi-class classification approach for open-set recognition problems. For each class of the training set, Galaxy-X creates a minimum bounding hyper-sphere that encompasses the distribution of the class by enclosing all of its instances. In such manner, our method is able to distinguish instances resembling previously seen classes from those that are of unknown ones. To adequately evaluate open-set classification, we introduce a novel evaluation procedure. Experimental results on benchmark datasets show the efficiency of our approach in classifying novel instances from known as well as unknown classes. ",Computer Science - Machine Learning ; Computer Science - Artificial Intelligence ; Computer Science - Databases ; Computer Science - Information Retrieval ; ,"Dhifli, Wajdi ; Diallo, Abdoulaye Baniré ; ","Toward an Efficient Multi-class Classification in an Open Universe  Classification is a fundamental task in machine learning and data mining. Existing classification methods are designed to classify unknown instances within a set of previously known training classes. Such a classification takes the form of a prediction within a closed-set of classes. However, a more realistic scenario that fits real-world applications is to consider the possibility of encountering instances that do not belong to any of the training classes, $i.e.$, an open-set classification. In such situation, existing closed-set classifiers will assign a training label to these instances resulting in a misclassification. In this paper, we introduce Galaxy-X, a novel multi-class classification approach for open-set recognition problems. For each class of the training set, Galaxy-X creates a minimum bounding hyper-sphere that encompasses the distribution of the class by enclosing all of its instances. In such manner, our method is able to distinguish instances resembling previously seen classes from those that are of unknown ones. To adequately evaluate open-set classification, we introduce a novel evaluation procedure. Experimental results on benchmark datasets show the efficiency of our approach in classifying novel instances from known as well as unknown classes. ",toward efficient multi class classification open universe classification fundamental task machine learn data mine exist classification methods design classify unknown instance within set previously know train class classification take form prediction within close set class however realistic scenario fit real world applications consider possibility encounter instance belong train class open set classification situation exist close set classifiers assign train label instance result misclassification paper introduce galaxy novel multi class classification approach open set recognition problems class train set galaxy create minimum bound hyper sphere encompass distribution class enclose instance manner method able distinguish instance resemble previously see class unknown ones adequately evaluate open set classification introduce novel evaluation procedure experimental result benchmark datasets show efficiency approach classify novel instance know well unknown class,123,10,1511.00725.txt
http://arxiv.org/abs/1511.00736,ProtNN: Fast and Accurate Nearest Neighbor Protein Function Prediction   based on Graph Embedding in Structural and Topological Space,"  Studying the function of proteins is important for understanding the molecular mechanisms of life. The number of publicly available protein structures has increasingly become extremely large. Still, the determination of the function of a protein structure remains a difficult, costly, and time consuming task. The difficulties are often due to the essential role of spatial and topological structures in the determination of protein functions in living cells. In this paper, we propose ProtNN, a novel approach for protein function prediction. Given an unannotated protein structure and a set of annotated proteins, ProtNN finds the nearest neighbor annotated structures based on protein-graph pairwise similarities. Given a query protein, ProtNN finds the nearest neighbor reference proteins based on a graph representation model and a pairwise similarity between vector embedding of both query and reference protein-graphs in structural and topological spaces. ProtNN assigns to the query protein the function with the highest number of votes across the set of k nearest neighbor reference proteins, where k is a user-defined parameter. Experimental evaluation demonstrates that ProtNN is able to accurately classify several datasets in an extremely fast runtime compared to state-of-the-art approaches. We further show that ProtNN is able to scale up to a whole PDB dataset in a single-process mode with no parallelization, with a gain of thousands order of magnitude of runtime compared to state-of-the-art approaches. ",Computer Science - Machine Learning ; Computer Science - Social and Information Networks ; ,"Dhifli, Wajdi ; Diallo, Abdoulaye Baniré ; ","ProtNN: Fast and Accurate Nearest Neighbor Protein Function Prediction   based on Graph Embedding in Structural and Topological Space  Studying the function of proteins is important for understanding the molecular mechanisms of life. The number of publicly available protein structures has increasingly become extremely large. Still, the determination of the function of a protein structure remains a difficult, costly, and time consuming task. The difficulties are often due to the essential role of spatial and topological structures in the determination of protein functions in living cells. In this paper, we propose ProtNN, a novel approach for protein function prediction. Given an unannotated protein structure and a set of annotated proteins, ProtNN finds the nearest neighbor annotated structures based on protein-graph pairwise similarities. Given a query protein, ProtNN finds the nearest neighbor reference proteins based on a graph representation model and a pairwise similarity between vector embedding of both query and reference protein-graphs in structural and topological spaces. ProtNN assigns to the query protein the function with the highest number of votes across the set of k nearest neighbor reference proteins, where k is a user-defined parameter. Experimental evaluation demonstrates that ProtNN is able to accurately classify several datasets in an extremely fast runtime compared to state-of-the-art approaches. We further show that ProtNN is able to scale up to a whole PDB dataset in a single-process mode with no parallelization, with a gain of thousands order of magnitude of runtime compared to state-of-the-art approaches. ",protnn fast accurate nearest neighbor protein function prediction base graph embed structural topological space study function proteins important understand molecular mechanisms life number publicly available protein structure increasingly become extremely large still determination function protein structure remain difficult costly time consume task difficulties often due essential role spatial topological structure determination protein function live cells paper propose protnn novel approach protein function prediction give unannotated protein structure set annotate proteins protnn find nearest neighbor annotate structure base protein graph pairwise similarities give query protein protnn find nearest neighbor reference proteins base graph representation model pairwise similarity vector embed query reference protein graph structural topological space protnn assign query protein function highest number vote across set nearest neighbor reference proteins user define parameter experimental evaluation demonstrate protnn able accurately classify several datasets extremely fast runtime compare state art approach show protnn able scale whole pdb dataset single process mode parallelization gain thousands order magnitude runtime compare state art approach,158,11,1511.00736.txt
http://arxiv.org/abs/1511.00867,Dynamic Gossip,"  A gossip protocol is a procedure for spreading secrets among a group of agents, using a connection graph. The goal is for all agents to get to know all secrets, in which case we call the execution of the protocol successful. We consider distributed and dynamic gossip protocols. In distributed gossip the agents themselves instead of a global scheduler determine whom to call. In dynamic gossip not only secrets are exchanged but also telephone numbers (agent identities). This results in increased graph connectivity. We define six such distributed dynamic gossip protocols, and we characterize them in terms of the topology of the graphs on which they are successful, wherein we distinguish strong success (the protocol always terminates, possibly assuming fair scheduling) from weak success (the protocol sometimes terminates). For five of these protocols strong (fair) and weak success are characterized by weakly connected graphs. This result is surprising because the protocols are fairly different. In the sixth protocol an agent may only call another agent if it does not know the other agent's secret. Strong success for this protocol is characterized by graphs for which the set of non-terminal nodes is strongly connected. Weak success for this protocol is characterized by weakly connected graphs satisfying further topological constraints that we define in the paper. One direction of this characterization is surprisingly harder to prove than the other results in this contribution. ",Computer Science - Discrete Mathematics ; ,"van Ditmarsch, Hans ; van Eijck, Jan ; Pardo, Pere ; Ramezanian, Rahim ; Schwarzentruber, François ; ","Dynamic Gossip  A gossip protocol is a procedure for spreading secrets among a group of agents, using a connection graph. The goal is for all agents to get to know all secrets, in which case we call the execution of the protocol successful. We consider distributed and dynamic gossip protocols. In distributed gossip the agents themselves instead of a global scheduler determine whom to call. In dynamic gossip not only secrets are exchanged but also telephone numbers (agent identities). This results in increased graph connectivity. We define six such distributed dynamic gossip protocols, and we characterize them in terms of the topology of the graphs on which they are successful, wherein we distinguish strong success (the protocol always terminates, possibly assuming fair scheduling) from weak success (the protocol sometimes terminates). For five of these protocols strong (fair) and weak success are characterized by weakly connected graphs. This result is surprising because the protocols are fairly different. In the sixth protocol an agent may only call another agent if it does not know the other agent's secret. Strong success for this protocol is characterized by graphs for which the set of non-terminal nodes is strongly connected. Weak success for this protocol is characterized by weakly connected graphs satisfying further topological constraints that we define in the paper. One direction of this characterization is surprisingly harder to prove than the other results in this contribution. ",dynamic gossip gossip protocol procedure spread secrets among group agents use connection graph goal agents get know secrets case call execution protocol successful consider distribute dynamic gossip protocols distribute gossip agents instead global scheduler determine call dynamic gossip secrets exchange also telephone number agent identities result increase graph connectivity define six distribute dynamic gossip protocols characterize term topology graph successful wherein distinguish strong success protocol always terminate possibly assume fair schedule weak success protocol sometimes terminate five protocols strong fair weak success characterize weakly connect graph result surprise protocols fairly different sixth protocol agent may call another agent know agent secret strong success protocol characterize graph set non terminal nod strongly connect weak success protocol characterize weakly connect graph satisfy topological constraints define paper one direction characterization surprisingly harder prove result contribution,132,3,1511.00867.txt
http://arxiv.org/abs/1511.00876,Beating the Harmonic lower bound for online bin packing,"  In the online bin packing problem, items of sizes in (0,1] arrive online to be packed into bins of size 1. The goal is to minimize the number of used bins. In this paper, we present an online bin packing algorithm with asymptotic competitive ratio of 1.5813. This is the first improvement in fifteen years and reduces the gap to the lower bound by 15%. Within the well-known SuperHarmonic framework, no competitive ratio below 1.58333 can be achieved.   We make two crucial changes to that framework. First, some of our algorithm's decisions depend on exact sizes of items, instead of only their types. In particular, for each item with size in (1/3,1/2], we use its exact size to determine if it can be packed together with an item of size greater than 1/2. Second, we add constraints to the linear programs considered by Seiden, in order to better lower bound the optimal solution. These extra constraints are based on marks that we give to items based on how they are packed by our algorithm. We show that for each input, a single weighting function can be constructed to upper bound the competitive ratio on it.   We use this idea to simplify the analysis of SuperHarmonic, and show that the algorithm Harmonic++ is in fact 1.58880-competitive (Seiden proved 1.58889), and that 1.5884 can be achieved within the SuperHarmonic framework. Finally, we give a lower bound of 1.5762 for our new framework. ",Computer Science - Data Structures and Algorithms ; ,"Heydrich, Sandy ; van Stee, Rob ; ","Beating the Harmonic lower bound for online bin packing  In the online bin packing problem, items of sizes in (0,1] arrive online to be packed into bins of size 1. The goal is to minimize the number of used bins. In this paper, we present an online bin packing algorithm with asymptotic competitive ratio of 1.5813. This is the first improvement in fifteen years and reduces the gap to the lower bound by 15%. Within the well-known SuperHarmonic framework, no competitive ratio below 1.58333 can be achieved.   We make two crucial changes to that framework. First, some of our algorithm's decisions depend on exact sizes of items, instead of only their types. In particular, for each item with size in (1/3,1/2], we use its exact size to determine if it can be packed together with an item of size greater than 1/2. Second, we add constraints to the linear programs considered by Seiden, in order to better lower bound the optimal solution. These extra constraints are based on marks that we give to items based on how they are packed by our algorithm. We show that for each input, a single weighting function can be constructed to upper bound the competitive ratio on it.   We use this idea to simplify the analysis of SuperHarmonic, and show that the algorithm Harmonic++ is in fact 1.58880-competitive (Seiden proved 1.58889), and that 1.5884 can be achieved within the SuperHarmonic framework. Finally, we give a lower bound of 1.5762 for our new framework. ",beat harmonic lower bind online bin pack online bin pack problem items size arrive online pack bin size goal minimize number use bin paper present online bin pack algorithm asymptotic competitive ratio first improvement fifteen years reduce gap lower bind within well know superharmonic framework competitive ratio achieve make two crucial change framework first algorithm decisions depend exact size items instead type particular item size use exact size determine pack together item size greater second add constraints linear program consider seiden order better lower bind optimal solution extra constraints base mark give items base pack algorithm show input single weight function construct upper bind competitive ratio use idea simplify analysis superharmonic show algorithm harmonic fact competitive seiden prove achieve within superharmonic framework finally give lower bind new framework,128,8,1511.00876.txt
http://arxiv.org/abs/1511.00925,Do Prices Coordinate Markets?,"  Walrasian equilibrium prices can be said to coordinate markets: They support a welfare optimal allocation in which each buyer is buying bundle of goods that is individually most preferred. However, this clean story has two caveats. First, the prices alone are not sufficient to coordinate the market, and buyers may need to select among their most preferred bundles in a coordinated way to find a feasible allocation. Second, we don't in practice expect to encounter exact equilibrium prices tailored to the market, but instead only approximate prices, somehow encoding ""distributional"" information about the market. How well do prices work to coordinate markets when tie-breaking is not coordinated, and they encode only distributional information?   We answer this question. First, we provide a genericity condition such that for buyers with Matroid Based Valuations, overdemand with respect to equilibrium prices is at most 1, independent of the supply of goods, even when tie-breaking is done in an uncoordinated fashion. Second, we provide learning-theoretic results that show that such prices are robust to changing the buyers in the market, so long as all buyers are sampled from the same (unknown) distribution. ",Computer Science - Computer Science and Game Theory ; Computer Science - Machine Learning ; ,"Hsu, Justin ; Morgenstern, Jamie ; Rogers, Ryan ; Roth, Aaron ; Vohra, Rakesh ; ","Do Prices Coordinate Markets?  Walrasian equilibrium prices can be said to coordinate markets: They support a welfare optimal allocation in which each buyer is buying bundle of goods that is individually most preferred. However, this clean story has two caveats. First, the prices alone are not sufficient to coordinate the market, and buyers may need to select among their most preferred bundles in a coordinated way to find a feasible allocation. Second, we don't in practice expect to encounter exact equilibrium prices tailored to the market, but instead only approximate prices, somehow encoding ""distributional"" information about the market. How well do prices work to coordinate markets when tie-breaking is not coordinated, and they encode only distributional information?   We answer this question. First, we provide a genericity condition such that for buyers with Matroid Based Valuations, overdemand with respect to equilibrium prices is at most 1, independent of the supply of goods, even when tie-breaking is done in an uncoordinated fashion. Second, we provide learning-theoretic results that show that such prices are robust to changing the buyers in the market, so long as all buyers are sampled from the same (unknown) distribution. ",price coordinate market walrasian equilibrium price say coordinate market support welfare optimal allocation buyer buy bundle goods individually prefer however clean story two caveats first price alone sufficient coordinate market buyers may need select among prefer bundle coordinate way find feasible allocation second practice expect encounter exact equilibrium price tailor market instead approximate price somehow encode distributional information market well price work coordinate market tie break coordinate encode distributional information answer question first provide genericity condition buyers matroid base valuations overdemand respect equilibrium price independent supply goods even tie break do uncoordinated fashion second provide learn theoretic result show price robust change buyers market long buyers sample unknown distribution,109,0,1511.00925.txt
http://arxiv.org/abs/1511.01238,The wisdom of networks: A general adaptation and learning mechanism of   complex systems: The network core triggers fast responses to known stimuli;   innovations require the slow network periphery and are encoded by   core-remodeling,"  I hypothesize that re-occurring prior experience of complex systems mobilizes a fast response, whose attractor is encoded by their strongly connected network core. In contrast, responses to novel stimuli are often slow and require the weakly connected network periphery. Upon repeated stimulus, peripheral network nodes remodel the network core that encodes the attractor of the new response. This ""core-periphery learning"" theory reviews and generalizes the heretofore fragmented knowledge on attractor formation by neural networks, periphery-driven innovation and a number of recent reports on the adaptation of protein, neuronal and social networks. The coreperiphery learning theory may increase our understanding of signaling, memory formation, information encoding and decision-making processes. Moreover, the power of network periphery-related 'wisdom of crowds' inventing creative, novel responses indicates that deliberative democracy is a slow yet efficient learning strategy developed as the success of a billion-year evolution. ",Quantitative Biology - Molecular Networks ; Condensed Matter - Disordered Systems and Neural Networks ; Computer Science - Social and Information Networks ; Nonlinear Sciences - Adaptation and Self-Organizing Systems ; Physics - Biological Physics ; ,"Csermely, Peter ; ","The wisdom of networks: A general adaptation and learning mechanism of   complex systems: The network core triggers fast responses to known stimuli;   innovations require the slow network periphery and are encoded by   core-remodeling  I hypothesize that re-occurring prior experience of complex systems mobilizes a fast response, whose attractor is encoded by their strongly connected network core. In contrast, responses to novel stimuli are often slow and require the weakly connected network periphery. Upon repeated stimulus, peripheral network nodes remodel the network core that encodes the attractor of the new response. This ""core-periphery learning"" theory reviews and generalizes the heretofore fragmented knowledge on attractor formation by neural networks, periphery-driven innovation and a number of recent reports on the adaptation of protein, neuronal and social networks. The coreperiphery learning theory may increase our understanding of signaling, memory formation, information encoding and decision-making processes. Moreover, the power of network periphery-related 'wisdom of crowds' inventing creative, novel responses indicates that deliberative democracy is a slow yet efficient learning strategy developed as the success of a billion-year evolution. ",wisdom network general adaptation learn mechanism complex systems network core trigger fast responses know stimuli innovations require slow network periphery encode core remodel hypothesize occur prior experience complex systems mobilize fast response whose attractor encode strongly connect network core contrast responses novel stimuli often slow require weakly connect network periphery upon repeat stimulus peripheral network nod remodel network core encode attractor new response core periphery learn theory review generalize heretofore fragment knowledge attractor formation neural network periphery drive innovation number recent report adaptation protein neuronal social network coreperiphery learn theory may increase understand signal memory formation information encode decision make process moreover power network periphery relate wisdom crowd invent creative novel responses indicate deliberative democracy slow yet efficient learn strategy develop success billion year evolution,125,6,1511.01238.txt
http://arxiv.org/abs/1511.01249,Privacy by Design: On the Formal Design and Conformance Check of   Personal Data Protection Policies and Architectures,"  The new General Data Protection Regulation (GDPR) will take effect in May 2018, and hence, designing compliant data protection policies and system architectures became crucial for organizations to avoid penalties. Unfortunately, the regulations given in a textual format can be easily misinterpreted by the policy and system designers, which also making the conformance check error-prone for auditors. In this paper, we apply formal approach to facilitate systematic design of policies and architectures in an unambiguous way, and provide a framework for mathematically sound conformance checks against the current data protection regulations. We propose a (semi-)formal approach for specifying and reasoning about data protection policies and architectures as well as defining conformance relations between architectures and policies. The usability of our proposed approach is demonstrated on a smart metering service case study. ",Computer Science - Cryptography and Security ; ,"Ta, Vinh Thong ; ","Privacy by Design: On the Formal Design and Conformance Check of   Personal Data Protection Policies and Architectures  The new General Data Protection Regulation (GDPR) will take effect in May 2018, and hence, designing compliant data protection policies and system architectures became crucial for organizations to avoid penalties. Unfortunately, the regulations given in a textual format can be easily misinterpreted by the policy and system designers, which also making the conformance check error-prone for auditors. In this paper, we apply formal approach to facilitate systematic design of policies and architectures in an unambiguous way, and provide a framework for mathematically sound conformance checks against the current data protection regulations. We propose a (semi-)formal approach for specifying and reasoning about data protection policies and architectures as well as defining conformance relations between architectures and policies. The usability of our proposed approach is demonstrated on a smart metering service case study. ",privacy design formal design conformance check personal data protection policies architectures new general data protection regulation gdpr take effect may hence design compliant data protection policies system architectures become crucial organizations avoid penalties unfortunately regulations give textual format easily misinterpret policy system designers also make conformance check error prone auditors paper apply formal approach facilitate systematic design policies architectures unambiguous way provide framework mathematically sound conformance check current data protection regulations propose semi formal approach specify reason data protection policies architectures well define conformance relations architectures policies usability propose approach demonstrate smart meter service case study,96,0,1511.01249.txt
http://arxiv.org/abs/1511.01258,"Learn on Source, Refine on Target:A Model Transfer Learning Framework   with Random Forests","  We propose novel model transfer-learning methods that refine a decision forest model M learned within a ""source"" domain using a training set sampled from a ""target"" domain, assumed to be a variation of the source. We present two random forest transfer algorithms. The first algorithm searches greedily for locally optimal modifications of each tree structure by trying to locally expand or reduce the tree around individual nodes. The second algorithm does not modify structure, but only the parameter (thresholds) associated with decision nodes. We also propose to combine both methods by considering an ensemble that contains the union of the two forests. The proposed methods exhibit impressive experimental results over a range of problems. ",Computer Science - Machine Learning ; ,"Segev, Noam ; Harel, Maayan ; Mannor, Shie ; Crammer, Koby ; El-Yaniv, Ran ; ","Learn on Source, Refine on Target:A Model Transfer Learning Framework   with Random Forests  We propose novel model transfer-learning methods that refine a decision forest model M learned within a ""source"" domain using a training set sampled from a ""target"" domain, assumed to be a variation of the source. We present two random forest transfer algorithms. The first algorithm searches greedily for locally optimal modifications of each tree structure by trying to locally expand or reduce the tree around individual nodes. The second algorithm does not modify structure, but only the parameter (thresholds) associated with decision nodes. We also propose to combine both methods by considering an ensemble that contains the union of the two forests. The proposed methods exhibit impressive experimental results over a range of problems. ",learn source refine target model transfer learn framework random forest propose novel model transfer learn methods refine decision forest model learn within source domain use train set sample target domain assume variation source present two random forest transfer algorithms first algorithm search greedily locally optimal modifications tree structure try locally expand reduce tree around individual nod second algorithm modify structure parameter thresholds associate decision nod also propose combine methods consider ensemble contain union two forest propose methods exhibit impressive experimental result range problems,83,12,1511.01258.txt
http://arxiv.org/abs/1511.01807,The height of piecewise-testable languages and the complexity of the   logic of subwords,"  The height of a piecewise-testable language $L$ is the maximum length of the words needed to define $L$ by excluding and requiring given subwords. The height of $L$ is an important descriptive complexity measure that has not yet been investigated in a systematic way. This paper develops a series of new techniques for bounding the height of finite languages and of languages obtained by taking closures by subwords, superwords and related operations.   As an application of these results, we show that $\text{FO}^2(A^*,\sqsubseteq)$, the two-variable fragment of the first-order logic of sequences with the subword ordering, can only express piecewise-testable properties and has elementary complexity. ",Computer Science - Logic in Computer Science ; Computer Science - Formal Languages and Automata Theory ; F.4.1 ; F.4.3 ; F.3.1 ; ,"Karandikar, Prateek ; Schnoebelen, Philippe ; ","The height of piecewise-testable languages and the complexity of the   logic of subwords  The height of a piecewise-testable language $L$ is the maximum length of the words needed to define $L$ by excluding and requiring given subwords. The height of $L$ is an important descriptive complexity measure that has not yet been investigated in a systematic way. This paper develops a series of new techniques for bounding the height of finite languages and of languages obtained by taking closures by subwords, superwords and related operations.   As an application of these results, we show that $\text{FO}^2(A^*,\sqsubseteq)$, the two-variable fragment of the first-order logic of sequences with the subword ordering, can only express piecewise-testable properties and has elementary complexity. ",height piecewise testable languages complexity logic subwords height piecewise testable language maximum length word need define exclude require give subwords height important descriptive complexity measure yet investigate systematic way paper develop series new techniques bound height finite languages languages obtain take closure subwords superwords relate operations application result show text fo sqsubseteq two variable fragment first order logic sequence subword order express piecewise testable properties elementary complexity,67,14,1511.01807.txt
http://arxiv.org/abs/1511.02166,Evaluation of the Intel Xeon Phi 7120 and NVIDIA K80 as accelerators for   two-dimensional panel codes,"  To optimize the geometry of airfoils for a specific application is an important engineering problem. In this context genetic algorithms have enjoyed some success as they are able to explore the search space without getting stuck in local optima. However, these algorithms require the computation of aerodynamic properties for a significant number of airfoil geometries. Consequently, for low-speed aerodynamics, panel methods are most often used as the inner solver.   In this paper we evaluate the performance of such an optimization algorithm on modern accelerators (more specifically, the Intel Xeon Phi 7120 and the NVIDIA K80). For that purpose, we have implemented an optimized version of the algorithm on the CPU and Xeon Phi (based on OpenMP, vectorization, and the Intel MKL library) and on the GPU (based on CUDA and the MAGMA library). We present timing results for all codes and discuss the similarities and differences between the three implementations. Overall, we observe a speedup of approximately $2.5$ for adding an Intel Xeon Phi 7120 to a dual socket workstation and a speedup between $3.4$ and $3.8$ for adding a NVIDIA K80 to a dual socket workstation. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Mathematical Software ; Physics - Computational Physics ; ","Einkemmer, Lukas ; ","Evaluation of the Intel Xeon Phi 7120 and NVIDIA K80 as accelerators for   two-dimensional panel codes  To optimize the geometry of airfoils for a specific application is an important engineering problem. In this context genetic algorithms have enjoyed some success as they are able to explore the search space without getting stuck in local optima. However, these algorithms require the computation of aerodynamic properties for a significant number of airfoil geometries. Consequently, for low-speed aerodynamics, panel methods are most often used as the inner solver.   In this paper we evaluate the performance of such an optimization algorithm on modern accelerators (more specifically, the Intel Xeon Phi 7120 and the NVIDIA K80). For that purpose, we have implemented an optimized version of the algorithm on the CPU and Xeon Phi (based on OpenMP, vectorization, and the Intel MKL library) and on the GPU (based on CUDA and the MAGMA library). We present timing results for all codes and discuss the similarities and differences between the three implementations. Overall, we observe a speedup of approximately $2.5$ for adding an Intel Xeon Phi 7120 to a dual socket workstation and a speedup between $3.4$ and $3.8$ for adding a NVIDIA K80 to a dual socket workstation. ",evaluation intel xeon phi nvidia accelerators two dimensional panel cod optimize geometry airfoils specific application important engineer problem context genetic algorithms enjoy success able explore search space without get stick local optima however algorithms require computation aerodynamic properties significant number airfoil geometries consequently low speed aerodynamics panel methods often use inner solver paper evaluate performance optimization algorithm modern accelerators specifically intel xeon phi nvidia purpose implement optimize version algorithm cpu xeon phi base openmp vectorization intel mkl library gpu base cuda magma library present time result cod discuss similarities differences three implementations overall observe speedup approximately add intel xeon phi dual socket workstation speedup add nvidia dual socket workstation,109,4,1511.02166.txt
http://arxiv.org/abs/1511.02475,On Sylvester Colorings of Cubic Graphs,"  If $G$ and $H$ are two cubic graphs, then an $H$-coloring of $G$ is a proper edge-coloring $f$ with edges of $H$, such that for each vertex $x$ of $G$, there is a vertex $y$ of $H$ with $f(\partial_G(x))=\partial_H(y)$. If $G$ admits an $H$-coloring, then we will write $H\prec G$. The Petersen coloring conjecture of Jaeger states that for any bridgeless cubic graph $G$, one has: $P\prec G$. The second author has recently introduced the Sylvester coloring conjecture, which states that for any cubic graph $G$ one has: $S\prec G$. Here $S$ is the Sylvester graph on $10$ vertices. In this paper, we prove the analogue of Sylvester coloring conjecture for cubic pseudo-graphs. Moreover, we show that if $G$ is any connected simple cubic graph $G$ with $G\prec P$, then $G = P$. This implies that the Petersen graph does not admit an $S_{16}$-coloring, where $S_{16}$ is the smallest connected simple cubic graph without a perfect matching. $S_{16}$ has $16$ vertices. %We conjecture that there are infinitely many connected cubic simple graphs which do not admit an %$S_{16}$-coloring. Finally, we obtain $2$ results towards the Sylvester coloring conjecture. The first result states that any cubic graph $G$ has a coloring with edges of Sylvester graph $S$ such that at least $\frac45$ of vertices of $G$ meet the conditions of Sylvester coloring conjecture. The second result states that any claw-free cubic graph graph admits an $S$-coloring. This results is an application of our result on cubic pseudo-graphs. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Hakobyan, Anush ; Mkrtchyan, Vahan ; ","On Sylvester Colorings of Cubic Graphs  If $G$ and $H$ are two cubic graphs, then an $H$-coloring of $G$ is a proper edge-coloring $f$ with edges of $H$, such that for each vertex $x$ of $G$, there is a vertex $y$ of $H$ with $f(\partial_G(x))=\partial_H(y)$. If $G$ admits an $H$-coloring, then we will write $H\prec G$. The Petersen coloring conjecture of Jaeger states that for any bridgeless cubic graph $G$, one has: $P\prec G$. The second author has recently introduced the Sylvester coloring conjecture, which states that for any cubic graph $G$ one has: $S\prec G$. Here $S$ is the Sylvester graph on $10$ vertices. In this paper, we prove the analogue of Sylvester coloring conjecture for cubic pseudo-graphs. Moreover, we show that if $G$ is any connected simple cubic graph $G$ with $G\prec P$, then $G = P$. This implies that the Petersen graph does not admit an $S_{16}$-coloring, where $S_{16}$ is the smallest connected simple cubic graph without a perfect matching. $S_{16}$ has $16$ vertices. %We conjecture that there are infinitely many connected cubic simple graphs which do not admit an %$S_{16}$-coloring. Finally, we obtain $2$ results towards the Sylvester coloring conjecture. The first result states that any cubic graph $G$ has a coloring with edges of Sylvester graph $S$ such that at least $\frac45$ of vertices of $G$ meet the conditions of Sylvester coloring conjecture. The second result states that any claw-free cubic graph graph admits an $S$-coloring. This results is an application of our result on cubic pseudo-graphs. ",sylvester color cubic graph two cubic graph color proper edge color edge vertex vertex partial partial admit color write prec petersen color conjecture jaeger state bridgeless cubic graph one prec second author recently introduce sylvester color conjecture state cubic graph one prec sylvester graph vertices paper prove analogue sylvester color conjecture cubic pseudo graph moreover show connect simple cubic graph prec imply petersen graph admit color smallest connect simple cubic graph without perfect match vertices conjecture infinitely many connect cubic simple graph admit color finally obtain result towards sylvester color conjecture first result state cubic graph color edge sylvester graph least frac vertices meet condition sylvester color conjecture second result state claw free cubic graph graph admit color result application result cubic pseudo graph,124,13,1511.02475.txt
http://arxiv.org/abs/1511.02476,Statistical physics of inference: Thresholds and algorithms,"  Many questions of fundamental interest in todays science can be formulated as inference problems: Some partial, or noisy, observations are performed over a set of variables and the goal is to recover, or infer, the values of the variables based on the indirect information contained in the measurements. For such problems, the central scientific questions are: Under what conditions is the information contained in the measurements sufficient for a satisfactory inference to be possible? What are the most efficient algorithms for this task? A growing body of work has shown that often we can understand and locate these fundamental barriers by thinking of them as phase transitions in the sense of statistical physics. Moreover, it turned out that we can use the gained physical insight to develop new promising algorithms. Connection between inference and statistical physics is currently witnessing an impressive renaissance and we review here the current state-of-the-art, with a pedagogical focus on the Ising model which formulated as an inference problem we call the planted spin glass. In terms of applications we review two classes of problems: (i) inference of clusters on graphs and networks, with community detection as a special case and (ii) estimating a signal from its noisy linear measurements, with compressed sensing as a case of sparse estimation. Our goal is to provide a pedagogical review for researchers in physics and other fields interested in this fascinating topic. ",Condensed Matter - Statistical Mechanics ; Computer Science - Data Structures and Algorithms ; Statistics - Machine Learning ; ,"Zdeborová, Lenka ; Krzakala, Florent ; ","Statistical physics of inference: Thresholds and algorithms  Many questions of fundamental interest in todays science can be formulated as inference problems: Some partial, or noisy, observations are performed over a set of variables and the goal is to recover, or infer, the values of the variables based on the indirect information contained in the measurements. For such problems, the central scientific questions are: Under what conditions is the information contained in the measurements sufficient for a satisfactory inference to be possible? What are the most efficient algorithms for this task? A growing body of work has shown that often we can understand and locate these fundamental barriers by thinking of them as phase transitions in the sense of statistical physics. Moreover, it turned out that we can use the gained physical insight to develop new promising algorithms. Connection between inference and statistical physics is currently witnessing an impressive renaissance and we review here the current state-of-the-art, with a pedagogical focus on the Ising model which formulated as an inference problem we call the planted spin glass. In terms of applications we review two classes of problems: (i) inference of clusters on graphs and networks, with community detection as a special case and (ii) estimating a signal from its noisy linear measurements, with compressed sensing as a case of sparse estimation. Our goal is to provide a pedagogical review for researchers in physics and other fields interested in this fascinating topic. ",statistical physics inference thresholds algorithms many question fundamental interest todays science formulate inference problems partial noisy observations perform set variables goal recover infer value variables base indirect information contain measurements problems central scientific question condition information contain measurements sufficient satisfactory inference possible efficient algorithms task grow body work show often understand locate fundamental barriers think phase transition sense statistical physics moreover turn use gain physical insight develop new promise algorithms connection inference statistical physics currently witness impressive renaissance review current state art pedagogical focus ising model formulate inference problem call plant spin glass term applications review two class problems inference cluster graph network community detection special case ii estimate signal noisy linear measurements compress sense case sparse estimation goal provide pedagogical review researchers physics field interest fascinate topic,128,9,1511.02476.txt
http://arxiv.org/abs/1511.02599,Waste Makes Haste: Bounded Time Protocols for Envy-Free Cake Cutting   with Free Disposal,"  We consider the classic problem of envy-free division of a heterogeneous good (""cake"") among several agents. It is known that, when the allotted pieces must be connected, the problem cannot be solved by a finite algorithm for 3 or more agents. The impossibility result, however, assumes that the entire cake must be allocated. In this paper we replace the entire-allocation requirement with a weaker \emph{partial-proportionality} requirement: the piece given to each agent must be worth for it at least a certain positive fraction of the entire cake value. We prove that this version of the problem is solvable in bounded time even when the pieces must be connected. We present simple, bounded-time envy-free cake-cutting algorithms for: (1) giving each of $n$ agents a connected piece with a positive value; (2) giving each of 3 agents a connected piece worth at least 1/3; (3) giving each of 4 agents a connected piece worth at least 1/7; (4) giving each of 4 agents a disconnected piece worth at least 1/4; (5) giving each of $n$ agents a disconnected piece worth at least $(1-\epsilon)/n$ for any positive $\epsilon$. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computer Science and Game Theory ; F.2.2 ; ,"Segal-Halevi, Erel ; Hassidim, Avinatan ; Aumann, Yonatan ; ","Waste Makes Haste: Bounded Time Protocols for Envy-Free Cake Cutting   with Free Disposal  We consider the classic problem of envy-free division of a heterogeneous good (""cake"") among several agents. It is known that, when the allotted pieces must be connected, the problem cannot be solved by a finite algorithm for 3 or more agents. The impossibility result, however, assumes that the entire cake must be allocated. In this paper we replace the entire-allocation requirement with a weaker \emph{partial-proportionality} requirement: the piece given to each agent must be worth for it at least a certain positive fraction of the entire cake value. We prove that this version of the problem is solvable in bounded time even when the pieces must be connected. We present simple, bounded-time envy-free cake-cutting algorithms for: (1) giving each of $n$ agents a connected piece with a positive value; (2) giving each of 3 agents a connected piece worth at least 1/3; (3) giving each of 4 agents a connected piece worth at least 1/7; (4) giving each of 4 agents a disconnected piece worth at least 1/4; (5) giving each of $n$ agents a disconnected piece worth at least $(1-\epsilon)/n$ for any positive $\epsilon$. ",waste make haste bound time protocols envy free cake cut free disposal consider classic problem envy free division heterogeneous good cake among several agents know allot piece must connect problem cannot solve finite algorithm agents impossibility result however assume entire cake must allocate paper replace entire allocation requirement weaker emph partial proportionality requirement piece give agent must worth least certain positive fraction entire cake value prove version problem solvable bound time even piece must connect present simple bound time envy free cake cut algorithms give agents connect piece positive value give agents connect piece worth least give agents connect piece worth least give agents disconnect piece worth least give agents disconnect piece worth least epsilon positive epsilon,117,7,1511.02599.txt
http://arxiv.org/abs/1511.02683,A Light CNN for Deep Face Representation with Noisy Labels,"  The volume of convolutional neural network (CNN) models proposed for face recognition has been continuously growing larger to better fit large amount of training data. When training data are obtained from internet, the labels are likely to be ambiguous and inaccurate. This paper presents a Light CNN framework to learn a compact embedding on the large-scale face data with massive noisy labels. First, we introduce a variation of maxout activation, called Max-Feature-Map (MFM), into each convolutional layer of CNN. Different from maxout activation that uses many feature maps to linearly approximate an arbitrary convex activation function, MFM does so via a competitive relationship. MFM can not only separate noisy and informative signals but also play the role of feature selection between two feature maps. Second, three networks are carefully designed to obtain better performance meanwhile reducing the number of parameters and computational costs. Lastly, a semantic bootstrapping method is proposed to make the prediction of the networks more consistent with noisy labels. Experimental results show that the proposed framework can utilize large-scale noisy data to learn a Light model that is efficient in computational costs and storage spaces. The learned single network with a 256-D representation achieves state-of-the-art results on various face benchmarks without fine-tuning. The code is released on https://github.com/AlfredXiangWu/LightCNN. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Wu, Xiang ; He, Ran ; Sun, Zhenan ; Tan, Tieniu ; ","A Light CNN for Deep Face Representation with Noisy Labels  The volume of convolutional neural network (CNN) models proposed for face recognition has been continuously growing larger to better fit large amount of training data. When training data are obtained from internet, the labels are likely to be ambiguous and inaccurate. This paper presents a Light CNN framework to learn a compact embedding on the large-scale face data with massive noisy labels. First, we introduce a variation of maxout activation, called Max-Feature-Map (MFM), into each convolutional layer of CNN. Different from maxout activation that uses many feature maps to linearly approximate an arbitrary convex activation function, MFM does so via a competitive relationship. MFM can not only separate noisy and informative signals but also play the role of feature selection between two feature maps. Second, three networks are carefully designed to obtain better performance meanwhile reducing the number of parameters and computational costs. Lastly, a semantic bootstrapping method is proposed to make the prediction of the networks more consistent with noisy labels. Experimental results show that the proposed framework can utilize large-scale noisy data to learn a Light model that is efficient in computational costs and storage spaces. The learned single network with a 256-D representation achieves state-of-the-art results on various face benchmarks without fine-tuning. The code is released on https://github.com/AlfredXiangWu/LightCNN. ",light cnn deep face representation noisy label volume convolutional neural network cnn model propose face recognition continuously grow larger better fit large amount train data train data obtain internet label likely ambiguous inaccurate paper present light cnn framework learn compact embed large scale face data massive noisy label first introduce variation maxout activation call max feature map mfm convolutional layer cnn different maxout activation use many feature map linearly approximate arbitrary convex activation function mfm via competitive relationship mfm separate noisy informative signal also play role feature selection two feature map second three network carefully design obtain better performance meanwhile reduce number parameters computational cost lastly semantic bootstrapping method propose make prediction network consistent noisy label experimental result show propose framework utilize large scale noisy data learn light model efficient computational cost storage space learn single network representation achieve state art result various face benchmarks without fine tune code release https github com alfredxiangwu lightcnn,155,6,1511.02683.txt
http://arxiv.org/abs/1511.02899,On the Combinatorial Version of the Slepian-Wolf Problem,"  We study the following combinatorial version of the Slepian-Wolf coding scheme. Two isolated Senders are given binary strings $X$ and $Y$ respectively; the length of each string is equal to $n$, and the Hamming distance between the strings is at most $\alpha n$. The Senders compress their strings and communicate the results to the Receiver. Then the Receiver must reconstruct both strings $X$ and $Y$. The aim is to minimise the lengths of the transmitted messages.   For an asymmetric variant of this problem (where one of the Senders transmits the input string to the Receiver without compression) with deterministic encoding a nontrivial lower bound was found by A.Orlitsky and K.Viswanathany. In our paper we prove a new lower bound for the schemes with syndrome coding, where at least one of the Senders uses linear encoding of the input string.   For the combinatorial Slepian-Wolf problem with randomized encoding the theoretical optimum of communication complexity was recently found by the first author, though effective protocols with optimal lengths of messages remained unknown. We close this gap and present a polynomial time randomized protocol that achieves the optimal communication complexity. ",Computer Science - Information Theory ; Computer Science - Discrete Mathematics ; ,"Chumbalov, Daniyar ; Romashchenko, Andrei ; ","On the Combinatorial Version of the Slepian-Wolf Problem  We study the following combinatorial version of the Slepian-Wolf coding scheme. Two isolated Senders are given binary strings $X$ and $Y$ respectively; the length of each string is equal to $n$, and the Hamming distance between the strings is at most $\alpha n$. The Senders compress their strings and communicate the results to the Receiver. Then the Receiver must reconstruct both strings $X$ and $Y$. The aim is to minimise the lengths of the transmitted messages.   For an asymmetric variant of this problem (where one of the Senders transmits the input string to the Receiver without compression) with deterministic encoding a nontrivial lower bound was found by A.Orlitsky and K.Viswanathany. In our paper we prove a new lower bound for the schemes with syndrome coding, where at least one of the Senders uses linear encoding of the input string.   For the combinatorial Slepian-Wolf problem with randomized encoding the theoretical optimum of communication complexity was recently found by the first author, though effective protocols with optimal lengths of messages remained unknown. We close this gap and present a polynomial time randomized protocol that achieves the optimal communication complexity. ",combinatorial version slepian wolf problem study follow combinatorial version slepian wolf cod scheme two isolate senders give binary string respectively length string equal ham distance string alpha senders compress string communicate result receiver receiver must reconstruct string aim minimise lengths transmit message asymmetric variant problem one senders transmit input string receiver without compression deterministic encode nontrivial lower bind find orlitsky viswanathany paper prove new lower bind scheme syndrome cod least one senders use linear encode input string combinatorial slepian wolf problem randomize encode theoretical optimum communication complexity recently find first author though effective protocols optimal lengths message remain unknown close gap present polynomial time randomize protocol achieve optimal communication complexity,110,12,1511.02899.txt
http://arxiv.org/abs/1511.03005,ELDA: Towards Efficient and Lightweight Detection of Cache Pollution   Attacks in NDN,"  As a promising architectural design for future Internet, named data networking (NDN) relies on in-network caching to efficiently deliver name-based content. However, the in-network caching is vulnerable to cache pollution attacks (CPA), which can reduce cache hits by violating cache locality and significantly degrade the overall performance of NDN. To defend against CPA attacks, the most effective way is to first detect the attacks and then throttle them. Since the CPA attack itself has already imposed a huge burden on victims, to avoid exhausting the remaining resources on the victims for detection purpose, we expect a lightweight detection solution. We thus propose ELDA, an Efficient and Lightweight Detection scheme against cache pollution Attacks, in which we design a Lightweight Flajolet-Martin (LFM) sketch to monitor the interest traffic. Our analysis and simulations demonstrate that, by consuming a few computation and memory resources, ELDA can effectively and efficiently detect CPA attacks. ",Computer Science - Cryptography and Security ; Computer Science - Networking and Internet Architecture ; Computer Science - Performance ; B.2.4 ; C.4 ; ,"Xu, Zhiwei ; Chen, Bo ; Wang, Ninghan ; Zhang, Yujun ; Li, Zhongcheng ; ","ELDA: Towards Efficient and Lightweight Detection of Cache Pollution   Attacks in NDN  As a promising architectural design for future Internet, named data networking (NDN) relies on in-network caching to efficiently deliver name-based content. However, the in-network caching is vulnerable to cache pollution attacks (CPA), which can reduce cache hits by violating cache locality and significantly degrade the overall performance of NDN. To defend against CPA attacks, the most effective way is to first detect the attacks and then throttle them. Since the CPA attack itself has already imposed a huge burden on victims, to avoid exhausting the remaining resources on the victims for detection purpose, we expect a lightweight detection solution. We thus propose ELDA, an Efficient and Lightweight Detection scheme against cache pollution Attacks, in which we design a Lightweight Flajolet-Martin (LFM) sketch to monitor the interest traffic. Our analysis and simulations demonstrate that, by consuming a few computation and memory resources, ELDA can effectively and efficiently detect CPA attacks. ",elda towards efficient lightweight detection cache pollution attack ndn promise architectural design future internet name data network ndn rely network cache efficiently deliver name base content however network cache vulnerable cache pollution attack cpa reduce cache hit violate cache locality significantly degrade overall performance ndn defend cpa attack effective way first detect attack throttle since cpa attack already impose huge burden victims avoid exhaust remain resources victims detection purpose expect lightweight detection solution thus propose elda efficient lightweight detection scheme cache pollution attack design lightweight flajolet martin lfm sketch monitor interest traffic analysis simulations demonstrate consume computation memory resources elda effectively efficiently detect cpa attack,105,6,1511.03005.txt
http://arxiv.org/abs/1511.03234,A general framework for Noetherian well ordered polynomial reductions,"  Polynomial reduction is one of the main tools in computational algebra with innumerable applications in many areas, both pure and applied. Since many years both the theory and an efficient design of the related algorithm have been solidly established.   This paper presents a general definition of polynomial reduction structure, studies its features and highlights the aspects needed in order to grant and to efficiently test the main properties (noetherianity, confluence, ideal membership).   The most significant aspect of this analysis is a negative reappraisal of the role of the notion of term order which is usually considered a central and crucial tool in the theory. In fact, as it was already established in the computer science context in relation with termination of algorithms, most of the properties can be obtained simply considering a well-founded ordering, while the classical requirement that it be preserved by multiplication is irrelevant.   The last part of the paper shows how the polynomial basis concepts present in literature are interpreted in our language and their properties are consequences of the general results established in the first part of the paper. ","Mathematics - Commutative Algebra ; Computer Science - Symbolic Computation ; 14C05, 14Q20, 13P10 ; ","Ceria, Michela ; Mora, Teo ; Roggero, Margherita ; ","A general framework for Noetherian well ordered polynomial reductions  Polynomial reduction is one of the main tools in computational algebra with innumerable applications in many areas, both pure and applied. Since many years both the theory and an efficient design of the related algorithm have been solidly established.   This paper presents a general definition of polynomial reduction structure, studies its features and highlights the aspects needed in order to grant and to efficiently test the main properties (noetherianity, confluence, ideal membership).   The most significant aspect of this analysis is a negative reappraisal of the role of the notion of term order which is usually considered a central and crucial tool in the theory. In fact, as it was already established in the computer science context in relation with termination of algorithms, most of the properties can be obtained simply considering a well-founded ordering, while the classical requirement that it be preserved by multiplication is irrelevant.   The last part of the paper shows how the polynomial basis concepts present in literature are interpreted in our language and their properties are consequences of the general results established in the first part of the paper. ",general framework noetherian well order polynomial reductions polynomial reduction one main tool computational algebra innumerable applications many areas pure apply since many years theory efficient design relate algorithm solidly establish paper present general definition polynomial reduction structure study feature highlight aspects need order grant efficiently test main properties noetherianity confluence ideal membership significant aspect analysis negative reappraisal role notion term order usually consider central crucial tool theory fact already establish computer science context relation termination algorithms properties obtain simply consider well found order classical requirement preserve multiplication irrelevant last part paper show polynomial basis concepts present literature interpret language properties consequences general result establish first part paper,107,8,1511.03234.txt
http://arxiv.org/abs/1511.03407,Reorganizing topologies of Steiner trees to accelerate their elimination,"  We describe a technique to reorganize topologies of Steiner trees by exchanging neighbors of adjacent Steiner points. We explain how to use the systematic way of building trees, and therefore topologies, to find the correct topology after nodes have been exchanged. Topology reorganizations can be inserted into the enumeration scheme commonly used by exact algorithms for the Euclidean Steiner tree problem in $d$-space, providing a method of improvement different than the usual approaches. As an example, we show how topology reorganizations can be used to dynamically change the exploration of the usual branch-and-bound tree when two Steiner points collide during the optimization process. We also turn our attention to the erroneous use of a pre-optimization lower bound in the original algorithm and give an example to confirm its usage is incorrect. In order to provide numerical results on correct solutions, we use planar equilateral points to quickly compute this lower bound, even in dimensions higher than two. Finally, we describe planar twin trees, identical trees yielded by different topologies, whose generalization to higher dimensions could open a new way of building Steiner trees. ","Computer Science - Data Structures and Algorithms ; 90C35, 05C05, 90C57 ; ","Grodet, Aymeric ; Tsuchiya, Takuya ; ","Reorganizing topologies of Steiner trees to accelerate their elimination  We describe a technique to reorganize topologies of Steiner trees by exchanging neighbors of adjacent Steiner points. We explain how to use the systematic way of building trees, and therefore topologies, to find the correct topology after nodes have been exchanged. Topology reorganizations can be inserted into the enumeration scheme commonly used by exact algorithms for the Euclidean Steiner tree problem in $d$-space, providing a method of improvement different than the usual approaches. As an example, we show how topology reorganizations can be used to dynamically change the exploration of the usual branch-and-bound tree when two Steiner points collide during the optimization process. We also turn our attention to the erroneous use of a pre-optimization lower bound in the original algorithm and give an example to confirm its usage is incorrect. In order to provide numerical results on correct solutions, we use planar equilateral points to quickly compute this lower bound, even in dimensions higher than two. Finally, we describe planar twin trees, identical trees yielded by different topologies, whose generalization to higher dimensions could open a new way of building Steiner trees. ",reorganize topologies steiner tree accelerate elimination describe technique reorganize topologies steiner tree exchange neighbor adjacent steiner point explain use systematic way build tree therefore topologies find correct topology nod exchange topology reorganizations insert enumeration scheme commonly use exact algorithms euclidean steiner tree problem space provide method improvement different usual approach example show topology reorganizations use dynamically change exploration usual branch bind tree two steiner point collide optimization process also turn attention erroneous use pre optimization lower bind original algorithm give example confirm usage incorrect order provide numerical result correct solutions use planar equilateral point quickly compute lower bind even dimension higher two finally describe planar twin tree identical tree yield different topologies whose generalization higher dimension could open new way build steiner tree,123,4,1511.03407.txt
http://arxiv.org/abs/1511.03501,"Eliminating Higher-Multiplicity Intersections, III. Codimension 2","  We study conditions under which a finite simplicial complex $K$ can be mapped to $\mathbb R^d$ without higher-multiplicity intersections. An almost $r$-embedding is a map $f: K\to \mathbb R^d$ such that the images of any $r$ pairwise disjoint simplices of $K$ do not have a common point. We show that if $r$ is not a prime power and $d\geq 2r+1$, then there is a counterexample to the topological Tverberg conjecture, i.e., there is an almost $r$-embedding of the $(d+1)(r-1)$-simplex in $\mathbb R^d$. This improves on previous constructions of counterexamples (for $d\geq 3r$) based on a series of papers by M. \""Ozaydin, M. Gromov, P. Blagojevi\'c, F. Frick, G. Ziegler, and the second and fourth present authors.   The counterexamples are obtained by proving the following algebraic criterion in codimension 2: If $r\ge3$ and if $K$ is a finite $2(r-1)$-complex then there exists an almost $r$-embedding $K\to \mathbb R^{2r}$ if and only if there exists a general position PL map $f:K\to \mathbb R^{2r}$ such that the algebraic intersection number of the $f$-images of any $r$ pairwise disjoint simplices of $K$ is zero. This result can be restated in terms of cohomological obstructions or equivariant maps, and extends an analogous codimension 3 criterion by the second and fourth authors. As another application we classify ornaments $f:S^3 \sqcup S^3\sqcup S^3\to \mathbb R^5$ up to ornament concordance.   It follows from work of M. Freedman, V. Krushkal and P. Teichner that the analogous criterion for $r=2$ is false. We prove a lemma on singular higher-dimensional Borromean rings, yielding an elementary proof of the counterexample. ","Mathematics - Geometric Topology ; Computer Science - Computational Geometry ; Mathematics - Combinatorics ; 57Q35, 55S91, 52A35 ; ","Avvakumov, S. ; Mabillard, I. ; Skopenkov, A. ; Wagner, U. ; ","Eliminating Higher-Multiplicity Intersections, III. Codimension 2  We study conditions under which a finite simplicial complex $K$ can be mapped to $\mathbb R^d$ without higher-multiplicity intersections. An almost $r$-embedding is a map $f: K\to \mathbb R^d$ such that the images of any $r$ pairwise disjoint simplices of $K$ do not have a common point. We show that if $r$ is not a prime power and $d\geq 2r+1$, then there is a counterexample to the topological Tverberg conjecture, i.e., there is an almost $r$-embedding of the $(d+1)(r-1)$-simplex in $\mathbb R^d$. This improves on previous constructions of counterexamples (for $d\geq 3r$) based on a series of papers by M. \""Ozaydin, M. Gromov, P. Blagojevi\'c, F. Frick, G. Ziegler, and the second and fourth present authors.   The counterexamples are obtained by proving the following algebraic criterion in codimension 2: If $r\ge3$ and if $K$ is a finite $2(r-1)$-complex then there exists an almost $r$-embedding $K\to \mathbb R^{2r}$ if and only if there exists a general position PL map $f:K\to \mathbb R^{2r}$ such that the algebraic intersection number of the $f$-images of any $r$ pairwise disjoint simplices of $K$ is zero. This result can be restated in terms of cohomological obstructions or equivariant maps, and extends an analogous codimension 3 criterion by the second and fourth authors. As another application we classify ornaments $f:S^3 \sqcup S^3\sqcup S^3\to \mathbb R^5$ up to ornament concordance.   It follows from work of M. Freedman, V. Krushkal and P. Teichner that the analogous criterion for $r=2$ is false. We prove a lemma on singular higher-dimensional Borromean rings, yielding an elementary proof of the counterexample. ",eliminate higher multiplicity intersections iii codimension study condition finite simplicial complex map mathbb without higher multiplicity intersections almost embed map mathbb image pairwise disjoint simplices common point show prime power geq counterexample topological tverberg conjecture almost embed simplex mathbb improve previous constructions counterexamples geq base series paper ozaydin gromov blagojevi frick ziegler second fourth present author counterexamples obtain prove follow algebraic criterion codimension ge finite complex exist almost embed mathbb exist general position pl map mathbb algebraic intersection number image pairwise disjoint simplices zero result restate term cohomological obstructions equivariant map extend analogous codimension criterion second fourth author another application classify ornament sqcup sqcup mathbb ornament concordance follow work freedman krushkal teichner analogous criterion false prove lemma singular higher dimensional borromean ring yield elementary proof counterexample,126,4,1511.03501.txt
http://arxiv.org/abs/1511.03518,Diffusion-like recommendation with enhanced similarity of objects,"  In last decades, diversity and accuracy have been regarded as two important measures in evaluating a recommendation model. However, a clear concern is that a model focusing excessively on one measure will put the other one at risk, thus it is not easy to greatly improve diversity and accuracy simultaneously. In this paper, we propose to enhance the Resource-Allocation (RA) similarity in resource transfer equations of diffusion-like models, by giving a tunable exponent to the RA similarity, and traversing the value of the exponent to achieve the optimal recommendation results. In this way, we can increase the recommendation scores (allocated resource) of many unpopular objects. Experiments on three benchmark data sets, MovieLens, Netflix, and RateYourMusic show that the modified models can yield remarkable performance improvement compared with the original ones. ",Computer Science - Information Retrieval ; Computer Science - Social and Information Networks ; ,"An, Ya-Hui ; Dong, Qiang ; Sun, Chong-Jing ; Nie, Da-Cheng ; Fu, Yan ; ","Diffusion-like recommendation with enhanced similarity of objects  In last decades, diversity and accuracy have been regarded as two important measures in evaluating a recommendation model. However, a clear concern is that a model focusing excessively on one measure will put the other one at risk, thus it is not easy to greatly improve diversity and accuracy simultaneously. In this paper, we propose to enhance the Resource-Allocation (RA) similarity in resource transfer equations of diffusion-like models, by giving a tunable exponent to the RA similarity, and traversing the value of the exponent to achieve the optimal recommendation results. In this way, we can increase the recommendation scores (allocated resource) of many unpopular objects. Experiments on three benchmark data sets, MovieLens, Netflix, and RateYourMusic show that the modified models can yield remarkable performance improvement compared with the original ones. ",diffusion like recommendation enhance similarity object last decades diversity accuracy regard two important measure evaluate recommendation model however clear concern model focus excessively one measure put one risk thus easy greatly improve diversity accuracy simultaneously paper propose enhance resource allocation ra similarity resource transfer equations diffusion like model give tunable exponent ra similarity traverse value exponent achieve optimal recommendation result way increase recommendation score allocate resource many unpopular object experiment three benchmark data set movielens netflix rateyourmusic show modify model yield remarkable performance improvement compare original ones,87,2,1511.03518.txt
http://arxiv.org/abs/1511.03693,Game characterizations and lower cones in the Weihrauch degrees,"  We introduce a parametrized version of the Wadge game for functions and show that each lower cone in the Weihrauch degrees is characterized by such a game. These parametrized Wadge games subsume the original Wadge game, the eraser and backtrack games as well as Semmes's tree games. In particular, we propose that the lower cones in the Weihrauch degrees are the answer to Andretta's question on which classes of functions admit game characterizations. We then discuss some applications of such parametrized Wadge games. Using machinery from Weihrauch reducibility theory, we introduce games characterizing every (transfinite) level of the Baire hierarchy via an iteration of a pruning derivative on countably branching trees. ","Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03E15, 54H05, 03D60, 03F15 ; ","Nobrega, Hugo ; Pauly, Arno ; ","Game characterizations and lower cones in the Weihrauch degrees  We introduce a parametrized version of the Wadge game for functions and show that each lower cone in the Weihrauch degrees is characterized by such a game. These parametrized Wadge games subsume the original Wadge game, the eraser and backtrack games as well as Semmes's tree games. In particular, we propose that the lower cones in the Weihrauch degrees are the answer to Andretta's question on which classes of functions admit game characterizations. We then discuss some applications of such parametrized Wadge games. Using machinery from Weihrauch reducibility theory, we introduce games characterizing every (transfinite) level of the Baire hierarchy via an iteration of a pruning derivative on countably branching trees. ",game characterizations lower con weihrauch degrees introduce parametrized version wadge game function show lower cone weihrauch degrees characterize game parametrized wadge game subsume original wadge game eraser backtrack game well semmes tree game particular propose lower con weihrauch degrees answer andretta question class function admit game characterizations discuss applications parametrized wadge game use machinery weihrauch reducibility theory introduce game characterize every transfinite level baire hierarchy via iteration prune derivative countably branch tree,72,8,1511.03693.txt
http://arxiv.org/abs/1511.03894,The Game of Phishing,"  The current implementation of TLS involves your browser displaying a padlock, and a green bar, after successfully verifying the digital signature on the TLS certificate. Proposed is a solution where your browser's response to successful verification of a TLS certificate is to display a login window. That login window displays the identity credentials from the TLS certificate, to allow the user to authenticate Bob. It also displays a 'user-browser' shared secret i.e. a specific picture from your hard disk. This is not SiteKey, the image is shared between the computer user and their browser. It is never transmitted over the internet. Since sandboxed websites cannot access your hard disk this image cannot be counterfeited by phishing websites. Basically if you view the installed software component of your browser as an actor in the cryptography protocol, then the solution to phishing attacks is classic cryptography, as documented in any cryptography textbook. ",Computer Science - Cryptography and Security ; ,"Kilcullen, Joseph ; ","The Game of Phishing  The current implementation of TLS involves your browser displaying a padlock, and a green bar, after successfully verifying the digital signature on the TLS certificate. Proposed is a solution where your browser's response to successful verification of a TLS certificate is to display a login window. That login window displays the identity credentials from the TLS certificate, to allow the user to authenticate Bob. It also displays a 'user-browser' shared secret i.e. a specific picture from your hard disk. This is not SiteKey, the image is shared between the computer user and their browser. It is never transmitted over the internet. Since sandboxed websites cannot access your hard disk this image cannot be counterfeited by phishing websites. Basically if you view the installed software component of your browser as an actor in the cryptography protocol, then the solution to phishing attacks is classic cryptography, as documented in any cryptography textbook. ",game phishing current implementation tls involve browser display padlock green bar successfully verify digital signature tls certificate propose solution browser response successful verification tls certificate display login window login window display identity credentials tls certificate allow user authenticate bob also display user browser share secret specific picture hard disk sitekey image share computer user browser never transmit internet since sandboxed websites cannot access hard disk image cannot counterfeit phishing websites basically view instal software component browser actor cryptography protocol solution phishing attack classic cryptography document cryptography textbook,87,11,1511.03894.txt
http://arxiv.org/abs/1511.04731,Hardness of RNA Folding Problem with Four Symbols,"  An RNA sequence is a string composed of four types of nucleotides, $A, C, G$, and $U$. The goal of the RNA folding problem is to find a maximum cardinality set of crossing-free pairs of the form $\{A,U\}$ or $\{C,G\}$ in a given RNA sequence. The problem is central in bioinformatics and has received much attention over the years. Abboud, Backurs, and Williams (FOCS 2015) demonstrated a conditional lower bound for a generalized version of the RNA folding problem based on a conjectured hardness of the $k$-clique problem. Their lower bound requires the RNA sequence to have at least 36 types of symbols, making the result not applicable to the RNA folding problem in real life (i.e., alphabet size 4). In this paper, we present an improved lower bound that works for the alphabet size 4 case.   We also investigate the Dyck edit distance problem, which is a string problem closely related to RNA folding. We demonstrate a reduction from RNA folding to Dyck edit distance with alphabet size 10. This leads to a much simpler proof of the conditional lower bound for Dyck edit distance problem given by Abboud, Backurs, and Williams (FOCS 2015), and lowers the alphabet size requirement. ",Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; ,"Chang, Yi-Jun ; ","Hardness of RNA Folding Problem with Four Symbols  An RNA sequence is a string composed of four types of nucleotides, $A, C, G$, and $U$. The goal of the RNA folding problem is to find a maximum cardinality set of crossing-free pairs of the form $\{A,U\}$ or $\{C,G\}$ in a given RNA sequence. The problem is central in bioinformatics and has received much attention over the years. Abboud, Backurs, and Williams (FOCS 2015) demonstrated a conditional lower bound for a generalized version of the RNA folding problem based on a conjectured hardness of the $k$-clique problem. Their lower bound requires the RNA sequence to have at least 36 types of symbols, making the result not applicable to the RNA folding problem in real life (i.e., alphabet size 4). In this paper, we present an improved lower bound that works for the alphabet size 4 case.   We also investigate the Dyck edit distance problem, which is a string problem closely related to RNA folding. We demonstrate a reduction from RNA folding to Dyck edit distance with alphabet size 10. This leads to a much simpler proof of the conditional lower bound for Dyck edit distance problem given by Abboud, Backurs, and Williams (FOCS 2015), and lowers the alphabet size requirement. ",hardness rna fold problem four symbols rna sequence string compose four type nucleotides goal rna fold problem find maximum cardinality set cross free pair form give rna sequence problem central bioinformatics receive much attention years abboud backurs williams focs demonstrate conditional lower bind generalize version rna fold problem base conjecture hardness clique problem lower bind require rna sequence least type symbols make result applicable rna fold problem real life alphabet size paper present improve lower bind work alphabet size case also investigate dyck edit distance problem string problem closely relate rna fold demonstrate reduction rna fold dyck edit distance alphabet size lead much simpler proof conditional lower bind dyck edit distance problem give abboud backurs williams focs lower alphabet size requirement,121,8,1511.04731.txt
http://arxiv.org/abs/1511.04798,"Heterogeneous Knowledge Transfer in Video Emotion Recognition,   Attribution and Summarization","  Emotion is a key element in user-generated videos. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames expressing emotion. In this paper, for the first time, we study the problem of transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in understanding video emotion: emotion recognition, emotion attribution and emotion-oriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpora for zero-shot recognition of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization. A comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Artificial Intelligence ; Computer Science - Multimedia ; ,"Xu, Baohan ; Fu, Yanwei ; Jiang, Yu-Gang ; Li, Boyang ; Sigal, Leonid ; ","Heterogeneous Knowledge Transfer in Video Emotion Recognition,   Attribution and Summarization  Emotion is a key element in user-generated videos. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames expressing emotion. In this paper, for the first time, we study the problem of transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in understanding video emotion: emotion recognition, emotion attribution and emotion-oriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpora for zero-shot recognition of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization. A comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework. ",heterogeneous knowledge transfer video emotion recognition attribution summarization emotion key element user generate videos however difficult understand emotions convey videos due complex unstructured nature user generate content sparsity video frame express emotion paper first time study problem transfer knowledge heterogeneous external source include image textual data facilitate three relate task understand video emotion emotion recognition emotion attribution emotion orient summarization specifically framework learn video encode auxiliary emotional image dataset order improve supervise video emotion recognition transfer knowledge auxiliary textual corpora zero shoot recognition emotion class unseen train propose technique knowledge transfer facilitate novel applications emotion attribution emotion orient summarization comprehensive set experiment multiple datasets demonstrate effectiveness framework,107,10,1511.04798.txt
http://arxiv.org/abs/1511.04855,"Deep learning is a good steganalysis tool when embedding key is reused   for different images, even if there is a cover source-mismatch","  Since the BOSS competition, in 2010, most steganalysis approaches use a learning methodology involving two steps: feature extraction, such as the Rich Models (RM), for the image representation, and use of the Ensemble Classifier (EC) for the learning step. In 2015, Qian et al. have shown that the use of a deep learning approach that jointly learns and computes the features, is very promising for the steganalysis. In this paper, we follow-up the study of Qian et al., and show that, due to intrinsic joint minimization, the results obtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural Network (FNN), if well parameterized, surpass the conventional use of a RM with an EC. First, numerous experiments were conducted in order to find the best "" shape "" of the CNN. Second, experiments were carried out in the clairvoyant scenario in order to compare the CNN and FNN to an RM with an EC. The results show more than 16% reduction in the classification error with our CNN or FNN. Third, experiments were also performed in a cover-source mismatch setting. The results show that the CNN and FNN are naturally robust to the mismatch problem. In Addition to the experiments, we provide discussions on the internal mechanisms of a CNN, and weave links with some previously stated ideas, in order to understand the impressive results we obtained. ",Computer Science - Multimedia ; Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; ,"Pibre, Lionel ; Jérôme, Pasquet ; Ienco, Dino ; Chaumont, Marc ; ","Deep learning is a good steganalysis tool when embedding key is reused   for different images, even if there is a cover source-mismatch  Since the BOSS competition, in 2010, most steganalysis approaches use a learning methodology involving two steps: feature extraction, such as the Rich Models (RM), for the image representation, and use of the Ensemble Classifier (EC) for the learning step. In 2015, Qian et al. have shown that the use of a deep learning approach that jointly learns and computes the features, is very promising for the steganalysis. In this paper, we follow-up the study of Qian et al., and show that, due to intrinsic joint minimization, the results obtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural Network (FNN), if well parameterized, surpass the conventional use of a RM with an EC. First, numerous experiments were conducted in order to find the best "" shape "" of the CNN. Second, experiments were carried out in the clairvoyant scenario in order to compare the CNN and FNN to an RM with an EC. The results show more than 16% reduction in the classification error with our CNN or FNN. Third, experiments were also performed in a cover-source mismatch setting. The results show that the CNN and FNN are naturally robust to the mismatch problem. In Addition to the experiments, we provide discussions on the internal mechanisms of a CNN, and weave links with some previously stated ideas, in order to understand the impressive results we obtained. ",deep learn good steganalysis tool embed key reuse different image even cover source mismatch since boss competition steganalysis approach use learn methodology involve two step feature extraction rich model rm image representation use ensemble classifier ec learn step qian et al show use deep learn approach jointly learn compute feature promise steganalysis paper follow study qian et al show due intrinsic joint minimization result obtain convolutional neural network cnn fully connect neural network fnn well parameterized surpass conventional use rm ec first numerous experiment conduct order find best shape cnn second experiment carry clairvoyant scenario order compare cnn fnn rm ec result show reduction classification error cnn fnn third experiment also perform cover source mismatch set result show cnn fnn naturally robust mismatch problem addition experiment provide discussions internal mechanisms cnn weave link previously state ideas order understand impressive result obtain,141,6,1511.04855.txt
http://arxiv.org/abs/1511.04944,NASCUP: Nucleic Acid Sequence Classification by Universal Probability,"  Motivated by the need for fast and accurate classification of unlabeled nucleotide sequences on a large scale, we developed NASCUP, a new classification method that captures statistical structures of nucleotide sequences by compact context-tree models and universal probability from information theory. NASCUP achieved BLAST-like classification accuracy consistently for several large-scale databases in orders-of-magnitude reduced runtime, and was applied to other bioinformatics tasks such as outlier detection and synthetic sequence generation. ",Quantitative Biology - Genomics ; Computer Science - Information Theory ; ,"Kwon, Sunyoung ; Kim, Gyuwan ; Lee, Byunghan ; Chun, Jongsik ; Yoon, Sungroh ; Kim, Young-Han ; ","NASCUP: Nucleic Acid Sequence Classification by Universal Probability  Motivated by the need for fast and accurate classification of unlabeled nucleotide sequences on a large scale, we developed NASCUP, a new classification method that captures statistical structures of nucleotide sequences by compact context-tree models and universal probability from information theory. NASCUP achieved BLAST-like classification accuracy consistently for several large-scale databases in orders-of-magnitude reduced runtime, and was applied to other bioinformatics tasks such as outlier detection and synthetic sequence generation. ",nascup nucleic acid sequence classification universal probability motivate need fast accurate classification unlabeled nucleotide sequence large scale develop nascup new classification method capture statistical structure nucleotide sequence compact context tree model universal probability information theory nascup achieve blast like classification accuracy consistently several large scale databases order magnitude reduce runtime apply bioinformatics task outlier detection synthetic sequence generation,58,11,1511.04944.txt
http://arxiv.org/abs/1511.05174,Cross-scale predictive dictionaries,"  Sparse representations using data dictionaries provide an efficient model particularly for signals that do not enjoy alternate analytic sparsifying transformations. However, solving inverse problems with sparsifying dictionaries can be computationally expensive, especially when the dictionary under consideration has a large number of atoms. In this paper, we incorporate additional structure on to dictionary-based sparse representations for visual signals to enable speedups when solving sparse approximation problems. The specific structure that we endow onto sparse models is that of a multi-scale modeling where the sparse representation at each scale is constrained by the sparse representation at coarser scales. We show that this cross-scale predictive model delivers significant speedups, often in the range of 10-60$\times$, with little loss in accuracy for linear inverse problems associated with images, videos, and light fields. ",Computer Science - Computer Vision and Pattern Recognition ; Statistics - Machine Learning ; ,"Saragadam, Vishwanath ; Li, Xin ; Sankaranarayanan, Aswin ; ","Cross-scale predictive dictionaries  Sparse representations using data dictionaries provide an efficient model particularly for signals that do not enjoy alternate analytic sparsifying transformations. However, solving inverse problems with sparsifying dictionaries can be computationally expensive, especially when the dictionary under consideration has a large number of atoms. In this paper, we incorporate additional structure on to dictionary-based sparse representations for visual signals to enable speedups when solving sparse approximation problems. The specific structure that we endow onto sparse models is that of a multi-scale modeling where the sparse representation at each scale is constrained by the sparse representation at coarser scales. We show that this cross-scale predictive model delivers significant speedups, often in the range of 10-60$\times$, with little loss in accuracy for linear inverse problems associated with images, videos, and light fields. ",cross scale predictive dictionaries sparse representations use data dictionaries provide efficient model particularly signal enjoy alternate analytic sparsifying transformations however solve inverse problems sparsifying dictionaries computationally expensive especially dictionary consideration large number atoms paper incorporate additional structure dictionary base sparse representations visual signal enable speedups solve sparse approximation problems specific structure endow onto sparse model multi scale model sparse representation scale constrain sparse representation coarser scale show cross scale predictive model deliver significant speedups often range time little loss accuracy linear inverse problems associate image videos light field,88,9,1511.05174.txt
http://arxiv.org/abs/1511.05432,Understanding Adversarial Training: Increasing Local Stability of Neural   Nets through Robust Optimization,"  We propose a general framework for increasing local stability of Artificial Neural Nets (ANNs) using Robust Optimization (RO). We achieve this through an alternating minimization-maximization procedure, in which the loss of the network is minimized over perturbed examples that are generated at each parameter update. We show that adversarial training of ANNs is in fact robustification of the network optimization, and that our proposed framework generalizes previous approaches for increasing local stability of ANNs. Experimental results reveal that our approach increases the robustness of the network to existing adversarial examples, while making it harder to generate new ones. Furthermore, our algorithm improves the accuracy of the network also on the original test data. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; ,"Shaham, Uri ; Yamada, Yutaro ; Negahban, Sahand ; ","Understanding Adversarial Training: Increasing Local Stability of Neural   Nets through Robust Optimization  We propose a general framework for increasing local stability of Artificial Neural Nets (ANNs) using Robust Optimization (RO). We achieve this through an alternating minimization-maximization procedure, in which the loss of the network is minimized over perturbed examples that are generated at each parameter update. We show that adversarial training of ANNs is in fact robustification of the network optimization, and that our proposed framework generalizes previous approaches for increasing local stability of ANNs. Experimental results reveal that our approach increases the robustness of the network to existing adversarial examples, while making it harder to generate new ones. Furthermore, our algorithm improves the accuracy of the network also on the original test data. ",understand adversarial train increase local stability neural net robust optimization propose general framework increase local stability artificial neural net anns use robust optimization ro achieve alternate minimization maximization procedure loss network minimize perturb examples generate parameter update show adversarial train anns fact robustification network optimization propose framework generalize previous approach increase local stability anns experimental result reveal approach increase robustness network exist adversarial examples make harder generate new ones furthermore algorithm improve accuracy network also original test data,78,6,1511.05432.txt
http://arxiv.org/abs/1511.05546,Complexity and Approximability of Parameterized MAX-CSPs,"  We study the optimization version of constraint satisfaction problems (Max-CSPs) in the framework of parameterized complexity; the goal is to compute the maximum fraction of constraints that can be satisfied simultaneously. In standard CSPs, we want to decide whether this fraction equals one. The parameters we investigate are structural measures, such as the treewidth or the clique-width of the variable-constraint incidence graph of the CSP instance.   We consider Max-CSPs with the constraint types AND, OR, PARITY, and MAJORITY, and with various parameters k, and we attempt to fully classify them into the following three cases: 1. The exact optimum can be computed in FPT time. 2. It is W[1]-hard to compute the exact optimum, but there is a randomized FPT approximation scheme (FPTAS), which computes a $(1-\epsilon)$-approximation in time $f(k,\epsilon)\cdot poly(n)$. 3. There is no FPTAS unless FPT=W[1].   For the corresponding standard CSPs, we establish FPT vs. W[1]-hardness results. ",Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; ,"Dell, Holger ; Kim, Eun Jung ; Lampis, Michael ; Mitsou, Valia ; Mömke, Tobias ; ","Complexity and Approximability of Parameterized MAX-CSPs  We study the optimization version of constraint satisfaction problems (Max-CSPs) in the framework of parameterized complexity; the goal is to compute the maximum fraction of constraints that can be satisfied simultaneously. In standard CSPs, we want to decide whether this fraction equals one. The parameters we investigate are structural measures, such as the treewidth or the clique-width of the variable-constraint incidence graph of the CSP instance.   We consider Max-CSPs with the constraint types AND, OR, PARITY, and MAJORITY, and with various parameters k, and we attempt to fully classify them into the following three cases: 1. The exact optimum can be computed in FPT time. 2. It is W[1]-hard to compute the exact optimum, but there is a randomized FPT approximation scheme (FPTAS), which computes a $(1-\epsilon)$-approximation in time $f(k,\epsilon)\cdot poly(n)$. 3. There is no FPTAS unless FPT=W[1].   For the corresponding standard CSPs, we establish FPT vs. W[1]-hardness results. ",complexity approximability parameterized max csps study optimization version constraint satisfaction problems max csps framework parameterized complexity goal compute maximum fraction constraints satisfy simultaneously standard csps want decide whether fraction equal one parameters investigate structural measure treewidth clique width variable constraint incidence graph csp instance consider max csps constraint type parity majority various parameters attempt fully classify follow three case exact optimum compute fpt time hard compute exact optimum randomize fpt approximation scheme fptas compute epsilon approximation time epsilon cdot poly fptas unless fpt correspond standard csps establish fpt vs hardness result,91,1,1511.05546.txt
http://arxiv.org/abs/1511.05635,Competitive Multi-scale Convolution,"  In this paper, we introduce a new deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; ,"Liao, Zhibin ; Carneiro, Gustavo ; ","Competitive Multi-scale Convolution  In this paper, we introduce a new deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN. ",competitive multi scale convolution paper introduce new deep convolutional neural network convnet module promote competition among set multi scale convolutional filter new module inspire inception module replace original collaborative pool stage consist concatenation multi scale filter output competitive pool represent maxout activation unit extension follow two objectives selection maximum response among multi scale filter prevent filter co adaptation allow formation multiple sub network within model show facilitate train complex learn problems maxout unit reduce dimensionality output multi scale filter show use propose module typical deep convnets produce classification result either better comparable state art follow benchmark datasets mnist cifar cifar svhn,101,2,1511.05635.txt
http://arxiv.org/abs/1511.05646,The Invisible Hand of Dynamic Market Pricing,"  Walrasian prices, if they exist, have the property that one can assign every buyer some bundle in her demand set, such that the resulting assignment will maximize social welfare. Unfortunately, this assumes carefully breaking ties amongst different bundles in the buyer demand set. Presumably, the shopkeeper cleverly convinces the buyer to break ties in a manner consistent with maximizing social welfare. Lacking such a shopkeeper, if buyers arrive sequentially and simply choose some arbitrary bundle in their demand set, the social welfare may be arbitrarily bad. In the context of matching markets, we show how to compute dynamic prices, based upon the current inventory, that guarantee that social welfare is maximized. Such prices are set without knowing the identity of the next buyer to arrive. We also show that this is impossible in general (e.g., for coverage valuations), but consider other scenarios where this can be done. We further extend our results to Bayesian and bounded rationality models. ",Computer Science - Computer Science and Game Theory ; Computer Science - Data Structures and Algorithms ; ,"Cohen-Addad, Vincent ; Eden, Alon ; Feldman, Michal ; Fiat, Amos ; ","The Invisible Hand of Dynamic Market Pricing  Walrasian prices, if they exist, have the property that one can assign every buyer some bundle in her demand set, such that the resulting assignment will maximize social welfare. Unfortunately, this assumes carefully breaking ties amongst different bundles in the buyer demand set. Presumably, the shopkeeper cleverly convinces the buyer to break ties in a manner consistent with maximizing social welfare. Lacking such a shopkeeper, if buyers arrive sequentially and simply choose some arbitrary bundle in their demand set, the social welfare may be arbitrarily bad. In the context of matching markets, we show how to compute dynamic prices, based upon the current inventory, that guarantee that social welfare is maximized. Such prices are set without knowing the identity of the next buyer to arrive. We also show that this is impossible in general (e.g., for coverage valuations), but consider other scenarios where this can be done. We further extend our results to Bayesian and bounded rationality models. ",invisible hand dynamic market price walrasian price exist property one assign every buyer bundle demand set result assignment maximize social welfare unfortunately assume carefully break tie amongst different bundle buyer demand set presumably shopkeeper cleverly convince buyer break tie manner consistent maximize social welfare lack shopkeeper buyers arrive sequentially simply choose arbitrary bundle demand set social welfare may arbitrarily bad context match market show compute dynamic price base upon current inventory guarantee social welfare maximize price set without know identity next buyer arrive also show impossible general coverage valuations consider scenarios do extend result bayesian bound rationality model,98,0,1511.05646.txt
http://arxiv.org/abs/1511.05710,Complex-Valued Gaussian Processes for Regression,"  In this paper we propose a novel Bayesian solution for nonlinear regression in complex fields. Previous solutions for kernels methods usually assume a complexification approach, where the real-valued kernel is replaced by a complex-valued one. This approach is limited. Based on results in complex-valued linear theory and Gaussian random processes we show that a pseudo-kernel must be included. This is the starting point to develop the new complex-valued formulation for Gaussian process for regression (CGPR). We face the design of the covariance and pseudo-covariance based on a convolution approach and for several scenarios. Just in the particular case where the outputs are proper, the pseudo-kernel cancels. Also, the hyperparameters of the covariance {can be learnt} maximizing the marginal likelihood using Wirtinger's calculus and patterned complex-valued matrix derivatives. In the experiments included, we show how CGPR successfully solve systems where real and imaginary parts are correlated. Besides, we successfully solve the nonlinear channel equalization problem by developing a recursive solution with basis removal. We report remarkable improvements compared to previous solutions: a 2-4 dB reduction of the MSE with {just a quarter} of the training samples used by previous approaches. ",Computer Science - Machine Learning ; ,"Boloix-Tortosa, Rafael ; Arias-de-Reyna, Eva ; Payan-Somet, F. Javier ; Murillo-Fuentes, Juan J. ; ","Complex-Valued Gaussian Processes for Regression  In this paper we propose a novel Bayesian solution for nonlinear regression in complex fields. Previous solutions for kernels methods usually assume a complexification approach, where the real-valued kernel is replaced by a complex-valued one. This approach is limited. Based on results in complex-valued linear theory and Gaussian random processes we show that a pseudo-kernel must be included. This is the starting point to develop the new complex-valued formulation for Gaussian process for regression (CGPR). We face the design of the covariance and pseudo-covariance based on a convolution approach and for several scenarios. Just in the particular case where the outputs are proper, the pseudo-kernel cancels. Also, the hyperparameters of the covariance {can be learnt} maximizing the marginal likelihood using Wirtinger's calculus and patterned complex-valued matrix derivatives. In the experiments included, we show how CGPR successfully solve systems where real and imaginary parts are correlated. Besides, we successfully solve the nonlinear channel equalization problem by developing a recursive solution with basis removal. We report remarkable improvements compared to previous solutions: a 2-4 dB reduction of the MSE with {just a quarter} of the training samples used by previous approaches. ",complex value gaussian process regression paper propose novel bayesian solution nonlinear regression complex field previous solutions kernels methods usually assume complexification approach real value kernel replace complex value one approach limit base result complex value linear theory gaussian random process show pseudo kernel must include start point develop new complex value formulation gaussian process regression cgpr face design covariance pseudo covariance base convolution approach several scenarios particular case output proper pseudo kernel cancel also hyperparameters covariance learn maximize marginal likelihood use wirtinger calculus pattern complex value matrix derivatives experiment include show cgpr successfully solve systems real imaginary part correlate besides successfully solve nonlinear channel equalization problem develop recursive solution basis removal report remarkable improvements compare previous solutions db reduction mse quarter train sample use previous approach,126,11,1511.05710.txt
http://arxiv.org/abs/1511.06030,BIRDNEST: Bayesian Inference for Ratings-Fraud Detection,"  Review fraud is a pervasive problem in online commerce, in which fraudulent sellers write or purchase fake reviews to manipulate perception of their products and services. Fake reviews are often detected based on several signs, including 1) they occur in short bursts of time; 2) fraudulent user accounts have skewed rating distributions. However, these may both be true in any given dataset. Hence, in this paper, we propose an approach for detecting fraudulent reviews which combines these 2 approaches in a principled manner, allowing successful detection even when one of these signs is not present. To combine these 2 approaches, we formulate our Bayesian Inference for Rating Data (BIRD) model, a flexible Bayesian model of user rating behavior. Based on our model we formulate a likelihood-based suspiciousness metric, Normalized Expected Surprise Total (NEST). We propose a linear-time algorithm for performing Bayesian inference using our model and computing the metric. Experiments on real data show that BIRDNEST successfully spots review fraud in large, real-world graphs: the 50 most suspicious users of the Flipkart platform flagged by our algorithm were investigated and all identified as fraudulent by domain experts at Flipkart. ",Computer Science - Artificial Intelligence ; Computer Science - Social and Information Networks ; ,"Hooi, Bryan ; Shah, Neil ; Beutel, Alex ; Gunnemann, Stephan ; Akoglu, Leman ; Kumar, Mohit ; Makhija, Disha ; Faloutsos, Christos ; ","BIRDNEST: Bayesian Inference for Ratings-Fraud Detection  Review fraud is a pervasive problem in online commerce, in which fraudulent sellers write or purchase fake reviews to manipulate perception of their products and services. Fake reviews are often detected based on several signs, including 1) they occur in short bursts of time; 2) fraudulent user accounts have skewed rating distributions. However, these may both be true in any given dataset. Hence, in this paper, we propose an approach for detecting fraudulent reviews which combines these 2 approaches in a principled manner, allowing successful detection even when one of these signs is not present. To combine these 2 approaches, we formulate our Bayesian Inference for Rating Data (BIRD) model, a flexible Bayesian model of user rating behavior. Based on our model we formulate a likelihood-based suspiciousness metric, Normalized Expected Surprise Total (NEST). We propose a linear-time algorithm for performing Bayesian inference using our model and computing the metric. Experiments on real data show that BIRDNEST successfully spots review fraud in large, real-world graphs: the 50 most suspicious users of the Flipkart platform flagged by our algorithm were investigated and all identified as fraudulent by domain experts at Flipkart. ",birdnest bayesian inference rat fraud detection review fraud pervasive problem online commerce fraudulent sellers write purchase fake review manipulate perception products service fake review often detect base several sign include occur short burst time fraudulent user account skew rat distributions however may true give dataset hence paper propose approach detect fraudulent review combine approach principled manner allow successful detection even one sign present combine approach formulate bayesian inference rat data bird model flexible bayesian model user rat behavior base model formulate likelihood base suspiciousness metric normalize expect surprise total nest propose linear time algorithm perform bayesian inference use model compute metric experiment real data show birdnest successfully spot review fraud large real world graph suspicious users flipkart platform flag algorithm investigate identify fraudulent domain experts flipkart,126,11,1511.06030.txt
http://arxiv.org/abs/1511.06033,EigenRec: Generalizing PureSVD for Effective and Efficient Top-N   Recommendations,"  We introduce EigenRec; a versatile and efficient Latent-Factor framework for Top-N Recommendations that includes the well-known PureSVD algorithm as a special case. EigenRec builds a low dimensional model of an inter-item proximity matrix that combines a similarity component, with a scaling operator, designed to control the influence of the prior item popularity on the final model. Seeing PureSVD within our framework provides intuition about its inner workings, exposes its inherent limitations, and also, paves the path towards painlessly improving its recommendation performance. A comprehensive set of experiments on the MovieLens and the Yahoo datasets based on widely applied performance metrics, indicate that EigenRec outperforms several state-of-the-art algorithms, in terms of Standard and Long-Tail recommendation accuracy, exhibiting low susceptibility to sparsity, even in its most extreme manifestations -- the Cold-Start problems. At the same time EigenRec has an attractive computational profile and it can apply readily in large-scale recommendation settings. ","Computer Science - Information Retrieval ; Computer Science - Databases ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Numerical Analysis ; Computer Science - Social and Information Networks ; H.3.3 ; H.2.8 ; G.1.3 ; ","Nikolakopoulos, Athanasios N. ; Kalantzis, Vassilis ; Gallopoulos, Efstratios ; Garofalakis, John D. ; ","EigenRec: Generalizing PureSVD for Effective and Efficient Top-N   Recommendations  We introduce EigenRec; a versatile and efficient Latent-Factor framework for Top-N Recommendations that includes the well-known PureSVD algorithm as a special case. EigenRec builds a low dimensional model of an inter-item proximity matrix that combines a similarity component, with a scaling operator, designed to control the influence of the prior item popularity on the final model. Seeing PureSVD within our framework provides intuition about its inner workings, exposes its inherent limitations, and also, paves the path towards painlessly improving its recommendation performance. A comprehensive set of experiments on the MovieLens and the Yahoo datasets based on widely applied performance metrics, indicate that EigenRec outperforms several state-of-the-art algorithms, in terms of Standard and Long-Tail recommendation accuracy, exhibiting low susceptibility to sparsity, even in its most extreme manifestations -- the Cold-Start problems. At the same time EigenRec has an attractive computational profile and it can apply readily in large-scale recommendation settings. ",eigenrec generalize puresvd effective efficient top recommendations introduce eigenrec versatile efficient latent factor framework top recommendations include well know puresvd algorithm special case eigenrec build low dimensional model inter item proximity matrix combine similarity component scale operator design control influence prior item popularity final model see puresvd within framework provide intuition inner work expose inherent limitations also pave path towards painlessly improve recommendation performance comprehensive set experiment movielens yahoo datasets base widely apply performance metrics indicate eigenrec outperform several state art algorithms term standard long tail recommendation accuracy exhibit low susceptibility sparsity even extreme manifestations cold start problems time eigenrec attractive computational profile apply readily large scale recommendation settings,109,9,1511.06033.txt
http://arxiv.org/abs/1511.06324,Global Convergence of ADMM in Nonconvex Nonsmooth Optimization,"  In this paper, we analyze the convergence of the alternating direction method of multipliers (ADMM) for minimizing a nonconvex and possibly nonsmooth objective function, $\phi(x_0,\ldots,x_p,y)$, subject to coupled linear equality constraints. Our ADMM updates each of the primal variables $x_0,\ldots,x_p,y$, followed by updating the dual variable. We separate the variable $y$ from $x_i$'s as it has a special role in our analysis.   The developed convergence guarantee covers a variety of nonconvex functions such as piecewise linear functions, $\ell_q$ quasi-norm, Schatten-$q$ quasi-norm ($0<q<1$), minimax concave penalty (MCP), and smoothly clipped absolute deviation (SCAD) penalty. It also allows nonconvex constraints such as compact manifolds (e.g., spherical, Stiefel, and Grassman manifolds) and linear complementarity constraints. Also, the $x_0$-block can be almost any lower semi-continuous function.   By applying our analysis, we show, for the first time, that several ADMM algorithms applied to solve nonconvex models in statistical learning, optimization on manifold, and matrix decomposition are guaranteed to converge.   Our results provide sufficient conditions for ADMM to converge on (convex or nonconvex) monotropic programs with three or more blocks, as they are special cases of our model.   ADMM has been regarded as a variant to the augmented Lagrangian method (ALM). We present a simple example to illustrate how ADMM converges but ALM diverges with bounded penalty parameter $\beta$. Indicated by this example and other analysis in this paper, ADMM might be a better choice than ALM for some nonconvex \emph{nonsmooth} problems, because ADMM is not only easier to implement, it is also more likely to converge for the concerned scenarios. ",Mathematics - Optimization and Control ; Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; ,"Wang, Yu ; Yin, Wotao ; Zeng, Jinshan ; ","Global Convergence of ADMM in Nonconvex Nonsmooth Optimization  In this paper, we analyze the convergence of the alternating direction method of multipliers (ADMM) for minimizing a nonconvex and possibly nonsmooth objective function, $\phi(x_0,\ldots,x_p,y)$, subject to coupled linear equality constraints. Our ADMM updates each of the primal variables $x_0,\ldots,x_p,y$, followed by updating the dual variable. We separate the variable $y$ from $x_i$'s as it has a special role in our analysis.   The developed convergence guarantee covers a variety of nonconvex functions such as piecewise linear functions, $\ell_q$ quasi-norm, Schatten-$q$ quasi-norm ($0<q<1$), minimax concave penalty (MCP), and smoothly clipped absolute deviation (SCAD) penalty. It also allows nonconvex constraints such as compact manifolds (e.g., spherical, Stiefel, and Grassman manifolds) and linear complementarity constraints. Also, the $x_0$-block can be almost any lower semi-continuous function.   By applying our analysis, we show, for the first time, that several ADMM algorithms applied to solve nonconvex models in statistical learning, optimization on manifold, and matrix decomposition are guaranteed to converge.   Our results provide sufficient conditions for ADMM to converge on (convex or nonconvex) monotropic programs with three or more blocks, as they are special cases of our model.   ADMM has been regarded as a variant to the augmented Lagrangian method (ALM). We present a simple example to illustrate how ADMM converges but ALM diverges with bounded penalty parameter $\beta$. Indicated by this example and other analysis in this paper, ADMM might be a better choice than ALM for some nonconvex \emph{nonsmooth} problems, because ADMM is not only easier to implement, it is also more likely to converge for the concerned scenarios. ",global convergence admm nonconvex nonsmooth optimization paper analyze convergence alternate direction method multipliers admm minimize nonconvex possibly nonsmooth objective function phi ldots subject couple linear equality constraints admm update primal variables ldots follow update dual variable separate variable special role analysis develop convergence guarantee cover variety nonconvex function piecewise linear function ell quasi norm schatten quasi norm minimax concave penalty mcp smoothly clip absolute deviation scad penalty also allow nonconvex constraints compact manifold spherical stiefel grassman manifold linear complementarity constraints also block almost lower semi continuous function apply analysis show first time several admm algorithms apply solve nonconvex model statistical learn optimization manifold matrix decomposition guarantee converge result provide sufficient condition admm converge convex nonconvex monotropic program three block special case model admm regard variant augment lagrangian method alm present simple example illustrate admm converge alm diverge bound penalty parameter beta indicate example analysis paper admm might better choice alm nonconvex emph nonsmooth problems admm easier implement also likely converge concern scenarios,162,7,1511.06324.txt
http://arxiv.org/abs/1511.06382,Iterative Refinement of the Approximate Posterior for Directed Belief   Networks,"  Variational methods that rely on a recognition network to approximate the posterior of directed graphical models offer better inference and learning than previous methods. Recent advances that exploit the capacity and flexibility in this approach have expanded what kinds of models can be trained. However, as a proposal for the posterior, the capacity of the recognition network is limited, which can constrain the representational power of the generative model and increase the variance of Monte Carlo estimates. To address these issues, we introduce an iterative refinement procedure for improving the approximate posterior of the recognition network and show that training with the refined posterior is competitive with state-of-the-art methods. The advantages of refinement are further evident in an increased effective sample size, which implies a lower variance of gradient estimates. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Hjelm, R Devon ; Cho, Kyunghyun ; Chung, Junyoung ; Salakhutdinov, Russ ; Calhoun, Vince ; Jojic, Nebojsa ; ","Iterative Refinement of the Approximate Posterior for Directed Belief   Networks  Variational methods that rely on a recognition network to approximate the posterior of directed graphical models offer better inference and learning than previous methods. Recent advances that exploit the capacity and flexibility in this approach have expanded what kinds of models can be trained. However, as a proposal for the posterior, the capacity of the recognition network is limited, which can constrain the representational power of the generative model and increase the variance of Monte Carlo estimates. To address these issues, we introduce an iterative refinement procedure for improving the approximate posterior of the recognition network and show that training with the refined posterior is competitive with state-of-the-art methods. The advantages of refinement are further evident in an increased effective sample size, which implies a lower variance of gradient estimates. ",iterative refinement approximate posterior direct belief network variational methods rely recognition network approximate posterior direct graphical model offer better inference learn previous methods recent advance exploit capacity flexibility approach expand kinds model train however proposal posterior capacity recognition network limit constrain representational power generative model increase variance monte carlo estimate address issue introduce iterative refinement procedure improve approximate posterior recognition network show train refine posterior competitive state art methods advantage refinement evident increase effective sample size imply lower variance gradient estimate,81,6,1511.06382.txt
http://arxiv.org/abs/1511.06436,"On the robust hardness of Gr\""obner basis computation","  The computation of Gr\""obner bases is an established hard problem. By contrast with many other problems, however, there has been little investigation of whether this hardness is robust. In this paper, we frame and present results on the problem of approximate computation of Gr\""obner bases. We show that it is NP-hard to construct a Gr\""obner basis of the ideal generated by a set of polynomials, even when the algorithm is allowed to discard a $(1 - \epsilon)$ fraction of the generators, and likewise when the algorithm is allowed to discard variables (and the generators containing them). Our results shows that computation of Gr\""obner bases is robustly hard even for simple polynomial systems (e.g. maximum degree 2, with at most 3 variables per generator). We conclude by greatly strengthening results for the Strong $c$-Partial Gr\""obner problem posed by De Loera et al. Our proofs also establish interesting connections between the robust hardness of Gr\""obner bases and that of SAT variants and graph-coloring. ",Computer Science - Symbolic Computation ; ,"Spencer, Gwen ; Rolnick, David ; ","On the robust hardness of Gr\""obner basis computation  The computation of Gr\""obner bases is an established hard problem. By contrast with many other problems, however, there has been little investigation of whether this hardness is robust. In this paper, we frame and present results on the problem of approximate computation of Gr\""obner bases. We show that it is NP-hard to construct a Gr\""obner basis of the ideal generated by a set of polynomials, even when the algorithm is allowed to discard a $(1 - \epsilon)$ fraction of the generators, and likewise when the algorithm is allowed to discard variables (and the generators containing them). Our results shows that computation of Gr\""obner bases is robustly hard even for simple polynomial systems (e.g. maximum degree 2, with at most 3 variables per generator). We conclude by greatly strengthening results for the Strong $c$-Partial Gr\""obner problem posed by De Loera et al. Our proofs also establish interesting connections between the robust hardness of Gr\""obner bases and that of SAT variants and graph-coloring. ",robust hardness gr obner basis computation computation gr obner base establish hard problem contrast many problems however little investigation whether hardness robust paper frame present result problem approximate computation gr obner base show np hard construct gr obner basis ideal generate set polynomials even algorithm allow discard epsilon fraction generators likewise algorithm allow discard variables generators contain result show computation gr obner base robustly hard even simple polynomial systems maximum degree variables per generator conclude greatly strengthen result strong partial gr obner problem pose de loera et al proof also establish interest connections robust hardness gr obner base sit variants graph color,102,4,1511.06436.txt
http://arxiv.org/abs/1511.06444,Universal halting times in optimization and machine learning,"  The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that, after centering and scaling, remains unchanged even when the distribution on the landscape is changed. We observe two qualitative classes: A Gumbel-like distribution that appears in Google searches, human decision times, the QR eigenvalue algorithm and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions. ","Computer Science - Machine Learning ; Mathematics - Numerical Analysis ; Mathematics - Probability ; 65K10, 82D30, 37E20 ; ","Sagun, Levent ; Trogdon, Thomas ; LeCun, Yann ; ","Universal halting times in optimization and machine learning  The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that, after centering and scaling, remains unchanged even when the distribution on the landscape is changed. We observe two qualitative classes: A Gumbel-like distribution that appears in Google searches, human decision times, the QR eigenvalue algorithm and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions. ",universal halt time optimization machine learn author present empirical distributions halt time measure number iterations reach give accuracy optimization algorithms apply two random systems spin glass deep learn give algorithm take optimization routine form random landscape fluctuations halt time follow distribution center scale remain unchanged even distribution landscape change observe two qualitative class gumbel like distribution appear google search human decision time qr eigenvalue algorithm spin glass gaussian like distribution appear conjugate gradient method deep network mnist input data deep network random input data empirical evidence suggest presence class distributions halt time independent underlie distribution condition,96,6,1511.06444.txt
http://arxiv.org/abs/1511.06489,A Simple Hierarchical Pooling Data Structure for Loop Closure,"  We propose a data structure obtained by hierarchically averaging bag-of-word descriptors during a sequence of views that achieves average speedups in large-scale loop closure applications ranging from 4 to 20 times on benchmark datasets. Although simple, the method works as well as sophisticated agglomerative schemes at a fraction of the cost with minimal loss of performance. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Robotics ; ,"Fei, Xiaohan ; Tsotsos, Konstantine ; Soatto, Stefano ; ","A Simple Hierarchical Pooling Data Structure for Loop Closure  We propose a data structure obtained by hierarchically averaging bag-of-word descriptors during a sequence of views that achieves average speedups in large-scale loop closure applications ranging from 4 to 20 times on benchmark datasets. Although simple, the method works as well as sophisticated agglomerative schemes at a fraction of the cost with minimal loss of performance. ",simple hierarchical pool data structure loop closure propose data structure obtain hierarchically average bag word descriptors sequence view achieve average speedups large scale loop closure applications range time benchmark datasets although simple method work well sophisticate agglomerative scheme fraction cost minimal loss performance,43,11,1511.06489.txt
http://arxiv.org/abs/1511.06653,Recurrent Semi-supervised Classification and Constrained Adversarial   Generation with Motion Capture Data,"  We explore recurrent encoder multi-decoder neural network architectures for semi-supervised sequence classification and reconstruction. We find that the use of multiple reconstruction modules helps models generalize in a classification task when only a small amount of labeled data is available, which is often the case in practice. Such models provide useful high-level representations of motions allowing clustering, searching and faster labeling of new sequences. We also propose a new, realistic partitioning of a well-known, high quality motion-capture dataset for better evaluations. We further explore a novel formulation for future-predicting decoders based on conditional recurrent generative adversarial networks, for which we propose both soft and hard constraints for transition generation derived from desired physical properties of synthesized future movements and desired animation goals. We find that using such constraints allow to stabilize the training of recurrent adversarial architectures for animation generation. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Machine Learning ; ,"Harvey, Félix G. ; Roy, Julien ; Kanaa, David ; Pal, Christopher ; ","Recurrent Semi-supervised Classification and Constrained Adversarial   Generation with Motion Capture Data  We explore recurrent encoder multi-decoder neural network architectures for semi-supervised sequence classification and reconstruction. We find that the use of multiple reconstruction modules helps models generalize in a classification task when only a small amount of labeled data is available, which is often the case in practice. Such models provide useful high-level representations of motions allowing clustering, searching and faster labeling of new sequences. We also propose a new, realistic partitioning of a well-known, high quality motion-capture dataset for better evaluations. We further explore a novel formulation for future-predicting decoders based on conditional recurrent generative adversarial networks, for which we propose both soft and hard constraints for transition generation derived from desired physical properties of synthesized future movements and desired animation goals. We find that using such constraints allow to stabilize the training of recurrent adversarial architectures for animation generation. ",recurrent semi supervise classification constrain adversarial generation motion capture data explore recurrent encoder multi decoder neural network architectures semi supervise sequence classification reconstruction find use multiple reconstruction modules help model generalize classification task small amount label data available often case practice model provide useful high level representations motion allow cluster search faster label new sequence also propose new realistic partition well know high quality motion capture dataset better evaluations explore novel formulation future predict decoders base conditional recurrent generative adversarial network propose soft hard constraints transition generation derive desire physical properties synthesize future movements desire animation goals find use constraints allow stabilize train recurrent adversarial architectures animation generation,108,6,1511.06653.txt
http://arxiv.org/abs/1511.06860,Convex Sparse Spectral Clustering: Single-view to Multi-view,"  Spectral Clustering (SC) is one of the most widely used methods for data clustering. It first finds a low-dimensonal embedding $U$ of data by computing the eigenvectors of the normalized Laplacian matrix, and then performs k-means on $U^\top$ to get the final clustering result. In this work, we observe that, in the ideal case, $UU^\top$ should be block diagonal and thus sparse. Therefore we propose the Sparse Spectral Clustering (SSC) method which extends SC with sparse regularization on $UU^\top$. To address the computational issue of the nonconvex SSC model, we propose a novel convex relaxation of SSC based on the convex hull of the fixed rank projection matrices. Then the convex SSC model can be efficiently solved by the Alternating Direction Method of \canyi{Multipliers} (ADMM). Furthermore, we propose the Pairwise Sparse Spectral Clustering (PSSC) which extends SSC to boost the clustering performance by using the multi-view information of data. Experimental comparisons with several baselines on real-world datasets testify to the efficacy of our proposed methods. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Lu, Canyi ; Yan, Shuicheng ; Lin, Zhouchen ; ","Convex Sparse Spectral Clustering: Single-view to Multi-view  Spectral Clustering (SC) is one of the most widely used methods for data clustering. It first finds a low-dimensonal embedding $U$ of data by computing the eigenvectors of the normalized Laplacian matrix, and then performs k-means on $U^\top$ to get the final clustering result. In this work, we observe that, in the ideal case, $UU^\top$ should be block diagonal and thus sparse. Therefore we propose the Sparse Spectral Clustering (SSC) method which extends SC with sparse regularization on $UU^\top$. To address the computational issue of the nonconvex SSC model, we propose a novel convex relaxation of SSC based on the convex hull of the fixed rank projection matrices. Then the convex SSC model can be efficiently solved by the Alternating Direction Method of \canyi{Multipliers} (ADMM). Furthermore, we propose the Pairwise Sparse Spectral Clustering (PSSC) which extends SSC to boost the clustering performance by using the multi-view information of data. Experimental comparisons with several baselines on real-world datasets testify to the efficacy of our proposed methods. ",convex sparse spectral cluster single view multi view spectral cluster sc one widely use methods data cluster first find low dimensonal embed data compute eigenvectors normalize laplacian matrix perform mean top get final cluster result work observe ideal case uu top block diagonal thus sparse therefore propose sparse spectral cluster ssc method extend sc sparse regularization uu top address computational issue nonconvex ssc model propose novel convex relaxation ssc base convex hull fix rank projection matrices convex ssc model efficiently solve alternate direction method canyi multipliers admm furthermore propose pairwise sparse spectral cluster pssc extend ssc boost cluster performance use multi view information data experimental comparisons several baselines real world datasets testify efficacy propose methods,115,9,1511.06860.txt
http://arxiv.org/abs/1511.06971,A General Framework for the Design and Analysis of Sparse FIR Linear   Equalizers,"  Complexity of linear finite-impulse-response (FIR) equalizers is proportional to the square of the number of nonzero taps in the filter. This makes equalization of channels with long impulse responses using either zero-forcing or minimum mean square error (MMSE) filters computationally expensive. Sparse equalization is a widely-used technique to solve this problem. In this paper, a general framework is provided that transforms the problem of sparse linear equalizers (LEs) design into the problem of sparsest-approximation of a vector in different dictionaries. In addition, some possible choices of sparsifying dictionaries in this framework are discussed. Furthermore, the worst-case coherence of some of these dictionaries, which determines their sparsifying strength, are analytically and/or numerically evaluated. Finally, the usefulness of the proposed framework for the design of sparse FIR LEs is validated through numerical experiments. ",Computer Science - Information Theory ; ,"Al-Abbasi, Abubakr O. ; Hamila, Ridha ; Bajwa, Waheed U. ; Al-Dhahir, Naofal ; ","A General Framework for the Design and Analysis of Sparse FIR Linear   Equalizers  Complexity of linear finite-impulse-response (FIR) equalizers is proportional to the square of the number of nonzero taps in the filter. This makes equalization of channels with long impulse responses using either zero-forcing or minimum mean square error (MMSE) filters computationally expensive. Sparse equalization is a widely-used technique to solve this problem. In this paper, a general framework is provided that transforms the problem of sparse linear equalizers (LEs) design into the problem of sparsest-approximation of a vector in different dictionaries. In addition, some possible choices of sparsifying dictionaries in this framework are discussed. Furthermore, the worst-case coherence of some of these dictionaries, which determines their sparsifying strength, are analytically and/or numerically evaluated. Finally, the usefulness of the proposed framework for the design of sparse FIR LEs is validated through numerical experiments. ",general framework design analysis sparse fir linear equalizers complexity linear finite impulse response fir equalizers proportional square number nonzero tap filter make equalization channel long impulse responses use either zero force minimum mean square error mmse filter computationally expensive sparse equalization widely use technique solve problem paper general framework provide transform problem sparse linear equalizers les design problem sparsest approximation vector different dictionaries addition possible choices sparsifying dictionaries framework discuss furthermore worst case coherence dictionaries determine sparsifying strength analytically numerically evaluate finally usefulness propose framework design sparse fir les validate numerical experiment,92,9,1511.06971.txt
http://arxiv.org/abs/1511.07174,Developing a High Performance Software Library with MPI and CUDA for   Matrix Computations,"  Nowadays, the paradigm of parallel computing is changing. CUDA is now a popular programming model for general purpose computations on GPUs and a great number of applications were ported to CUDA obtaining speedups of orders of magnitude comparing to optimized CPU implementations. Hybrid approaches that combine the message passing model with the shared memory model for parallel computing are a solution for very large applications. We considered a heterogeneous cluster that combines the CPU and GPU computations using MPI and CUDA for developing a high performance linear algebra library. Our library deals with large linear systems solvers because they are a common problem in the fields of science and engineering. Direct methods for computing the solution of such systems can be very expensive due to high memory requirements and computational cost. An efficient alternative are iterative methods which computes only an approximation of the solution. In this paper we present an implementation of a library that uses a hybrid model of computation using MPI and CUDA implementing both direct and iterative linear systems solvers. Our library implements LU and Cholesky factorization based solvers and some of the non-stationary iterative methods using the MPI/CUDA combination. We compared the performance of our MPI/CUDA implementation with classic programs written to be run on a single CPU. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Mathematical Software ; ","Oancea, Bogdan ; Andrei, Tudorel ; ","Developing a High Performance Software Library with MPI and CUDA for   Matrix Computations  Nowadays, the paradigm of parallel computing is changing. CUDA is now a popular programming model for general purpose computations on GPUs and a great number of applications were ported to CUDA obtaining speedups of orders of magnitude comparing to optimized CPU implementations. Hybrid approaches that combine the message passing model with the shared memory model for parallel computing are a solution for very large applications. We considered a heterogeneous cluster that combines the CPU and GPU computations using MPI and CUDA for developing a high performance linear algebra library. Our library deals with large linear systems solvers because they are a common problem in the fields of science and engineering. Direct methods for computing the solution of such systems can be very expensive due to high memory requirements and computational cost. An efficient alternative are iterative methods which computes only an approximation of the solution. In this paper we present an implementation of a library that uses a hybrid model of computation using MPI and CUDA implementing both direct and iterative linear systems solvers. Our library implements LU and Cholesky factorization based solvers and some of the non-stationary iterative methods using the MPI/CUDA combination. We compared the performance of our MPI/CUDA implementation with classic programs written to be run on a single CPU. ",develop high performance software library mpi cuda matrix computations nowadays paradigm parallel compute change cuda popular program model general purpose computations gpus great number applications port cuda obtain speedups order magnitude compare optimize cpu implementations hybrid approach combine message pass model share memory model parallel compute solution large applications consider heterogeneous cluster combine cpu gpu computations use mpi cuda develop high performance linear algebra library library deal large linear systems solvers common problem field science engineer direct methods compute solution systems expensive due high memory requirements computational cost efficient alternative iterative methods compute approximation solution paper present implementation library use hybrid model computation use mpi cuda implement direct iterative linear systems solvers library implement lu cholesky factorization base solvers non stationary iterative methods use mpi cuda combination compare performance mpi cuda implementation classic program write run single cpu,138,4,1511.07174.txt
http://arxiv.org/abs/1511.07212,Face Alignment Across Large Poses: A 3D Solution,"  Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in CV community. However, most algorithms are designed for faces in small to medium poses (below 45 degree), lacking the ability to align faces in large poses up to 90 degree. The challenges are three-fold: Firstly, the commonly used landmark-based face model assumes that all the landmarks are visible and is therefore not suitable for profile views. Secondly, the face appearance varies more dramatically across large poses, ranging from frontal view to profile view. Thirdly, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose a solution to the three problems in an new alignment framework, called 3D Dense Face Alignment (3DDFA), in which a dense 3D face model is fitted to the image via convolutional neutral network (CNN). We also propose a method to synthesize large-scale training samples in profile views to solve the third problem of data labelling. Experiments on the challenging AFLW database show that our approach achieves significant improvements over state-of-the-art methods. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Zhu, Xiangyu ; Lei, Zhen ; Liu, Xiaoming ; Shi, Hailin ; Li, Stan Z. ; ","Face Alignment Across Large Poses: A 3D Solution  Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in CV community. However, most algorithms are designed for faces in small to medium poses (below 45 degree), lacking the ability to align faces in large poses up to 90 degree. The challenges are three-fold: Firstly, the commonly used landmark-based face model assumes that all the landmarks are visible and is therefore not suitable for profile views. Secondly, the face appearance varies more dramatically across large poses, ranging from frontal view to profile view. Thirdly, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose a solution to the three problems in an new alignment framework, called 3D Dense Face Alignment (3DDFA), in which a dense 3D face model is fitted to the image via convolutional neutral network (CNN). We also propose a method to synthesize large-scale training samples in profile views to solve the third problem of data labelling. Experiments on the challenging AFLW database show that our approach achieves significant improvements over state-of-the-art methods. ",face alignment across large pose solution face alignment fit face model image extract semantic mean facial pixels important topic cv community however algorithms design face small medium pose degree lack ability align face large pose degree challenge three fold firstly commonly use landmark base face model assume landmarks visible therefore suitable profile view secondly face appearance vary dramatically across large pose range frontal view profile view thirdly label landmarks large pose extremely challenge since invisible landmarks guess paper propose solution three problems new alignment framework call dense face alignment ddfa dense face model fit image via convolutional neutral network cnn also propose method synthesize large scale train sample profile view solve third problem data label experiment challenge aflw database show approach achieve significant improvements state art methods,127,11,1511.07212.txt
http://arxiv.org/abs/1511.07860,Super-Linear Gate and Super-Quadratic Wire Lower Bounds for Depth-Two   and Depth-Three Threshold Circuits,"  In order to formally understand the power of neural computing, we first need to crack the frontier of threshold circuits with two and three layers, a regime that has been surprisingly intractable to analyze. We prove the first super-linear gate lower bounds and the first super-quadratic wire lower bounds for depth-two linear threshold circuits with arbitrary weights, and depth-three majority circuits computing an explicit function.   $\bullet$ We prove that for all $\epsilon\gg \sqrt{\log(n)/n}$, the linear-time computable Andreev's function cannot be computed on a $(1/2+\epsilon)$-fraction of $n$-bit inputs by depth-two linear threshold circuits of $o(\epsilon^3 n^{3/2}/\log^3 n)$ gates, nor can it be computed with $o(\epsilon^{3} n^{5/2}/\log^{7/2} n)$ wires. This establishes an average-case ``size hierarchy'' for threshold circuits, as Andreev's function is computable by uniform depth-two circuits of $o(n^3)$ linear threshold gates, and by uniform depth-three circuits of $O(n)$ majority gates.   $\bullet$ We present a new function in $P$ based on small-biased sets, which we prove cannot be computed by a majority vote of depth-two linear threshold circuits with $o(n^{3/2}/\log^3 n)$ gates, nor with $o(n^{5/2}/\log^{7/2}n)$ wires.   $\bullet$ We give tight average-case (gate and wire) complexity results for computing PARITY with depth-two threshold circuits; the answer turns out to be the same as for depth-two majority circuits.   The key is a new random restriction lemma for linear threshold functions. Our main analytical tool is the Littlewood-Offord Lemma from additive combinatorics. ",Computer Science - Computational Complexity ; Computer Science - Neural and Evolutionary Computing ; 68Q17 ; C.1.3 ; F.1.3 ; ,"Kane, Daniel M. ; Williams, Ryan ; ","Super-Linear Gate and Super-Quadratic Wire Lower Bounds for Depth-Two   and Depth-Three Threshold Circuits  In order to formally understand the power of neural computing, we first need to crack the frontier of threshold circuits with two and three layers, a regime that has been surprisingly intractable to analyze. We prove the first super-linear gate lower bounds and the first super-quadratic wire lower bounds for depth-two linear threshold circuits with arbitrary weights, and depth-three majority circuits computing an explicit function.   $\bullet$ We prove that for all $\epsilon\gg \sqrt{\log(n)/n}$, the linear-time computable Andreev's function cannot be computed on a $(1/2+\epsilon)$-fraction of $n$-bit inputs by depth-two linear threshold circuits of $o(\epsilon^3 n^{3/2}/\log^3 n)$ gates, nor can it be computed with $o(\epsilon^{3} n^{5/2}/\log^{7/2} n)$ wires. This establishes an average-case ``size hierarchy'' for threshold circuits, as Andreev's function is computable by uniform depth-two circuits of $o(n^3)$ linear threshold gates, and by uniform depth-three circuits of $O(n)$ majority gates.   $\bullet$ We present a new function in $P$ based on small-biased sets, which we prove cannot be computed by a majority vote of depth-two linear threshold circuits with $o(n^{3/2}/\log^3 n)$ gates, nor with $o(n^{5/2}/\log^{7/2}n)$ wires.   $\bullet$ We give tight average-case (gate and wire) complexity results for computing PARITY with depth-two threshold circuits; the answer turns out to be the same as for depth-two majority circuits.   The key is a new random restriction lemma for linear threshold functions. Our main analytical tool is the Littlewood-Offord Lemma from additive combinatorics. ",super linear gate super quadratic wire lower bound depth two depth three threshold circuit order formally understand power neural compute first need crack frontier threshold circuit two three layer regime surprisingly intractable analyze prove first super linear gate lower bound first super quadratic wire lower bound depth two linear threshold circuit arbitrary weight depth three majority circuit compute explicit function bullet prove epsilon gg sqrt log linear time computable andreev function cannot compute epsilon fraction bite input depth two linear threshold circuit epsilon log gate compute epsilon log wire establish average case size hierarchy threshold circuit andreev function computable uniform depth two circuit linear threshold gate uniform depth three circuit majority gate bullet present new function base small bias set prove cannot compute majority vote depth two linear threshold circuit log gate log wire bullet give tight average case gate wire complexity result compute parity depth two threshold circuit answer turn depth two majority circuit key new random restriction lemma linear threshold function main analytical tool littlewood offord lemma additive combinatorics,171,1,1511.07860.txt
http://arxiv.org/abs/1511.08152,Max-Cut under Graph Constraints,"  An instance of the graph-constrained max-cut (GCMC) problem consists of (i) an undirected graph G and (ii) edge-weights on a complete undirected graph on the same vertex set. The objective is to find a subset of vertices satisfying some graph-based constraint in G that maximizes the total weight of edges in the cut. The types of graph constraints we can handle include independent set, vertex cover, dominating set and connectivity. Our main results are for the case when G is a graph with bounded treewidth, where we obtain a 0.5-approximation algorithm. Our algorithm uses an LP relaxation based on the Sherali-Adams hierarchy. It can handle any graph constraint for which there is a (certain type of) dynamic program that exactly optimizes linear objectives. Using known decomposition results, these imply essentially the same approximation ratio for GCMC under constraints such as independent set, dominating set and connectivity on a planar graph G (more generally for bounded-genus or excluded-minor graphs). ",Computer Science - Data Structures and Algorithms ; ,"Lee, Jon ; Nagarajan, Viswanath ; Shen, Xiangkun ; ","Max-Cut under Graph Constraints  An instance of the graph-constrained max-cut (GCMC) problem consists of (i) an undirected graph G and (ii) edge-weights on a complete undirected graph on the same vertex set. The objective is to find a subset of vertices satisfying some graph-based constraint in G that maximizes the total weight of edges in the cut. The types of graph constraints we can handle include independent set, vertex cover, dominating set and connectivity. Our main results are for the case when G is a graph with bounded treewidth, where we obtain a 0.5-approximation algorithm. Our algorithm uses an LP relaxation based on the Sherali-Adams hierarchy. It can handle any graph constraint for which there is a (certain type of) dynamic program that exactly optimizes linear objectives. Using known decomposition results, these imply essentially the same approximation ratio for GCMC under constraints such as independent set, dominating set and connectivity on a planar graph G (more generally for bounded-genus or excluded-minor graphs). ",max cut graph constraints instance graph constrain max cut gcmc problem consist undirected graph ii edge weight complete undirected graph vertex set objective find subset vertices satisfy graph base constraint maximize total weight edge cut type graph constraints handle include independent set vertex cover dominate set connectivity main result case graph bound treewidth obtain approximation algorithm algorithm use lp relaxation base sherali adams hierarchy handle graph constraint certain type dynamic program exactly optimize linear objectives use know decomposition result imply essentially approximation ratio gcmc constraints independent set dominate set connectivity planar graph generally bound genus exclude minor graph,98,3,1511.08152.txt
http://arxiv.org/abs/1511.08189,Graph Isomorphism and Circuit Size,"  It is well-known [KST93] that the complexity of the Graph Automorphism problem is characterized by a special case of Graph Isomorphism, where the input graphs satisfy the ""promise"" of being rigid (that is, having no nontrivial automorphisms). In this brief note, we observe that the reduction of Graph Automorphism to the Rigid Graph Ismorphism problem can be accomplished even using Grollman and Selman's notion of a ""smart reduction"". ",Computer Science - Computational Complexity ; ,"Allender, Eric ; Grochow, Joshua A. ; van Melkebeek, Dieter ; Moore, Cristopher ; Morgan, Andrew ; ","Graph Isomorphism and Circuit Size  It is well-known [KST93] that the complexity of the Graph Automorphism problem is characterized by a special case of Graph Isomorphism, where the input graphs satisfy the ""promise"" of being rigid (that is, having no nontrivial automorphisms). In this brief note, we observe that the reduction of Graph Automorphism to the Rigid Graph Ismorphism problem can be accomplished even using Grollman and Selman's notion of a ""smart reduction"". ",graph isomorphism circuit size well know kst complexity graph automorphism problem characterize special case graph isomorphism input graph satisfy promise rigid nontrivial automorphisms brief note observe reduction graph automorphism rigid graph ismorphism problem accomplish even use grollman selman notion smart reduction,41,3,1511.08189.txt
http://arxiv.org/abs/1511.08280,Welfare of Sequential Allocation Mechanisms for Indivisible Goods,"  Sequential allocation is a simple and attractive mechanism for the allocation of indivisible goods. Agents take turns, according to a policy, to pick items. Sequential allocation is guaranteed to return an allocation which is efficient but may not have an optimal social welfare. We consider therefore the relation between welfare and efficiency. We study the (computational) questions of what welfare is possible or necessary depending on the choice of policy. We also consider a novel control problem in which the chair chooses a policy to improve social welfare. ",Computer Science - Artificial Intelligence ; Computer Science - Computer Science and Game Theory ; ,"Aziz, Haris ; Kalinowski, Thomas ; Walsh, Toby ; Xia, Lirong ; ","Welfare of Sequential Allocation Mechanisms for Indivisible Goods  Sequential allocation is a simple and attractive mechanism for the allocation of indivisible goods. Agents take turns, according to a policy, to pick items. Sequential allocation is guaranteed to return an allocation which is efficient but may not have an optimal social welfare. We consider therefore the relation between welfare and efficiency. We study the (computational) questions of what welfare is possible or necessary depending on the choice of policy. We also consider a novel control problem in which the chair chooses a policy to improve social welfare. ",welfare sequential allocation mechanisms indivisible goods sequential allocation simple attractive mechanism allocation indivisible goods agents take turn accord policy pick items sequential allocation guarantee return allocation efficient may optimal social welfare consider therefore relation welfare efficiency study computational question welfare possible necessary depend choice policy also consider novel control problem chair choose policy improve social welfare,56,0,1511.08280.txt
http://arxiv.org/abs/1511.08416,Who Can Win a Single-Elimination Tournament?,"  A single-elimination (SE) tournament is a popular way to select a winner in both sports competitions and in elections. A natural and well-studied question is the tournament fixing problem (TFP): given the set of all pairwise match outcomes, can a tournament organizer rig an SE tournament by adjusting the initial seeding so that their favorite player wins? We prove new sufficient conditions on the pairwise match outcome information and the favorite player, under which there is guaranteed to be a seeding where the player wins the tournament. Our results greatly generalize previous results. We also investigate the relationship between the set of players that can win an SE tournament under some seeding (so called SE winners) and other traditional tournament solutions. In addition, we generalize and strengthen prior work on probabilistic models for generating tournaments. For instance, we show that \emph{every} player in an $n$ player tournament generated by the Condorcet Random Model will be an SE winner even when the noise is as small as possible, $p=\Theta(\ln n/n)$; prior work only had such results for $p\geq \Omega(\sqrt{\ln n/n})$. We also establish new results for significantly more general generative models. ",Computer Science - Computer Science and Game Theory ; ,"Kim, Michael P. ; Suksompong, Warut ; Williams, Virginia Vassilevska ; ","Who Can Win a Single-Elimination Tournament?  A single-elimination (SE) tournament is a popular way to select a winner in both sports competitions and in elections. A natural and well-studied question is the tournament fixing problem (TFP): given the set of all pairwise match outcomes, can a tournament organizer rig an SE tournament by adjusting the initial seeding so that their favorite player wins? We prove new sufficient conditions on the pairwise match outcome information and the favorite player, under which there is guaranteed to be a seeding where the player wins the tournament. Our results greatly generalize previous results. We also investigate the relationship between the set of players that can win an SE tournament under some seeding (so called SE winners) and other traditional tournament solutions. In addition, we generalize and strengthen prior work on probabilistic models for generating tournaments. For instance, we show that \emph{every} player in an $n$ player tournament generated by the Condorcet Random Model will be an SE winner even when the noise is as small as possible, $p=\Theta(\ln n/n)$; prior work only had such results for $p\geq \Omega(\sqrt{\ln n/n})$. We also establish new results for significantly more general generative models. ",win single elimination tournament single elimination se tournament popular way select winner sport competitions elections natural well study question tournament fix problem tfp give set pairwise match outcomes tournament organizer rig se tournament adjust initial seed favorite player win prove new sufficient condition pairwise match outcome information favorite player guarantee seed player win tournament result greatly generalize previous result also investigate relationship set players win se tournament seed call se winners traditional tournament solutions addition generalize strengthen prior work probabilistic model generate tournaments instance show emph every player player tournament generate condorcet random model se winner even noise small possible theta ln prior work result geq omega sqrt ln also establish new result significantly general generative model,117,8,1511.08416.txt
http://arxiv.org/abs/1511.08575,A Modified Multiple OLS (m$^2$OLS) Algorithm for Signal Recovery in   Compressive Sensing,"  Orthogonal least square (OLS) is an important sparse signal recovery algorithm for compressive sensing, which enjoys superior probability of success over other well-known recovery algorithms under conditions of correlated measurement matrices. Multiple OLS (mOLS) is a recently proposed improved version of OLS which selects multiple candidates per iteration by generalizing the greedy selection principle used in OLS and enjoys faster convergence than OLS. In this paper, we present a refined version of the mOLS algorithm where at each step of the iteration, we first preselect a submatrix of the measurement matrix suitably and then apply the mOLS computations to the chosen submatrix. Since mOLS now works only on a submatrix and not on the overall matrix, computations reduce drastically. Convergence of the algorithm, however, requires ensuring passage of true candidates through the two stages of preselection and mOLS based selection successively. This paper presents convergence conditions for both noisy and noise free signal models. The proposed algorithm enjoys faster convergence properties similar to mOLS, at a much reduced computational complexity. ",Computer Science - Information Theory ; Statistics - Methodology ; ,"Mukhopadhyay, Samrat ; Satpathi, Siddhartha ; Chakraborty, Mrityunjoy ; ","A Modified Multiple OLS (m$^2$OLS) Algorithm for Signal Recovery in   Compressive Sensing  Orthogonal least square (OLS) is an important sparse signal recovery algorithm for compressive sensing, which enjoys superior probability of success over other well-known recovery algorithms under conditions of correlated measurement matrices. Multiple OLS (mOLS) is a recently proposed improved version of OLS which selects multiple candidates per iteration by generalizing the greedy selection principle used in OLS and enjoys faster convergence than OLS. In this paper, we present a refined version of the mOLS algorithm where at each step of the iteration, we first preselect a submatrix of the measurement matrix suitably and then apply the mOLS computations to the chosen submatrix. Since mOLS now works only on a submatrix and not on the overall matrix, computations reduce drastically. Convergence of the algorithm, however, requires ensuring passage of true candidates through the two stages of preselection and mOLS based selection successively. This paper presents convergence conditions for both noisy and noise free signal models. The proposed algorithm enjoys faster convergence properties similar to mOLS, at a much reduced computational complexity. ",modify multiple ols ols algorithm signal recovery compressive sense orthogonal least square ols important sparse signal recovery algorithm compressive sense enjoy superior probability success well know recovery algorithms condition correlate measurement matrices multiple ols mols recently propose improve version ols select multiple candidates per iteration generalize greedy selection principle use ols enjoy faster convergence ols paper present refine version mols algorithm step iteration first preselect submatrix measurement matrix suitably apply mols computations choose submatrix since mols work submatrix overall matrix computations reduce drastically convergence algorithm however require ensure passage true candidates two stag preselection mols base selection successively paper present convergence condition noisy noise free signal model propose algorithm enjoy faster convergence properties similar mols much reduce computational complexity,119,9,1511.08575.txt
http://arxiv.org/abs/1511.08585,Real-Time Residential-Side Joint Energy Storage Management and Load   Scheduling with Renewable Integration,"  We consider joint energy storage management and load scheduling at a residential site with integrated renewable generation. Assuming unknown arbitrary dynamics of renewable source, loads, and electricity price, we aim at optimizing the load scheduling and energy storage control simultaneously in order to minimize the overall system cost within a finite time period. Besides incorporating battery operational constraints and costs, we model each individual load task by its requested power intensity and service durations, as well as the maximum and average delay requirements. To tackle this finite time horizon stochastic problem, we propose a real-time scheduling and storage control solution by applying a sequence of modification and transformation to employ Lyapunov optimization that otherwise is not directly applicable. With our proposed algorithm, we show that the joint load scheduling and energy storage control can in fact be separated and sequentially determined. Furthermore, both scheduling and energy control decisions have closed-form solutions for simple implementation. Through analysis, we show that our proposed real-time algorithm has a bounded performance guarantee from the optimal T-slot look-ahead solution and is asymptotically equivalent to it as the battery capacity and time period goes to infinity. The effectiveness of joint load scheduling and energy storage control by our proposed algorithm is demonstrated through simulation as compared with alternative algorithms. ",Computer Science - Systems and Control ; ,"Li, Tianyi ; Dong, Min ; ","Real-Time Residential-Side Joint Energy Storage Management and Load   Scheduling with Renewable Integration  We consider joint energy storage management and load scheduling at a residential site with integrated renewable generation. Assuming unknown arbitrary dynamics of renewable source, loads, and electricity price, we aim at optimizing the load scheduling and energy storage control simultaneously in order to minimize the overall system cost within a finite time period. Besides incorporating battery operational constraints and costs, we model each individual load task by its requested power intensity and service durations, as well as the maximum and average delay requirements. To tackle this finite time horizon stochastic problem, we propose a real-time scheduling and storage control solution by applying a sequence of modification and transformation to employ Lyapunov optimization that otherwise is not directly applicable. With our proposed algorithm, we show that the joint load scheduling and energy storage control can in fact be separated and sequentially determined. Furthermore, both scheduling and energy control decisions have closed-form solutions for simple implementation. Through analysis, we show that our proposed real-time algorithm has a bounded performance guarantee from the optimal T-slot look-ahead solution and is asymptotically equivalent to it as the battery capacity and time period goes to infinity. The effectiveness of joint load scheduling and energy storage control by our proposed algorithm is demonstrated through simulation as compared with alternative algorithms. ",real time residential side joint energy storage management load schedule renewable integration consider joint energy storage management load schedule residential site integrate renewable generation assume unknown arbitrary dynamics renewable source load electricity price aim optimize load schedule energy storage control simultaneously order minimize overall system cost within finite time period besides incorporate battery operational constraints cost model individual load task request power intensity service durations well maximum average delay requirements tackle finite time horizon stochastic problem propose real time schedule storage control solution apply sequence modification transformation employ lyapunov optimization otherwise directly applicable propose algorithm show joint load schedule energy storage control fact separate sequentially determine furthermore schedule energy control decisions close form solutions simple implementation analysis show propose real time algorithm bound performance guarantee optimal slot look ahead solution asymptotically equivalent battery capacity time period go infinity effectiveness joint load schedule energy storage control propose algorithm demonstrate simulation compare alternative algorithms,152,11,1511.08585.txt
http://arxiv.org/abs/1511.08861,Loss Functions for Neural Networks for Image Processing,"  Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is L2. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Zhao, Hang ; Gallo, Orazio ; Frosio, Iuri ; Kautz, Jan ; ","Loss Functions for Neural Networks for Image Processing  Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is L2. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged. ",loss function neural network image process neural network become central several areas computer vision image process different architectures propose solve specific problems impact loss layer neural network however receive much attention context image process default virtually choice paper bring attention alternative choices image restoration particular show importance perceptually motivate losses result image evaluate human observer compare performance several losses propose novel differentiable error function show quality result improve significantly better loss function even network architecture leave unchanged,77,6,1511.08861.txt
http://arxiv.org/abs/1511.08887,On the Degrees of Freedom of the Symmetric Multi-Relay MIMO Y Channel,"  In this paper, we study the degrees of freedom (DoF) of the symmetric multi-relay multiple-input multiple-output (MIMO) Y channel, where three user nodes, each with M antennas, communicate via K geographically separated relay nodes, each with N antennas. For this model, we establish a general DoF achievability framework based on linear precoding and post-processing methods. The framework poses a nonlinear problem with respect to user precoders and post-processors, as well as relay precoders. To solve this problem, we adopt an uplink-downlink asymmetric strategy, where the user precoders are designed for signal alignment and the user post-processors are used for interference neutralization. With the user precoder and post-processor designs fixed as such, the original problem then reduces to a problem of relay precoder design. To address the solvability of the system, we propose a general method for solving matrix equations. This method is also useful to the DoF analysis of many other multiway relay networks. Together with the techniques of antenna disablement and symbol extension, an achievable DoF of the symmetric multi-relay MIMO Y channel is derived for an arbitrary setup of (K, M, N). We show that, for K >= 2, the optimal DoF is achieved for M/N in [0, max{sqrt(3K)/3,1}) and [(3K+sqrt(9K^2-12K))/6,infinity). We also show that the uplink-downlink asymmetric design proposed in this paper considerably outperforms the conventional approach based on uplink-downlink symmetry. ",Computer Science - Information Theory ; ,"Ding, Tian ; Yuan, Xiaojun ; Liew, Soung Chang ; ","On the Degrees of Freedom of the Symmetric Multi-Relay MIMO Y Channel  In this paper, we study the degrees of freedom (DoF) of the symmetric multi-relay multiple-input multiple-output (MIMO) Y channel, where three user nodes, each with M antennas, communicate via K geographically separated relay nodes, each with N antennas. For this model, we establish a general DoF achievability framework based on linear precoding and post-processing methods. The framework poses a nonlinear problem with respect to user precoders and post-processors, as well as relay precoders. To solve this problem, we adopt an uplink-downlink asymmetric strategy, where the user precoders are designed for signal alignment and the user post-processors are used for interference neutralization. With the user precoder and post-processor designs fixed as such, the original problem then reduces to a problem of relay precoder design. To address the solvability of the system, we propose a general method for solving matrix equations. This method is also useful to the DoF analysis of many other multiway relay networks. Together with the techniques of antenna disablement and symbol extension, an achievable DoF of the symmetric multi-relay MIMO Y channel is derived for an arbitrary setup of (K, M, N). We show that, for K >= 2, the optimal DoF is achieved for M/N in [0, max{sqrt(3K)/3,1}) and [(3K+sqrt(9K^2-12K))/6,infinity). We also show that the uplink-downlink asymmetric design proposed in this paper considerably outperforms the conventional approach based on uplink-downlink symmetry. ",degrees freedom symmetric multi relay mimo channel paper study degrees freedom dof symmetric multi relay multiple input multiple output mimo channel three user nod antennas communicate via geographically separate relay nod antennas model establish general dof achievability framework base linear precoding post process methods framework pose nonlinear problem respect user precoders post processors well relay precoders solve problem adopt uplink downlink asymmetric strategy user precoders design signal alignment user post processors use interference neutralization user precoder post processor design fix original problem reduce problem relay precoder design address solvability system propose general method solve matrix equations method also useful dof analysis many multiway relay network together techniques antenna disablement symbol extension achievable dof symmetric multi relay mimo channel derive arbitrary setup show optimal dof achieve max sqrt sqrt infinity also show uplink downlink asymmetric design propose paper considerably outperform conventional approach base uplink downlink symmetry,145,9,1511.08887.txt
http://arxiv.org/abs/1511.09156,Constant-approximation algorithms for highly connected multi-dominating   sets in unit disk graphs,"  Given an undirected graph on a node set $V$ and positive integers $k$ and $m$, a $k$-connected $m$-dominating set ($(k,m)$-CDS) is defined as a subset $S$ of $V$ such that each node in $V \setminus S$ has at least $m$ neighbors in $S$, and a $k$-connected subgraph is induced by $S$. The weighted $(k,m)$-CDS problem is to find a minimum weight $(k,m)$-CDS in a given node-weighted graph. The problem is called the unweighted $(k,m)$-CDS problem if the objective is to minimize the cardinality of a $(k,m)$-CDS. These problems have been actively studied for unit disk graphs, motivated by the application of constructing a virtual backbone in a wireless ad hoc network. However, constant-approximation algorithms are known only for $k \leq 3$ in the unweighted $(k,m)$-CDS problem, and for $(k,m)=(1,1)$ in the weighted $(k,m)$-CDS problem. In this paper, we consider the case in which $m \geq k$, and we present a simple $O(5^k k!)$-approximation algorithm for the unweighted $(k,m)$-CDS problem, and a primal-dual $O(k^2 \log k)$-approximation algorithm for the weighted $(k,m)$-CDS problem. Both algorithms achieve constant approximation factors when $k$ is a fixed constant. ",Computer Science - Data Structures and Algorithms ; ,"Fukunaga, Takuro ; ","Constant-approximation algorithms for highly connected multi-dominating   sets in unit disk graphs  Given an undirected graph on a node set $V$ and positive integers $k$ and $m$, a $k$-connected $m$-dominating set ($(k,m)$-CDS) is defined as a subset $S$ of $V$ such that each node in $V \setminus S$ has at least $m$ neighbors in $S$, and a $k$-connected subgraph is induced by $S$. The weighted $(k,m)$-CDS problem is to find a minimum weight $(k,m)$-CDS in a given node-weighted graph. The problem is called the unweighted $(k,m)$-CDS problem if the objective is to minimize the cardinality of a $(k,m)$-CDS. These problems have been actively studied for unit disk graphs, motivated by the application of constructing a virtual backbone in a wireless ad hoc network. However, constant-approximation algorithms are known only for $k \leq 3$ in the unweighted $(k,m)$-CDS problem, and for $(k,m)=(1,1)$ in the weighted $(k,m)$-CDS problem. In this paper, we consider the case in which $m \geq k$, and we present a simple $O(5^k k!)$-approximation algorithm for the unweighted $(k,m)$-CDS problem, and a primal-dual $O(k^2 \log k)$-approximation algorithm for the weighted $(k,m)$-CDS problem. Both algorithms achieve constant approximation factors when $k$ is a fixed constant. ",constant approximation algorithms highly connect multi dominate set unit disk graph give undirected graph node set positive integers connect dominate set cds define subset node setminus least neighbor connect subgraph induce weight cds problem find minimum weight cds give node weight graph problem call unweighted cds problem objective minimize cardinality cds problems actively study unit disk graph motivate application construct virtual backbone wireless ad hoc network however constant approximation algorithms know leq unweighted cds problem weight cds problem paper consider case geq present simple approximation algorithm unweighted cds problem primal dual log approximation algorithm weight cds problem algorithms achieve constant approximation factor fix constant,104,1,1511.09156.txt
http://arxiv.org/abs/1511.09259,The Alternating Stock Size Problem and the Gasoline Puzzle,"  Given a set S of integers whose sum is zero, consider the problem of finding a permutation of these integers such that: (i) all prefix sums of the ordering are nonnegative, and (ii) the maximum value of a prefix sum is minimized. Kellerer et al. referred to this problem as the ""Stock Size Problem"" and showed that it can be approximated to within 3/2. They also showed that an approximation ratio of 2 can be achieved via several simple algorithms.   We consider a related problem, which we call the ""Alternating Stock Size Problem"", where the number of positive and negative integers in the input set S are equal. The problem is the same as above, but we are additionally required to alternate the positive and negative numbers in the output ordering. This problem also has several simple 2-approximations. We show that it can be approximated to within 1.79.   Then we show that this problem is closely related to an optimization version of the gasoline puzzle due to Lov\'asz, in which we want to minimize the size of the gas tank necessary to go around the track. We present a 2-approximation for this problem, using a natural linear programming relaxation whose feasible solutions are doubly stochastic matrices. Our novel rounding algorithm is based on a transformation that yields another doubly stochastic matrix with special properties, from which we can extract a suitable permutation. ",Computer Science - Data Structures and Algorithms ; ,"Newman, Alantha ; Röglin, Heiko ; Seif, Johanna ; ","The Alternating Stock Size Problem and the Gasoline Puzzle  Given a set S of integers whose sum is zero, consider the problem of finding a permutation of these integers such that: (i) all prefix sums of the ordering are nonnegative, and (ii) the maximum value of a prefix sum is minimized. Kellerer et al. referred to this problem as the ""Stock Size Problem"" and showed that it can be approximated to within 3/2. They also showed that an approximation ratio of 2 can be achieved via several simple algorithms.   We consider a related problem, which we call the ""Alternating Stock Size Problem"", where the number of positive and negative integers in the input set S are equal. The problem is the same as above, but we are additionally required to alternate the positive and negative numbers in the output ordering. This problem also has several simple 2-approximations. We show that it can be approximated to within 1.79.   Then we show that this problem is closely related to an optimization version of the gasoline puzzle due to Lov\'asz, in which we want to minimize the size of the gas tank necessary to go around the track. We present a 2-approximation for this problem, using a natural linear programming relaxation whose feasible solutions are doubly stochastic matrices. Our novel rounding algorithm is based on a transformation that yields another doubly stochastic matrix with special properties, from which we can extract a suitable permutation. ",alternate stock size problem gasoline puzzle give set integers whose sum zero consider problem find permutation integers prefix sum order nonnegative ii maximum value prefix sum minimize kellerer et al refer problem stock size problem show approximate within also show approximation ratio achieve via several simple algorithms consider relate problem call alternate stock size problem number positive negative integers input set equal problem additionally require alternate positive negative number output order problem also several simple approximations show approximate within show problem closely relate optimization version gasoline puzzle due lov asz want minimize size gas tank necessary go around track present approximation problem use natural linear program relaxation whose feasible solutions doubly stochastic matrices novel round algorithm base transformation yield another doubly stochastic matrix special properties extract suitable permutation,128,7,1511.09259.txt
http://arxiv.org/abs/1511.09324,Isomorphisms considered as equalities: Projecting functions and   enhancing partial application through and implementation of lambda+,"  We propose an implementation of lambda+, a recently introduced simply typed lambda-calculus with pairs where isomorphic types are made equal. The rewrite system of lambda+ is a rewrite system modulo an equivalence relation, which makes its implementation non-trivial. We also extend lambda+ with natural numbers and general recursion and use Beki\'c's theorem to split mutual recursions. This splitting, together with the features of lambda+, allows for a novel way of program transformation by reduction, by projecting a function before it is applied in order to simplify it. Also, currying together with the associativity and commutativity of pairs gives an enhanced form of partial application. ",Computer Science - Logic in Computer Science ; F.4.1 ; ,"Díaz-Caro, Alejandro ; López, Pablo E. Martínez ; ","Isomorphisms considered as equalities: Projecting functions and   enhancing partial application through and implementation of lambda+  We propose an implementation of lambda+, a recently introduced simply typed lambda-calculus with pairs where isomorphic types are made equal. The rewrite system of lambda+ is a rewrite system modulo an equivalence relation, which makes its implementation non-trivial. We also extend lambda+ with natural numbers and general recursion and use Beki\'c's theorem to split mutual recursions. This splitting, together with the features of lambda+, allows for a novel way of program transformation by reduction, by projecting a function before it is applied in order to simplify it. Also, currying together with the associativity and commutativity of pairs gives an enhanced form of partial application. ",isomorphisms consider equalities project function enhance partial application implementation lambda propose implementation lambda recently introduce simply type lambda calculus pair isomorphic type make equal rewrite system lambda rewrite system modulo equivalence relation make implementation non trivial also extend lambda natural number general recursion use beki theorem split mutual recursions split together feature lambda allow novel way program transformation reduction project function apply order simplify also curry together associativity commutativity pair give enhance form partial application,75,8,1511.09324.txt
http://arxiv.org/abs/1512.00135,Entropies of weighted sums in cyclic groups and an application to polar   codes,"  In this note, the following basic question is explored: in a cyclic group, how are the Shannon entropies of the sum and difference of i.i.d. random variables related to each other? For the integer group, we show that they can differ by any real number additively, but not too much multiplicatively; on the other hand, for $\mathbb{Z}/3\mathbb{Z}$, the entropy of the difference is always at least as large as that of the sum. These results are closely related to the study of more-sum-than-difference (i.e. MSTD) sets in additive combinatorics. We also investigate polar codes for $q$-ary input channels using non-canonical kernels to construct the generator matrix, and present applications of our results to constructing polar codes with significantly improved error probability compared to the canonical construction. ",Computer Science - Information Theory ; ,"Abbe, Emmanuel ; Li, Jiange ; Madiman, Mokshay ; ","Entropies of weighted sums in cyclic groups and an application to polar   codes  In this note, the following basic question is explored: in a cyclic group, how are the Shannon entropies of the sum and difference of i.i.d. random variables related to each other? For the integer group, we show that they can differ by any real number additively, but not too much multiplicatively; on the other hand, for $\mathbb{Z}/3\mathbb{Z}$, the entropy of the difference is always at least as large as that of the sum. These results are closely related to the study of more-sum-than-difference (i.e. MSTD) sets in additive combinatorics. We also investigate polar codes for $q$-ary input channels using non-canonical kernels to construct the generator matrix, and present applications of our results to constructing polar codes with significantly improved error probability compared to the canonical construction. ",entropies weight sum cyclic group application polar cod note follow basic question explore cyclic group shannon entropies sum difference random variables relate integer group show differ real number additively much multiplicatively hand mathbb mathbb entropy difference always least large sum result closely relate study sum difference mstd set additive combinatorics also investigate polar cod ary input channel use non canonical kernels construct generator matrix present applications result construct polar cod significantly improve error probability compare canonical construction,77,5,1512.00135.txt
http://arxiv.org/abs/1512.00327,Technical Privacy Metrics: a Systematic Survey,"  The goal of privacy metrics is to measure the degree of privacy enjoyed by users in a system and the amount of protection offered by privacy-enhancing technologies. In this way, privacy metrics contribute to improving user privacy in the digital world. The diversity and complexity of privacy metrics in the literature makes an informed choice of metrics challenging. As a result, instead of using existing metrics, new metrics are proposed frequently, and privacy studies are often incomparable. In this survey we alleviate these problems by structuring the landscape of privacy metrics. To this end, we explain and discuss a selection of over eighty privacy metrics and introduce categorizations based on the aspect of privacy they measure, their required inputs, and the type of data that needs protection. In addition, we present a method on how to choose privacy metrics based on nine questions that help identify the right privacy metrics for a given scenario, and highlight topics where additional work on privacy metrics is needed. Our survey spans multiple privacy domains and can be understood as a general framework for privacy measurement. ",Computer Science - Cryptography and Security ; Computer Science - Information Theory ; Computer Science - Performance ; ,"Wagner, Isabel ; Eckhoff, David ; ","Technical Privacy Metrics: a Systematic Survey  The goal of privacy metrics is to measure the degree of privacy enjoyed by users in a system and the amount of protection offered by privacy-enhancing technologies. In this way, privacy metrics contribute to improving user privacy in the digital world. The diversity and complexity of privacy metrics in the literature makes an informed choice of metrics challenging. As a result, instead of using existing metrics, new metrics are proposed frequently, and privacy studies are often incomparable. In this survey we alleviate these problems by structuring the landscape of privacy metrics. To this end, we explain and discuss a selection of over eighty privacy metrics and introduce categorizations based on the aspect of privacy they measure, their required inputs, and the type of data that needs protection. In addition, we present a method on how to choose privacy metrics based on nine questions that help identify the right privacy metrics for a given scenario, and highlight topics where additional work on privacy metrics is needed. Our survey spans multiple privacy domains and can be understood as a general framework for privacy measurement. ",technical privacy metrics systematic survey goal privacy metrics measure degree privacy enjoy users system amount protection offer privacy enhance technologies way privacy metrics contribute improve user privacy digital world diversity complexity privacy metrics literature make inform choice metrics challenge result instead use exist metrics new metrics propose frequently privacy study often incomparable survey alleviate problems structure landscape privacy metrics end explain discuss selection eighty privacy metrics introduce categorizations base aspect privacy measure require input type data need protection addition present method choose privacy metrics base nine question help identify right privacy metrics give scenario highlight topics additional work privacy metrics need survey span multiple privacy domains understand general framework privacy measurement,111,10,1512.00327.txt
